I1116 19:15:09.280910      26 test_context.go:414] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-420283327
I1116 19:15:09.281083      26 e2e.go:92] Starting e2e run "d57824ea-ee17-4ae5-aefb-7e90c824a524" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1605554107 - Will randomize all specs
Will run 276 of 4897 specs

Nov 16 19:15:09.303: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
Nov 16 19:15:09.307: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Nov 16 19:15:09.355: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Nov 16 19:15:09.415: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Nov 16 19:15:09.415: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Nov 16 19:15:09.415: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Nov 16 19:15:09.430: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Nov 16 19:15:09.430: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
Nov 16 19:15:09.430: INFO: e2e test version: v1.16.2
Nov 16 19:15:09.433: INFO: kube-apiserver version: v1.16.2+853223d
Nov 16 19:15:09.433: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
Nov 16 19:15:09.445: INFO: Cluster IP family: ipv4
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:15:09.446: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename daemonsets
Nov 16 19:15:09.580: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 19:15:09.642: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Nov 16 19:15:09.682: INFO: Number of nodes with available pods: 0
Nov 16 19:15:09.682: INFO: Node 10.240.167.206 is running more than one daemon pod
Nov 16 19:15:10.704: INFO: Number of nodes with available pods: 0
Nov 16 19:15:10.704: INFO: Node 10.240.167.206 is running more than one daemon pod
Nov 16 19:15:11.705: INFO: Number of nodes with available pods: 0
Nov 16 19:15:11.705: INFO: Node 10.240.167.206 is running more than one daemon pod
Nov 16 19:15:12.708: INFO: Number of nodes with available pods: 0
Nov 16 19:15:12.708: INFO: Node 10.240.167.206 is running more than one daemon pod
Nov 16 19:15:13.709: INFO: Number of nodes with available pods: 0
Nov 16 19:15:13.709: INFO: Node 10.240.167.206 is running more than one daemon pod
Nov 16 19:15:14.709: INFO: Number of nodes with available pods: 0
Nov 16 19:15:14.709: INFO: Node 10.240.167.206 is running more than one daemon pod
Nov 16 19:15:15.707: INFO: Number of nodes with available pods: 0
Nov 16 19:15:15.707: INFO: Node 10.240.167.206 is running more than one daemon pod
Nov 16 19:15:16.762: INFO: Number of nodes with available pods: 0
Nov 16 19:15:16.762: INFO: Node 10.240.167.206 is running more than one daemon pod
Nov 16 19:15:17.707: INFO: Number of nodes with available pods: 0
Nov 16 19:15:17.707: INFO: Node 10.240.167.206 is running more than one daemon pod
Nov 16 19:15:18.706: INFO: Number of nodes with available pods: 0
Nov 16 19:15:18.706: INFO: Node 10.240.167.206 is running more than one daemon pod
Nov 16 19:15:19.707: INFO: Number of nodes with available pods: 1
Nov 16 19:15:19.707: INFO: Node 10.240.167.206 is running more than one daemon pod
Nov 16 19:15:20.705: INFO: Number of nodes with available pods: 2
Nov 16 19:15:20.705: INFO: Node 10.240.167.206 is running more than one daemon pod
Nov 16 19:15:21.707: INFO: Number of nodes with available pods: 3
Nov 16 19:15:21.707: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Nov 16 19:15:21.795: INFO: Wrong image for pod: daemon-set-vw6ff. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:21.795: INFO: Wrong image for pod: daemon-set-w6g8d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:21.795: INFO: Wrong image for pod: daemon-set-zpl46. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:22.819: INFO: Wrong image for pod: daemon-set-vw6ff. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:22.819: INFO: Wrong image for pod: daemon-set-w6g8d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:22.819: INFO: Wrong image for pod: daemon-set-zpl46. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:23.819: INFO: Wrong image for pod: daemon-set-vw6ff. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:23.819: INFO: Wrong image for pod: daemon-set-w6g8d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:23.819: INFO: Wrong image for pod: daemon-set-zpl46. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:24.823: INFO: Wrong image for pod: daemon-set-vw6ff. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:24.823: INFO: Wrong image for pod: daemon-set-w6g8d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:24.823: INFO: Wrong image for pod: daemon-set-zpl46. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:25.818: INFO: Wrong image for pod: daemon-set-vw6ff. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:25.818: INFO: Wrong image for pod: daemon-set-w6g8d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:25.818: INFO: Wrong image for pod: daemon-set-zpl46. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:25.818: INFO: Pod daemon-set-zpl46 is not available
Nov 16 19:15:26.818: INFO: Wrong image for pod: daemon-set-vw6ff. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:26.818: INFO: Wrong image for pod: daemon-set-w6g8d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:26.818: INFO: Wrong image for pod: daemon-set-zpl46. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:26.818: INFO: Pod daemon-set-zpl46 is not available
Nov 16 19:15:27.817: INFO: Wrong image for pod: daemon-set-vw6ff. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:27.817: INFO: Wrong image for pod: daemon-set-w6g8d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:27.817: INFO: Wrong image for pod: daemon-set-zpl46. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:27.817: INFO: Pod daemon-set-zpl46 is not available
Nov 16 19:15:28.819: INFO: Wrong image for pod: daemon-set-vw6ff. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:28.819: INFO: Wrong image for pod: daemon-set-w6g8d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:28.819: INFO: Wrong image for pod: daemon-set-zpl46. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:28.819: INFO: Pod daemon-set-zpl46 is not available
Nov 16 19:15:29.818: INFO: Pod daemon-set-nzps4 is not available
Nov 16 19:15:29.818: INFO: Wrong image for pod: daemon-set-vw6ff. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:29.818: INFO: Wrong image for pod: daemon-set-w6g8d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:30.818: INFO: Pod daemon-set-nzps4 is not available
Nov 16 19:15:30.818: INFO: Wrong image for pod: daemon-set-vw6ff. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:30.818: INFO: Wrong image for pod: daemon-set-w6g8d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:31.818: INFO: Pod daemon-set-nzps4 is not available
Nov 16 19:15:31.818: INFO: Wrong image for pod: daemon-set-vw6ff. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:31.818: INFO: Wrong image for pod: daemon-set-w6g8d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:32.818: INFO: Pod daemon-set-nzps4 is not available
Nov 16 19:15:32.818: INFO: Wrong image for pod: daemon-set-vw6ff. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:32.818: INFO: Wrong image for pod: daemon-set-w6g8d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:33.818: INFO: Pod daemon-set-nzps4 is not available
Nov 16 19:15:33.818: INFO: Wrong image for pod: daemon-set-vw6ff. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:33.818: INFO: Wrong image for pod: daemon-set-w6g8d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:34.820: INFO: Pod daemon-set-nzps4 is not available
Nov 16 19:15:34.820: INFO: Wrong image for pod: daemon-set-vw6ff. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:34.820: INFO: Wrong image for pod: daemon-set-w6g8d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:35.819: INFO: Pod daemon-set-nzps4 is not available
Nov 16 19:15:35.819: INFO: Wrong image for pod: daemon-set-vw6ff. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:35.819: INFO: Wrong image for pod: daemon-set-w6g8d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:36.819: INFO: Pod daemon-set-nzps4 is not available
Nov 16 19:15:36.819: INFO: Wrong image for pod: daemon-set-vw6ff. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:36.819: INFO: Wrong image for pod: daemon-set-w6g8d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:37.823: INFO: Wrong image for pod: daemon-set-vw6ff. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:37.823: INFO: Wrong image for pod: daemon-set-w6g8d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:38.818: INFO: Wrong image for pod: daemon-set-vw6ff. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:38.818: INFO: Wrong image for pod: daemon-set-w6g8d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:39.817: INFO: Wrong image for pod: daemon-set-vw6ff. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:39.817: INFO: Wrong image for pod: daemon-set-w6g8d. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:39.817: INFO: Pod daemon-set-w6g8d is not available
Nov 16 19:15:40.818: INFO: Pod daemon-set-lntzs is not available
Nov 16 19:15:40.819: INFO: Wrong image for pod: daemon-set-vw6ff. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:41.818: INFO: Pod daemon-set-lntzs is not available
Nov 16 19:15:41.819: INFO: Wrong image for pod: daemon-set-vw6ff. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:42.817: INFO: Pod daemon-set-lntzs is not available
Nov 16 19:15:42.817: INFO: Wrong image for pod: daemon-set-vw6ff. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:43.819: INFO: Pod daemon-set-lntzs is not available
Nov 16 19:15:43.819: INFO: Wrong image for pod: daemon-set-vw6ff. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:44.819: INFO: Pod daemon-set-lntzs is not available
Nov 16 19:15:44.819: INFO: Wrong image for pod: daemon-set-vw6ff. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:45.817: INFO: Pod daemon-set-lntzs is not available
Nov 16 19:15:45.817: INFO: Wrong image for pod: daemon-set-vw6ff. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:46.817: INFO: Pod daemon-set-lntzs is not available
Nov 16 19:15:46.817: INFO: Wrong image for pod: daemon-set-vw6ff. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:47.817: INFO: Pod daemon-set-lntzs is not available
Nov 16 19:15:47.817: INFO: Wrong image for pod: daemon-set-vw6ff. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:48.818: INFO: Wrong image for pod: daemon-set-vw6ff. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:49.818: INFO: Wrong image for pod: daemon-set-vw6ff. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:50.819: INFO: Wrong image for pod: daemon-set-vw6ff. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Nov 16 19:15:50.819: INFO: Pod daemon-set-vw6ff is not available
Nov 16 19:15:51.817: INFO: Pod daemon-set-jdz48 is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Nov 16 19:15:51.852: INFO: Number of nodes with available pods: 2
Nov 16 19:15:51.852: INFO: Node 10.240.167.209 is running more than one daemon pod
Nov 16 19:15:52.879: INFO: Number of nodes with available pods: 2
Nov 16 19:15:52.879: INFO: Node 10.240.167.209 is running more than one daemon pod
Nov 16 19:15:53.888: INFO: Number of nodes with available pods: 2
Nov 16 19:15:53.888: INFO: Node 10.240.167.209 is running more than one daemon pod
Nov 16 19:15:54.877: INFO: Number of nodes with available pods: 2
Nov 16 19:15:54.877: INFO: Node 10.240.167.209 is running more than one daemon pod
Nov 16 19:15:55.877: INFO: Number of nodes with available pods: 2
Nov 16 19:15:55.877: INFO: Node 10.240.167.209 is running more than one daemon pod
Nov 16 19:15:56.876: INFO: Number of nodes with available pods: 2
Nov 16 19:15:56.876: INFO: Node 10.240.167.209 is running more than one daemon pod
Nov 16 19:15:57.875: INFO: Number of nodes with available pods: 2
Nov 16 19:15:57.875: INFO: Node 10.240.167.209 is running more than one daemon pod
Nov 16 19:15:58.880: INFO: Number of nodes with available pods: 2
Nov 16 19:15:58.880: INFO: Node 10.240.167.209 is running more than one daemon pod
Nov 16 19:15:59.876: INFO: Number of nodes with available pods: 3
Nov 16 19:15:59.876: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2368, will wait for the garbage collector to delete the pods
Nov 16 19:16:00.005: INFO: Deleting DaemonSet.extensions daemon-set took: 23.188633ms
Nov 16 19:16:00.505: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.307532ms
Nov 16 19:16:06.916: INFO: Number of nodes with available pods: 0
Nov 16 19:16:06.916: INFO: Number of running nodes: 0, number of available pods: 0
Nov 16 19:16:06.926: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2368/daemonsets","resourceVersion":"40832"},"items":null}

Nov 16 19:16:06.936: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2368/pods","resourceVersion":"40832"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:16:06.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2368" for this suite.
Nov 16 19:16:15.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:16:16.922: INFO: namespace daemonsets-2368 deletion completed in 9.93245705s

• [SLOW TEST:67.477 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:16:16.922: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 16 19:16:17.102: INFO: Waiting up to 5m0s for pod "downwardapi-volume-499277d8-b5b7-4a98-9bec-167de768d79d" in namespace "downward-api-7231" to be "success or failure"
Nov 16 19:16:17.124: INFO: Pod "downwardapi-volume-499277d8-b5b7-4a98-9bec-167de768d79d": Phase="Pending", Reason="", readiness=false. Elapsed: 21.778794ms
Nov 16 19:16:19.135: INFO: Pod "downwardapi-volume-499277d8-b5b7-4a98-9bec-167de768d79d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032674061s
Nov 16 19:16:21.146: INFO: Pod "downwardapi-volume-499277d8-b5b7-4a98-9bec-167de768d79d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0430371s
Nov 16 19:16:23.157: INFO: Pod "downwardapi-volume-499277d8-b5b7-4a98-9bec-167de768d79d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054274566s
Nov 16 19:16:25.175: INFO: Pod "downwardapi-volume-499277d8-b5b7-4a98-9bec-167de768d79d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.072761751s
Nov 16 19:16:27.186: INFO: Pod "downwardapi-volume-499277d8-b5b7-4a98-9bec-167de768d79d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.083089155s
STEP: Saw pod success
Nov 16 19:16:27.186: INFO: Pod "downwardapi-volume-499277d8-b5b7-4a98-9bec-167de768d79d" satisfied condition "success or failure"
Nov 16 19:16:27.198: INFO: Trying to get logs from node 10.240.167.254 pod downwardapi-volume-499277d8-b5b7-4a98-9bec-167de768d79d container client-container: <nil>
STEP: delete the pod
Nov 16 19:16:27.294: INFO: Waiting for pod downwardapi-volume-499277d8-b5b7-4a98-9bec-167de768d79d to disappear
Nov 16 19:16:27.304: INFO: Pod downwardapi-volume-499277d8-b5b7-4a98-9bec-167de768d79d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:16:27.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7231" for this suite.
Nov 16 19:16:35.642: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:16:37.507: INFO: namespace downward-api-7231 deletion completed in 10.190238295s

• [SLOW TEST:20.584 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:16:37.508: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 19:16:38.376: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 16 19:16:40.401: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741150998, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741150998, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741150998, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741150998, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 19:16:42.410: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741150998, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741150998, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741150998, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741150998, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 19:16:44.409: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741150998, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741150998, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741150998, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741150998, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 19:16:46.409: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741150998, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741150998, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741150998, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741150998, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 19:16:49.438: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:16:49.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3951" for this suite.
Nov 16 19:16:57.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:16:59.418: INFO: namespace webhook-3951 deletion completed in 9.947140168s
STEP: Destroying namespace "webhook-3951-markers" for this suite.
Nov 16 19:17:07.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:17:09.353: INFO: namespace webhook-3951-markers deletion completed in 9.93540939s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:31.892 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:17:09.399: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:17:25.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8676" for this suite.
Nov 16 19:17:33.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:17:35.756: INFO: namespace resourcequota-8676 deletion completed in 9.934994009s

• [SLOW TEST:26.357 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:17:35.757: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 19:17:35.971: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"de91d536-ae50-4865-b1d5-33af4ccbf0a7", Controller:(*bool)(0xc001da5dc2), BlockOwnerDeletion:(*bool)(0xc001da5dc3)}}
Nov 16 19:17:35.984: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"c7fa0904-0fe8-4667-8a6e-1fdfe85fa8de", Controller:(*bool)(0xc0007c5032), BlockOwnerDeletion:(*bool)(0xc0007c5033)}}
Nov 16 19:17:36.010: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"b4704e76-c795-4098-b7fc-72ff4f12e31f", Controller:(*bool)(0xc000527556), BlockOwnerDeletion:(*bool)(0xc000527557)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:17:41.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9109" for this suite.
Nov 16 19:17:49.081: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:17:51.003: INFO: namespace gc-9109 deletion completed in 9.948547164s

• [SLOW TEST:15.246 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:17:51.003: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-736e0746-f037-4a8e-b2e8-76676ba1f9b3
STEP: Creating a pod to test consume configMaps
Nov 16 19:17:51.213: INFO: Waiting up to 5m0s for pod "pod-configmaps-170bb36d-e76f-4c3f-8603-060892379c18" in namespace "configmap-5863" to be "success or failure"
Nov 16 19:17:51.222: INFO: Pod "pod-configmaps-170bb36d-e76f-4c3f-8603-060892379c18": Phase="Pending", Reason="", readiness=false. Elapsed: 8.670711ms
Nov 16 19:17:53.233: INFO: Pod "pod-configmaps-170bb36d-e76f-4c3f-8603-060892379c18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019836041s
STEP: Saw pod success
Nov 16 19:17:53.233: INFO: Pod "pod-configmaps-170bb36d-e76f-4c3f-8603-060892379c18" satisfied condition "success or failure"
Nov 16 19:17:53.242: INFO: Trying to get logs from node 10.240.167.254 pod pod-configmaps-170bb36d-e76f-4c3f-8603-060892379c18 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 16 19:17:53.292: INFO: Waiting for pod pod-configmaps-170bb36d-e76f-4c3f-8603-060892379c18 to disappear
Nov 16 19:17:53.300: INFO: Pod pod-configmaps-170bb36d-e76f-4c3f-8603-060892379c18 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:17:53.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5863" for this suite.
Nov 16 19:18:01.341: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:18:03.246: INFO: namespace configmap-5863 deletion completed in 9.934135734s

• [SLOW TEST:12.243 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:18:03.247: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 16 19:18:03.415: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c89f0ed3-9b1a-438e-bd16-c54209e7837f" in namespace "projected-7540" to be "success or failure"
Nov 16 19:18:03.425: INFO: Pod "downwardapi-volume-c89f0ed3-9b1a-438e-bd16-c54209e7837f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.825676ms
Nov 16 19:18:05.436: INFO: Pod "downwardapi-volume-c89f0ed3-9b1a-438e-bd16-c54209e7837f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021033815s
Nov 16 19:18:07.448: INFO: Pod "downwardapi-volume-c89f0ed3-9b1a-438e-bd16-c54209e7837f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032978165s
STEP: Saw pod success
Nov 16 19:18:07.448: INFO: Pod "downwardapi-volume-c89f0ed3-9b1a-438e-bd16-c54209e7837f" satisfied condition "success or failure"
Nov 16 19:18:07.458: INFO: Trying to get logs from node 10.240.167.254 pod downwardapi-volume-c89f0ed3-9b1a-438e-bd16-c54209e7837f container client-container: <nil>
STEP: delete the pod
Nov 16 19:18:07.509: INFO: Waiting for pod downwardapi-volume-c89f0ed3-9b1a-438e-bd16-c54209e7837f to disappear
Nov 16 19:18:07.517: INFO: Pod downwardapi-volume-c89f0ed3-9b1a-438e-bd16-c54209e7837f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:18:07.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7540" for this suite.
Nov 16 19:18:15.557: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:18:17.465: INFO: namespace projected-7540 deletion completed in 9.93477403s

• [SLOW TEST:14.218 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:18:17.465: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 19:18:18.013: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 19:18:21.071: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:18:21.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2304" for this suite.
Nov 16 19:18:29.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:18:31.497: INFO: namespace webhook-2304 deletion completed in 9.938306154s
STEP: Destroying namespace "webhook-2304-markers" for this suite.
Nov 16 19:18:39.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:18:41.434: INFO: namespace webhook-2304-markers deletion completed in 9.936524693s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:24.013 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:18:41.478: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-5e0cfb01-f553-4120-9319-170cbb01bf0c
STEP: Creating a pod to test consume configMaps
Nov 16 19:18:41.714: INFO: Waiting up to 5m0s for pod "pod-configmaps-3d5ff41c-9f97-4867-bc67-a4773cb10f48" in namespace "configmap-856" to be "success or failure"
Nov 16 19:18:41.724: INFO: Pod "pod-configmaps-3d5ff41c-9f97-4867-bc67-a4773cb10f48": Phase="Pending", Reason="", readiness=false. Elapsed: 10.042821ms
Nov 16 19:18:43.734: INFO: Pod "pod-configmaps-3d5ff41c-9f97-4867-bc67-a4773cb10f48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020354407s
Nov 16 19:18:45.746: INFO: Pod "pod-configmaps-3d5ff41c-9f97-4867-bc67-a4773cb10f48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032618855s
STEP: Saw pod success
Nov 16 19:18:45.746: INFO: Pod "pod-configmaps-3d5ff41c-9f97-4867-bc67-a4773cb10f48" satisfied condition "success or failure"
Nov 16 19:18:45.756: INFO: Trying to get logs from node 10.240.167.254 pod pod-configmaps-3d5ff41c-9f97-4867-bc67-a4773cb10f48 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 16 19:18:45.813: INFO: Waiting for pod pod-configmaps-3d5ff41c-9f97-4867-bc67-a4773cb10f48 to disappear
Nov 16 19:18:45.823: INFO: Pod pod-configmaps-3d5ff41c-9f97-4867-bc67-a4773cb10f48 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:18:45.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-856" for this suite.
Nov 16 19:18:53.863: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:18:55.770: INFO: namespace configmap-856 deletion completed in 9.934159619s

• [SLOW TEST:14.292 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:18:55.770: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-e35ae61d-1603-4bdf-a9f7-ebc9d1e1c5d7
STEP: Creating a pod to test consume configMaps
Nov 16 19:18:55.959: INFO: Waiting up to 5m0s for pod "pod-configmaps-212bf4a2-3566-4ec2-a365-ff6b0b055657" in namespace "configmap-5849" to be "success or failure"
Nov 16 19:18:55.968: INFO: Pod "pod-configmaps-212bf4a2-3566-4ec2-a365-ff6b0b055657": Phase="Pending", Reason="", readiness=false. Elapsed: 9.255286ms
Nov 16 19:18:57.981: INFO: Pod "pod-configmaps-212bf4a2-3566-4ec2-a365-ff6b0b055657": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02203693s
STEP: Saw pod success
Nov 16 19:18:57.981: INFO: Pod "pod-configmaps-212bf4a2-3566-4ec2-a365-ff6b0b055657" satisfied condition "success or failure"
Nov 16 19:18:57.990: INFO: Trying to get logs from node 10.240.167.254 pod pod-configmaps-212bf4a2-3566-4ec2-a365-ff6b0b055657 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 16 19:18:58.044: INFO: Waiting for pod pod-configmaps-212bf4a2-3566-4ec2-a365-ff6b0b055657 to disappear
Nov 16 19:18:58.054: INFO: Pod pod-configmaps-212bf4a2-3566-4ec2-a365-ff6b0b055657 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:18:58.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5849" for this suite.
Nov 16 19:19:06.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:19:08.047: INFO: namespace configmap-5849 deletion completed in 9.98190531s

• [SLOW TEST:12.277 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:19:08.048: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:19:14.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-50" for this suite.
Nov 16 19:20:04.293: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:20:06.199: INFO: namespace kubelet-test-50 deletion completed in 51.93651239s

• [SLOW TEST:58.151 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:20:06.201: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Nov 16 19:20:06.306: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 16 19:20:06.345: INFO: Waiting for terminating namespaces to be deleted...
Nov 16 19:20:06.355: INFO: 
Logging pods the kubelet thinks is on node 10.240.167.206 before test
Nov 16 19:20:06.436: INFO: multus-gnk8t from openshift-multus started at 2020-11-16 17:39:32 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.436: INFO: 	Container kube-multus ready: true, restart count 0
Nov 16 19:20:06.436: INFO: ibmcloud-block-storage-driver-2s8xr from kube-system started at 2020-11-16 17:39:34 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.436: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 16 19:20:06.436: INFO: prometheus-adapter-554fc6c4db-6qn5b from openshift-monitoring started at 2020-11-16 17:45:54 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.436: INFO: 	Container prometheus-adapter ready: true, restart count 0
Nov 16 19:20:06.436: INFO: openshift-state-metrics-86c5b47587-nhfn8 from openshift-monitoring started at 2020-11-16 17:41:13 +0000 UTC (3 container statuses recorded)
Nov 16 19:20:06.437: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Nov 16 19:20:06.437: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Nov 16 19:20:06.437: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Nov 16 19:20:06.437: INFO: calico-typha-b5486f777-9gw72 from calico-system started at 2020-11-16 17:42:09 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.437: INFO: 	Container calico-typha ready: true, restart count 0
Nov 16 19:20:06.437: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-11-16 17:44:46 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.437: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Nov 16 19:20:06.437: INFO: thanos-querier-54b47b4fc4-rcx4j from openshift-monitoring started at 2020-11-16 17:46:47 +0000 UTC (4 container statuses recorded)
Nov 16 19:20:06.437: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 19:20:06.437: INFO: 	Container oauth-proxy ready: true, restart count 0
Nov 16 19:20:06.437: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 19:20:06.437: INFO: 	Container thanos-querier ready: true, restart count 0
Nov 16 19:20:06.437: INFO: openshift-kube-proxy-5kpn9 from openshift-kube-proxy started at 2020-11-16 17:39:32 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.437: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 16 19:20:06.437: INFO: multus-admission-controller-x6jqb from openshift-multus started at 2020-11-16 17:40:32 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.438: INFO: 	Container multus-admission-controller ready: true, restart count 0
Nov 16 19:20:06.438: INFO: tuned-9m9tq from openshift-cluster-node-tuning-operator started at 2020-11-16 17:40:38 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.438: INFO: 	Container tuned ready: true, restart count 0
Nov 16 19:20:06.438: INFO: node-ca-2pgxt from openshift-image-registry started at 2020-11-16 17:41:42 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.438: INFO: 	Container node-ca ready: true, restart count 0
Nov 16 19:20:06.438: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-11-16 17:47:07 +0000 UTC (7 container statuses recorded)
Nov 16 19:20:06.438: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 19:20:06.438: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 19:20:06.438: INFO: 	Container prometheus ready: true, restart count 1
Nov 16 19:20:06.438: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Nov 16 19:20:06.438: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov 16 19:20:06.438: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Nov 16 19:20:06.438: INFO: 	Container thanos-sidecar ready: true, restart count 0
Nov 16 19:20:06.438: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-11-16 17:46:19 +0000 UTC (3 container statuses recorded)
Nov 16 19:20:06.438: INFO: 	Container alertmanager ready: true, restart count 0
Nov 16 19:20:06.439: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 16 19:20:06.439: INFO: 	Container config-reloader ready: true, restart count 0
Nov 16 19:20:06.439: INFO: sonobuoy-systemd-logs-daemon-set-18804f2d5c884991-lmbvb from sonobuoy started at 2020-11-16 19:14:33 +0000 UTC (2 container statuses recorded)
Nov 16 19:20:06.439: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 19:20:06.439: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 16 19:20:06.439: INFO: ibm-keepalived-watcher-2t6l4 from kube-system started at 2020-11-16 17:39:32 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.439: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 19:20:06.439: INFO: kube-state-metrics-776f8894df-p8xsw from openshift-monitoring started at 2020-11-16 17:41:15 +0000 UTC (3 container statuses recorded)
Nov 16 19:20:06.439: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Nov 16 19:20:06.439: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Nov 16 19:20:06.439: INFO: 	Container kube-state-metrics ready: true, restart count 0
Nov 16 19:20:06.439: INFO: packageserver-68c9b7ddbb-rqt2z from openshift-operator-lifecycle-manager started at 2020-11-16 17:44:04 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.439: INFO: 	Container packageserver ready: true, restart count 0
Nov 16 19:20:06.440: INFO: dns-default-zqnbq from openshift-dns started at 2020-11-16 17:42:09 +0000 UTC (2 container statuses recorded)
Nov 16 19:20:06.440: INFO: 	Container dns ready: true, restart count 0
Nov 16 19:20:06.440: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 16 19:20:06.440: INFO: calico-node-pk4fl from calico-system started at 2020-11-16 17:40:12 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.440: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 19:20:06.440: INFO: cluster-samples-operator-55cf746658-rtxxg from openshift-cluster-samples-operator started at 2020-11-16 17:42:11 +0000 UTC (2 container statuses recorded)
Nov 16 19:20:06.440: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Nov 16 19:20:06.440: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Nov 16 19:20:06.440: INFO: console-7f85d88fbc-4r5lv from openshift-console started at 2020-11-16 17:42:45 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.440: INFO: 	Container console ready: true, restart count 0
Nov 16 19:20:06.440: INFO: ibm-master-proxy-static-10.240.167.206 from kube-system started at 2020-11-16 17:39:29 +0000 UTC (2 container statuses recorded)
Nov 16 19:20:06.440: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 19:20:06.440: INFO: 	Container pause ready: true, restart count 0
Nov 16 19:20:06.440: INFO: service-serving-cert-signer-787695f6b4-s4fqz from openshift-service-ca started at 2020-11-16 17:41:13 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.440: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
Nov 16 19:20:06.441: INFO: node-exporter-jssfh from openshift-monitoring started at 2020-11-16 17:41:17 +0000 UTC (2 container statuses recorded)
Nov 16 19:20:06.441: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 19:20:06.441: INFO: 	Container node-exporter ready: true, restart count 0
Nov 16 19:20:06.441: INFO: router-default-589c4b7b87-7299r from openshift-ingress started at 2020-11-16 17:41:37 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.441: INFO: 	Container router ready: true, restart count 0
Nov 16 19:20:06.441: INFO: 
Logging pods the kubelet thinks is on node 10.240.167.209 before test
Nov 16 19:20:06.523: INFO: community-operators-587cdbd4fb-5mwwt from openshift-marketplace started at 2020-11-16 17:42:34 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.523: INFO: 	Container community-operators ready: true, restart count 0
Nov 16 19:20:06.523: INFO: vpn-679798bf87-s5h4z from kube-system started at 2020-11-16 17:44:16 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.523: INFO: 	Container vpn ready: true, restart count 0
Nov 16 19:20:06.523: INFO: prometheus-adapter-554fc6c4db-8f7cc from openshift-monitoring started at 2020-11-16 17:45:54 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.523: INFO: 	Container prometheus-adapter ready: true, restart count 0
Nov 16 19:20:06.523: INFO: ibm-master-proxy-static-10.240.167.209 from kube-system started at 2020-11-16 17:39:17 +0000 UTC (2 container statuses recorded)
Nov 16 19:20:06.523: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 19:20:06.523: INFO: 	Container pause ready: true, restart count 0
Nov 16 19:20:06.523: INFO: openshift-kube-proxy-qfwg7 from openshift-kube-proxy started at 2020-11-16 17:39:31 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.523: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 16 19:20:06.523: INFO: apiservice-cabundle-injector-75dcd4f8db-sllzv from openshift-service-ca started at 2020-11-16 17:41:13 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.523: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
Nov 16 19:20:06.523: INFO: registry-pvc-permissions-l4jgc from openshift-image-registry started at 2020-11-16 17:44:28 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.523: INFO: 	Container pvc-permissions ready: false, restart count 0
Nov 16 19:20:06.523: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-11-16 17:46:57 +0000 UTC (7 container statuses recorded)
Nov 16 19:20:06.523: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 19:20:06.523: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 19:20:06.523: INFO: 	Container prometheus ready: true, restart count 1
Nov 16 19:20:06.523: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Nov 16 19:20:06.523: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov 16 19:20:06.523: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Nov 16 19:20:06.523: INFO: 	Container thanos-sidecar ready: true, restart count 0
Nov 16 19:20:06.523: INFO: multus-dfpk4 from openshift-multus started at 2020-11-16 17:39:24 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.523: INFO: 	Container kube-multus ready: true, restart count 0
Nov 16 19:20:06.523: INFO: node-ca-mg4vg from openshift-image-registry started at 2020-11-16 17:41:42 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.523: INFO: 	Container node-ca ready: true, restart count 0
Nov 16 19:20:06.523: INFO: certified-operators-676df69595-kgtfx from openshift-marketplace started at 2020-11-16 17:42:33 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.523: INFO: 	Container certified-operators ready: true, restart count 0
Nov 16 19:20:06.523: INFO: dns-default-m5gqp from openshift-dns started at 2020-11-16 17:42:09 +0000 UTC (2 container statuses recorded)
Nov 16 19:20:06.523: INFO: 	Container dns ready: true, restart count 0
Nov 16 19:20:06.523: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 16 19:20:06.523: INFO: ibm-cloud-provider-ip-161-156-99-92-596ff786f9-gqmvr from ibm-system started at 2020-11-16 17:46:26 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.523: INFO: 	Container ibm-cloud-provider-ip-161-156-99-92 ready: true, restart count 0
Nov 16 19:20:06.523: INFO: ibmcloud-block-storage-driver-6zlqm from kube-system started at 2020-11-16 17:39:27 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.523: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 16 19:20:06.523: INFO: image-registry-77d48f6786-gqts2 from openshift-image-registry started at 2020-11-16 17:44:28 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.523: INFO: 	Container registry ready: true, restart count 0
Nov 16 19:20:06.523: INFO: console-7f85d88fbc-fdc4m from openshift-console started at 2020-11-16 17:41:44 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.523: INFO: 	Container console ready: true, restart count 0
Nov 16 19:20:06.523: INFO: calico-typha-b5486f777-7g7gg from calico-system started at 2020-11-16 17:40:11 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.523: INFO: 	Container calico-typha ready: true, restart count 0
Nov 16 19:20:06.523: INFO: packageserver-68c9b7ddbb-phc28 from openshift-operator-lifecycle-manager started at 2020-11-16 17:44:00 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.523: INFO: 	Container packageserver ready: true, restart count 0
Nov 16 19:20:06.523: INFO: thanos-querier-54b47b4fc4-zvlf7 from openshift-monitoring started at 2020-11-16 17:46:57 +0000 UTC (4 container statuses recorded)
Nov 16 19:20:06.523: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 19:20:06.523: INFO: 	Container oauth-proxy ready: true, restart count 0
Nov 16 19:20:06.523: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 19:20:06.523: INFO: 	Container thanos-querier ready: true, restart count 0
Nov 16 19:20:06.523: INFO: calico-node-tc4r7 from calico-system started at 2020-11-16 17:40:12 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.523: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 19:20:06.523: INFO: tuned-cp7vt from openshift-cluster-node-tuning-operator started at 2020-11-16 17:40:38 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.523: INFO: 	Container tuned ready: true, restart count 0
Nov 16 19:20:06.523: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-11-16 17:46:06 +0000 UTC (3 container statuses recorded)
Nov 16 19:20:06.523: INFO: 	Container alertmanager ready: true, restart count 0
Nov 16 19:20:06.523: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 16 19:20:06.523: INFO: 	Container config-reloader ready: true, restart count 0
Nov 16 19:20:06.523: INFO: router-default-589c4b7b87-74cdn from openshift-ingress started at 2020-11-16 17:41:37 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.523: INFO: 	Container router ready: true, restart count 0
Nov 16 19:20:06.523: INFO: redhat-operators-7f9c6c576-rs2f7 from openshift-marketplace started at 2020-11-16 17:42:33 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.523: INFO: 	Container redhat-operators ready: true, restart count 0
Nov 16 19:20:06.523: INFO: sonobuoy-systemd-logs-daemon-set-18804f2d5c884991-rfhks from sonobuoy started at 2020-11-16 19:14:33 +0000 UTC (2 container statuses recorded)
Nov 16 19:20:06.523: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 19:20:06.523: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 16 19:20:06.523: INFO: node-exporter-lqwgh from openshift-monitoring started at 2020-11-16 17:41:17 +0000 UTC (2 container statuses recorded)
Nov 16 19:20:06.523: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 19:20:06.523: INFO: 	Container node-exporter ready: true, restart count 0
Nov 16 19:20:06.523: INFO: prometheus-operator-7646fdcb7b-tw6jj from openshift-monitoring started at 2020-11-16 17:45:42 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.523: INFO: 	Container prometheus-operator ready: true, restart count 0
Nov 16 19:20:06.523: INFO: grafana-58c46d5bb-hs8tj from openshift-monitoring started at 2020-11-16 17:46:00 +0000 UTC (2 container statuses recorded)
Nov 16 19:20:06.523: INFO: 	Container grafana ready: true, restart count 0
Nov 16 19:20:06.523: INFO: 	Container grafana-proxy ready: true, restart count 0
Nov 16 19:20:06.523: INFO: ibm-keepalived-watcher-x4nqc from kube-system started at 2020-11-16 17:39:19 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.523: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 19:20:06.523: INFO: multus-admission-controller-tf7n7 from openshift-multus started at 2020-11-16 17:40:30 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.523: INFO: 	Container multus-admission-controller ready: true, restart count 0
Nov 16 19:20:06.523: INFO: configmap-cabundle-injector-5bd6fcf58-hlwpk from openshift-service-ca started at 2020-11-16 17:41:14 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.523: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
Nov 16 19:20:06.523: INFO: 
Logging pods the kubelet thinks is on node 10.240.167.254 before test
Nov 16 19:20:06.597: INFO: downloads-5dbb4f4ff7-mtjcb from openshift-console started at 2020-11-16 17:40:25 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.598: INFO: 	Container download-server ready: true, restart count 0
Nov 16 19:20:06.598: INFO: downloads-5dbb4f4ff7-krzbx from openshift-console started at 2020-11-16 17:40:25 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.598: INFO: 	Container download-server ready: true, restart count 0
Nov 16 19:20:06.598: INFO: node-exporter-4kz5x from openshift-monitoring started at 2020-11-16 17:41:17 +0000 UTC (2 container statuses recorded)
Nov 16 19:20:06.598: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 19:20:06.598: INFO: 	Container node-exporter ready: true, restart count 0
Nov 16 19:20:06.598: INFO: network-operator-74fc599569-c45s6 from openshift-network-operator started at 2020-11-16 17:39:02 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.598: INFO: 	Container network-operator ready: true, restart count 0
Nov 16 19:20:06.598: INFO: ibmcloud-block-storage-driver-qdh9z from kube-system started at 2020-11-16 17:39:02 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.599: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 16 19:20:06.599: INFO: calico-node-s6qzg from calico-system started at 2020-11-16 17:40:12 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.599: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 19:20:06.599: INFO: cluster-monitoring-operator-7d44956445-lqkh9 from openshift-monitoring started at 2020-11-16 17:40:24 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.599: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Nov 16 19:20:06.599: INFO: cluster-image-registry-operator-8d47696c5-hpv5z from openshift-image-registry started at 2020-11-16 17:40:25 +0000 UTC (2 container statuses recorded)
Nov 16 19:20:06.599: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Nov 16 19:20:06.599: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
Nov 16 19:20:06.599: INFO: tuned-n7h6s from openshift-cluster-node-tuning-operator started at 2020-11-16 17:40:38 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.599: INFO: 	Container tuned ready: true, restart count 0
Nov 16 19:20:06.599: INFO: telemeter-client-95c48d495-ctf9b from openshift-monitoring started at 2020-11-16 17:45:52 +0000 UTC (3 container statuses recorded)
Nov 16 19:20:06.599: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 19:20:06.600: INFO: 	Container reload ready: true, restart count 0
Nov 16 19:20:06.600: INFO: 	Container telemeter-client ready: true, restart count 0
Nov 16 19:20:06.600: INFO: sonobuoy-systemd-logs-daemon-set-18804f2d5c884991-ffhv2 from sonobuoy started at 2020-11-16 19:14:33 +0000 UTC (2 container statuses recorded)
Nov 16 19:20:06.600: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 19:20:06.600: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 16 19:20:06.600: INFO: openshift-kube-proxy-smf7r from openshift-kube-proxy started at 2020-11-16 17:39:31 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.600: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 16 19:20:06.600: INFO: openshift-service-catalog-apiserver-operator-86b98d6fff-znhvn from openshift-service-catalog-apiserver-operator started at 2020-11-16 17:40:25 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.600: INFO: 	Container operator ready: true, restart count 1
Nov 16 19:20:06.600: INFO: sonobuoy from sonobuoy started at 2020-11-16 19:14:22 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.600: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 16 19:20:06.601: INFO: sonobuoy-e2e-job-b6bf7e56608b4535 from sonobuoy started at 2020-11-16 19:14:33 +0000 UTC (2 container statuses recorded)
Nov 16 19:20:06.601: INFO: 	Container e2e ready: true, restart count 0
Nov 16 19:20:06.601: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 19:20:06.601: INFO: multus-gf29b from openshift-multus started at 2020-11-16 17:39:24 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.601: INFO: 	Container kube-multus ready: true, restart count 0
Nov 16 19:20:06.601: INFO: marketplace-operator-54847664c-cbqsc from openshift-marketplace started at 2020-11-16 17:40:25 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.601: INFO: 	Container marketplace-operator ready: true, restart count 0
Nov 16 19:20:06.601: INFO: calico-kube-controllers-599969f895-4vf88 from calico-system started at 2020-11-16 17:40:25 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.601: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Nov 16 19:20:06.601: INFO: ibmcloud-block-storage-plugin-79495594d5-6wqx5 from kube-system started at 2020-11-16 17:40:25 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.602: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Nov 16 19:20:06.602: INFO: dns-default-rtvgx from openshift-dns started at 2020-11-16 17:42:09 +0000 UTC (2 container statuses recorded)
Nov 16 19:20:06.602: INFO: 	Container dns ready: true, restart count 0
Nov 16 19:20:06.602: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 16 19:20:06.602: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-11-16 17:45:54 +0000 UTC (3 container statuses recorded)
Nov 16 19:20:06.602: INFO: 	Container alertmanager ready: true, restart count 0
Nov 16 19:20:06.602: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 16 19:20:06.602: INFO: 	Container config-reloader ready: true, restart count 0
Nov 16 19:20:06.602: INFO: ibm-cloud-provider-ip-161-156-99-92-596ff786f9-8zlk8 from ibm-system started at 2020-11-16 17:46:19 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.602: INFO: 	Container ibm-cloud-provider-ip-161-156-99-92 ready: true, restart count 0
Nov 16 19:20:06.602: INFO: console-operator-c9844474-csp4h from openshift-console-operator started at 2020-11-16 17:40:25 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.602: INFO: 	Container console-operator ready: true, restart count 1
Nov 16 19:20:06.603: INFO: catalog-operator-86d68f4684-n8kk4 from openshift-operator-lifecycle-manager started at 2020-11-16 17:40:27 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.603: INFO: 	Container catalog-operator ready: true, restart count 0
Nov 16 19:20:06.603: INFO: calico-typha-b5486f777-km6dh from calico-system started at 2020-11-16 17:42:09 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.603: INFO: 	Container calico-typha ready: true, restart count 0
Nov 16 19:20:06.603: INFO: ibm-master-proxy-static-10.240.167.254 from kube-system started at 2020-11-16 17:38:51 +0000 UTC (2 container statuses recorded)
Nov 16 19:20:06.603: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 19:20:06.603: INFO: 	Container pause ready: true, restart count 0
Nov 16 19:20:06.603: INFO: ibm-keepalived-watcher-gw7ng from kube-system started at 2020-11-16 17:38:54 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.603: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 19:20:06.603: INFO: service-ca-operator-5db7bd4f-qmnrv from openshift-service-ca-operator started at 2020-11-16 17:40:25 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.603: INFO: 	Container operator ready: true, restart count 0
Nov 16 19:20:06.603: INFO: ingress-operator-57d688547-8j97m from openshift-ingress-operator started at 2020-11-16 17:40:25 +0000 UTC (2 container statuses recorded)
Nov 16 19:20:06.603: INFO: 	Container ingress-operator ready: true, restart count 0
Nov 16 19:20:06.604: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 19:20:06.604: INFO: ibm-file-plugin-75bbff878-w8grr from kube-system started at 2020-11-16 17:40:27 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.604: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Nov 16 19:20:06.604: INFO: tigera-operator-798cfbf7dd-6rmk9 from tigera-operator started at 2020-11-16 17:39:02 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.604: INFO: 	Container tigera-operator ready: true, restart count 2
Nov 16 19:20:06.604: INFO: openshift-service-catalog-controller-manager-operator-595fxjf2f from openshift-service-catalog-controller-manager-operator started at 2020-11-16 17:40:24 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.604: INFO: 	Container operator ready: true, restart count 1
Nov 16 19:20:06.604: INFO: olm-operator-88bfd79df-7mm7b from openshift-operator-lifecycle-manager started at 2020-11-16 17:40:24 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.604: INFO: 	Container olm-operator ready: true, restart count 0
Nov 16 19:20:06.604: INFO: ibm-storage-watcher-69d9c445b4-k5bvh from kube-system started at 2020-11-16 17:40:24 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.605: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Nov 16 19:20:06.605: INFO: cluster-storage-operator-6488c9f77b-wq8vv from openshift-cluster-storage-operator started at 2020-11-16 17:40:27 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.605: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Nov 16 19:20:06.605: INFO: dns-operator-847f45b78c-49hz4 from openshift-dns-operator started at 2020-11-16 17:40:24 +0000 UTC (2 container statuses recorded)
Nov 16 19:20:06.605: INFO: 	Container dns-operator ready: true, restart count 0
Nov 16 19:20:06.605: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 19:20:06.605: INFO: cluster-node-tuning-operator-5c44ccc99b-6g94b from openshift-cluster-node-tuning-operator started at 2020-11-16 17:40:25 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.605: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Nov 16 19:20:06.605: INFO: multus-admission-controller-xw5h7 from openshift-multus started at 2020-11-16 17:40:25 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.605: INFO: 	Container multus-admission-controller ready: true, restart count 0
Nov 16 19:20:06.606: INFO: node-ca-852zk from openshift-image-registry started at 2020-11-16 17:41:42 +0000 UTC (1 container statuses recorded)
Nov 16 19:20:06.606: INFO: 	Container node-ca ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-b92973fe-6e2c-4c5f-ac27-4e1eab82e76f 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-b92973fe-6e2c-4c5f-ac27-4e1eab82e76f off the node 10.240.167.254
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b92973fe-6e2c-4c5f-ac27-4e1eab82e76f
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:20:16.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5653" for this suite.
Nov 16 19:20:34.946: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:20:36.856: INFO: namespace sched-pred-5653 deletion completed in 19.936696928s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:30.655 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:20:36.857: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Starting the proxy
Nov 16 19:20:36.960: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-420283327 proxy --unix-socket=/tmp/kubectl-proxy-unix180721146/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:20:37.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1557" for this suite.
Nov 16 19:20:45.066: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:20:46.973: INFO: namespace kubectl-1557 deletion completed in 9.934282819s

• [SLOW TEST:10.116 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:20:46.974: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override command
Nov 16 19:20:47.133: INFO: Waiting up to 5m0s for pod "client-containers-4501add9-c62c-4cf5-936c-89af38ab5e92" in namespace "containers-741" to be "success or failure"
Nov 16 19:20:47.142: INFO: Pod "client-containers-4501add9-c62c-4cf5-936c-89af38ab5e92": Phase="Pending", Reason="", readiness=false. Elapsed: 8.672579ms
Nov 16 19:20:49.152: INFO: Pod "client-containers-4501add9-c62c-4cf5-936c-89af38ab5e92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019407681s
STEP: Saw pod success
Nov 16 19:20:49.152: INFO: Pod "client-containers-4501add9-c62c-4cf5-936c-89af38ab5e92" satisfied condition "success or failure"
Nov 16 19:20:49.162: INFO: Trying to get logs from node 10.240.167.254 pod client-containers-4501add9-c62c-4cf5-936c-89af38ab5e92 container test-container: <nil>
STEP: delete the pod
Nov 16 19:20:49.217: INFO: Waiting for pod client-containers-4501add9-c62c-4cf5-936c-89af38ab5e92 to disappear
Nov 16 19:20:49.228: INFO: Pod client-containers-4501add9-c62c-4cf5-936c-89af38ab5e92 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:20:49.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-741" for this suite.
Nov 16 19:20:57.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:20:59.172: INFO: namespace containers-741 deletion completed in 9.931117062s

• [SLOW TEST:12.198 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:20:59.173: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 16 19:20:59.342: INFO: Waiting up to 5m0s for pod "downwardapi-volume-592a2281-2f5a-4496-b522-cf70f6cc520d" in namespace "downward-api-7628" to be "success or failure"
Nov 16 19:20:59.351: INFO: Pod "downwardapi-volume-592a2281-2f5a-4496-b522-cf70f6cc520d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.604275ms
Nov 16 19:21:01.363: INFO: Pod "downwardapi-volume-592a2281-2f5a-4496-b522-cf70f6cc520d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021089821s
Nov 16 19:21:03.374: INFO: Pod "downwardapi-volume-592a2281-2f5a-4496-b522-cf70f6cc520d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032234685s
Nov 16 19:21:05.385: INFO: Pod "downwardapi-volume-592a2281-2f5a-4496-b522-cf70f6cc520d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.043347519s
Nov 16 19:21:07.396: INFO: Pod "downwardapi-volume-592a2281-2f5a-4496-b522-cf70f6cc520d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.053974306s
STEP: Saw pod success
Nov 16 19:21:07.396: INFO: Pod "downwardapi-volume-592a2281-2f5a-4496-b522-cf70f6cc520d" satisfied condition "success or failure"
Nov 16 19:21:07.406: INFO: Trying to get logs from node 10.240.167.206 pod downwardapi-volume-592a2281-2f5a-4496-b522-cf70f6cc520d container client-container: <nil>
STEP: delete the pod
Nov 16 19:21:07.462: INFO: Waiting for pod downwardapi-volume-592a2281-2f5a-4496-b522-cf70f6cc520d to disappear
Nov 16 19:21:07.473: INFO: Pod downwardapi-volume-592a2281-2f5a-4496-b522-cf70f6cc520d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:21:07.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7628" for this suite.
Nov 16 19:21:15.513: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:21:17.431: INFO: namespace downward-api-7628 deletion completed in 9.945662972s

• [SLOW TEST:18.258 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:21:17.431: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl replace
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1704
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Nov 16 19:21:17.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-151'
Nov 16 19:21:17.825: INFO: stderr: ""
Nov 16 19:21:17.825: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Nov 16 19:21:22.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pod e2e-test-httpd-pod --namespace=kubectl-151 -o json'
Nov 16 19:21:22.998: INFO: stderr: ""
Nov 16 19:21:22.998: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"172.30.216.220/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.216.220/32\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.216.220\\\"\\n    ],\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2020-11-16T19:21:17Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-151\",\n        \"resourceVersion\": \"43467\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-151/pods/e2e-test-httpd-pod\",\n        \"uid\": \"441d83c2-a55f-4588-934f-4b7ceac1bf85\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-xtz7w\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-k4w6n\"\n            }\n        ],\n        \"nodeName\": \"10.240.167.254\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c42,c9\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-xtz7w\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-xtz7w\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-16T19:21:17Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-16T19:21:19Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-16T19:21:19Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-16T19:21:17Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://389967f838ad2148ee45bdfa72b7ad65219119c68f6fb2e4543bc691c63eff9b\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-11-16T19:21:18Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.240.167.254\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.216.220\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.216.220\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-11-16T19:21:17Z\"\n    }\n}\n"
STEP: replace the image in the pod
Nov 16 19:21:22.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 replace -f - --namespace=kubectl-151'
Nov 16 19:21:23.516: INFO: stderr: ""
Nov 16 19:21:23.516: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
Nov 16 19:21:23.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 delete pods e2e-test-httpd-pod --namespace=kubectl-151'
Nov 16 19:21:31.673: INFO: stderr: ""
Nov 16 19:21:31.673: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:21:31.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-151" for this suite.
Nov 16 19:21:39.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:21:41.627: INFO: namespace kubectl-151 deletion completed in 9.939962842s

• [SLOW TEST:24.196 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1700
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:21:41.628: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating all guestbook components
Nov 16 19:21:41.766: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Nov 16 19:21:41.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 create -f - --namespace=kubectl-9358'
Nov 16 19:21:42.100: INFO: stderr: ""
Nov 16 19:21:42.100: INFO: stdout: "service/redis-slave created\n"
Nov 16 19:21:42.100: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Nov 16 19:21:42.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 create -f - --namespace=kubectl-9358'
Nov 16 19:21:42.562: INFO: stderr: ""
Nov 16 19:21:42.562: INFO: stdout: "service/redis-master created\n"
Nov 16 19:21:42.563: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Nov 16 19:21:42.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 create -f - --namespace=kubectl-9358'
Nov 16 19:21:43.085: INFO: stderr: ""
Nov 16 19:21:43.085: INFO: stdout: "service/frontend created\n"
Nov 16 19:21:43.088: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Nov 16 19:21:43.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 create -f - --namespace=kubectl-9358'
Nov 16 19:21:43.388: INFO: stderr: ""
Nov 16 19:21:43.388: INFO: stdout: "deployment.apps/frontend created\n"
Nov 16 19:21:43.388: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: docker.io/library/redis:5.0.5-alpine
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Nov 16 19:21:43.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 create -f - --namespace=kubectl-9358'
Nov 16 19:21:43.874: INFO: stderr: ""
Nov 16 19:21:43.874: INFO: stdout: "deployment.apps/redis-master created\n"
Nov 16 19:21:43.874: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: docker.io/library/redis:5.0.5-alpine
        # We are only implementing the dns option of:
        # https://github.com/kubernetes/examples/blob/97c7ed0eb6555a4b667d2877f965d392e00abc45/guestbook/redis-slave/run.sh
        command: [ "redis-server", "--slaveof", "redis-master", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Nov 16 19:21:43.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 create -f - --namespace=kubectl-9358'
Nov 16 19:21:44.430: INFO: stderr: ""
Nov 16 19:21:44.430: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Nov 16 19:21:44.430: INFO: Waiting for all frontend pods to be Running.
Nov 16 19:22:09.481: INFO: Waiting for frontend to serve content.
Nov 16 19:22:09.517: INFO: Trying to add a new entry to the guestbook.
Nov 16 19:22:09.545: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Nov 16 19:22:09.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 delete --grace-period=0 --force -f - --namespace=kubectl-9358'
Nov 16 19:22:09.729: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 16 19:22:09.729: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Nov 16 19:22:09.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 delete --grace-period=0 --force -f - --namespace=kubectl-9358'
Nov 16 19:22:09.890: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 16 19:22:09.890: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Nov 16 19:22:09.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 delete --grace-period=0 --force -f - --namespace=kubectl-9358'
Nov 16 19:22:10.041: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 16 19:22:10.041: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Nov 16 19:22:10.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 delete --grace-period=0 --force -f - --namespace=kubectl-9358'
Nov 16 19:22:10.183: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 16 19:22:10.183: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Nov 16 19:22:10.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 delete --grace-period=0 --force -f - --namespace=kubectl-9358'
Nov 16 19:22:10.298: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 16 19:22:10.298: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Nov 16 19:22:10.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 delete --grace-period=0 --force -f - --namespace=kubectl-9358'
Nov 16 19:22:10.415: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 16 19:22:10.415: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:22:10.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9358" for this suite.
Nov 16 19:22:24.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:22:26.364: INFO: namespace kubectl-9358 deletion completed in 15.937188812s

• [SLOW TEST:44.736 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:333
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:22:26.364: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 19:22:26.502: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Nov 16 19:22:28.603: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:22:29.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4306" for this suite.
Nov 16 19:22:37.659: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:22:39.569: INFO: namespace replication-controller-4306 deletion completed in 9.934625334s

• [SLOW TEST:13.204 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:22:39.569: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:22:46.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3454" for this suite.
Nov 16 19:22:54.774: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:22:56.681: INFO: namespace resourcequota-3454 deletion completed in 9.936900419s

• [SLOW TEST:17.112 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:22:56.681: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-5662
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating stateful set ss in namespace statefulset-5662
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5662
Nov 16 19:22:56.854: INFO: Found 0 stateful pods, waiting for 1
Nov 16 19:23:06.865: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Nov 16 19:23:06.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 16 19:23:07.164: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 16 19:23:07.164: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 16 19:23:07.164: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 16 19:23:07.175: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov 16 19:23:17.187: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 16 19:23:17.187: INFO: Waiting for statefulset status.replicas updated to 0
Nov 16 19:23:17.228: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Nov 16 19:23:17.228: INFO: ss-0  10.240.167.254  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:22:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:22:56 +0000 UTC  }]
Nov 16 19:23:17.228: INFO: 
Nov 16 19:23:17.228: INFO: StatefulSet ss has not reached scale 3, at 1
Nov 16 19:23:18.239: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989238932s
Nov 16 19:23:19.253: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.978123854s
Nov 16 19:23:20.265: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.964169712s
Nov 16 19:23:21.276: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.95197816s
Nov 16 19:23:22.288: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.940883895s
Nov 16 19:23:23.300: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.929347566s
Nov 16 19:23:24.311: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.916674312s
Nov 16 19:23:25.323: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.9055685s
Nov 16 19:23:26.334: INFO: Verifying statefulset ss doesn't scale past 3 for another 894.176701ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5662
Nov 16 19:23:27.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:23:27.673: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 16 19:23:27.673: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 16 19:23:27.673: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 16 19:23:27.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:23:27.987: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Nov 16 19:23:27.987: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 16 19:23:27.987: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 16 19:23:27.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:23:28.279: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Nov 16 19:23:28.279: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 16 19:23:28.279: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 16 19:23:28.291: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 19:23:28.291: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 19:23:28.291: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Nov 16 19:23:28.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 16 19:23:28.602: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 16 19:23:28.602: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 16 19:23:28.602: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 16 19:23:28.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 16 19:23:28.943: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 16 19:23:28.943: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 16 19:23:28.943: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 16 19:23:28.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 16 19:23:29.222: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 16 19:23:29.222: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 16 19:23:29.222: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 16 19:23:29.222: INFO: Waiting for statefulset status.replicas updated to 0
Nov 16 19:23:29.233: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Nov 16 19:23:39.254: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 16 19:23:39.254: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov 16 19:23:39.254: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov 16 19:23:39.287: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Nov 16 19:23:39.287: INFO: ss-0  10.240.167.254  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:22:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:22:56 +0000 UTC  }]
Nov 16 19:23:39.287: INFO: ss-1  10.240.167.206  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:17 +0000 UTC  }]
Nov 16 19:23:39.287: INFO: ss-2  10.240.167.209  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:17 +0000 UTC  }]
Nov 16 19:23:39.287: INFO: 
Nov 16 19:23:39.287: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 16 19:23:40.299: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Nov 16 19:23:40.299: INFO: ss-0  10.240.167.254  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:22:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:22:56 +0000 UTC  }]
Nov 16 19:23:40.300: INFO: ss-1  10.240.167.206  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:17 +0000 UTC  }]
Nov 16 19:23:40.300: INFO: ss-2  10.240.167.209  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:17 +0000 UTC  }]
Nov 16 19:23:40.300: INFO: 
Nov 16 19:23:40.300: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 16 19:23:41.311: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Nov 16 19:23:41.311: INFO: ss-0  10.240.167.254  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:22:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:22:56 +0000 UTC  }]
Nov 16 19:23:41.311: INFO: ss-1  10.240.167.206  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:17 +0000 UTC  }]
Nov 16 19:23:41.311: INFO: ss-2  10.240.167.209  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:17 +0000 UTC  }]
Nov 16 19:23:41.311: INFO: 
Nov 16 19:23:41.311: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 16 19:23:42.323: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Nov 16 19:23:42.323: INFO: ss-0  10.240.167.254  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:22:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:22:56 +0000 UTC  }]
Nov 16 19:23:42.323: INFO: ss-1  10.240.167.206  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:17 +0000 UTC  }]
Nov 16 19:23:42.323: INFO: 
Nov 16 19:23:42.323: INFO: StatefulSet ss has not reached scale 0, at 2
Nov 16 19:23:43.335: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Nov 16 19:23:43.335: INFO: ss-0  10.240.167.254  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:22:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:22:56 +0000 UTC  }]
Nov 16 19:23:43.335: INFO: ss-1  10.240.167.206  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:17 +0000 UTC  }]
Nov 16 19:23:43.335: INFO: 
Nov 16 19:23:43.335: INFO: StatefulSet ss has not reached scale 0, at 2
Nov 16 19:23:44.346: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Nov 16 19:23:44.346: INFO: ss-0  10.240.167.254  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:22:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:22:56 +0000 UTC  }]
Nov 16 19:23:44.346: INFO: ss-1  10.240.167.206  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:17 +0000 UTC  }]
Nov 16 19:23:44.346: INFO: 
Nov 16 19:23:44.346: INFO: StatefulSet ss has not reached scale 0, at 2
Nov 16 19:23:45.357: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Nov 16 19:23:45.357: INFO: ss-0  10.240.167.254  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:22:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:22:56 +0000 UTC  }]
Nov 16 19:23:45.357: INFO: ss-1  10.240.167.206  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:17 +0000 UTC  }]
Nov 16 19:23:45.357: INFO: 
Nov 16 19:23:45.357: INFO: StatefulSet ss has not reached scale 0, at 2
Nov 16 19:23:46.368: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Nov 16 19:23:46.368: INFO: ss-0  10.240.167.254  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:22:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:22:56 +0000 UTC  }]
Nov 16 19:23:46.368: INFO: ss-1  10.240.167.206  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:17 +0000 UTC  }]
Nov 16 19:23:46.368: INFO: 
Nov 16 19:23:46.368: INFO: StatefulSet ss has not reached scale 0, at 2
Nov 16 19:23:47.385: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Nov 16 19:23:47.385: INFO: ss-0  10.240.167.254  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:22:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:22:56 +0000 UTC  }]
Nov 16 19:23:47.385: INFO: ss-1  10.240.167.206  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:17 +0000 UTC  }]
Nov 16 19:23:47.385: INFO: 
Nov 16 19:23:47.385: INFO: StatefulSet ss has not reached scale 0, at 2
Nov 16 19:23:48.397: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Nov 16 19:23:48.397: INFO: ss-0  10.240.167.254  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:22:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:22:56 +0000 UTC  }]
Nov 16 19:23:48.397: INFO: ss-1  10.240.167.206  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-16 19:23:17 +0000 UTC  }]
Nov 16 19:23:48.397: INFO: 
Nov 16 19:23:48.397: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5662
Nov 16 19:23:49.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:23:49.581: INFO: rc: 1
Nov 16 19:23:49.581: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  error: unable to upgrade connection: container not found ("webserver")
 [] <nil> 0xc0029fced0 exit status 1 <nil> <nil> true [0xc001944350 0xc001944368 0xc001944380] [0xc001944350 0xc001944368 0xc001944380] [0xc001944360 0xc001944378] [0x10efce0 0x10efce0] 0xc0022ae960 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 16 19:23:59.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:23:59.707: INFO: rc: 1
Nov 16 19:23:59.707: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002294960 exit status 1 <nil> <nil> true [0xc00250ad30 0xc00250ad48 0xc00250ad60] [0xc00250ad30 0xc00250ad48 0xc00250ad60] [0xc00250ad40 0xc00250ad58] [0x10efce0 0x10efce0] 0xc002cc1bc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 16 19:24:09.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:24:09.824: INFO: rc: 1
Nov 16 19:24:09.824: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002294c90 exit status 1 <nil> <nil> true [0xc00250ad68 0xc00250ad80 0xc00250ad98] [0xc00250ad68 0xc00250ad80 0xc00250ad98] [0xc00250ad78 0xc00250ad90] [0x10efce0 0x10efce0] 0xc0030b44e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 16 19:24:19.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:24:19.940: INFO: rc: 1
Nov 16 19:24:19.941: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001b5de60 exit status 1 <nil> <nil> true [0xc001184128 0xc001184140 0xc001184180] [0xc001184128 0xc001184140 0xc001184180] [0xc001184138 0xc001184168] [0x10efce0 0x10efce0] 0xc003876ea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 16 19:24:29.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:24:30.052: INFO: rc: 1
Nov 16 19:24:30.052: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0022950e0 exit status 1 <nil> <nil> true [0xc00250ada0 0xc00250adb8 0xc00250add0] [0xc00250ada0 0xc00250adb8 0xc00250add0] [0xc00250adb0 0xc00250adc8] [0x10efce0 0x10efce0] 0xc0030b5b60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 16 19:24:40.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:24:40.166: INFO: rc: 1
Nov 16 19:24:40.166: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0029fd230 exit status 1 <nil> <nil> true [0xc001944388 0xc0019443a0 0xc0019443b8] [0xc001944388 0xc0019443a0 0xc0019443b8] [0xc001944398 0xc0019443b0] [0x10efce0 0x10efce0] 0xc0022aefc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 16 19:24:50.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:24:50.295: INFO: rc: 1
Nov 16 19:24:50.295: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002364330 exit status 1 <nil> <nil> true [0xc001faa028 0xc001faa068 0xc001faa0b0] [0xc001faa028 0xc001faa068 0xc001faa0b0] [0xc001faa050 0xc001faa0a8] [0x10efce0 0x10efce0] 0xc0027ce540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 16 19:25:00.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:25:00.406: INFO: rc: 1
Nov 16 19:25:00.406: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001e82330 exit status 1 <nil> <nil> true [0xc0004eac28 0xc0004eb220 0xc0004eb4f0] [0xc0004eac28 0xc0004eb220 0xc0004eb4f0] [0xc0004eb1f8 0xc0004eb3c8] [0x10efce0 0x10efce0] 0xc002cc0420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 16 19:25:10.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:25:10.520: INFO: rc: 1
Nov 16 19:25:10.520: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001388330 exit status 1 <nil> <nil> true [0xc0007fcea8 0xc000011530 0xc000011ee0] [0xc0007fcea8 0xc000011530 0xc000011ee0] [0xc000010c08 0xc000011b68] [0x10efce0 0x10efce0] 0xc00212aa80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 16 19:25:20.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:25:20.643: INFO: rc: 1
Nov 16 19:25:20.643: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001e826c0 exit status 1 <nil> <nil> true [0xc0004eb568 0xc001b04230 0xc001b04770] [0xc0004eb568 0xc001b04230 0xc001b04770] [0xc001b04128 0xc001b04470] [0x10efce0 0x10efce0] 0xc002cc0ba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 16 19:25:30.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:25:30.752: INFO: rc: 1
Nov 16 19:25:30.752: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002364690 exit status 1 <nil> <nil> true [0xc001faa0b8 0xc001faa0f8 0xc001faa128] [0xc001faa0b8 0xc001faa0f8 0xc001faa128] [0xc001faa0d8 0xc001faa118] [0x10efce0 0x10efce0] 0xc0027ceba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 16 19:25:40.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:25:40.870: INFO: rc: 1
Nov 16 19:25:40.870: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001388690 exit status 1 <nil> <nil> true [0xc000011fe0 0xc0019cc280 0xc0019cc468] [0xc000011fe0 0xc0019cc280 0xc0019cc468] [0xc0019cc158 0xc0019cc330] [0x10efce0 0x10efce0] 0xc00212b740 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 16 19:25:50.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:25:50.999: INFO: rc: 1
Nov 16 19:25:50.999: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001e82a80 exit status 1 <nil> <nil> true [0xc001b04ab8 0xc001b04f50 0xc001b05200] [0xc001b04ab8 0xc001b04f50 0xc001b05200] [0xc001b04e38 0xc001b05148] [0x10efce0 0x10efce0] 0xc002cc17a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 16 19:26:00.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:26:01.120: INFO: rc: 1
Nov 16 19:26:01.120: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001388a80 exit status 1 <nil> <nil> true [0xc0019cc5e8 0xc0019cc780 0xc0019cc808] [0xc0019cc5e8 0xc0019cc780 0xc0019cc808] [0xc0019cc738 0xc0019cc7b8] [0x10efce0 0x10efce0] 0xc00212bf80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 16 19:26:11.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:26:11.241: INFO: rc: 1
Nov 16 19:26:11.241: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002364a20 exit status 1 <nil> <nil> true [0xc001faa130 0xc001faa160 0xc001faa180] [0xc001faa130 0xc001faa160 0xc001faa180] [0xc001faa150 0xc001faa178] [0x10efce0 0x10efce0] 0xc0027cf0e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 16 19:26:21.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:26:21.366: INFO: rc: 1
Nov 16 19:26:21.367: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001388e70 exit status 1 <nil> <nil> true [0xc0019cc828 0xc0019cc880 0xc0019cca48] [0xc0019cc828 0xc0019cc880 0xc0019cca48] [0xc0019cc850 0xc0019cc970] [0x10efce0 0x10efce0] 0xc002c2c660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 16 19:26:31.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:26:31.487: INFO: rc: 1
Nov 16 19:26:31.487: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002364d80 exit status 1 <nil> <nil> true [0xc001faa190 0xc001faa1d8 0xc001faa200] [0xc001faa190 0xc001faa1d8 0xc001faa200] [0xc001faa1b8 0xc001faa1f0] [0x10efce0 0x10efce0] 0xc0027cf740 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 16 19:26:41.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:26:41.647: INFO: rc: 1
Nov 16 19:26:41.647: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001389260 exit status 1 <nil> <nil> true [0xc0019cca58 0xc0019ccb50 0xc0019ccc98] [0xc0019cca58 0xc0019ccb50 0xc0019ccc98] [0xc0019ccb08 0xc0019ccc28] [0x10efce0 0x10efce0] 0xc002c2cc60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 16 19:26:51.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:26:51.769: INFO: rc: 1
Nov 16 19:26:51.769: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001388300 exit status 1 <nil> <nil> true [0xc000011530 0xc000011ee0 0xc0007fdca8] [0xc000011530 0xc000011ee0 0xc0007fdca8] [0xc000011b68 0xc0007fcea8] [0x10efce0 0x10efce0] 0xc00212aa80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 16 19:27:01.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:27:01.891: INFO: rc: 1
Nov 16 19:27:01.891: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001e82300 exit status 1 <nil> <nil> true [0xc0004ea250 0xc0004eb1f8 0xc0004eb3c8] [0xc0004ea250 0xc0004eb1f8 0xc0004eb3c8] [0xc0004eaeb0 0xc0004eb2a0] [0x10efce0 0x10efce0] 0xc002c2c480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 16 19:27:11.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:27:12.027: INFO: rc: 1
Nov 16 19:27:12.027: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002364300 exit status 1 <nil> <nil> true [0xc001b04128 0xc001b04470 0xc001b04c80] [0xc001b04128 0xc001b04470 0xc001b04c80] [0xc001b043c8 0xc001b04ab8] [0x10efce0 0x10efce0] 0xc002cc0420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 16 19:27:22.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:27:22.148: INFO: rc: 1
Nov 16 19:27:22.148: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001e82690 exit status 1 <nil> <nil> true [0xc0004eb4f0 0xc0019cc0a8 0xc0019cc2d8] [0xc0004eb4f0 0xc0019cc0a8 0xc0019cc2d8] [0xc0004eb5b8 0xc0019cc280] [0x10efce0 0x10efce0] 0xc002c2cae0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 16 19:27:32.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:27:32.253: INFO: rc: 1
Nov 16 19:27:32.253: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001e82a20 exit status 1 <nil> <nil> true [0xc0019cc330 0xc0019cc718 0xc0019cc798] [0xc0019cc330 0xc0019cc718 0xc0019cc798] [0xc0019cc5e8 0xc0019cc780] [0x10efce0 0x10efce0] 0xc002c2d200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 16 19:27:42.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:27:42.376: INFO: rc: 1
Nov 16 19:27:42.376: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001e82de0 exit status 1 <nil> <nil> true [0xc0019cc7b8 0xc0019cc838 0xc0019cc8e8] [0xc0019cc7b8 0xc0019cc838 0xc0019cc8e8] [0xc0019cc828 0xc0019cc880] [0x10efce0 0x10efce0] 0xc002c2d740 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 16 19:27:52.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:27:52.497: INFO: rc: 1
Nov 16 19:27:52.497: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002364720 exit status 1 <nil> <nil> true [0xc001b04e38 0xc001b05148 0xc001b053a0] [0xc001b04e38 0xc001b05148 0xc001b053a0] [0xc001b04fd8 0xc001b05220] [0x10efce0 0x10efce0] 0xc002cc0ba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 16 19:28:02.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:28:02.617: INFO: rc: 1
Nov 16 19:28:02.617: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0013886c0 exit status 1 <nil> <nil> true [0xc001faa018 0xc001faa050 0xc001faa0a8] [0xc001faa018 0xc001faa050 0xc001faa0a8] [0xc001faa040 0xc001faa088] [0x10efce0 0x10efce0] 0xc00212b740 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 16 19:28:12.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:28:12.740: INFO: rc: 1
Nov 16 19:28:12.740: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002364ae0 exit status 1 <nil> <nil> true [0xc001b054d8 0xc001b05798 0xc001b05bf8] [0xc001b054d8 0xc001b05798 0xc001b05bf8] [0xc001b056d0 0xc001b05b80] [0x10efce0 0x10efce0] 0xc002cc17a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 16 19:28:22.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:28:22.857: INFO: rc: 1
Nov 16 19:28:22.857: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001e83140 exit status 1 <nil> <nil> true [0xc0019cc970 0xc0019ccaa0 0xc0019ccb98] [0xc0019cc970 0xc0019ccaa0 0xc0019ccb98] [0xc0019cca58 0xc0019ccb50] [0x10efce0 0x10efce0] 0xc002c2db00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 16 19:28:32.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:28:32.987: INFO: rc: 1
Nov 16 19:28:32.987: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0010ba360 exit status 1 <nil> <nil> true [0xc00254e000 0xc00254e030 0xc00254e048] [0xc00254e000 0xc00254e030 0xc00254e048] [0xc00254e028 0xc00254e040] [0x10efce0 0x10efce0] 0xc0027ce600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 16 19:28:42.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:28:43.094: INFO: rc: 1
Nov 16 19:28:43.094: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001388ab0 exit status 1 <nil> <nil> true [0xc001faa0b0 0xc001faa0d8 0xc001faa118] [0xc001faa0b0 0xc001faa0d8 0xc001faa118] [0xc001faa0c8 0xc001faa108] [0x10efce0 0x10efce0] 0xc00212bf80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 16 19:28:53.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-5662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:28:53.214: INFO: rc: 1
Nov 16 19:28:53.214: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: 
Nov 16 19:28:53.214: INFO: Scaling statefulset ss to 0
Nov 16 19:28:53.250: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Nov 16 19:28:53.258: INFO: Deleting all statefulset in ns statefulset-5662
Nov 16 19:28:53.265: INFO: Scaling statefulset ss to 0
Nov 16 19:28:53.288: INFO: Waiting for statefulset status.replicas updated to 0
Nov 16 19:28:53.295: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:28:53.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5662" for this suite.
Nov 16 19:29:01.371: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:29:03.286: INFO: namespace statefulset-5662 deletion completed in 9.947636273s

• [SLOW TEST:366.605 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:29:03.287: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Nov 16 19:29:03.461: INFO: Waiting up to 5m0s for pod "downward-api-bc39c89e-424b-417d-90f2-debc2c78a227" in namespace "downward-api-1628" to be "success or failure"
Nov 16 19:29:03.468: INFO: Pod "downward-api-bc39c89e-424b-417d-90f2-debc2c78a227": Phase="Pending", Reason="", readiness=false. Elapsed: 6.726903ms
Nov 16 19:29:05.476: INFO: Pod "downward-api-bc39c89e-424b-417d-90f2-debc2c78a227": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014469575s
Nov 16 19:29:07.485: INFO: Pod "downward-api-bc39c89e-424b-417d-90f2-debc2c78a227": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023121959s
Nov 16 19:29:09.492: INFO: Pod "downward-api-bc39c89e-424b-417d-90f2-debc2c78a227": Phase="Pending", Reason="", readiness=false. Elapsed: 6.030614208s
Nov 16 19:29:11.500: INFO: Pod "downward-api-bc39c89e-424b-417d-90f2-debc2c78a227": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.038131629s
STEP: Saw pod success
Nov 16 19:29:11.500: INFO: Pod "downward-api-bc39c89e-424b-417d-90f2-debc2c78a227" satisfied condition "success or failure"
Nov 16 19:29:11.506: INFO: Trying to get logs from node 10.240.167.206 pod downward-api-bc39c89e-424b-417d-90f2-debc2c78a227 container dapi-container: <nil>
STEP: delete the pod
Nov 16 19:29:11.574: INFO: Waiting for pod downward-api-bc39c89e-424b-417d-90f2-debc2c78a227 to disappear
Nov 16 19:29:11.582: INFO: Pod downward-api-bc39c89e-424b-417d-90f2-debc2c78a227 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:29:11.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1628" for this suite.
Nov 16 19:29:19.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:29:21.529: INFO: namespace downward-api-1628 deletion completed in 9.936507935s

• [SLOW TEST:18.242 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:29:21.530: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-164973b5-d2e9-43af-9db4-1aaf81661e87
STEP: Creating a pod to test consume secrets
Nov 16 19:29:21.791: INFO: Waiting up to 5m0s for pod "pod-secrets-911ee369-2432-4b7e-b122-d00e31e9bbc2" in namespace "secrets-8295" to be "success or failure"
Nov 16 19:29:21.798: INFO: Pod "pod-secrets-911ee369-2432-4b7e-b122-d00e31e9bbc2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.111499ms
Nov 16 19:29:23.807: INFO: Pod "pod-secrets-911ee369-2432-4b7e-b122-d00e31e9bbc2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01532519s
STEP: Saw pod success
Nov 16 19:29:23.807: INFO: Pod "pod-secrets-911ee369-2432-4b7e-b122-d00e31e9bbc2" satisfied condition "success or failure"
Nov 16 19:29:23.813: INFO: Trying to get logs from node 10.240.167.254 pod pod-secrets-911ee369-2432-4b7e-b122-d00e31e9bbc2 container secret-volume-test: <nil>
STEP: delete the pod
Nov 16 19:29:23.874: INFO: Waiting for pod pod-secrets-911ee369-2432-4b7e-b122-d00e31e9bbc2 to disappear
Nov 16 19:29:23.880: INFO: Pod pod-secrets-911ee369-2432-4b7e-b122-d00e31e9bbc2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:29:23.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8295" for this suite.
Nov 16 19:29:31.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:29:33.839: INFO: namespace secrets-8295 deletion completed in 9.947855159s
STEP: Destroying namespace "secret-namespace-9718" for this suite.
Nov 16 19:29:41.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:29:43.786: INFO: namespace secret-namespace-9718 deletion completed in 9.947441257s

• [SLOW TEST:22.257 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:29:43.787: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Nov 16 19:29:43.922: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
Nov 16 19:29:51.470: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:30:19.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8513" for this suite.
Nov 16 19:30:27.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:30:29.201: INFO: namespace crd-publish-openapi-8513 deletion completed in 9.933944156s

• [SLOW TEST:45.414 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:30:29.202: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-07c76223-71e3-40e1-9461-7740e1577eeb
STEP: Creating a pod to test consume configMaps
Nov 16 19:30:29.357: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5d3b86d6-6753-4573-b017-9b0fc6c2aa43" in namespace "projected-3766" to be "success or failure"
Nov 16 19:30:29.365: INFO: Pod "pod-projected-configmaps-5d3b86d6-6753-4573-b017-9b0fc6c2aa43": Phase="Pending", Reason="", readiness=false. Elapsed: 7.321801ms
Nov 16 19:30:31.373: INFO: Pod "pod-projected-configmaps-5d3b86d6-6753-4573-b017-9b0fc6c2aa43": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015127111s
Nov 16 19:30:33.381: INFO: Pod "pod-projected-configmaps-5d3b86d6-6753-4573-b017-9b0fc6c2aa43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023527731s
STEP: Saw pod success
Nov 16 19:30:33.381: INFO: Pod "pod-projected-configmaps-5d3b86d6-6753-4573-b017-9b0fc6c2aa43" satisfied condition "success or failure"
Nov 16 19:30:33.388: INFO: Trying to get logs from node 10.240.167.254 pod pod-projected-configmaps-5d3b86d6-6753-4573-b017-9b0fc6c2aa43 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 16 19:30:33.447: INFO: Waiting for pod pod-projected-configmaps-5d3b86d6-6753-4573-b017-9b0fc6c2aa43 to disappear
Nov 16 19:30:33.454: INFO: Pod pod-projected-configmaps-5d3b86d6-6753-4573-b017-9b0fc6c2aa43 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:30:33.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3766" for this suite.
Nov 16 19:30:41.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:30:43.402: INFO: namespace projected-3766 deletion completed in 9.937852288s

• [SLOW TEST:14.200 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:30:43.403: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 19:31:05.560: INFO: Container started at 2020-11-16 19:30:50 +0000 UTC, pod became ready at 2020-11-16 19:31:05 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:31:05.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1107" for this suite.
Nov 16 19:31:35.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:31:37.518: INFO: namespace container-probe-1107 deletion completed in 31.946110736s

• [SLOW TEST:54.116 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:31:37.521: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 19:31:38.112: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 16 19:31:40.140: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741151898, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741151898, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741151898, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741151898, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 19:31:43.175: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
Nov 16 19:31:44.300: INFO: Waiting for webhook configuration to be ready...
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:31:55.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3473" for this suite.
Nov 16 19:32:03.699: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:32:05.604: INFO: namespace webhook-3473 deletion completed in 9.932801627s
STEP: Destroying namespace "webhook-3473-markers" for this suite.
Nov 16 19:32:13.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:32:15.539: INFO: namespace webhook-3473-markers deletion completed in 9.93546285s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:38.060 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:32:15.582: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1914.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1914.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1914.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1914.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1914.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1914.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1914.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1914.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1914.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1914.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1914.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1914.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1914.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 175.45.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.45.175_udp@PTR;check="$$(dig +tcp +noall +answer +search 175.45.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.45.175_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1914.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1914.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1914.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1914.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1914.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1914.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1914.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1914.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1914.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1914.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1914.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1914.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1914.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 175.45.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.45.175_udp@PTR;check="$$(dig +tcp +noall +answer +search 175.45.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.45.175_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 16 19:32:41.833: INFO: Unable to read wheezy_udp@dns-test-service.dns-1914.svc.cluster.local from pod dns-1914/dns-test-eb2db5e0-fd6a-4112-8dac-1368c8f75060: the server could not find the requested resource (get pods dns-test-eb2db5e0-fd6a-4112-8dac-1368c8f75060)
Nov 16 19:32:41.845: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1914.svc.cluster.local from pod dns-1914/dns-test-eb2db5e0-fd6a-4112-8dac-1368c8f75060: the server could not find the requested resource (get pods dns-test-eb2db5e0-fd6a-4112-8dac-1368c8f75060)
Nov 16 19:32:41.857: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1914.svc.cluster.local from pod dns-1914/dns-test-eb2db5e0-fd6a-4112-8dac-1368c8f75060: the server could not find the requested resource (get pods dns-test-eb2db5e0-fd6a-4112-8dac-1368c8f75060)
Nov 16 19:32:41.868: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1914.svc.cluster.local from pod dns-1914/dns-test-eb2db5e0-fd6a-4112-8dac-1368c8f75060: the server could not find the requested resource (get pods dns-test-eb2db5e0-fd6a-4112-8dac-1368c8f75060)
Nov 16 19:32:41.950: INFO: Unable to read jessie_udp@dns-test-service.dns-1914.svc.cluster.local from pod dns-1914/dns-test-eb2db5e0-fd6a-4112-8dac-1368c8f75060: the server could not find the requested resource (get pods dns-test-eb2db5e0-fd6a-4112-8dac-1368c8f75060)
Nov 16 19:32:41.962: INFO: Unable to read jessie_tcp@dns-test-service.dns-1914.svc.cluster.local from pod dns-1914/dns-test-eb2db5e0-fd6a-4112-8dac-1368c8f75060: the server could not find the requested resource (get pods dns-test-eb2db5e0-fd6a-4112-8dac-1368c8f75060)
Nov 16 19:32:41.973: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1914.svc.cluster.local from pod dns-1914/dns-test-eb2db5e0-fd6a-4112-8dac-1368c8f75060: the server could not find the requested resource (get pods dns-test-eb2db5e0-fd6a-4112-8dac-1368c8f75060)
Nov 16 19:32:41.987: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1914.svc.cluster.local from pod dns-1914/dns-test-eb2db5e0-fd6a-4112-8dac-1368c8f75060: the server could not find the requested resource (get pods dns-test-eb2db5e0-fd6a-4112-8dac-1368c8f75060)
Nov 16 19:32:42.063: INFO: Lookups using dns-1914/dns-test-eb2db5e0-fd6a-4112-8dac-1368c8f75060 failed for: [wheezy_udp@dns-test-service.dns-1914.svc.cluster.local wheezy_tcp@dns-test-service.dns-1914.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1914.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1914.svc.cluster.local jessie_udp@dns-test-service.dns-1914.svc.cluster.local jessie_tcp@dns-test-service.dns-1914.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1914.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1914.svc.cluster.local]

Nov 16 19:32:47.298: INFO: DNS probes using dns-1914/dns-test-eb2db5e0-fd6a-4112-8dac-1368c8f75060 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:32:47.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1914" for this suite.
Nov 16 19:32:55.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:32:57.339: INFO: namespace dns-1914 deletion completed in 9.937202208s

• [SLOW TEST:41.757 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:32:57.339: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Nov 16 19:33:07.526: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-420283327 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Nov 16 19:33:22.683: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:33:22.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3183" for this suite.
Nov 16 19:33:30.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:33:32.648: INFO: namespace pods-3183 deletion completed in 9.946653195s

• [SLOW TEST:35.309 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be submitted and removed [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:33:32.650: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Nov 16 19:33:32.842: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7102 /api/v1/namespaces/watch-7102/configmaps/e2e-watch-test-resource-version a9509528-e5a6-408a-b7f2-742213e794c9 47873 0 2020-11-16 19:33:32 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Nov 16 19:33:32.842: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7102 /api/v1/namespaces/watch-7102/configmaps/e2e-watch-test-resource-version a9509528-e5a6-408a-b7f2-742213e794c9 47875 0 2020-11-16 19:33:32 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:33:32.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7102" for this suite.
Nov 16 19:33:40.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:33:42.785: INFO: namespace watch-7102 deletion completed in 9.932086806s

• [SLOW TEST:10.135 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:33:42.785: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1540
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Nov 16 19:33:42.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-1158'
Nov 16 19:33:43.175: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Nov 16 19:33:43.175: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
Nov 16 19:33:47.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 delete deployment e2e-test-httpd-deployment --namespace=kubectl-1158'
Nov 16 19:33:47.328: INFO: stderr: ""
Nov 16 19:33:47.328: INFO: stdout: "deployment.extensions \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:33:47.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1158" for this suite.
Nov 16 19:33:55.366: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:33:57.286: INFO: namespace kubectl-1158 deletion completed in 9.948281122s

• [SLOW TEST:14.501 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1536
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:33:57.286: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Nov 16 19:33:57.412: INFO: Waiting up to 5m0s for pod "pod-1e83c16c-3ce6-4155-8368-7b66029df815" in namespace "emptydir-8181" to be "success or failure"
Nov 16 19:33:57.420: INFO: Pod "pod-1e83c16c-3ce6-4155-8368-7b66029df815": Phase="Pending", Reason="", readiness=false. Elapsed: 7.936833ms
Nov 16 19:33:59.428: INFO: Pod "pod-1e83c16c-3ce6-4155-8368-7b66029df815": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016110688s
STEP: Saw pod success
Nov 16 19:33:59.428: INFO: Pod "pod-1e83c16c-3ce6-4155-8368-7b66029df815" satisfied condition "success or failure"
Nov 16 19:33:59.435: INFO: Trying to get logs from node 10.240.167.254 pod pod-1e83c16c-3ce6-4155-8368-7b66029df815 container test-container: <nil>
STEP: delete the pod
Nov 16 19:33:59.476: INFO: Waiting for pod pod-1e83c16c-3ce6-4155-8368-7b66029df815 to disappear
Nov 16 19:33:59.484: INFO: Pod pod-1e83c16c-3ce6-4155-8368-7b66029df815 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:33:59.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8181" for this suite.
Nov 16 19:34:07.524: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:34:09.444: INFO: namespace emptydir-8181 deletion completed in 9.948717525s

• [SLOW TEST:12.158 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:34:09.444: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:34:16.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-278" for this suite.
Nov 16 19:34:24.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:34:26.844: INFO: namespace namespaces-278 deletion completed in 9.947086837s
STEP: Destroying namespace "nsdeletetest-135" for this suite.
Nov 16 19:34:26.851: INFO: Namespace nsdeletetest-135 was already deleted
STEP: Destroying namespace "nsdeletetest-3214" for this suite.
Nov 16 19:34:34.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:34:36.784: INFO: namespace nsdeletetest-3214 deletion completed in 9.933078876s

• [SLOW TEST:27.340 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:34:36.785: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov 16 19:34:36.994: INFO: Number of nodes with available pods: 0
Nov 16 19:34:36.994: INFO: Node 10.240.167.206 is running more than one daemon pod
Nov 16 19:34:38.011: INFO: Number of nodes with available pods: 0
Nov 16 19:34:38.011: INFO: Node 10.240.167.206 is running more than one daemon pod
Nov 16 19:34:39.014: INFO: Number of nodes with available pods: 3
Nov 16 19:34:39.014: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Nov 16 19:34:39.062: INFO: Number of nodes with available pods: 2
Nov 16 19:34:39.062: INFO: Node 10.240.167.209 is running more than one daemon pod
Nov 16 19:34:40.084: INFO: Number of nodes with available pods: 2
Nov 16 19:34:40.085: INFO: Node 10.240.167.209 is running more than one daemon pod
Nov 16 19:34:41.084: INFO: Number of nodes with available pods: 2
Nov 16 19:34:41.084: INFO: Node 10.240.167.209 is running more than one daemon pod
Nov 16 19:34:42.082: INFO: Number of nodes with available pods: 2
Nov 16 19:34:42.082: INFO: Node 10.240.167.209 is running more than one daemon pod
Nov 16 19:34:43.083: INFO: Number of nodes with available pods: 2
Nov 16 19:34:43.083: INFO: Node 10.240.167.209 is running more than one daemon pod
Nov 16 19:34:44.081: INFO: Number of nodes with available pods: 2
Nov 16 19:34:44.082: INFO: Node 10.240.167.209 is running more than one daemon pod
Nov 16 19:34:45.082: INFO: Number of nodes with available pods: 2
Nov 16 19:34:45.082: INFO: Node 10.240.167.209 is running more than one daemon pod
Nov 16 19:34:46.081: INFO: Number of nodes with available pods: 2
Nov 16 19:34:46.081: INFO: Node 10.240.167.209 is running more than one daemon pod
Nov 16 19:34:47.081: INFO: Number of nodes with available pods: 2
Nov 16 19:34:47.081: INFO: Node 10.240.167.209 is running more than one daemon pod
Nov 16 19:34:48.082: INFO: Number of nodes with available pods: 2
Nov 16 19:34:48.082: INFO: Node 10.240.167.209 is running more than one daemon pod
Nov 16 19:34:49.083: INFO: Number of nodes with available pods: 3
Nov 16 19:34:49.083: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7448, will wait for the garbage collector to delete the pods
Nov 16 19:34:49.177: INFO: Deleting DaemonSet.extensions daemon-set took: 24.107033ms
Nov 16 19:34:49.578: INFO: Terminating DaemonSet.extensions daemon-set pods took: 401.044928ms
Nov 16 19:35:01.689: INFO: Number of nodes with available pods: 0
Nov 16 19:35:01.689: INFO: Number of running nodes: 0, number of available pods: 0
Nov 16 19:35:01.698: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7448/daemonsets","resourceVersion":"48698"},"items":null}

Nov 16 19:35:01.705: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7448/pods","resourceVersion":"48698"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:35:01.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7448" for this suite.
Nov 16 19:35:09.777: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:35:11.695: INFO: namespace daemonsets-7448 deletion completed in 9.946462989s

• [SLOW TEST:34.910 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:35:11.696: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7683
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-7683
STEP: Creating statefulset with conflicting port in namespace statefulset-7683
STEP: Waiting until pod test-pod will start running in namespace statefulset-7683
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7683
Nov 16 19:35:15.893: INFO: Observed stateful pod in namespace: statefulset-7683, name: ss-0, uid: abaa2ffc-e0b8-4f2f-9307-7b2181027b27, status phase: Pending. Waiting for statefulset controller to delete.
Nov 16 19:35:15.983: INFO: Observed stateful pod in namespace: statefulset-7683, name: ss-0, uid: abaa2ffc-e0b8-4f2f-9307-7b2181027b27, status phase: Failed. Waiting for statefulset controller to delete.
Nov 16 19:35:15.997: INFO: Observed stateful pod in namespace: statefulset-7683, name: ss-0, uid: abaa2ffc-e0b8-4f2f-9307-7b2181027b27, status phase: Failed. Waiting for statefulset controller to delete.
Nov 16 19:35:16.004: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7683
STEP: Removing pod with conflicting port in namespace statefulset-7683
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7683 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Nov 16 19:35:20.052: INFO: Deleting all statefulset in ns statefulset-7683
Nov 16 19:35:20.059: INFO: Scaling statefulset ss to 0
Nov 16 19:35:30.096: INFO: Waiting for statefulset status.replicas updated to 0
Nov 16 19:35:30.104: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:35:30.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7683" for this suite.
Nov 16 19:35:38.171: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:35:40.078: INFO: namespace statefulset-7683 deletion completed in 9.933037157s

• [SLOW TEST:28.382 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:35:40.079: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 19:35:40.829: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 16 19:35:42.857: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741152140, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741152140, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741152140, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741152140, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 19:35:45.903: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:35:46.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-229" for this suite.
Nov 16 19:35:54.397: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:35:56.306: INFO: namespace webhook-229 deletion completed in 9.935283033s
STEP: Destroying namespace "webhook-229-markers" for this suite.
Nov 16 19:36:04.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:36:06.240: INFO: namespace webhook-229-markers deletion completed in 9.934249036s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:26.203 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:36:06.281: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 19:36:06.883: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 16 19:36:08.910: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741152166, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741152166, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741152166, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741152166, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 19:36:11.969: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:36:12.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5171" for this suite.
Nov 16 19:36:26.186: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:36:28.091: INFO: namespace webhook-5171 deletion completed in 15.932062264s
STEP: Destroying namespace "webhook-5171-markers" for this suite.
Nov 16 19:36:36.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:36:38.024: INFO: namespace webhook-5171-markers deletion completed in 9.933072938s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:31.784 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:36:38.069: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 19:36:38.185: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-7f9d89e6-074f-4056-9b59-23a3bdaa6054
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:36:54.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3795" for this suite.
Nov 16 19:37:08.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:37:10.252: INFO: namespace configmap-3795 deletion completed in 15.931253479s

• [SLOW TEST:32.183 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:37:10.252: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 16 19:37:10.388: INFO: Waiting up to 5m0s for pod "downwardapi-volume-06f641e3-4323-4eed-ba0a-c30ff9b122f3" in namespace "projected-9763" to be "success or failure"
Nov 16 19:37:10.398: INFO: Pod "downwardapi-volume-06f641e3-4323-4eed-ba0a-c30ff9b122f3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.763965ms
Nov 16 19:37:12.406: INFO: Pod "downwardapi-volume-06f641e3-4323-4eed-ba0a-c30ff9b122f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01772054s
STEP: Saw pod success
Nov 16 19:37:12.406: INFO: Pod "downwardapi-volume-06f641e3-4323-4eed-ba0a-c30ff9b122f3" satisfied condition "success or failure"
Nov 16 19:37:12.413: INFO: Trying to get logs from node 10.240.167.254 pod downwardapi-volume-06f641e3-4323-4eed-ba0a-c30ff9b122f3 container client-container: <nil>
STEP: delete the pod
Nov 16 19:37:12.478: INFO: Waiting for pod downwardapi-volume-06f641e3-4323-4eed-ba0a-c30ff9b122f3 to disappear
Nov 16 19:37:12.486: INFO: Pod downwardapi-volume-06f641e3-4323-4eed-ba0a-c30ff9b122f3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:37:12.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9763" for this suite.
Nov 16 19:37:20.522: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:37:22.426: INFO: namespace projected-9763 deletion completed in 9.931101919s

• [SLOW TEST:12.174 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:37:22.427: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Nov 16 19:37:25.644: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:37:26.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1124" for this suite.
Nov 16 19:37:56.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:37:58.639: INFO: namespace replicaset-1124 deletion completed in 31.935108572s

• [SLOW TEST:36.212 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:37:58.639: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Nov 16 19:37:58.775: INFO: Waiting up to 5m0s for pod "pod-dab1599d-a557-49c3-b1f9-7baa056a25c5" in namespace "emptydir-1898" to be "success or failure"
Nov 16 19:37:58.787: INFO: Pod "pod-dab1599d-a557-49c3-b1f9-7baa056a25c5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.225627ms
Nov 16 19:38:00.795: INFO: Pod "pod-dab1599d-a557-49c3-b1f9-7baa056a25c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0209284s
STEP: Saw pod success
Nov 16 19:38:00.796: INFO: Pod "pod-dab1599d-a557-49c3-b1f9-7baa056a25c5" satisfied condition "success or failure"
Nov 16 19:38:00.803: INFO: Trying to get logs from node 10.240.167.254 pod pod-dab1599d-a557-49c3-b1f9-7baa056a25c5 container test-container: <nil>
STEP: delete the pod
Nov 16 19:38:00.845: INFO: Waiting for pod pod-dab1599d-a557-49c3-b1f9-7baa056a25c5 to disappear
Nov 16 19:38:00.852: INFO: Pod pod-dab1599d-a557-49c3-b1f9-7baa056a25c5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:38:00.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1898" for this suite.
Nov 16 19:38:08.889: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:38:10.794: INFO: namespace emptydir-1898 deletion completed in 9.931518474s

• [SLOW TEST:12.155 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:38:10.796: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 19:38:10.897: INFO: Creating ReplicaSet my-hostname-basic-d3d9c3f3-7e7e-4137-8e9c-993e3e72c5a7
Nov 16 19:38:10.915: INFO: Pod name my-hostname-basic-d3d9c3f3-7e7e-4137-8e9c-993e3e72c5a7: Found 0 pods out of 1
Nov 16 19:38:15.925: INFO: Pod name my-hostname-basic-d3d9c3f3-7e7e-4137-8e9c-993e3e72c5a7: Found 1 pods out of 1
Nov 16 19:38:15.925: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-d3d9c3f3-7e7e-4137-8e9c-993e3e72c5a7" is running
Nov 16 19:38:15.932: INFO: Pod "my-hostname-basic-d3d9c3f3-7e7e-4137-8e9c-993e3e72c5a7-p4st9" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-16 19:38:10 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-16 19:38:13 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-16 19:38:13 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-16 19:38:10 +0000 UTC Reason: Message:}])
Nov 16 19:38:15.932: INFO: Trying to dial the pod
Nov 16 19:38:20.962: INFO: Controller my-hostname-basic-d3d9c3f3-7e7e-4137-8e9c-993e3e72c5a7: Got expected result from replica 1 [my-hostname-basic-d3d9c3f3-7e7e-4137-8e9c-993e3e72c5a7-p4st9]: "my-hostname-basic-d3d9c3f3-7e7e-4137-8e9c-993e3e72c5a7-p4st9", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:38:20.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-527" for this suite.
Nov 16 19:38:29.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:38:30.906: INFO: namespace replicaset-527 deletion completed in 9.931043082s

• [SLOW TEST:20.111 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:38:30.907: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Nov 16 19:38:35.567: INFO: Successfully updated pod "adopt-release-p2l4k"
STEP: Checking that the Job readopts the Pod
Nov 16 19:38:35.567: INFO: Waiting up to 15m0s for pod "adopt-release-p2l4k" in namespace "job-8676" to be "adopted"
Nov 16 19:38:35.575: INFO: Pod "adopt-release-p2l4k": Phase="Running", Reason="", readiness=true. Elapsed: 7.778711ms
Nov 16 19:38:37.584: INFO: Pod "adopt-release-p2l4k": Phase="Running", Reason="", readiness=true. Elapsed: 2.016341676s
Nov 16 19:38:37.584: INFO: Pod "adopt-release-p2l4k" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Nov 16 19:38:38.118: INFO: Successfully updated pod "adopt-release-p2l4k"
STEP: Checking that the Job releases the Pod
Nov 16 19:38:38.118: INFO: Waiting up to 15m0s for pod "adopt-release-p2l4k" in namespace "job-8676" to be "released"
Nov 16 19:38:38.128: INFO: Pod "adopt-release-p2l4k": Phase="Running", Reason="", readiness=true. Elapsed: 10.132581ms
Nov 16 19:38:40.140: INFO: Pod "adopt-release-p2l4k": Phase="Running", Reason="", readiness=true. Elapsed: 2.022376724s
Nov 16 19:38:40.140: INFO: Pod "adopt-release-p2l4k" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:38:40.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8676" for this suite.
Nov 16 19:39:30.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:39:32.101: INFO: namespace job-8676 deletion completed in 51.949764331s

• [SLOW TEST:61.195 seconds]
[sig-apps] Job
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:39:32.104: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 19:39:32.612: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 16 19:39:34.637: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741152372, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741152372, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741152372, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741152372, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 19:39:37.673: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 19:39:37.680: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2168-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:39:39.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8746" for this suite.
Nov 16 19:39:47.049: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:39:48.954: INFO: namespace webhook-8746 deletion completed in 9.931865166s
STEP: Destroying namespace "webhook-8746-markers" for this suite.
Nov 16 19:39:56.980: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:39:58.890: INFO: namespace webhook-8746-markers deletion completed in 9.935333041s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:26.826 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:39:58.930: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 19:39:59.071: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-7177262f-53e4-4600-aeca-bddb1e7363f9" in namespace "security-context-test-655" to be "success or failure"
Nov 16 19:39:59.077: INFO: Pod "busybox-privileged-false-7177262f-53e4-4600-aeca-bddb1e7363f9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.211696ms
Nov 16 19:40:01.087: INFO: Pod "busybox-privileged-false-7177262f-53e4-4600-aeca-bddb1e7363f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015772757s
Nov 16 19:40:01.087: INFO: Pod "busybox-privileged-false-7177262f-53e4-4600-aeca-bddb1e7363f9" satisfied condition "success or failure"
Nov 16 19:40:01.126: INFO: Got logs for pod "busybox-privileged-false-7177262f-53e4-4600-aeca-bddb1e7363f9": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:40:01.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-655" for this suite.
Nov 16 19:40:09.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:40:11.085: INFO: namespace security-context-test-655 deletion completed in 9.948149911s

• [SLOW TEST:12.155 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with privileged
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:226
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:40:11.088: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2800.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2800.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2800.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2800.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2800.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2800.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 16 19:40:15.388: INFO: DNS probes using dns-2800/dns-test-96779499-90e2-4bbd-8820-b6ccf63ce2f6 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:40:15.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2800" for this suite.
Nov 16 19:40:23.492: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:40:25.415: INFO: namespace dns-2800 deletion completed in 9.958550097s

• [SLOW TEST:14.328 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:40:25.415: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-d3c6697d-db81-464f-a681-0bfd7279fc56 in namespace container-probe-7750
Nov 16 19:40:30.577: INFO: Started pod liveness-d3c6697d-db81-464f-a681-0bfd7279fc56 in namespace container-probe-7750
STEP: checking the pod's current state and verifying that restartCount is present
Nov 16 19:40:30.586: INFO: Initial restart count of pod liveness-d3c6697d-db81-464f-a681-0bfd7279fc56 is 0
Nov 16 19:40:46.657: INFO: Restart count of pod container-probe-7750/liveness-d3c6697d-db81-464f-a681-0bfd7279fc56 is now 1 (16.070629385s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:40:46.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7750" for this suite.
Nov 16 19:40:54.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:40:56.640: INFO: namespace container-probe-7750 deletion completed in 9.938645588s

• [SLOW TEST:31.225 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:40:56.644: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:41:13.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1397" for this suite.
Nov 16 19:41:21.893: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:41:23.817: INFO: namespace resourcequota-1397 deletion completed in 9.952114013s

• [SLOW TEST:27.173 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:41:23.817: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-2af2fb76-f31d-4251-8762-8924eb5f8b6e
STEP: Creating a pod to test consume secrets
Nov 16 19:41:23.983: INFO: Waiting up to 5m0s for pod "pod-secrets-a152019b-8a8f-4af8-be37-6686f57ab261" in namespace "secrets-5043" to be "success or failure"
Nov 16 19:41:23.990: INFO: Pod "pod-secrets-a152019b-8a8f-4af8-be37-6686f57ab261": Phase="Pending", Reason="", readiness=false. Elapsed: 6.23524ms
Nov 16 19:41:25.998: INFO: Pod "pod-secrets-a152019b-8a8f-4af8-be37-6686f57ab261": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014091429s
STEP: Saw pod success
Nov 16 19:41:25.998: INFO: Pod "pod-secrets-a152019b-8a8f-4af8-be37-6686f57ab261" satisfied condition "success or failure"
Nov 16 19:41:26.005: INFO: Trying to get logs from node 10.240.167.254 pod pod-secrets-a152019b-8a8f-4af8-be37-6686f57ab261 container secret-volume-test: <nil>
STEP: delete the pod
Nov 16 19:41:26.049: INFO: Waiting for pod pod-secrets-a152019b-8a8f-4af8-be37-6686f57ab261 to disappear
Nov 16 19:41:26.057: INFO: Pod pod-secrets-a152019b-8a8f-4af8-be37-6686f57ab261 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:41:26.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5043" for this suite.
Nov 16 19:41:34.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:41:36.002: INFO: namespace secrets-5043 deletion completed in 9.935550565s

• [SLOW TEST:12.185 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:41:36.003: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Nov 16 19:41:36.145: INFO: Waiting up to 5m0s for pod "pod-707ffba5-b41c-48cb-a7e3-665740c6fcb6" in namespace "emptydir-8707" to be "success or failure"
Nov 16 19:41:36.152: INFO: Pod "pod-707ffba5-b41c-48cb-a7e3-665740c6fcb6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.774874ms
Nov 16 19:41:38.161: INFO: Pod "pod-707ffba5-b41c-48cb-a7e3-665740c6fcb6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015964148s
Nov 16 19:41:40.169: INFO: Pod "pod-707ffba5-b41c-48cb-a7e3-665740c6fcb6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023392141s
Nov 16 19:41:42.176: INFO: Pod "pod-707ffba5-b41c-48cb-a7e3-665740c6fcb6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.030779343s
Nov 16 19:41:44.184: INFO: Pod "pod-707ffba5-b41c-48cb-a7e3-665740c6fcb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.039086811s
STEP: Saw pod success
Nov 16 19:41:44.184: INFO: Pod "pod-707ffba5-b41c-48cb-a7e3-665740c6fcb6" satisfied condition "success or failure"
Nov 16 19:41:44.192: INFO: Trying to get logs from node 10.240.167.254 pod pod-707ffba5-b41c-48cb-a7e3-665740c6fcb6 container test-container: <nil>
STEP: delete the pod
Nov 16 19:41:44.230: INFO: Waiting for pod pod-707ffba5-b41c-48cb-a7e3-665740c6fcb6 to disappear
Nov 16 19:41:44.238: INFO: Pod pod-707ffba5-b41c-48cb-a7e3-665740c6fcb6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:41:44.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8707" for this suite.
Nov 16 19:41:52.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:41:54.184: INFO: namespace emptydir-8707 deletion completed in 9.935826193s

• [SLOW TEST:18.182 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:41:54.190: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 16 19:41:54.349: INFO: Waiting up to 5m0s for pod "downwardapi-volume-368e7594-6707-4fc1-86d0-50c25a4ec62f" in namespace "projected-3695" to be "success or failure"
Nov 16 19:41:54.361: INFO: Pod "downwardapi-volume-368e7594-6707-4fc1-86d0-50c25a4ec62f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.017138ms
Nov 16 19:41:56.369: INFO: Pod "downwardapi-volume-368e7594-6707-4fc1-86d0-50c25a4ec62f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019437415s
STEP: Saw pod success
Nov 16 19:41:56.370: INFO: Pod "downwardapi-volume-368e7594-6707-4fc1-86d0-50c25a4ec62f" satisfied condition "success or failure"
Nov 16 19:41:56.377: INFO: Trying to get logs from node 10.240.167.209 pod downwardapi-volume-368e7594-6707-4fc1-86d0-50c25a4ec62f container client-container: <nil>
STEP: delete the pod
Nov 16 19:41:56.439: INFO: Waiting for pod downwardapi-volume-368e7594-6707-4fc1-86d0-50c25a4ec62f to disappear
Nov 16 19:41:56.445: INFO: Pod downwardapi-volume-368e7594-6707-4fc1-86d0-50c25a4ec62f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:41:56.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3695" for this suite.
Nov 16 19:42:04.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:42:06.404: INFO: namespace projected-3695 deletion completed in 9.948173756s

• [SLOW TEST:12.214 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:42:06.406: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 19:42:06.530: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Nov 16 19:42:11.538: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 16 19:42:11.538: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Nov 16 19:42:11.592: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7581 /apis/apps/v1/namespaces/deployment-7581/deployments/test-cleanup-deployment db8b55ab-5202-4512-a466-8a7a83fd0c43 52484 1 2020-11-16 19:42:11 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0070ebad8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Nov 16 19:42:11.600: INFO: New ReplicaSet "test-cleanup-deployment-65db99849b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-65db99849b  deployment-7581 /apis/apps/v1/namespaces/deployment-7581/replicasets/test-cleanup-deployment-65db99849b 3019362c-43cc-4000-8298-11f0ee7c3637 52486 1 2020-11-16 19:42:11 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment db8b55ab-5202-4512-a466-8a7a83fd0c43 0xc0070ebf37 0xc0070ebf38}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 65db99849b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0070ebf98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 16 19:42:11.600: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Nov 16 19:42:11.600: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-7581 /apis/apps/v1/namespaces/deployment-7581/replicasets/test-cleanup-controller 40735abf-f299-4b15-88a0-73935ca61d74 52485 1 2020-11-16 19:42:06 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment db8b55ab-5202-4512-a466-8a7a83fd0c43 0xc0070ebe67 0xc0070ebe68}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0070ebec8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 16 19:42:11.608: INFO: Pod "test-cleanup-controller-dk4lj" is available:
&Pod{ObjectMeta:{test-cleanup-controller-dk4lj test-cleanup-controller- deployment-7581 /api/v1/namespaces/deployment-7581/pods/test-cleanup-controller-dk4lj 4fad0027-ae22-42fd-b91d-03271555ae80 52469 0 2020-11-16 19:42:06 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:172.30.216.247/32 cni.projectcalico.org/podIPs:172.30.216.247/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.216.247"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-cleanup-controller 40735abf-f299-4b15-88a0-73935ca61d74 0xc009244417 0xc009244418}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-25nfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-25nfw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-25nfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.167.254,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:42:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:42:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:42:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:42:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.167.254,PodIP:172.30.216.247,StartTime:2020-11-16 19:42:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-16 19:42:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://0f2b24c309ae1a96bdfaa5bebf4dbf81fff41a5ced191bcfbaf81566ecb66fc3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.216.247,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:42:11.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7581" for this suite.
Nov 16 19:42:19.647: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:42:21.552: INFO: namespace deployment-7581 deletion completed in 9.934473926s

• [SLOW TEST:15.146 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:42:21.552: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Nov 16 19:42:24.250: INFO: Successfully updated pod "pod-update-486f377c-3909-4c9a-ab39-12cde1d680d9"
STEP: verifying the updated pod is in kubernetes
Nov 16 19:42:24.270: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:42:24.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5173" for this suite.
Nov 16 19:42:38.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:42:40.214: INFO: namespace pods-5173 deletion completed in 15.9335905s

• [SLOW TEST:18.662 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:42:40.215: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:42:45.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5761" for this suite.
Nov 16 19:42:54.098: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:42:56.018: INFO: namespace watch-5761 deletion completed in 10.024626515s

• [SLOW TEST:15.804 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:42:56.018: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 16 19:42:58.241: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:42:58.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-565" for this suite.
Nov 16 19:43:06.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:43:08.213: INFO: namespace container-runtime-565 deletion completed in 9.930984871s

• [SLOW TEST:12.195 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:43:08.214: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 19:43:08.316: INFO: Creating deployment "test-recreate-deployment"
Nov 16 19:43:08.328: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Nov 16 19:43:08.351: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Nov 16 19:43:10.369: INFO: Waiting deployment "test-recreate-deployment" to complete
Nov 16 19:43:10.378: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Nov 16 19:43:10.402: INFO: Updating deployment test-recreate-deployment
Nov 16 19:43:10.402: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Nov 16 19:43:10.538: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-3650 /apis/apps/v1/namespaces/deployment-3650/deployments/test-recreate-deployment e7f9d013-b5a2-4ea1-b27c-e216153d05dd 53142 2 2020-11-16 19:43:08 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003aa03e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-11-16 19:43:10 +0000 UTC,LastTransitionTime:2020-11-16 19:43:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-11-16 19:43:10 +0000 UTC,LastTransitionTime:2020-11-16 19:43:08 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Nov 16 19:43:10.546: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-3650 /apis/apps/v1/namespaces/deployment-3650/replicasets/test-recreate-deployment-5f94c574ff be3ce689-2e02-42be-99e4-b0d2bbf1fdbb 53140 1 2020-11-16 19:43:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment e7f9d013-b5a2-4ea1-b27c-e216153d05dd 0xc003aa07d7 0xc003aa07d8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003aa0838 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 16 19:43:10.546: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Nov 16 19:43:10.546: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-68fc85c7bb  deployment-3650 /apis/apps/v1/namespaces/deployment-3650/replicasets/test-recreate-deployment-68fc85c7bb 664d753c-290f-4b85-b9b9-084470491b95 53131 2 2020-11-16 19:43:08 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment e7f9d013-b5a2-4ea1-b27c-e216153d05dd 0xc003aa08a7 0xc003aa08a8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68fc85c7bb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003aa0908 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 16 19:43:10.554: INFO: Pod "test-recreate-deployment-5f94c574ff-mz7cs" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-mz7cs test-recreate-deployment-5f94c574ff- deployment-3650 /api/v1/namespaces/deployment-3650/pods/test-recreate-deployment-5f94c574ff-mz7cs 54ba10d5-1151-4ce1-b98e-b9ba9bedfa31 53143 0 2020-11-16 19:43:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff be3ce689-2e02-42be-99e4-b0d2bbf1fdbb 0xc003aa0da7 0xc003aa0da8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5b6sp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5b6sp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5b6sp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.167.254,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-hjl9j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:43:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:43:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:43:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:43:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.167.254,PodIP:,StartTime:2020-11-16 19:43:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:43:10.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3650" for this suite.
Nov 16 19:43:18.594: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:43:20.511: INFO: namespace deployment-3650 deletion completed in 9.945864047s

• [SLOW TEST:12.297 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:43:20.511: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on node default medium
Nov 16 19:43:20.711: INFO: Waiting up to 5m0s for pod "pod-c71cf43a-31a5-4359-8b20-074f31ce93df" in namespace "emptydir-6445" to be "success or failure"
Nov 16 19:43:20.718: INFO: Pod "pod-c71cf43a-31a5-4359-8b20-074f31ce93df": Phase="Pending", Reason="", readiness=false. Elapsed: 7.547588ms
Nov 16 19:43:22.726: INFO: Pod "pod-c71cf43a-31a5-4359-8b20-074f31ce93df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015417639s
STEP: Saw pod success
Nov 16 19:43:22.726: INFO: Pod "pod-c71cf43a-31a5-4359-8b20-074f31ce93df" satisfied condition "success or failure"
Nov 16 19:43:22.733: INFO: Trying to get logs from node 10.240.167.254 pod pod-c71cf43a-31a5-4359-8b20-074f31ce93df container test-container: <nil>
STEP: delete the pod
Nov 16 19:43:22.793: INFO: Waiting for pod pod-c71cf43a-31a5-4359-8b20-074f31ce93df to disappear
Nov 16 19:43:22.800: INFO: Pod pod-c71cf43a-31a5-4359-8b20-074f31ce93df no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:43:22.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6445" for this suite.
Nov 16 19:43:30.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:43:32.744: INFO: namespace emptydir-6445 deletion completed in 9.932833209s

• [SLOW TEST:12.233 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:43:32.744: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-2646
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-2646
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2646
Nov 16 19:43:32.903: INFO: Found 0 stateful pods, waiting for 1
Nov 16 19:43:42.912: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Nov 16 19:43:42.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-2646 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 16 19:43:43.251: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 16 19:43:43.251: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 16 19:43:43.251: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 16 19:43:43.259: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov 16 19:43:53.268: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 16 19:43:53.268: INFO: Waiting for statefulset status.replicas updated to 0
Nov 16 19:43:53.302: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998672s
Nov 16 19:43:54.311: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.992299078s
Nov 16 19:43:55.320: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.983494551s
Nov 16 19:43:56.329: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.974141715s
Nov 16 19:43:57.338: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.965237998s
Nov 16 19:43:58.346: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.956732663s
Nov 16 19:43:59.356: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.948222438s
Nov 16 19:44:00.366: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.93795646s
Nov 16 19:44:01.374: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.928672264s
Nov 16 19:44:02.384: INFO: Verifying statefulset ss doesn't scale past 1 for another 920.128466ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2646
Nov 16 19:44:03.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-2646 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:44:03.881: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 16 19:44:03.881: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 16 19:44:03.881: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 16 19:44:03.889: INFO: Found 1 stateful pods, waiting for 3
Nov 16 19:44:13.898: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 19:44:13.898: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 19:44:13.898: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Nov 16 19:44:13.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-2646 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 16 19:44:14.202: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 16 19:44:14.202: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 16 19:44:14.202: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 16 19:44:14.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-2646 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 16 19:44:14.519: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 16 19:44:14.519: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 16 19:44:14.519: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 16 19:44:14.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-2646 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 16 19:44:14.846: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 16 19:44:14.846: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 16 19:44:14.846: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 16 19:44:14.846: INFO: Waiting for statefulset status.replicas updated to 0
Nov 16 19:44:14.854: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Nov 16 19:44:24.870: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 16 19:44:24.870: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov 16 19:44:24.870: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov 16 19:44:24.898: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998673s
Nov 16 19:44:25.907: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.991794952s
Nov 16 19:44:26.916: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.983070991s
Nov 16 19:44:27.925: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.973508316s
Nov 16 19:44:28.933: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.965167373s
Nov 16 19:44:29.942: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.956605947s
Nov 16 19:44:30.951: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.947436999s
Nov 16 19:44:31.960: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.938866543s
Nov 16 19:44:32.968: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.930154256s
Nov 16 19:44:33.977: INFO: Verifying statefulset ss doesn't scale past 3 for another 921.71186ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2646
Nov 16 19:44:34.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-2646 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:44:35.273: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 16 19:44:35.273: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 16 19:44:35.273: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 16 19:44:35.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-2646 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:44:35.594: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 16 19:44:35.594: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 16 19:44:35.594: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 16 19:44:35.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-2646 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:44:35.885: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 16 19:44:35.885: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 16 19:44:35.885: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 16 19:44:35.885: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Nov 16 19:45:05.931: INFO: Deleting all statefulset in ns statefulset-2646
Nov 16 19:45:05.939: INFO: Scaling statefulset ss to 0
Nov 16 19:45:05.966: INFO: Waiting for statefulset status.replicas updated to 0
Nov 16 19:45:05.973: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:45:06.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2646" for this suite.
Nov 16 19:45:14.051: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:45:15.956: INFO: namespace statefulset-2646 deletion completed in 9.936270743s

• [SLOW TEST:103.212 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:45:15.956: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-e12e0606-65c1-499f-a815-3ed680d9eb2b
STEP: Creating a pod to test consume configMaps
Nov 16 19:45:16.117: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b315bf49-03e3-4b70-8329-dd658dfe7b58" in namespace "projected-3314" to be "success or failure"
Nov 16 19:45:16.126: INFO: Pod "pod-projected-configmaps-b315bf49-03e3-4b70-8329-dd658dfe7b58": Phase="Pending", Reason="", readiness=false. Elapsed: 9.558315ms
Nov 16 19:45:18.135: INFO: Pod "pod-projected-configmaps-b315bf49-03e3-4b70-8329-dd658dfe7b58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018159787s
STEP: Saw pod success
Nov 16 19:45:18.135: INFO: Pod "pod-projected-configmaps-b315bf49-03e3-4b70-8329-dd658dfe7b58" satisfied condition "success or failure"
Nov 16 19:45:18.143: INFO: Trying to get logs from node 10.240.167.254 pod pod-projected-configmaps-b315bf49-03e3-4b70-8329-dd658dfe7b58 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 16 19:45:18.214: INFO: Waiting for pod pod-projected-configmaps-b315bf49-03e3-4b70-8329-dd658dfe7b58 to disappear
Nov 16 19:45:18.221: INFO: Pod pod-projected-configmaps-b315bf49-03e3-4b70-8329-dd658dfe7b58 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:45:18.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3314" for this suite.
Nov 16 19:45:26.259: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:45:28.166: INFO: namespace projected-3314 deletion completed in 9.933967071s

• [SLOW TEST:12.210 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:45:28.166: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:45:36.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2218" for this suite.
Nov 16 19:45:44.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:45:46.248: INFO: namespace job-2218 deletion completed in 9.933078455s

• [SLOW TEST:18.082 seconds]
[sig-apps] Job
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:45:46.249: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:46:12.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7731" for this suite.
Nov 16 19:46:20.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:46:22.817: INFO: namespace container-runtime-7731 deletion completed in 9.928831485s

• [SLOW TEST:36.568 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    when starting a container that exits
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:46:22.818: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-c75636ce-e2f2-45e5-bd26-656855641025
STEP: Creating a pod to test consume secrets
Nov 16 19:46:22.968: INFO: Waiting up to 5m0s for pod "pod-secrets-527ade2e-1585-454f-93f8-5a0eb42b3e05" in namespace "secrets-6785" to be "success or failure"
Nov 16 19:46:22.975: INFO: Pod "pod-secrets-527ade2e-1585-454f-93f8-5a0eb42b3e05": Phase="Pending", Reason="", readiness=false. Elapsed: 6.879944ms
Nov 16 19:46:24.987: INFO: Pod "pod-secrets-527ade2e-1585-454f-93f8-5a0eb42b3e05": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018774231s
Nov 16 19:46:26.995: INFO: Pod "pod-secrets-527ade2e-1585-454f-93f8-5a0eb42b3e05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02721886s
STEP: Saw pod success
Nov 16 19:46:26.995: INFO: Pod "pod-secrets-527ade2e-1585-454f-93f8-5a0eb42b3e05" satisfied condition "success or failure"
Nov 16 19:46:27.002: INFO: Trying to get logs from node 10.240.167.254 pod pod-secrets-527ade2e-1585-454f-93f8-5a0eb42b3e05 container secret-env-test: <nil>
STEP: delete the pod
Nov 16 19:46:27.044: INFO: Waiting for pod pod-secrets-527ade2e-1585-454f-93f8-5a0eb42b3e05 to disappear
Nov 16 19:46:27.051: INFO: Pod pod-secrets-527ade2e-1585-454f-93f8-5a0eb42b3e05 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:46:27.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6785" for this suite.
Nov 16 19:46:35.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:46:36.996: INFO: namespace secrets-6785 deletion completed in 9.934571371s

• [SLOW TEST:14.179 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:46:36.996: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Nov 16 19:46:37.139: INFO: Waiting up to 5m0s for pod "pod-206eab4b-14a1-4a5b-a563-e28c1fae0e92" in namespace "emptydir-2585" to be "success or failure"
Nov 16 19:46:37.146: INFO: Pod "pod-206eab4b-14a1-4a5b-a563-e28c1fae0e92": Phase="Pending", Reason="", readiness=false. Elapsed: 7.100499ms
Nov 16 19:46:39.154: INFO: Pod "pod-206eab4b-14a1-4a5b-a563-e28c1fae0e92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01463296s
STEP: Saw pod success
Nov 16 19:46:39.154: INFO: Pod "pod-206eab4b-14a1-4a5b-a563-e28c1fae0e92" satisfied condition "success or failure"
Nov 16 19:46:39.160: INFO: Trying to get logs from node 10.240.167.254 pod pod-206eab4b-14a1-4a5b-a563-e28c1fae0e92 container test-container: <nil>
STEP: delete the pod
Nov 16 19:46:39.201: INFO: Waiting for pod pod-206eab4b-14a1-4a5b-a563-e28c1fae0e92 to disappear
Nov 16 19:46:39.208: INFO: Pod pod-206eab4b-14a1-4a5b-a563-e28c1fae0e92 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:46:39.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2585" for this suite.
Nov 16 19:46:47.247: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:46:49.153: INFO: namespace emptydir-2585 deletion completed in 9.935654069s

• [SLOW TEST:12.157 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:46:49.154: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Nov 16 19:46:49.961: INFO: Pod name wrapped-volume-race-0d1bc3ae-62f2-4f1d-972d-2cceb1c3be1a: Found 0 pods out of 5
Nov 16 19:46:54.973: INFO: Pod name wrapped-volume-race-0d1bc3ae-62f2-4f1d-972d-2cceb1c3be1a: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-0d1bc3ae-62f2-4f1d-972d-2cceb1c3be1a in namespace emptydir-wrapper-1749, will wait for the garbage collector to delete the pods
Nov 16 19:46:55.091: INFO: Deleting ReplicationController wrapped-volume-race-0d1bc3ae-62f2-4f1d-972d-2cceb1c3be1a took: 16.796408ms
Nov 16 19:46:55.191: INFO: Terminating ReplicationController wrapped-volume-race-0d1bc3ae-62f2-4f1d-972d-2cceb1c3be1a pods took: 100.262532ms
STEP: Creating RC which spawns configmap-volume pods
Nov 16 19:47:38.930: INFO: Pod name wrapped-volume-race-9d5c3948-3689-4924-a20f-3fce95f8ec66: Found 0 pods out of 5
Nov 16 19:47:43.943: INFO: Pod name wrapped-volume-race-9d5c3948-3689-4924-a20f-3fce95f8ec66: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-9d5c3948-3689-4924-a20f-3fce95f8ec66 in namespace emptydir-wrapper-1749, will wait for the garbage collector to delete the pods
Nov 16 19:47:44.067: INFO: Deleting ReplicationController wrapped-volume-race-9d5c3948-3689-4924-a20f-3fce95f8ec66 took: 27.09589ms
Nov 16 19:47:44.568: INFO: Terminating ReplicationController wrapped-volume-race-9d5c3948-3689-4924-a20f-3fce95f8ec66 pods took: 500.376428ms
STEP: Creating RC which spawns configmap-volume pods
Nov 16 19:48:29.106: INFO: Pod name wrapped-volume-race-8dc1d2e3-e8c9-42cd-8d0e-e08f89986569: Found 0 pods out of 5
Nov 16 19:48:34.117: INFO: Pod name wrapped-volume-race-8dc1d2e3-e8c9-42cd-8d0e-e08f89986569: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-8dc1d2e3-e8c9-42cd-8d0e-e08f89986569 in namespace emptydir-wrapper-1749, will wait for the garbage collector to delete the pods
Nov 16 19:48:34.225: INFO: Deleting ReplicationController wrapped-volume-race-8dc1d2e3-e8c9-42cd-8d0e-e08f89986569 took: 15.756013ms
Nov 16 19:48:34.725: INFO: Terminating ReplicationController wrapped-volume-race-8dc1d2e3-e8c9-42cd-8d0e-e08f89986569 pods took: 500.351393ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:49:20.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1749" for this suite.
Nov 16 19:49:28.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:49:30.218: INFO: namespace emptydir-wrapper-1749 deletion completed in 9.932897392s

• [SLOW TEST:161.064 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:49:30.218: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 19:49:30.341: INFO: Creating deployment "webserver-deployment"
Nov 16 19:49:30.355: INFO: Waiting for observed generation 1
Nov 16 19:49:32.375: INFO: Waiting for all required pods to come up
Nov 16 19:49:32.383: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Nov 16 19:49:34.402: INFO: Waiting for deployment "webserver-deployment" to complete
Nov 16 19:49:34.419: INFO: Updating deployment "webserver-deployment" with a non-existent image
Nov 16 19:49:34.496: INFO: Updating deployment webserver-deployment
Nov 16 19:49:34.496: INFO: Waiting for observed generation 2
Nov 16 19:49:36.517: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Nov 16 19:49:36.524: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Nov 16 19:49:36.531: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Nov 16 19:49:36.552: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Nov 16 19:49:36.552: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Nov 16 19:49:36.559: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Nov 16 19:49:36.571: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Nov 16 19:49:36.572: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Nov 16 19:49:36.595: INFO: Updating deployment webserver-deployment
Nov 16 19:49:36.595: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Nov 16 19:49:36.610: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Nov 16 19:49:36.620: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Nov 16 19:49:36.637: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-6473 /apis/apps/v1/namespaces/deployment-6473/deployments/webserver-deployment 1c6dd40f-0f96-4528-8113-cd47a3328a23 56470 3 2020-11-16 19:49:30 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0007c5448 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-11-16 19:49:34 +0000 UTC,LastTransitionTime:2020-11-16 19:49:30 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-11-16 19:49:36 +0000 UTC,LastTransitionTime:2020-11-16 19:49:36 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Nov 16 19:49:36.648: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-6473 /apis/apps/v1/namespaces/deployment-6473/replicasets/webserver-deployment-c7997dcc8 d3bbfa90-84cc-4447-b9e3-de48b3c88444 56469 3 2020-11-16 19:49:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 1c6dd40f-0f96-4528-8113-cd47a3328a23 0xc0007c59d7 0xc0007c59d8}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0007c5a58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 16 19:49:36.648: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Nov 16 19:49:36.648: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-6473 /apis/apps/v1/namespaces/deployment-6473/replicasets/webserver-deployment-595b5b9587 fb6a5701-bac6-4612-ada8-33e5ce6f459a 56467 3 2020-11-16 19:49:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 1c6dd40f-0f96-4528-8113-cd47a3328a23 0xc0007c5917 0xc0007c5918}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0007c5978 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Nov 16 19:49:36.658: INFO: Pod "webserver-deployment-595b5b9587-2rbw7" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-2rbw7 webserver-deployment-595b5b9587- deployment-6473 /api/v1/namespaces/deployment-6473/pods/webserver-deployment-595b5b9587-2rbw7 ff290f2b-4a20-44c6-b76b-5fa79f127ec5 56337 0 2020-11-16 19:49:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.194.221/32 cni.projectcalico.org/podIPs:172.30.194.221/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.194.221"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fb6a5701-bac6-4612-ada8-33e5ce6f459a 0xc000646837 0xc000646838}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-psd6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-psd6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-psd6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.167.206,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-h7hg7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.167.206,PodIP:172.30.194.221,StartTime:2020-11-16 19:49:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-16 19:49:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://f39a04cf99f584dc868e2bbf3c7a217c3cd310ec4bebfa5432afe88a5b05b3c0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.194.221,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:49:36.658: INFO: Pod "webserver-deployment-595b5b9587-4m45h" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-4m45h webserver-deployment-595b5b9587- deployment-6473 /api/v1/namespaces/deployment-6473/pods/webserver-deployment-595b5b9587-4m45h 798369de-5dd2-4194-83ad-f934be3c9070 56473 0 2020-11-16 19:49:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fb6a5701-bac6-4612-ada8-33e5ce6f459a 0xc000646eb7 0xc000646eb8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-psd6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-psd6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-psd6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.167.209,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-h7hg7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:49:36.658: INFO: Pod "webserver-deployment-595b5b9587-7kx85" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-7kx85 webserver-deployment-595b5b9587- deployment-6473 /api/v1/namespaces/deployment-6473/pods/webserver-deployment-595b5b9587-7kx85 77b405a5-9907-468c-a0d6-5c48d7f07f73 56347 0 2020-11-16 19:49:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.158.200/32 cni.projectcalico.org/podIPs:172.30.158.200/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.158.200"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fb6a5701-bac6-4612-ada8-33e5ce6f459a 0xc000647420 0xc000647421}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-psd6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-psd6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-psd6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.167.209,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-h7hg7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.167.209,PodIP:172.30.158.200,StartTime:2020-11-16 19:49:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-16 19:49:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://c8202c8e0e2c32a5a8c37614d48f1fa5751a0a53720e1d0923f33085c1f43651,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.158.200,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:49:36.658: INFO: Pod "webserver-deployment-595b5b9587-9wbp9" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-9wbp9 webserver-deployment-595b5b9587- deployment-6473 /api/v1/namespaces/deployment-6473/pods/webserver-deployment-595b5b9587-9wbp9 1073d737-04d8-4d5c-bedd-bea2cef432de 56360 0 2020-11-16 19:49:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.216.230/32 cni.projectcalico.org/podIPs:172.30.216.230/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.216.230"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fb6a5701-bac6-4612-ada8-33e5ce6f459a 0xc000647797 0xc000647798}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-psd6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-psd6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-psd6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.167.254,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-h7hg7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.167.254,PodIP:172.30.216.230,StartTime:2020-11-16 19:49:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-16 19:49:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://365f94844520359a20a6085add5a1b587cc4ae86d2074718f0a9c3d77bf3a02b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.216.230,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:49:36.658: INFO: Pod "webserver-deployment-595b5b9587-ccfpf" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-ccfpf webserver-deployment-595b5b9587- deployment-6473 /api/v1/namespaces/deployment-6473/pods/webserver-deployment-595b5b9587-ccfpf 427f7dbc-8697-4418-bb7a-c61b04b3ec2d 56332 0 2020-11-16 19:49:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.194.222/32 cni.projectcalico.org/podIPs:172.30.194.222/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.194.222"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fb6a5701-bac6-4612-ada8-33e5ce6f459a 0xc0006479a7 0xc0006479a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-psd6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-psd6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-psd6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.167.206,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-h7hg7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.167.206,PodIP:172.30.194.222,StartTime:2020-11-16 19:49:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-16 19:49:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://36c668fc462e8475bcfc5fd7882e62034048c88f86b9405ee5768f0aeb63a317,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.194.222,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:49:36.658: INFO: Pod "webserver-deployment-595b5b9587-n9fbn" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-n9fbn webserver-deployment-595b5b9587- deployment-6473 /api/v1/namespaces/deployment-6473/pods/webserver-deployment-595b5b9587-n9fbn b99694ed-684b-4891-aff9-b5efa7bbfb41 56367 0 2020-11-16 19:49:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.194.223/32 cni.projectcalico.org/podIPs:172.30.194.223/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.194.223"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fb6a5701-bac6-4612-ada8-33e5ce6f459a 0xc000647ea7 0xc000647ea8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-psd6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-psd6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-psd6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.167.206,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-h7hg7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.167.206,PodIP:172.30.194.223,StartTime:2020-11-16 19:49:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-16 19:49:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://a0cd27ed49543e58d76591788563d66189273be62a15a267a81384957c737e8f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.194.223,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:49:36.658: INFO: Pod "webserver-deployment-595b5b9587-pcq6v" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-pcq6v webserver-deployment-595b5b9587- deployment-6473 /api/v1/namespaces/deployment-6473/pods/webserver-deployment-595b5b9587-pcq6v 1aaa3fd9-f4a3-4787-b97c-6933c590aa6b 56350 0 2020-11-16 19:49:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.158.212/32 cni.projectcalico.org/podIPs:172.30.158.212/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.158.212"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fb6a5701-bac6-4612-ada8-33e5ce6f459a 0xc00074cf07 0xc00074cf08}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-psd6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-psd6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-psd6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.167.209,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-h7hg7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.167.209,PodIP:172.30.158.212,StartTime:2020-11-16 19:49:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-16 19:49:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://98b80fb6fb113c5d2075aaff102f6980f993af035949a127e70f295a1b3711bb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.158.212,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:49:36.659: INFO: Pod "webserver-deployment-595b5b9587-rgz2q" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rgz2q webserver-deployment-595b5b9587- deployment-6473 /api/v1/namespaces/deployment-6473/pods/webserver-deployment-595b5b9587-rgz2q b3178b25-ae01-49db-8b10-7c00f37078b6 56364 0 2020-11-16 19:49:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.216.228/32 cni.projectcalico.org/podIPs:172.30.216.228/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.216.228"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fb6a5701-bac6-4612-ada8-33e5ce6f459a 0xc00074d3f7 0xc00074d3f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-psd6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-psd6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-psd6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.167.254,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-h7hg7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.167.254,PodIP:172.30.216.228,StartTime:2020-11-16 19:49:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-16 19:49:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://7075e70be693467bdc56b8f66a89e6027579afa369f867774fb3dd5abca75e6b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.216.228,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:49:36.659: INFO: Pod "webserver-deployment-595b5b9587-zg8tq" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-zg8tq webserver-deployment-595b5b9587- deployment-6473 /api/v1/namespaces/deployment-6473/pods/webserver-deployment-595b5b9587-zg8tq 7b917976-edc5-433d-869f-605307a90d67 56358 0 2020-11-16 19:49:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.216.224/32 cni.projectcalico.org/podIPs:172.30.216.224/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.216.224"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fb6a5701-bac6-4612-ada8-33e5ce6f459a 0xc000708067 0xc000708068}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-psd6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-psd6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-psd6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.167.254,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-h7hg7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.167.254,PodIP:172.30.216.224,StartTime:2020-11-16 19:49:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-16 19:49:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://81154841f613c6c478a7c025496fb7981c144d280d9116285c9d02d0b81fb730,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.216.224,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:49:36.659: INFO: Pod "webserver-deployment-c7997dcc8-d6gnh" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-d6gnh webserver-deployment-c7997dcc8- deployment-6473 /api/v1/namespaces/deployment-6473/pods/webserver-deployment-c7997dcc8-d6gnh 798f991b-f0ac-429e-8a6f-c279a3f989b9 56455 0 2020-11-16 19:49:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.158.195/32 cni.projectcalico.org/podIPs:172.30.158.195/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.158.195"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d3bbfa90-84cc-4447-b9e3-de48b3c88444 0xc0007083f7 0xc0007083f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-psd6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-psd6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-psd6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.167.209,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-h7hg7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.167.209,PodIP:,StartTime:2020-11-16 19:49:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:49:36.659: INFO: Pod "webserver-deployment-c7997dcc8-w2sbx" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-w2sbx webserver-deployment-c7997dcc8- deployment-6473 /api/v1/namespaces/deployment-6473/pods/webserver-deployment-c7997dcc8-w2sbx bd1f2394-8572-4d03-831e-e4ba5f21c2df 56447 0 2020-11-16 19:49:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.194.224/32 cni.projectcalico.org/podIPs:172.30.194.224/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.194.224"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d3bbfa90-84cc-4447-b9e3-de48b3c88444 0xc0007086c7 0xc0007086c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-psd6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-psd6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-psd6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.167.206,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-h7hg7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.167.206,PodIP:,StartTime:2020-11-16 19:49:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:49:36.659: INFO: Pod "webserver-deployment-c7997dcc8-xddld" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-xddld webserver-deployment-c7997dcc8- deployment-6473 /api/v1/namespaces/deployment-6473/pods/webserver-deployment-c7997dcc8-xddld 7aea2f66-96e1-4007-acb1-29c930123e93 56445 0 2020-11-16 19:49:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.216.226/32 cni.projectcalico.org/podIPs:172.30.216.226/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.216.226"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d3bbfa90-84cc-4447-b9e3-de48b3c88444 0xc0007089f7 0xc0007089f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-psd6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-psd6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-psd6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.167.254,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-h7hg7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.167.254,PodIP:,StartTime:2020-11-16 19:49:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:49:36.659: INFO: Pod "webserver-deployment-c7997dcc8-xmrg2" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-xmrg2 webserver-deployment-c7997dcc8- deployment-6473 /api/v1/namespaces/deployment-6473/pods/webserver-deployment-c7997dcc8-xmrg2 ba9d9fb2-efa7-4efa-8f50-f37d2a0b5e32 56464 0 2020-11-16 19:49:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.158.204/32 cni.projectcalico.org/podIPs:172.30.158.204/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.158.204"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d3bbfa90-84cc-4447-b9e3-de48b3c88444 0xc000708e47 0xc000708e48}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-psd6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-psd6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-psd6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.167.209,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-h7hg7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.167.209,PodIP:,StartTime:2020-11-16 19:49:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:49:36.659: INFO: Pod "webserver-deployment-c7997dcc8-xqlpl" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-xqlpl webserver-deployment-c7997dcc8- deployment-6473 /api/v1/namespaces/deployment-6473/pods/webserver-deployment-c7997dcc8-xqlpl 4392d061-ad9f-400a-9e8b-d98c35e1cdba 56458 0 2020-11-16 19:49:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.194.225/32 cni.projectcalico.org/podIPs:172.30.194.225/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.194.225"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d3bbfa90-84cc-4447-b9e3-de48b3c88444 0xc0007092c7 0xc0007092c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-psd6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-psd6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-psd6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.167.206,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-h7hg7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 19:49:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.167.206,PodIP:,StartTime:2020-11-16 19:49:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:49:36.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6473" for this suite.
Nov 16 19:49:44.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:49:46.601: INFO: namespace deployment-6473 deletion completed in 9.930626205s

• [SLOW TEST:16.383 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:49:46.603: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 19:49:46.812: INFO: (0) /api/v1/nodes/10.240.167.206:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 41.299876ms)
Nov 16 19:49:46.826: INFO: (1) /api/v1/nodes/10.240.167.206:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 13.842128ms)
Nov 16 19:49:46.839: INFO: (2) /api/v1/nodes/10.240.167.206:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 13.459253ms)
Nov 16 19:49:46.854: INFO: (3) /api/v1/nodes/10.240.167.206:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 14.765606ms)
Nov 16 19:49:46.867: INFO: (4) /api/v1/nodes/10.240.167.206:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 13.066876ms)
Nov 16 19:49:46.884: INFO: (5) /api/v1/nodes/10.240.167.206:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 17.327703ms)
Nov 16 19:49:46.899: INFO: (6) /api/v1/nodes/10.240.167.206:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 14.459982ms)
Nov 16 19:49:46.911: INFO: (7) /api/v1/nodes/10.240.167.206:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 12.315842ms)
Nov 16 19:49:46.927: INFO: (8) /api/v1/nodes/10.240.167.206:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 15.794124ms)
Nov 16 19:49:46.942: INFO: (9) /api/v1/nodes/10.240.167.206:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 14.503781ms)
Nov 16 19:49:46.955: INFO: (10) /api/v1/nodes/10.240.167.206:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 13.079822ms)
Nov 16 19:49:46.970: INFO: (11) /api/v1/nodes/10.240.167.206:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 15.160451ms)
Nov 16 19:49:46.988: INFO: (12) /api/v1/nodes/10.240.167.206:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 17.520012ms)
Nov 16 19:49:47.020: INFO: (13) /api/v1/nodes/10.240.167.206:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 32.732483ms)
Nov 16 19:49:47.035: INFO: (14) /api/v1/nodes/10.240.167.206:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 14.96964ms)
Nov 16 19:49:47.050: INFO: (15) /api/v1/nodes/10.240.167.206:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 14.165381ms)
Nov 16 19:49:47.063: INFO: (16) /api/v1/nodes/10.240.167.206:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 12.886745ms)
Nov 16 19:49:47.083: INFO: (17) /api/v1/nodes/10.240.167.206:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 20.719357ms)
Nov 16 19:49:47.096: INFO: (18) /api/v1/nodes/10.240.167.206:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 12.870618ms)
Nov 16 19:49:47.109: INFO: (19) /api/v1/nodes/10.240.167.206:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 13.169387ms)
[AfterEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:49:47.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6175" for this suite.
Nov 16 19:49:55.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:49:55.905: INFO: namespace proxy-6175 deletion completed in 8.785441213s

• [SLOW TEST:9.303 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:49:55.906: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Nov 16 19:49:56.053: INFO: Waiting up to 5m0s for pod "downward-api-ee195be6-adf6-4619-8364-ad14d341d49c" in namespace "downward-api-5176" to be "success or failure"
Nov 16 19:49:56.059: INFO: Pod "downward-api-ee195be6-adf6-4619-8364-ad14d341d49c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.255472ms
Nov 16 19:49:58.067: INFO: Pod "downward-api-ee195be6-adf6-4619-8364-ad14d341d49c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014487933s
Nov 16 19:50:00.076: INFO: Pod "downward-api-ee195be6-adf6-4619-8364-ad14d341d49c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023534644s
STEP: Saw pod success
Nov 16 19:50:00.076: INFO: Pod "downward-api-ee195be6-adf6-4619-8364-ad14d341d49c" satisfied condition "success or failure"
Nov 16 19:50:00.084: INFO: Trying to get logs from node 10.240.167.206 pod downward-api-ee195be6-adf6-4619-8364-ad14d341d49c container dapi-container: <nil>
STEP: delete the pod
Nov 16 19:50:00.132: INFO: Waiting for pod downward-api-ee195be6-adf6-4619-8364-ad14d341d49c to disappear
Nov 16 19:50:00.139: INFO: Pod downward-api-ee195be6-adf6-4619-8364-ad14d341d49c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:50:00.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5176" for this suite.
Nov 16 19:50:08.178: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:50:10.082: INFO: namespace downward-api-5176 deletion completed in 9.931789336s

• [SLOW TEST:14.176 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:50:10.083: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 19:50:10.640: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 16 19:50:12.668: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153010, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153010, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153010, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153010, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 19:50:15.698: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:50:15.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1458" for this suite.
Nov 16 19:50:23.829: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:50:25.733: INFO: namespace webhook-1458 deletion completed in 9.932485952s
STEP: Destroying namespace "webhook-1458-markers" for this suite.
Nov 16 19:50:33.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:50:35.668: INFO: namespace webhook-1458-markers deletion completed in 9.934427985s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:25.626 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:50:35.710: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service endpoint-test2 in namespace services-2804
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2804 to expose endpoints map[]
Nov 16 19:50:35.902: INFO: Get endpoints failed (7.118801ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Nov 16 19:50:36.909: INFO: successfully validated that service endpoint-test2 in namespace services-2804 exposes endpoints map[] (1.01397843s elapsed)
STEP: Creating pod pod1 in namespace services-2804
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2804 to expose endpoints map[pod1:[80]]
Nov 16 19:50:38.989: INFO: successfully validated that service endpoint-test2 in namespace services-2804 exposes endpoints map[pod1:[80]] (2.047939516s elapsed)
STEP: Creating pod pod2 in namespace services-2804
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2804 to expose endpoints map[pod1:[80] pod2:[80]]
Nov 16 19:50:43.121: INFO: Unexpected endpoints: found map[97c364ff-820f-4f83-b6ec-ad3c815aa1e5:[80]], expected map[pod1:[80] pod2:[80]] (4.105605556s elapsed, will retry)
Nov 16 19:50:48.232: INFO: Unexpected endpoints: found map[97c364ff-820f-4f83-b6ec-ad3c815aa1e5:[80]], expected map[pod1:[80] pod2:[80]] (9.216749124s elapsed, will retry)
Nov 16 19:50:52.320: INFO: successfully validated that service endpoint-test2 in namespace services-2804 exposes endpoints map[pod1:[80] pod2:[80]] (13.305085726s elapsed)
STEP: Deleting pod pod1 in namespace services-2804
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2804 to expose endpoints map[pod2:[80]]
Nov 16 19:50:53.364: INFO: successfully validated that service endpoint-test2 in namespace services-2804 exposes endpoints map[pod2:[80]] (1.029961816s elapsed)
STEP: Deleting pod pod2 in namespace services-2804
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2804 to expose endpoints map[]
Nov 16 19:50:54.393: INFO: successfully validated that service endpoint-test2 in namespace services-2804 exposes endpoints map[] (1.014115128s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:50:54.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2804" for this suite.
Nov 16 19:51:08.479: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:51:10.383: INFO: namespace services-2804 deletion completed in 15.9359853s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:34.673 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:51:10.384: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl label
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
STEP: creating the pod
Nov 16 19:51:10.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 create -f - --namespace=kubectl-2590'
Nov 16 19:51:11.058: INFO: stderr: ""
Nov 16 19:51:11.058: INFO: stdout: "pod/pause created\n"
Nov 16 19:51:11.058: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Nov 16 19:51:11.058: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2590" to be "running and ready"
Nov 16 19:51:11.066: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 7.490441ms
Nov 16 19:51:13.074: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015387146s
Nov 16 19:51:15.082: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.023906997s
Nov 16 19:51:15.083: INFO: Pod "pause" satisfied condition "running and ready"
Nov 16 19:51:15.083: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: adding the label testing-label with value testing-label-value to a pod
Nov 16 19:51:15.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 label pods pause testing-label=testing-label-value --namespace=kubectl-2590'
Nov 16 19:51:15.223: INFO: stderr: ""
Nov 16 19:51:15.223: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Nov 16 19:51:15.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pod pause -L testing-label --namespace=kubectl-2590'
Nov 16 19:51:15.341: INFO: stderr: ""
Nov 16 19:51:15.341: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Nov 16 19:51:15.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 label pods pause testing-label- --namespace=kubectl-2590'
Nov 16 19:51:15.478: INFO: stderr: ""
Nov 16 19:51:15.478: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Nov 16 19:51:15.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pod pause -L testing-label --namespace=kubectl-2590'
Nov 16 19:51:15.610: INFO: stderr: ""
Nov 16 19:51:15.610: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1199
STEP: using delete to clean up resources
Nov 16 19:51:15.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 delete --grace-period=0 --force -f - --namespace=kubectl-2590'
Nov 16 19:51:15.794: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 16 19:51:15.794: INFO: stdout: "pod \"pause\" force deleted\n"
Nov 16 19:51:15.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get rc,svc -l name=pause --no-headers --namespace=kubectl-2590'
Nov 16 19:51:15.921: INFO: stderr: "No resources found in kubectl-2590 namespace.\n"
Nov 16 19:51:15.922: INFO: stdout: ""
Nov 16 19:51:15.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods -l name=pause --namespace=kubectl-2590 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 16 19:51:16.046: INFO: stderr: ""
Nov 16 19:51:16.046: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:51:16.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2590" for this suite.
Nov 16 19:51:24.086: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:51:26.006: INFO: namespace kubectl-2590 deletion completed in 9.949540546s

• [SLOW TEST:15.622 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1189
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:51:26.006: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 19:51:26.142: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-775a013d-f30a-42bd-9a86-793a0a8c94b5" in namespace "security-context-test-9996" to be "success or failure"
Nov 16 19:51:26.169: INFO: Pod "busybox-readonly-false-775a013d-f30a-42bd-9a86-793a0a8c94b5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.991015ms
Nov 16 19:51:28.176: INFO: Pod "busybox-readonly-false-775a013d-f30a-42bd-9a86-793a0a8c94b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034023747s
Nov 16 19:51:30.183: INFO: Pod "busybox-readonly-false-775a013d-f30a-42bd-9a86-793a0a8c94b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041278031s
Nov 16 19:51:30.183: INFO: Pod "busybox-readonly-false-775a013d-f30a-42bd-9a86-793a0a8c94b5" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:51:30.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9996" for this suite.
Nov 16 19:51:38.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:51:40.130: INFO: namespace security-context-test-9996 deletion completed in 9.935566593s

• [SLOW TEST:14.124 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with readOnlyRootFilesystem
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:165
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:51:40.132: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service multi-endpoint-test in namespace services-6429
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6429 to expose endpoints map[]
Nov 16 19:51:40.294: INFO: successfully validated that service multi-endpoint-test in namespace services-6429 exposes endpoints map[] (8.957856ms elapsed)
STEP: Creating pod pod1 in namespace services-6429
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6429 to expose endpoints map[pod1:[100]]
Nov 16 19:51:42.393: INFO: successfully validated that service multi-endpoint-test in namespace services-6429 exposes endpoints map[pod1:[100]] (2.064033182s elapsed)
STEP: Creating pod pod2 in namespace services-6429
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6429 to expose endpoints map[pod1:[100] pod2:[101]]
Nov 16 19:51:46.544: INFO: Unexpected endpoints: found map[04ad5d78-2e0d-4a71-8c57-57ab0e8198cb:[100]], expected map[pod1:[100] pod2:[101]] (4.123726067s elapsed, will retry)
Nov 16 19:51:51.654: INFO: Unexpected endpoints: found map[04ad5d78-2e0d-4a71-8c57-57ab0e8198cb:[100]], expected map[pod1:[100] pod2:[101]] (9.23341149s elapsed, will retry)
Nov 16 19:51:53.698: INFO: successfully validated that service multi-endpoint-test in namespace services-6429 exposes endpoints map[pod1:[100] pod2:[101]] (11.277768652s elapsed)
STEP: Deleting pod pod1 in namespace services-6429
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6429 to expose endpoints map[pod2:[101]]
Nov 16 19:51:54.745: INFO: successfully validated that service multi-endpoint-test in namespace services-6429 exposes endpoints map[pod2:[101]] (1.031813883s elapsed)
STEP: Deleting pod pod2 in namespace services-6429
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6429 to expose endpoints map[]
Nov 16 19:51:55.774: INFO: successfully validated that service multi-endpoint-test in namespace services-6429 exposes endpoints map[] (1.01491345s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:51:55.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6429" for this suite.
Nov 16 19:52:03.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:52:05.767: INFO: namespace services-6429 deletion completed in 9.939738911s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:25.635 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:52:05.768: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Nov 16 19:52:05.921: INFO: Waiting up to 5m0s for pod "downward-api-d0290ce6-ea27-4296-9dbd-e5ca32339722" in namespace "downward-api-4175" to be "success or failure"
Nov 16 19:52:05.929: INFO: Pod "downward-api-d0290ce6-ea27-4296-9dbd-e5ca32339722": Phase="Pending", Reason="", readiness=false. Elapsed: 8.181613ms
Nov 16 19:52:07.937: INFO: Pod "downward-api-d0290ce6-ea27-4296-9dbd-e5ca32339722": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01675809s
STEP: Saw pod success
Nov 16 19:52:07.938: INFO: Pod "downward-api-d0290ce6-ea27-4296-9dbd-e5ca32339722" satisfied condition "success or failure"
Nov 16 19:52:07.945: INFO: Trying to get logs from node 10.240.167.206 pod downward-api-d0290ce6-ea27-4296-9dbd-e5ca32339722 container dapi-container: <nil>
STEP: delete the pod
Nov 16 19:52:08.004: INFO: Waiting for pod downward-api-d0290ce6-ea27-4296-9dbd-e5ca32339722 to disappear
Nov 16 19:52:08.012: INFO: Pod downward-api-d0290ce6-ea27-4296-9dbd-e5ca32339722 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:52:08.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4175" for this suite.
Nov 16 19:52:16.052: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:52:17.974: INFO: namespace downward-api-4175 deletion completed in 9.950348709s

• [SLOW TEST:12.206 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:52:17.975: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 19:52:18.073: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:52:18.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8903" for this suite.
Nov 16 19:52:26.723: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:52:28.630: INFO: namespace custom-resource-definition-8903 deletion completed in 9.93596295s

• [SLOW TEST:10.656 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:52:28.630: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9284.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9284.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9284.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9284.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 16 19:52:32.835: INFO: DNS probes using dns-test-fd9c3b2d-11ec-4672-b249-1d5326f2997e succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9284.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9284.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9284.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9284.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 16 19:52:36.946: INFO: File wheezy_udp@dns-test-service-3.dns-9284.svc.cluster.local from pod  dns-9284/dns-test-dbb4db64-fdcc-4f05-949d-2607f4d1761d contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 16 19:52:36.964: INFO: Lookups using dns-9284/dns-test-dbb4db64-fdcc-4f05-949d-2607f4d1761d failed for: [wheezy_udp@dns-test-service-3.dns-9284.svc.cluster.local]

Nov 16 19:52:41.988: INFO: DNS probes using dns-test-dbb4db64-fdcc-4f05-949d-2607f4d1761d succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9284.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-9284.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9284.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-9284.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 16 19:52:46.127: INFO: DNS probes using dns-test-e8f08fba-ce26-4892-9074-dd24e6212d32 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:52:46.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9284" for this suite.
Nov 16 19:52:54.229: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:52:56.135: INFO: namespace dns-9284 deletion completed in 9.932220444s

• [SLOW TEST:27.505 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:52:56.135: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Nov 16 19:52:56.266: INFO: namespace kubectl-9348
Nov 16 19:52:56.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 create -f - --namespace=kubectl-9348'
Nov 16 19:52:56.784: INFO: stderr: ""
Nov 16 19:52:56.784: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Nov 16 19:52:57.792: INFO: Selector matched 1 pods for map[app:redis]
Nov 16 19:52:57.792: INFO: Found 0 / 1
Nov 16 19:52:58.792: INFO: Selector matched 1 pods for map[app:redis]
Nov 16 19:52:58.793: INFO: Found 1 / 1
Nov 16 19:52:58.793: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 16 19:52:58.800: INFO: Selector matched 1 pods for map[app:redis]
Nov 16 19:52:58.800: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 16 19:52:58.800: INFO: wait on redis-master startup in kubectl-9348 
Nov 16 19:52:58.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 logs redis-master-6nmnb redis-master --namespace=kubectl-9348'
Nov 16 19:52:58.975: INFO: stderr: ""
Nov 16 19:52:58.975: INFO: stdout: "1:C 16 Nov 2020 19:52:57.984 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 16 Nov 2020 19:52:57.985 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 16 Nov 2020 19:52:57.985 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\n1:M 16 Nov 2020 19:52:57.987 * Running mode=standalone, port=6379.\n1:M 16 Nov 2020 19:52:57.987 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 16 Nov 2020 19:52:57.987 # Server initialized\n1:M 16 Nov 2020 19:52:57.987 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 16 Nov 2020 19:52:57.987 * Ready to accept connections\n"
STEP: exposing RC
Nov 16 19:52:58.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-9348'
Nov 16 19:52:59.159: INFO: stderr: ""
Nov 16 19:52:59.159: INFO: stdout: "service/rm2 exposed\n"
Nov 16 19:52:59.171: INFO: Service rm2 in namespace kubectl-9348 found.
STEP: exposing service
Nov 16 19:53:01.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-9348'
Nov 16 19:53:01.345: INFO: stderr: ""
Nov 16 19:53:01.345: INFO: stdout: "service/rm3 exposed\n"
Nov 16 19:53:01.352: INFO: Service rm3 in namespace kubectl-9348 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:53:03.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9348" for this suite.
Nov 16 19:53:17.404: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:53:19.324: INFO: namespace kubectl-9348 deletion completed in 15.947906169s

• [SLOW TEST:23.189 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should create services for rc  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:53:19.324: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name projected-secret-test-185e18b9-a7b6-4bd2-b8bb-903006827e01
STEP: Creating a pod to test consume secrets
Nov 16 19:53:19.467: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-665c5770-51a6-486d-8137-ad68e9ec6912" in namespace "projected-2138" to be "success or failure"
Nov 16 19:53:19.476: INFO: Pod "pod-projected-secrets-665c5770-51a6-486d-8137-ad68e9ec6912": Phase="Pending", Reason="", readiness=false. Elapsed: 8.671782ms
Nov 16 19:53:21.484: INFO: Pod "pod-projected-secrets-665c5770-51a6-486d-8137-ad68e9ec6912": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016723245s
STEP: Saw pod success
Nov 16 19:53:21.484: INFO: Pod "pod-projected-secrets-665c5770-51a6-486d-8137-ad68e9ec6912" satisfied condition "success or failure"
Nov 16 19:53:21.491: INFO: Trying to get logs from node 10.240.167.254 pod pod-projected-secrets-665c5770-51a6-486d-8137-ad68e9ec6912 container secret-volume-test: <nil>
STEP: delete the pod
Nov 16 19:53:21.548: INFO: Waiting for pod pod-projected-secrets-665c5770-51a6-486d-8137-ad68e9ec6912 to disappear
Nov 16 19:53:21.557: INFO: Pod pod-projected-secrets-665c5770-51a6-486d-8137-ad68e9ec6912 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:53:21.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2138" for this suite.
Nov 16 19:53:29.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:53:31.498: INFO: namespace projected-2138 deletion completed in 9.930896679s

• [SLOW TEST:12.174 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:53:31.499: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6368.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6368.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 16 19:53:35.833: INFO: DNS probes using dns-6368/dns-test-9225b2c6-969a-4140-845c-6c7a933cc538 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:53:35.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6368" for this suite.
Nov 16 19:53:43.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:53:45.800: INFO: namespace dns-6368 deletion completed in 9.93124534s

• [SLOW TEST:14.301 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:53:45.800: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Nov 16 19:53:45.971: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3238 /api/v1/namespaces/watch-3238/configmaps/e2e-watch-test-label-changed d32fdd65-7c20-4bdd-93cc-f8e70be23add 58945 0 2020-11-16 19:53:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Nov 16 19:53:45.974: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3238 /api/v1/namespaces/watch-3238/configmaps/e2e-watch-test-label-changed d32fdd65-7c20-4bdd-93cc-f8e70be23add 58949 0 2020-11-16 19:53:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Nov 16 19:53:45.974: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3238 /api/v1/namespaces/watch-3238/configmaps/e2e-watch-test-label-changed d32fdd65-7c20-4bdd-93cc-f8e70be23add 58950 0 2020-11-16 19:53:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Nov 16 19:53:56.063: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3238 /api/v1/namespaces/watch-3238/configmaps/e2e-watch-test-label-changed d32fdd65-7c20-4bdd-93cc-f8e70be23add 58990 0 2020-11-16 19:53:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Nov 16 19:53:56.063: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3238 /api/v1/namespaces/watch-3238/configmaps/e2e-watch-test-label-changed d32fdd65-7c20-4bdd-93cc-f8e70be23add 58991 0 2020-11-16 19:53:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Nov 16 19:53:56.063: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3238 /api/v1/namespaces/watch-3238/configmaps/e2e-watch-test-label-changed d32fdd65-7c20-4bdd-93cc-f8e70be23add 58992 0 2020-11-16 19:53:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:53:56.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3238" for this suite.
Nov 16 19:54:04.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:54:06.029: INFO: namespace watch-3238 deletion completed in 9.956203682s

• [SLOW TEST:20.229 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:54:06.032: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Nov 16 19:54:08.772: INFO: Successfully updated pod "annotationupdate3fd004b5-d4ea-4ee9-9c85-0e06d3a4c76e"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:54:10.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4977" for this suite.
Nov 16 19:54:24.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:54:26.753: INFO: namespace downward-api-4977 deletion completed in 15.933502925s

• [SLOW TEST:20.721 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:54:26.757: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 16 19:54:29.982: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:54:30.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8252" for this suite.
Nov 16 19:54:38.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:54:39.974: INFO: namespace container-runtime-8252 deletion completed in 9.942455438s

• [SLOW TEST:13.217 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:54:39.974: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-projected-all-test-volume-3476fad7-8f3b-458a-82fb-b48748927866
STEP: Creating secret with name secret-projected-all-test-volume-898859e7-0e73-4b69-82dc-a60faa34678a
STEP: Creating a pod to test Check all projections for projected volume plugin
Nov 16 19:54:40.139: INFO: Waiting up to 5m0s for pod "projected-volume-e4efd58c-3392-45bc-bec1-b63522fa3f55" in namespace "projected-9271" to be "success or failure"
Nov 16 19:54:40.145: INFO: Pod "projected-volume-e4efd58c-3392-45bc-bec1-b63522fa3f55": Phase="Pending", Reason="", readiness=false. Elapsed: 6.026219ms
Nov 16 19:54:42.153: INFO: Pod "projected-volume-e4efd58c-3392-45bc-bec1-b63522fa3f55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014347659s
STEP: Saw pod success
Nov 16 19:54:42.154: INFO: Pod "projected-volume-e4efd58c-3392-45bc-bec1-b63522fa3f55" satisfied condition "success or failure"
Nov 16 19:54:42.160: INFO: Trying to get logs from node 10.240.167.254 pod projected-volume-e4efd58c-3392-45bc-bec1-b63522fa3f55 container projected-all-volume-test: <nil>
STEP: delete the pod
Nov 16 19:54:42.207: INFO: Waiting for pod projected-volume-e4efd58c-3392-45bc-bec1-b63522fa3f55 to disappear
Nov 16 19:54:42.214: INFO: Pod projected-volume-e4efd58c-3392-45bc-bec1-b63522fa3f55 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:54:42.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9271" for this suite.
Nov 16 19:54:50.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:54:52.155: INFO: namespace projected-9271 deletion completed in 9.931493449s

• [SLOW TEST:12.181 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:54:52.155: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov 16 19:54:52.338: INFO: Number of nodes with available pods: 0
Nov 16 19:54:52.338: INFO: Node 10.240.167.206 is running more than one daemon pod
Nov 16 19:54:53.357: INFO: Number of nodes with available pods: 0
Nov 16 19:54:53.357: INFO: Node 10.240.167.206 is running more than one daemon pod
Nov 16 19:54:54.356: INFO: Number of nodes with available pods: 2
Nov 16 19:54:54.356: INFO: Node 10.240.167.209 is running more than one daemon pod
Nov 16 19:54:55.356: INFO: Number of nodes with available pods: 3
Nov 16 19:54:55.356: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Nov 16 19:54:55.401: INFO: Number of nodes with available pods: 2
Nov 16 19:54:55.401: INFO: Node 10.240.167.209 is running more than one daemon pod
Nov 16 19:54:56.419: INFO: Number of nodes with available pods: 2
Nov 16 19:54:56.419: INFO: Node 10.240.167.209 is running more than one daemon pod
Nov 16 19:54:57.423: INFO: Number of nodes with available pods: 2
Nov 16 19:54:57.423: INFO: Node 10.240.167.209 is running more than one daemon pod
Nov 16 19:54:58.420: INFO: Number of nodes with available pods: 3
Nov 16 19:54:58.420: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2402, will wait for the garbage collector to delete the pods
Nov 16 19:54:58.517: INFO: Deleting DaemonSet.extensions daemon-set took: 21.780394ms
Nov 16 19:54:58.617: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.272629ms
Nov 16 19:55:11.727: INFO: Number of nodes with available pods: 0
Nov 16 19:55:11.727: INFO: Number of running nodes: 0, number of available pods: 0
Nov 16 19:55:11.736: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2402/daemonsets","resourceVersion":"59641"},"items":null}

Nov 16 19:55:11.742: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2402/pods","resourceVersion":"59641"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:55:11.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2402" for this suite.
Nov 16 19:55:19.817: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:55:21.741: INFO: namespace daemonsets-2402 deletion completed in 9.951634571s

• [SLOW TEST:29.586 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:55:21.741: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 19:55:21.917: INFO: (0) /api/v1/nodes/10.240.167.206/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 40.318567ms)
Nov 16 19:55:21.930: INFO: (1) /api/v1/nodes/10.240.167.206/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 13.446719ms)
Nov 16 19:55:21.944: INFO: (2) /api/v1/nodes/10.240.167.206/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 13.743865ms)
Nov 16 19:55:21.957: INFO: (3) /api/v1/nodes/10.240.167.206/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 13.1058ms)
Nov 16 19:55:21.971: INFO: (4) /api/v1/nodes/10.240.167.206/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 13.495282ms)
Nov 16 19:55:22.002: INFO: (5) /api/v1/nodes/10.240.167.206/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 31.159128ms)
Nov 16 19:55:22.037: INFO: (6) /api/v1/nodes/10.240.167.206/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 34.341149ms)
Nov 16 19:55:22.052: INFO: (7) /api/v1/nodes/10.240.167.206/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 14.654083ms)
Nov 16 19:55:22.066: INFO: (8) /api/v1/nodes/10.240.167.206/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 14.116552ms)
Nov 16 19:55:22.080: INFO: (9) /api/v1/nodes/10.240.167.206/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 13.531831ms)
Nov 16 19:55:22.096: INFO: (10) /api/v1/nodes/10.240.167.206/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 15.628282ms)
Nov 16 19:55:22.110: INFO: (11) /api/v1/nodes/10.240.167.206/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 14.372691ms)
Nov 16 19:55:22.122: INFO: (12) /api/v1/nodes/10.240.167.206/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 11.610331ms)
Nov 16 19:55:22.143: INFO: (13) /api/v1/nodes/10.240.167.206/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 21.053106ms)
Nov 16 19:55:22.158: INFO: (14) /api/v1/nodes/10.240.167.206/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 15.030432ms)
Nov 16 19:55:22.173: INFO: (15) /api/v1/nodes/10.240.167.206/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 15.270064ms)
Nov 16 19:55:22.188: INFO: (16) /api/v1/nodes/10.240.167.206/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 14.89776ms)
Nov 16 19:55:22.201: INFO: (17) /api/v1/nodes/10.240.167.206/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 12.924118ms)
Nov 16 19:55:22.223: INFO: (18) /api/v1/nodes/10.240.167.206/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 21.679421ms)
Nov 16 19:55:22.237: INFO: (19) /api/v1/nodes/10.240.167.206/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 13.926897ms)
[AfterEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:55:22.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3902" for this suite.
Nov 16 19:55:30.281: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:55:31.063: INFO: namespace proxy-3902 deletion completed in 8.812179068s

• [SLOW TEST:9.322 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:55:31.064: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-45bbb2d1-f4a7-4481-aae5-5f0773ac97e8
STEP: Creating a pod to test consume configMaps
Nov 16 19:55:31.222: INFO: Waiting up to 5m0s for pod "pod-configmaps-f2cb8552-66a2-45a8-9873-616e408b25eb" in namespace "configmap-7805" to be "success or failure"
Nov 16 19:55:31.230: INFO: Pod "pod-configmaps-f2cb8552-66a2-45a8-9873-616e408b25eb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.498722ms
Nov 16 19:55:33.237: INFO: Pod "pod-configmaps-f2cb8552-66a2-45a8-9873-616e408b25eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014889368s
Nov 16 19:55:35.245: INFO: Pod "pod-configmaps-f2cb8552-66a2-45a8-9873-616e408b25eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023242674s
STEP: Saw pod success
Nov 16 19:55:35.245: INFO: Pod "pod-configmaps-f2cb8552-66a2-45a8-9873-616e408b25eb" satisfied condition "success or failure"
Nov 16 19:55:35.253: INFO: Trying to get logs from node 10.240.167.254 pod pod-configmaps-f2cb8552-66a2-45a8-9873-616e408b25eb container configmap-volume-test: <nil>
STEP: delete the pod
Nov 16 19:55:35.306: INFO: Waiting for pod pod-configmaps-f2cb8552-66a2-45a8-9873-616e408b25eb to disappear
Nov 16 19:55:35.312: INFO: Pod pod-configmaps-f2cb8552-66a2-45a8-9873-616e408b25eb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:55:35.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7805" for this suite.
Nov 16 19:55:43.353: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:55:45.256: INFO: namespace configmap-7805 deletion completed in 9.935046841s

• [SLOW TEST:14.193 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:55:45.257: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-6527b755-d91d-4cb9-baaa-d89e07a222ee
STEP: Creating a pod to test consume secrets
Nov 16 19:55:45.424: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ab6ecd51-8c17-4cd1-b217-c60acfa1848b" in namespace "projected-8003" to be "success or failure"
Nov 16 19:55:45.430: INFO: Pod "pod-projected-secrets-ab6ecd51-8c17-4cd1-b217-c60acfa1848b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.260778ms
Nov 16 19:55:47.438: INFO: Pod "pod-projected-secrets-ab6ecd51-8c17-4cd1-b217-c60acfa1848b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014245653s
STEP: Saw pod success
Nov 16 19:55:47.438: INFO: Pod "pod-projected-secrets-ab6ecd51-8c17-4cd1-b217-c60acfa1848b" satisfied condition "success or failure"
Nov 16 19:55:47.445: INFO: Trying to get logs from node 10.240.167.254 pod pod-projected-secrets-ab6ecd51-8c17-4cd1-b217-c60acfa1848b container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 16 19:55:47.487: INFO: Waiting for pod pod-projected-secrets-ab6ecd51-8c17-4cd1-b217-c60acfa1848b to disappear
Nov 16 19:55:47.494: INFO: Pod pod-projected-secrets-ab6ecd51-8c17-4cd1-b217-c60acfa1848b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:55:47.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8003" for this suite.
Nov 16 19:55:55.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:55:57.453: INFO: namespace projected-8003 deletion completed in 9.948833543s

• [SLOW TEST:12.196 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:55:57.457: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-1880
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Nov 16 19:55:57.591: INFO: Found 0 stateful pods, waiting for 3
Nov 16 19:56:07.600: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 19:56:07.600: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 19:56:07.600: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Nov 16 19:56:07.663: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Nov 16 19:56:17.728: INFO: Updating stateful set ss2
Nov 16 19:56:17.748: INFO: Waiting for Pod statefulset-1880/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Nov 16 19:56:27.826: INFO: Found 1 stateful pods, waiting for 3
Nov 16 19:56:37.834: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 19:56:37.835: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 19:56:37.835: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Nov 16 19:56:37.878: INFO: Updating stateful set ss2
Nov 16 19:56:37.895: INFO: Waiting for Pod statefulset-1880/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 16 19:56:47.945: INFO: Updating stateful set ss2
Nov 16 19:56:47.959: INFO: Waiting for StatefulSet statefulset-1880/ss2 to complete update
Nov 16 19:56:47.959: INFO: Waiting for Pod statefulset-1880/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 16 19:56:57.975: INFO: Waiting for StatefulSet statefulset-1880/ss2 to complete update
Nov 16 19:56:57.976: INFO: Waiting for Pod statefulset-1880/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 16 19:57:07.975: INFO: Waiting for StatefulSet statefulset-1880/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Nov 16 19:57:17.975: INFO: Deleting all statefulset in ns statefulset-1880
Nov 16 19:57:17.982: INFO: Scaling statefulset ss2 to 0
Nov 16 19:57:48.016: INFO: Waiting for statefulset status.replicas updated to 0
Nov 16 19:57:48.024: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:57:48.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1880" for this suite.
Nov 16 19:57:56.101: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:57:58.021: INFO: namespace statefulset-1880 deletion completed in 9.944870377s

• [SLOW TEST:120.564 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:57:58.022: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Nov 16 19:57:58.142: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Nov 16 19:58:30.611: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
Nov 16 19:58:37.512: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:59:03.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8474" for this suite.
Nov 16 19:59:11.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:59:13.394: INFO: namespace crd-publish-openapi-8474 deletion completed in 9.935266988s

• [SLOW TEST:75.372 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:59:13.396: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-9463/configmap-test-91ada1c6-4994-4534-885f-858cbb1287c3
STEP: Creating a pod to test consume configMaps
Nov 16 19:59:13.549: INFO: Waiting up to 5m0s for pod "pod-configmaps-89154e9e-c032-4d46-b272-4549835ee8f9" in namespace "configmap-9463" to be "success or failure"
Nov 16 19:59:13.558: INFO: Pod "pod-configmaps-89154e9e-c032-4d46-b272-4549835ee8f9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.301808ms
Nov 16 19:59:15.566: INFO: Pod "pod-configmaps-89154e9e-c032-4d46-b272-4549835ee8f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015558092s
STEP: Saw pod success
Nov 16 19:59:15.566: INFO: Pod "pod-configmaps-89154e9e-c032-4d46-b272-4549835ee8f9" satisfied condition "success or failure"
Nov 16 19:59:15.573: INFO: Trying to get logs from node 10.240.167.254 pod pod-configmaps-89154e9e-c032-4d46-b272-4549835ee8f9 container env-test: <nil>
STEP: delete the pod
Nov 16 19:59:15.637: INFO: Waiting for pod pod-configmaps-89154e9e-c032-4d46-b272-4549835ee8f9 to disappear
Nov 16 19:59:15.643: INFO: Pod pod-configmaps-89154e9e-c032-4d46-b272-4549835ee8f9 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:59:15.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9463" for this suite.
Nov 16 19:59:23.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:59:25.589: INFO: namespace configmap-9463 deletion completed in 9.936336656s

• [SLOW TEST:12.194 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:59:25.591: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run default
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1403
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Nov 16 19:59:25.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-3611'
Nov 16 19:59:26.017: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Nov 16 19:59:26.017: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
Nov 16 19:59:28.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 delete deployment e2e-test-httpd-deployment --namespace=kubectl-3611'
Nov 16 19:59:28.173: INFO: stderr: ""
Nov 16 19:59:28.173: INFO: stdout: "deployment.extensions \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:59:28.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3611" for this suite.
Nov 16 19:59:36.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:59:38.116: INFO: namespace kubectl-3611 deletion completed in 9.931673577s

• [SLOW TEST:12.525 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run default
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:59:38.116: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-914281ab-d3f5-4f14-9192-4428ad62a875
STEP: Creating a pod to test consume configMaps
Nov 16 19:59:38.253: INFO: Waiting up to 5m0s for pod "pod-configmaps-076c86fb-7d29-4c8b-b04d-607d8215c764" in namespace "configmap-4818" to be "success or failure"
Nov 16 19:59:38.259: INFO: Pod "pod-configmaps-076c86fb-7d29-4c8b-b04d-607d8215c764": Phase="Pending", Reason="", readiness=false. Elapsed: 6.076996ms
Nov 16 19:59:40.267: INFO: Pod "pod-configmaps-076c86fb-7d29-4c8b-b04d-607d8215c764": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013791346s
STEP: Saw pod success
Nov 16 19:59:40.267: INFO: Pod "pod-configmaps-076c86fb-7d29-4c8b-b04d-607d8215c764" satisfied condition "success or failure"
Nov 16 19:59:40.274: INFO: Trying to get logs from node 10.240.167.254 pod pod-configmaps-076c86fb-7d29-4c8b-b04d-607d8215c764 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 16 19:59:40.313: INFO: Waiting for pod pod-configmaps-076c86fb-7d29-4c8b-b04d-607d8215c764 to disappear
Nov 16 19:59:40.320: INFO: Pod pod-configmaps-076c86fb-7d29-4c8b-b04d-607d8215c764 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 19:59:40.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4818" for this suite.
Nov 16 19:59:48.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 19:59:50.265: INFO: namespace configmap-4818 deletion completed in 9.935971545s

• [SLOW TEST:12.149 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 19:59:50.265: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1116 20:00:00.542365      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Nov 16 20:00:00.542: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:00:00.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6334" for this suite.
Nov 16 20:00:08.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:00:10.485: INFO: namespace gc-6334 deletion completed in 9.933124604s

• [SLOW TEST:20.220 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:00:10.485: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 20:00:11.542: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 16 20:00:13.571: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153611, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153611, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153611, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153611, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 20:00:16.604: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:00:26.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-955" for this suite.
Nov 16 20:00:34.980: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:00:36.885: INFO: namespace webhook-955 deletion completed in 9.937990576s
STEP: Destroying namespace "webhook-955-markers" for this suite.
Nov 16 20:00:44.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:00:46.848: INFO: namespace webhook-955-markers deletion completed in 9.962457458s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:36.405 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:00:46.892: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:00:47.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5998" for this suite.
Nov 16 20:00:55.110: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:00:57.018: INFO: namespace kubelet-test-5998 deletion completed in 9.934325133s

• [SLOW TEST:10.126 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:00:57.018: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 16 20:00:57.160: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a760b975-f80f-415a-bd1e-58aca7122939" in namespace "downward-api-883" to be "success or failure"
Nov 16 20:00:57.166: INFO: Pod "downwardapi-volume-a760b975-f80f-415a-bd1e-58aca7122939": Phase="Pending", Reason="", readiness=false. Elapsed: 6.246802ms
Nov 16 20:00:59.173: INFO: Pod "downwardapi-volume-a760b975-f80f-415a-bd1e-58aca7122939": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013506723s
STEP: Saw pod success
Nov 16 20:00:59.173: INFO: Pod "downwardapi-volume-a760b975-f80f-415a-bd1e-58aca7122939" satisfied condition "success or failure"
Nov 16 20:00:59.180: INFO: Trying to get logs from node 10.240.167.206 pod downwardapi-volume-a760b975-f80f-415a-bd1e-58aca7122939 container client-container: <nil>
STEP: delete the pod
Nov 16 20:00:59.247: INFO: Waiting for pod downwardapi-volume-a760b975-f80f-415a-bd1e-58aca7122939 to disappear
Nov 16 20:00:59.254: INFO: Pod downwardapi-volume-a760b975-f80f-415a-bd1e-58aca7122939 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:00:59.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-883" for this suite.
Nov 16 20:01:07.291: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:01:09.196: INFO: namespace downward-api-883 deletion completed in 9.931802141s

• [SLOW TEST:12.178 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:01:09.196: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:01:25.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4140" for this suite.
Nov 16 20:01:33.440: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:01:35.361: INFO: namespace resourcequota-4140 deletion completed in 9.950074043s

• [SLOW TEST:26.164 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:01:35.361: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Nov 16 20:01:35.474: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:02:08.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2028" for this suite.
Nov 16 20:02:16.589: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:02:18.510: INFO: namespace crd-publish-openapi-2028 deletion completed in 9.949401149s

• [SLOW TEST:43.149 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:02:18.511: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Nov 16 20:02:18.619: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the sample API server.
Nov 16 20:02:19.069: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Nov 16 20:02:21.176: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 20:02:23.186: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 20:02:25.187: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 20:02:27.186: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 20:02:29.187: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 20:02:31.186: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 20:02:33.187: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 20:02:35.186: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 20:02:37.185: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 20:02:39.187: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 20:02:41.187: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741153739, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 20:02:43.357: INFO: Waited 156.670397ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:02:45.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-5414" for this suite.
Nov 16 20:02:53.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:02:55.648: INFO: namespace aggregator-5414 deletion completed in 10.026766017s

• [SLOW TEST:37.138 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:02:55.649: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Nov 16 20:03:02.937: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 16 20:03:02.945: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 16 20:03:04.945: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 16 20:03:04.953: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 16 20:03:06.945: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 16 20:03:06.954: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:03:06.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-51" for this suite.
Nov 16 20:03:36.994: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:03:38.900: INFO: namespace container-lifecycle-hook-51 deletion completed in 31.935165913s

• [SLOW TEST:43.251 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:03:38.902: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
Nov 16 20:03:39.599: INFO: created pod pod-service-account-defaultsa
Nov 16 20:03:39.599: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Nov 16 20:03:39.622: INFO: created pod pod-service-account-mountsa
Nov 16 20:03:39.622: INFO: pod pod-service-account-mountsa service account token volume mount: true
Nov 16 20:03:39.644: INFO: created pod pod-service-account-nomountsa
Nov 16 20:03:39.644: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Nov 16 20:03:39.670: INFO: created pod pod-service-account-defaultsa-mountspec
Nov 16 20:03:39.670: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Nov 16 20:03:39.694: INFO: created pod pod-service-account-mountsa-mountspec
Nov 16 20:03:39.694: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Nov 16 20:03:39.716: INFO: created pod pod-service-account-nomountsa-mountspec
Nov 16 20:03:39.716: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Nov 16 20:03:39.740: INFO: created pod pod-service-account-defaultsa-nomountspec
Nov 16 20:03:39.740: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Nov 16 20:03:39.777: INFO: created pod pod-service-account-mountsa-nomountspec
Nov 16 20:03:39.777: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Nov 16 20:03:39.802: INFO: created pod pod-service-account-nomountsa-nomountspec
Nov 16 20:03:39.802: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:03:39.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2000" for this suite.
Nov 16 20:03:47.839: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:03:49.745: INFO: namespace svcaccounts-2000 deletion completed in 9.933850028s

• [SLOW TEST:10.844 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:03:49.746: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Nov 16 20:03:49.857: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:03:54.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1586" for this suite.
Nov 16 20:04:08.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:04:09.987: INFO: namespace init-container-1586 deletion completed in 15.949034054s

• [SLOW TEST:20.241 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:04:09.988: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Nov 16 20:04:14.271: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 16 20:04:14.278: INFO: Pod pod-with-prestop-http-hook still exists
Nov 16 20:04:16.278: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 16 20:04:16.290: INFO: Pod pod-with-prestop-http-hook still exists
Nov 16 20:04:18.278: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 16 20:04:18.287: INFO: Pod pod-with-prestop-http-hook still exists
Nov 16 20:04:20.278: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 16 20:04:20.286: INFO: Pod pod-with-prestop-http-hook still exists
Nov 16 20:04:22.279: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 16 20:04:22.287: INFO: Pod pod-with-prestop-http-hook still exists
Nov 16 20:04:24.278: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 16 20:04:24.286: INFO: Pod pod-with-prestop-http-hook still exists
Nov 16 20:04:26.278: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 16 20:04:26.287: INFO: Pod pod-with-prestop-http-hook still exists
Nov 16 20:04:28.278: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 16 20:04:28.287: INFO: Pod pod-with-prestop-http-hook still exists
Nov 16 20:04:30.278: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 16 20:04:30.287: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:04:30.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-104" for this suite.
Nov 16 20:04:44.351: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:04:46.258: INFO: namespace container-lifecycle-hook-104 deletion completed in 15.935762087s

• [SLOW TEST:36.270 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:04:46.260: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-c069d232-9992-4023-ab37-768eb7cc135f
STEP: Creating a pod to test consume configMaps
Nov 16 20:04:46.412: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8845ad55-dbb6-4dfd-b0dc-7e2d65297ad3" in namespace "projected-9080" to be "success or failure"
Nov 16 20:04:46.419: INFO: Pod "pod-projected-configmaps-8845ad55-dbb6-4dfd-b0dc-7e2d65297ad3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.277832ms
Nov 16 20:04:48.428: INFO: Pod "pod-projected-configmaps-8845ad55-dbb6-4dfd-b0dc-7e2d65297ad3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015718641s
STEP: Saw pod success
Nov 16 20:04:48.428: INFO: Pod "pod-projected-configmaps-8845ad55-dbb6-4dfd-b0dc-7e2d65297ad3" satisfied condition "success or failure"
Nov 16 20:04:48.436: INFO: Trying to get logs from node 10.240.167.254 pod pod-projected-configmaps-8845ad55-dbb6-4dfd-b0dc-7e2d65297ad3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 16 20:04:48.490: INFO: Waiting for pod pod-projected-configmaps-8845ad55-dbb6-4dfd-b0dc-7e2d65297ad3 to disappear
Nov 16 20:04:48.497: INFO: Pod pod-projected-configmaps-8845ad55-dbb6-4dfd-b0dc-7e2d65297ad3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:04:48.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9080" for this suite.
Nov 16 20:04:56.537: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:04:58.445: INFO: namespace projected-9080 deletion completed in 9.938305009s

• [SLOW TEST:12.185 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:04:58.446: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 16 20:04:59.642: INFO: Waiting up to 5m0s for pod "downwardapi-volume-16bf9c8c-5246-4ef2-ae90-f6568c338004" in namespace "projected-1721" to be "success or failure"
Nov 16 20:04:59.649: INFO: Pod "downwardapi-volume-16bf9c8c-5246-4ef2-ae90-f6568c338004": Phase="Pending", Reason="", readiness=false. Elapsed: 7.13124ms
Nov 16 20:05:01.657: INFO: Pod "downwardapi-volume-16bf9c8c-5246-4ef2-ae90-f6568c338004": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014841339s
STEP: Saw pod success
Nov 16 20:05:01.657: INFO: Pod "downwardapi-volume-16bf9c8c-5246-4ef2-ae90-f6568c338004" satisfied condition "success or failure"
Nov 16 20:05:01.664: INFO: Trying to get logs from node 10.240.167.209 pod downwardapi-volume-16bf9c8c-5246-4ef2-ae90-f6568c338004 container client-container: <nil>
STEP: delete the pod
Nov 16 20:05:01.736: INFO: Waiting for pod downwardapi-volume-16bf9c8c-5246-4ef2-ae90-f6568c338004 to disappear
Nov 16 20:05:01.742: INFO: Pod downwardapi-volume-16bf9c8c-5246-4ef2-ae90-f6568c338004 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:05:01.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1721" for this suite.
Nov 16 20:05:09.781: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:05:11.702: INFO: namespace projected-1721 deletion completed in 9.950625767s

• [SLOW TEST:13.256 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:05:11.703: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:05:27.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2638" for this suite.
Nov 16 20:05:35.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:05:37.086: INFO: namespace namespaces-2638 deletion completed in 9.936205181s
STEP: Destroying namespace "nsdeletetest-4981" for this suite.
Nov 16 20:05:37.093: INFO: Namespace nsdeletetest-4981 was already deleted
STEP: Destroying namespace "nsdeletetest-3245" for this suite.
Nov 16 20:05:45.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:05:47.026: INFO: namespace nsdeletetest-3245 deletion completed in 9.93341422s

• [SLOW TEST:35.324 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:05:47.026: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Nov 16 20:05:49.245: INFO: &Pod{ObjectMeta:{send-events-75cd71a6-537c-4166-8b6d-f77dcc428106  events-731 /api/v1/namespaces/events-731/pods/send-events-75cd71a6-537c-4166-8b6d-f77dcc428106 3c4f774d-59d3-4ed1-adba-646e61e16893 64824 0 2020-11-16 20:05:47 +0000 UTC <nil> <nil> map[name:foo time:169052780] map[cni.projectcalico.org/podIP:172.30.216.219/32 cni.projectcalico.org/podIPs:172.30.216.219/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.216.219"
    ],
    "dns": {}
}] openshift.io/scc:anyuid] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zl9nw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zl9nw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zl9nw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.167.254,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 20:05:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 20:05:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 20:05:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 20:05:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.167.254,PodIP:172.30.216.219,StartTime:2020-11-16 20:05:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-16 20:05:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:4057a5580c7b59c4fe10d8ab2732c9dec35eea80fd41f7bafc7bd5acc7edf727,ContainerID:cri-o://a7856fc76eb578fe95b28fc408e21eacaf5de1065730391e60d17cc216b05fb3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.216.219,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Nov 16 20:05:51.255: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Nov 16 20:05:53.265: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:05:53.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-731" for this suite.
Nov 16 20:06:41.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:06:43.238: INFO: namespace events-731 deletion completed in 49.947527116s

• [SLOW TEST:56.212 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:06:43.238: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: Gathering metrics
W1116 20:06:44.518531      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Nov 16 20:06:44.518: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:06:44.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9276" for this suite.
Nov 16 20:06:52.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:06:54.467: INFO: namespace gc-9276 deletion completed in 9.937799335s

• [SLOW TEST:11.228 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:06:54.467: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Nov 16 20:07:00.682: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6422 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 16 20:07:00.682: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
Nov 16 20:07:01.859: INFO: Exec stderr: ""
Nov 16 20:07:01.859: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6422 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 16 20:07:01.859: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
Nov 16 20:07:02.051: INFO: Exec stderr: ""
Nov 16 20:07:02.052: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6422 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 16 20:07:02.052: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
Nov 16 20:07:02.231: INFO: Exec stderr: ""
Nov 16 20:07:02.231: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6422 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 16 20:07:02.231: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
Nov 16 20:07:02.407: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Nov 16 20:07:02.407: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6422 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 16 20:07:02.407: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
Nov 16 20:07:02.594: INFO: Exec stderr: ""
Nov 16 20:07:02.594: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6422 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 16 20:07:02.594: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
Nov 16 20:07:02.784: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Nov 16 20:07:02.784: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6422 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 16 20:07:02.785: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
Nov 16 20:07:02.959: INFO: Exec stderr: ""
Nov 16 20:07:02.959: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6422 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 16 20:07:02.959: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
Nov 16 20:07:03.128: INFO: Exec stderr: ""
Nov 16 20:07:03.128: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6422 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 16 20:07:03.128: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
Nov 16 20:07:03.294: INFO: Exec stderr: ""
Nov 16 20:07:03.294: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6422 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 16 20:07:03.294: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
Nov 16 20:07:03.456: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:07:03.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-6422" for this suite.
Nov 16 20:07:51.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:07:53.399: INFO: namespace e2e-kubelet-etc-hosts-6422 deletion completed in 49.931481688s

• [SLOW TEST:58.932 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:07:53.399: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:07:53.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1510" for this suite.
Nov 16 20:08:01.545: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:08:03.450: INFO: namespace custom-resource-definition-1510 deletion completed in 9.930009874s

• [SLOW TEST:10.051 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:08:03.450: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:08:03.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1310" for this suite.
Nov 16 20:08:11.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:08:13.511: INFO: namespace services-1310 deletion completed in 9.935063414s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:10.061 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:08:13.512: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 20:08:14.706: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 20:08:17.765: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:08:17.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9002" for this suite.
Nov 16 20:08:25.989: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:08:27.894: INFO: namespace webhook-9002 deletion completed in 9.935259543s
STEP: Destroying namespace "webhook-9002-markers" for this suite.
Nov 16 20:08:35.923: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:08:37.831: INFO: namespace webhook-9002-markers deletion completed in 9.937450336s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:24.360 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:08:37.873: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1668
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Nov 16 20:08:37.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-2543'
Nov 16 20:08:38.111: INFO: stderr: ""
Nov 16 20:08:38.111: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1673
Nov 16 20:08:38.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 delete pods e2e-test-httpd-pod --namespace=kubectl-2543'
Nov 16 20:08:51.685: INFO: stderr: ""
Nov 16 20:08:51.685: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:08:51.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2543" for this suite.
Nov 16 20:08:59.733: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:09:01.639: INFO: namespace kubectl-2543 deletion completed in 9.94099787s

• [SLOW TEST:23.766 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1664
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:09:01.639: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 16 20:09:01.825: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ce628a4b-37ab-4335-ab60-006cae8ce05f" in namespace "downward-api-2172" to be "success or failure"
Nov 16 20:09:01.834: INFO: Pod "downwardapi-volume-ce628a4b-37ab-4335-ab60-006cae8ce05f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.679563ms
Nov 16 20:09:03.842: INFO: Pod "downwardapi-volume-ce628a4b-37ab-4335-ab60-006cae8ce05f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017343571s
STEP: Saw pod success
Nov 16 20:09:03.842: INFO: Pod "downwardapi-volume-ce628a4b-37ab-4335-ab60-006cae8ce05f" satisfied condition "success or failure"
Nov 16 20:09:03.850: INFO: Trying to get logs from node 10.240.167.254 pod downwardapi-volume-ce628a4b-37ab-4335-ab60-006cae8ce05f container client-container: <nil>
STEP: delete the pod
Nov 16 20:09:03.911: INFO: Waiting for pod downwardapi-volume-ce628a4b-37ab-4335-ab60-006cae8ce05f to disappear
Nov 16 20:09:03.918: INFO: Pod downwardapi-volume-ce628a4b-37ab-4335-ab60-006cae8ce05f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:09:03.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2172" for this suite.
Nov 16 20:09:11.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:09:13.863: INFO: namespace downward-api-2172 deletion completed in 9.935551827s

• [SLOW TEST:12.224 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:09:13.863: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:09:13.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-3638" for this suite.
Nov 16 20:09:22.019: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:09:23.939: INFO: namespace tables-3638 deletion completed in 9.949460845s

• [SLOW TEST:10.076 seconds]
[sig-api-machinery] Servers with support for Table transformation
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:09:23.940: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Nov 16 20:09:24.456: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Nov 16 20:09:26.483: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741154164, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741154164, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741154164, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741154164, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 20:09:29.520: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 20:09:29.529: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:09:30.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9922" for this suite.
Nov 16 20:09:38.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:09:40.896: INFO: namespace crd-webhook-9922 deletion completed in 9.932610276s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:17.000 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:09:40.940: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating replication controller my-hostname-basic-172c2368-9ed2-4c70-84c6-e9c6a4880f2b
Nov 16 20:09:41.120: INFO: Pod name my-hostname-basic-172c2368-9ed2-4c70-84c6-e9c6a4880f2b: Found 0 pods out of 1
Nov 16 20:09:46.129: INFO: Pod name my-hostname-basic-172c2368-9ed2-4c70-84c6-e9c6a4880f2b: Found 1 pods out of 1
Nov 16 20:09:46.129: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-172c2368-9ed2-4c70-84c6-e9c6a4880f2b" are running
Nov 16 20:09:46.137: INFO: Pod "my-hostname-basic-172c2368-9ed2-4c70-84c6-e9c6a4880f2b-8gvcl" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-16 20:09:41 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-16 20:09:42 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-16 20:09:42 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-16 20:09:41 +0000 UTC Reason: Message:}])
Nov 16 20:09:46.137: INFO: Trying to dial the pod
Nov 16 20:09:51.173: INFO: Controller my-hostname-basic-172c2368-9ed2-4c70-84c6-e9c6a4880f2b: Got expected result from replica 1 [my-hostname-basic-172c2368-9ed2-4c70-84c6-e9c6a4880f2b-8gvcl]: "my-hostname-basic-172c2368-9ed2-4c70-84c6-e9c6a4880f2b-8gvcl", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:09:51.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-581" for this suite.
Nov 16 20:09:59.211: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:10:01.117: INFO: namespace replication-controller-581 deletion completed in 9.933598176s

• [SLOW TEST:20.178 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:10:01.117: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 20:10:01.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 create -f - --namespace=kubectl-7750'
Nov 16 20:10:01.877: INFO: stderr: ""
Nov 16 20:10:01.877: INFO: stdout: "replicationcontroller/redis-master created\n"
Nov 16 20:10:01.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 create -f - --namespace=kubectl-7750'
Nov 16 20:10:02.444: INFO: stderr: ""
Nov 16 20:10:02.444: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Nov 16 20:10:03.460: INFO: Selector matched 1 pods for map[app:redis]
Nov 16 20:10:03.460: INFO: Found 0 / 1
Nov 16 20:10:04.453: INFO: Selector matched 1 pods for map[app:redis]
Nov 16 20:10:04.453: INFO: Found 1 / 1
Nov 16 20:10:04.453: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 16 20:10:04.461: INFO: Selector matched 1 pods for map[app:redis]
Nov 16 20:10:04.461: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 16 20:10:04.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 describe pod redis-master-9gndq --namespace=kubectl-7750'
Nov 16 20:10:04.611: INFO: stderr: ""
Nov 16 20:10:04.611: INFO: stdout: "Name:         redis-master-9gndq\nNamespace:    kubectl-7750\nPriority:     0\nNode:         10.240.167.254/10.240.167.254\nStart Time:   Mon, 16 Nov 2020 20:10:01 +0000\nLabels:       app=redis\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 172.30.216.231/32\n              cni.projectcalico.org/podIPs: 172.30.216.231/32\n              k8s.v1.cni.cncf.io/networks-status:\n                [{\n                    \"name\": \"k8s-pod-network\",\n                    \"ips\": [\n                        \"172.30.216.231\"\n                    ],\n                    \"dns\": {}\n                }]\n              openshift.io/scc: privileged\nStatus:       Running\nIP:           172.30.216.231\nIPs:\n  IP:           172.30.216.231\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   cri-o://e7d7ecbda31dc2a1884a70a4a443af35a10a96b99ba8aec5eea116124cef4f29\n    Image:          docker.io/library/redis:5.0.5-alpine\n    Image ID:       docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 16 Nov 2020 20:10:03 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-vwxsp (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-vwxsp:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-vwxsp\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From                     Message\n  ----    ------     ----       ----                     -------\n  Normal  Scheduled  <unknown>  default-scheduler        Successfully assigned kubectl-7750/redis-master-9gndq to 10.240.167.254\n  Normal  Pulled     2s         kubelet, 10.240.167.254  Container image \"docker.io/library/redis:5.0.5-alpine\" already present on machine\n  Normal  Created    1s         kubelet, 10.240.167.254  Created container redis-master\n  Normal  Started    1s         kubelet, 10.240.167.254  Started container redis-master\n"
Nov 16 20:10:04.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 describe rc redis-master --namespace=kubectl-7750'
Nov 16 20:10:04.764: INFO: stderr: ""
Nov 16 20:10:04.764: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-7750\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        docker.io/library/redis:5.0.5-alpine\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-9gndq\n"
Nov 16 20:10:04.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 describe service redis-master --namespace=kubectl-7750'
Nov 16 20:10:04.904: INFO: stderr: ""
Nov 16 20:10:04.904: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-7750\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                172.21.10.107\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.30.216.231:6379\nSession Affinity:  None\nEvents:            <none>\n"
Nov 16 20:10:04.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 describe node 10.240.167.206'
Nov 16 20:10:05.139: INFO: stderr: ""
Nov 16 20:10:05.139: INFO: stdout: "Name:               10.240.167.206\nRoles:              master,worker\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-de\n                    failure-domain.beta.kubernetes.io/zone=fra04\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=161.156.102.242\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.240.167.206\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=REDHAT_7_64\n                    ibm-cloud.kubernetes.io/region=eu-de\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-bupbauaf0p8tg6bo3mhg-kubee2epvgm-default-00000360\n                    ibm-cloud.kubernetes.io/worker-pool-id=bupbauaf0p8tg6bo3mhg-2a03369\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=4.3.40_1545_openshift\n                    ibm-cloud.kubernetes.io/zone=fra04\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.240.167.206\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    node.openshift.io/os_id=rhel\n                    privateVLAN=2723038\n                    publicVLAN=2722998\n                    topology.kubernetes.io/region=eu-de\n                    topology.kubernetes.io/zone=fra04\nAnnotations:        projectcalico.org/IPv4Address: 10.240.167.206/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.194.192\nCreationTimestamp:  Mon, 16 Nov 2020 17:39:31 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 16 Nov 2020 17:40:28 +0000   Mon, 16 Nov 2020 17:40:28 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 16 Nov 2020 20:10:02 +0000   Mon, 16 Nov 2020 17:39:31 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 16 Nov 2020 20:10:02 +0000   Mon, 16 Nov 2020 17:39:31 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 16 Nov 2020 20:10:02 +0000   Mon, 16 Nov 2020 17:39:31 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 16 Nov 2020 20:10:02 +0000   Mon, 16 Nov 2020 17:40:32 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.240.167.206\n  ExternalIP:  161.156.102.242\n  Hostname:    10.240.167.206\nCapacity:\n cpu:                4\n ephemeral-storage:  103078840Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             16260868Ki\n pods:               110\nAllocatable:\n cpu:                3910m\n ephemeral-storage:  100275095474\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             13484804Ki\n pods:               110\nSystem Info:\n Machine ID:                              accf713202284503b047a44e2b9c1e66\n System UUID:                             B5CB7076-1AEA-F46F-0B88-98341DB5CB32\n Boot ID:                                 a18624da-5a5f-483b-a73b-b4da841a7a26\n Kernel Version:                          3.10.0-1160.2.2.el7.x86_64\n OS Image:                                Red Hat\n Operating System:                        linux\n Architecture:                            amd64\n Container Runtime Version:               cri-o://1.16.6-18.rhaos4.3.git538d861.el7\n Kubelet Version:                         v1.16.2+853223d\n Kube-Proxy Version:                      v1.16.2+853223d\nProviderID:                               ibm://fee034388aa6435883a1f720010ab3a2///bupbauaf0p8tg6bo3mhg/kube-bupbauaf0p8tg6bo3mhg-kubee2epvgm-default-00000360\nNon-terminated Pods:                      (25 in total)\n  Namespace                               Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                               ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system                           calico-node-pk4fl                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         149m\n  calico-system                           calico-typha-b5486f777-9gw72                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         147m\n  default                                 test-k8s-e2e-pvg-master-verification                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         145m\n  kube-system                             ibm-keepalived-watcher-2t6l4                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         150m\n  kube-system                             ibm-master-proxy-static-10.240.167.206                     25m (0%)      300m (7%)   32M (0%)         512M (3%)      149m\n  kube-system                             ibmcloud-block-storage-driver-2s8xr                        50m (1%)      300m (7%)   100Mi (0%)       300Mi (2%)     150m\n  openshift-cluster-node-tuning-operator  tuned-9m9tq                                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         149m\n  openshift-cluster-samples-operator      cluster-samples-operator-55cf746658-rtxxg                  20m (0%)      0 (0%)      0 (0%)           0 (0%)         147m\n  openshift-console                       console-7f85d88fbc-4r5lv                                   10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         147m\n  openshift-dns                           dns-default-zqnbq                                          110m (2%)     0 (0%)      70Mi (0%)        512Mi (3%)     147m\n  openshift-image-registry                node-ca-2pgxt                                              10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         148m\n  openshift-ingress                       router-default-589c4b7b87-7299r                            100m (2%)     0 (0%)      256Mi (1%)       0 (0%)         148m\n  openshift-kube-proxy                    openshift-kube-proxy-5kpn9                                 100m (2%)     0 (0%)      200Mi (1%)       0 (0%)         150m\n  openshift-monitoring                    alertmanager-main-0                                        6m (0%)       0 (0%)      220Mi (1%)       0 (0%)         143m\n  openshift-monitoring                    kube-state-metrics-776f8894df-p8xsw                        4m (0%)       0 (0%)      120Mi (0%)       0 (0%)         148m\n  openshift-monitoring                    node-exporter-jssfh                                        9m (0%)       0 (0%)      210Mi (1%)       0 (0%)         148m\n  openshift-monitoring                    openshift-state-metrics-86c5b47587-nhfn8                   3m (0%)       0 (0%)      190Mi (1%)       0 (0%)         148m\n  openshift-monitoring                    prometheus-adapter-554fc6c4db-6qn5b                        1m (0%)       0 (0%)      20Mi (0%)        0 (0%)         144m\n  openshift-monitoring                    prometheus-k8s-0                                           76m (1%)      0 (0%)      1184Mi (8%)      0 (0%)         142m\n  openshift-monitoring                    thanos-querier-54b47b4fc4-rcx4j                            8m (0%)       0 (0%)      72Mi (0%)        0 (0%)         143m\n  openshift-multus                        multus-admission-controller-x6jqb                          10m (0%)      0 (0%)      0 (0%)           0 (0%)         149m\n  openshift-multus                        multus-gnk8t                                               10m (0%)      0 (0%)      150Mi (1%)       0 (0%)         150m\n  openshift-operator-lifecycle-manager    packageserver-68c9b7ddbb-rqt2z                             10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         146m\n  openshift-service-ca                    service-serving-cert-signer-787695f6b4-s4fqz               10m (0%)      0 (0%)      120Mi (0%)       0 (0%)         148m\n  sonobuoy                                sonobuoy-systemd-logs-daemon-set-18804f2d5c884991-lmbvb    0 (0%)        0 (0%)      0 (0%)           0 (0%)         55m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests         Limits\n  --------           --------         ------\n  cpu                587m (15%)       600m (15%)\n  memory             3238418Ki (24%)  1363443712 (9%)\n  ephemeral-storage  0 (0%)           0 (0%)\nEvents:\n  Type    Reason                   Age                  From                        Message\n  ----    ------                   ----                 ----                        -------\n  Normal  Starting                 150m                 kubelet, 10.240.167.206     Starting kubelet.\n  Normal  NodeAllocatableEnforced  150m                 kubelet, 10.240.167.206     Updated Node Allocatable limit across pods\n  Normal  NodeHasSufficientMemory  150m (x8 over 150m)  kubelet, 10.240.167.206     Node 10.240.167.206 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    150m (x8 over 150m)  kubelet, 10.240.167.206     Node 10.240.167.206 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     150m (x7 over 150m)  kubelet, 10.240.167.206     Node 10.240.167.206 status is now: NodeHasSufficientPID\n  Normal  Starting                 150m                 kube-proxy, 10.240.167.206  Starting kube-proxy.\n"
Nov 16 20:10:05.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 describe namespace kubectl-7750'
Nov 16 20:10:05.278: INFO: stderr: ""
Nov 16 20:10:05.278: INFO: stdout: "Name:         kubectl-7750\nLabels:       e2e-framework=kubectl\n              e2e-run=d57824ea-ee17-4ae5-aefb-7e90c824a524\nAnnotations:  openshift.io/sa.scc.mcs: s0:c54,c4\n              openshift.io/sa.scc.supplemental-groups: 1002870000/10000\n              openshift.io/sa.scc.uid-range: 1002870000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:10:05.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7750" for this suite.
Nov 16 20:10:35.318: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:10:37.223: INFO: namespace kubectl-7750 deletion completed in 31.934624394s

• [SLOW TEST:36.106 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1000
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:10:37.224: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 20:10:37.364: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-10498731-10fc-49e5-aef3-f1fc301a3943" in namespace "security-context-test-2509" to be "success or failure"
Nov 16 20:10:37.370: INFO: Pod "alpine-nnp-false-10498731-10fc-49e5-aef3-f1fc301a3943": Phase="Pending", Reason="", readiness=false. Elapsed: 6.146303ms
Nov 16 20:10:39.379: INFO: Pod "alpine-nnp-false-10498731-10fc-49e5-aef3-f1fc301a3943": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014423185s
Nov 16 20:10:41.387: INFO: Pod "alpine-nnp-false-10498731-10fc-49e5-aef3-f1fc301a3943": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022696629s
Nov 16 20:10:43.395: INFO: Pod "alpine-nnp-false-10498731-10fc-49e5-aef3-f1fc301a3943": Phase="Pending", Reason="", readiness=false. Elapsed: 6.031202724s
Nov 16 20:10:45.404: INFO: Pod "alpine-nnp-false-10498731-10fc-49e5-aef3-f1fc301a3943": Phase="Pending", Reason="", readiness=false. Elapsed: 8.039936819s
Nov 16 20:10:47.413: INFO: Pod "alpine-nnp-false-10498731-10fc-49e5-aef3-f1fc301a3943": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.048974489s
Nov 16 20:10:47.413: INFO: Pod "alpine-nnp-false-10498731-10fc-49e5-aef3-f1fc301a3943" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:10:47.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2509" for this suite.
Nov 16 20:10:55.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:10:57.406: INFO: namespace security-context-test-2509 deletion completed in 9.933350865s

• [SLOW TEST:20.183 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:277
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:10:57.407: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 20:10:57.530: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-5f6d4f60-c466-497f-b173-230f594d4d45
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-5f6d4f60-c466-497f-b173-230f594d4d45
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:12:08.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8645" for this suite.
Nov 16 20:12:22.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:12:24.238: INFO: namespace configmap-8645 deletion completed in 15.93334884s

• [SLOW TEST:86.832 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:12:24.239: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-9a2cb298-766e-4a6e-82e1-25e73b3b9dfa
STEP: Creating a pod to test consume configMaps
Nov 16 20:12:24.396: INFO: Waiting up to 5m0s for pod "pod-configmaps-191a7003-0e15-4cf1-be61-e5a7fcf1637a" in namespace "configmap-3268" to be "success or failure"
Nov 16 20:12:24.403: INFO: Pod "pod-configmaps-191a7003-0e15-4cf1-be61-e5a7fcf1637a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.651481ms
Nov 16 20:12:26.411: INFO: Pod "pod-configmaps-191a7003-0e15-4cf1-be61-e5a7fcf1637a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015043887s
STEP: Saw pod success
Nov 16 20:12:26.411: INFO: Pod "pod-configmaps-191a7003-0e15-4cf1-be61-e5a7fcf1637a" satisfied condition "success or failure"
Nov 16 20:12:26.418: INFO: Trying to get logs from node 10.240.167.254 pod pod-configmaps-191a7003-0e15-4cf1-be61-e5a7fcf1637a container configmap-volume-test: <nil>
STEP: delete the pod
Nov 16 20:12:26.465: INFO: Waiting for pod pod-configmaps-191a7003-0e15-4cf1-be61-e5a7fcf1637a to disappear
Nov 16 20:12:26.476: INFO: Pod pod-configmaps-191a7003-0e15-4cf1-be61-e5a7fcf1637a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:12:26.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3268" for this suite.
Nov 16 20:12:34.524: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:12:36.432: INFO: namespace configmap-3268 deletion completed in 9.938773954s

• [SLOW TEST:12.194 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:12:36.432: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Nov 16 20:12:39.144: INFO: Successfully updated pod "labelsupdate94d1de1d-8b36-4488-a496-db16c44cb48f"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:12:43.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7230" for this suite.
Nov 16 20:12:57.236: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:12:59.144: INFO: namespace projected-7230 deletion completed in 15.93322025s

• [SLOW TEST:22.712 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:12:59.145: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:13:59.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3380" for this suite.
Nov 16 20:14:29.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:14:31.283: INFO: namespace container-probe-3380 deletion completed in 31.951251773s

• [SLOW TEST:92.138 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:14:31.285: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:14:44.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7891" for this suite.
Nov 16 20:14:52.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:14:54.521: INFO: namespace resourcequota-7891 deletion completed in 9.945390004s

• [SLOW TEST:23.236 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:14:54.523: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Nov 16 20:14:58.770: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 16 20:14:58.777: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 16 20:15:00.778: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 16 20:15:00.785: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 16 20:15:02.778: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 16 20:15:02.786: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 16 20:15:04.778: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 16 20:15:04.786: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 16 20:15:06.778: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 16 20:15:06.787: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 16 20:15:08.778: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 16 20:15:08.786: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 16 20:15:10.778: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 16 20:15:10.786: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:15:10.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5952" for this suite.
Nov 16 20:15:24.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:15:26.766: INFO: namespace container-lifecycle-hook-5952 deletion completed in 15.935767883s

• [SLOW TEST:32.244 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:15:26.770: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:15:29.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9627" for this suite.
Nov 16 20:15:37.084: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:15:39.000: INFO: namespace emptydir-wrapper-9627 deletion completed in 9.947666796s

• [SLOW TEST:12.231 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:15:39.003: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Nov 16 20:15:39.190: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:15:42.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6251" for this suite.
Nov 16 20:15:50.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:15:52.622: INFO: namespace init-container-6251 deletion completed in 9.965507609s

• [SLOW TEST:13.619 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:15:52.624: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Nov 16 20:15:57.374: INFO: Successfully updated pod "annotationupdate3e714457-4edd-49ec-ba56-14fad502cd47"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:15:59.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4051" for this suite.
Nov 16 20:16:13.445: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:16:15.366: INFO: namespace projected-4051 deletion completed in 15.947812147s

• [SLOW TEST:22.742 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:16:15.366: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Nov 16 20:16:15.477: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:16:19.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3573" for this suite.
Nov 16 20:16:27.079: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:16:29.012: INFO: namespace init-container-3573 deletion completed in 9.960430557s

• [SLOW TEST:13.647 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:16:29.016: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 16 20:16:29.155: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0d46355e-177b-45aa-bf37-26a2e6518492" in namespace "downward-api-4950" to be "success or failure"
Nov 16 20:16:29.162: INFO: Pod "downwardapi-volume-0d46355e-177b-45aa-bf37-26a2e6518492": Phase="Pending", Reason="", readiness=false. Elapsed: 6.489064ms
Nov 16 20:16:31.170: INFO: Pod "downwardapi-volume-0d46355e-177b-45aa-bf37-26a2e6518492": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014649188s
STEP: Saw pod success
Nov 16 20:16:31.171: INFO: Pod "downwardapi-volume-0d46355e-177b-45aa-bf37-26a2e6518492" satisfied condition "success or failure"
Nov 16 20:16:31.178: INFO: Trying to get logs from node 10.240.167.206 pod downwardapi-volume-0d46355e-177b-45aa-bf37-26a2e6518492 container client-container: <nil>
STEP: delete the pod
Nov 16 20:16:31.247: INFO: Waiting for pod downwardapi-volume-0d46355e-177b-45aa-bf37-26a2e6518492 to disappear
Nov 16 20:16:31.253: INFO: Pod downwardapi-volume-0d46355e-177b-45aa-bf37-26a2e6518492 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:16:31.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4950" for this suite.
Nov 16 20:16:39.293: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:16:41.201: INFO: namespace downward-api-4950 deletion completed in 9.937385965s

• [SLOW TEST:12.186 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:16:41.202: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W1116 20:17:11.977069      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Nov 16 20:17:11.977: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:17:11.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4814" for this suite.
Nov 16 20:17:20.019: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:17:21.944: INFO: namespace gc-4814 deletion completed in 9.95541021s

• [SLOW TEST:40.742 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:17:21.945: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:17:33.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8673" for this suite.
Nov 16 20:17:41.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:17:43.157: INFO: namespace resourcequota-8673 deletion completed in 9.944644653s

• [SLOW TEST:21.212 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:17:43.157: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 20:17:43.733: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 16 20:17:45.760: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741154663, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741154663, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741154663, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741154663, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 20:17:48.810: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:17:48.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2465" for this suite.
Nov 16 20:17:56.966: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:17:58.903: INFO: namespace webhook-2465 deletion completed in 9.96522131s
STEP: Destroying namespace "webhook-2465-markers" for this suite.
Nov 16 20:18:06.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:18:08.852: INFO: namespace webhook-2465-markers deletion completed in 9.948239043s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:25.733 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:18:08.891: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Nov 16 20:18:09.012: INFO: Waiting up to 5m0s for pod "pod-cf95b786-332f-46d7-bff3-159137c7a877" in namespace "emptydir-7162" to be "success or failure"
Nov 16 20:18:09.018: INFO: Pod "pod-cf95b786-332f-46d7-bff3-159137c7a877": Phase="Pending", Reason="", readiness=false. Elapsed: 6.387422ms
Nov 16 20:18:11.026: INFO: Pod "pod-cf95b786-332f-46d7-bff3-159137c7a877": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014021628s
STEP: Saw pod success
Nov 16 20:18:11.026: INFO: Pod "pod-cf95b786-332f-46d7-bff3-159137c7a877" satisfied condition "success or failure"
Nov 16 20:18:11.033: INFO: Trying to get logs from node 10.240.167.254 pod pod-cf95b786-332f-46d7-bff3-159137c7a877 container test-container: <nil>
STEP: delete the pod
Nov 16 20:18:11.091: INFO: Waiting for pod pod-cf95b786-332f-46d7-bff3-159137c7a877 to disappear
Nov 16 20:18:11.098: INFO: Pod pod-cf95b786-332f-46d7-bff3-159137c7a877 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:18:11.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7162" for this suite.
Nov 16 20:18:19.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:18:21.042: INFO: namespace emptydir-7162 deletion completed in 9.932398403s

• [SLOW TEST:12.151 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:18:21.044: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Nov 16 20:18:21.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 create -f - --namespace=kubectl-2553'
Nov 16 20:18:21.593: INFO: stderr: ""
Nov 16 20:18:21.593: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 16 20:18:21.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2553'
Nov 16 20:18:21.724: INFO: stderr: ""
Nov 16 20:18:21.724: INFO: stdout: "update-demo-nautilus-t5pf6 update-demo-nautilus-wdgnr "
Nov 16 20:18:21.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods update-demo-nautilus-t5pf6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2553'
Nov 16 20:18:21.836: INFO: stderr: ""
Nov 16 20:18:21.836: INFO: stdout: ""
Nov 16 20:18:21.836: INFO: update-demo-nautilus-t5pf6 is created but not running
Nov 16 20:18:26.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2553'
Nov 16 20:18:26.947: INFO: stderr: ""
Nov 16 20:18:26.947: INFO: stdout: "update-demo-nautilus-t5pf6 update-demo-nautilus-wdgnr "
Nov 16 20:18:26.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods update-demo-nautilus-t5pf6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2553'
Nov 16 20:18:27.060: INFO: stderr: ""
Nov 16 20:18:27.060: INFO: stdout: ""
Nov 16 20:18:27.060: INFO: update-demo-nautilus-t5pf6 is created but not running
Nov 16 20:18:32.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2553'
Nov 16 20:18:32.177: INFO: stderr: ""
Nov 16 20:18:32.177: INFO: stdout: "update-demo-nautilus-t5pf6 update-demo-nautilus-wdgnr "
Nov 16 20:18:32.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods update-demo-nautilus-t5pf6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2553'
Nov 16 20:18:32.277: INFO: stderr: ""
Nov 16 20:18:32.277: INFO: stdout: "true"
Nov 16 20:18:32.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods update-demo-nautilus-t5pf6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2553'
Nov 16 20:18:32.386: INFO: stderr: ""
Nov 16 20:18:32.386: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 16 20:18:32.386: INFO: validating pod update-demo-nautilus-t5pf6
Nov 16 20:18:32.403: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 16 20:18:32.403: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 16 20:18:32.403: INFO: update-demo-nautilus-t5pf6 is verified up and running
Nov 16 20:18:32.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods update-demo-nautilus-wdgnr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2553'
Nov 16 20:18:32.517: INFO: stderr: ""
Nov 16 20:18:32.517: INFO: stdout: ""
Nov 16 20:18:32.517: INFO: update-demo-nautilus-wdgnr is created but not running
Nov 16 20:18:37.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2553'
Nov 16 20:18:37.636: INFO: stderr: ""
Nov 16 20:18:37.636: INFO: stdout: "update-demo-nautilus-t5pf6 update-demo-nautilus-wdgnr "
Nov 16 20:18:37.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods update-demo-nautilus-t5pf6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2553'
Nov 16 20:18:37.734: INFO: stderr: ""
Nov 16 20:18:37.734: INFO: stdout: "true"
Nov 16 20:18:37.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods update-demo-nautilus-t5pf6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2553'
Nov 16 20:18:37.854: INFO: stderr: ""
Nov 16 20:18:37.854: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 16 20:18:37.854: INFO: validating pod update-demo-nautilus-t5pf6
Nov 16 20:18:37.867: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 16 20:18:37.867: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 16 20:18:37.867: INFO: update-demo-nautilus-t5pf6 is verified up and running
Nov 16 20:18:37.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods update-demo-nautilus-wdgnr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2553'
Nov 16 20:18:37.987: INFO: stderr: ""
Nov 16 20:18:37.987: INFO: stdout: "true"
Nov 16 20:18:37.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods update-demo-nautilus-wdgnr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2553'
Nov 16 20:18:38.102: INFO: stderr: ""
Nov 16 20:18:38.102: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 16 20:18:38.102: INFO: validating pod update-demo-nautilus-wdgnr
Nov 16 20:18:38.121: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 16 20:18:38.122: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 16 20:18:38.122: INFO: update-demo-nautilus-wdgnr is verified up and running
STEP: scaling down the replication controller
Nov 16 20:18:38.125: INFO: scanned /root for discovery docs: <nil>
Nov 16 20:18:38.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-2553'
Nov 16 20:18:39.307: INFO: stderr: ""
Nov 16 20:18:39.307: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 16 20:18:39.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2553'
Nov 16 20:18:39.431: INFO: stderr: ""
Nov 16 20:18:39.431: INFO: stdout: "update-demo-nautilus-t5pf6 update-demo-nautilus-wdgnr "
STEP: Replicas for name=update-demo: expected=1 actual=2
Nov 16 20:18:44.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2553'
Nov 16 20:18:44.555: INFO: stderr: ""
Nov 16 20:18:44.555: INFO: stdout: "update-demo-nautilus-t5pf6 update-demo-nautilus-wdgnr "
STEP: Replicas for name=update-demo: expected=1 actual=2
Nov 16 20:18:49.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2553'
Nov 16 20:18:49.665: INFO: stderr: ""
Nov 16 20:18:49.665: INFO: stdout: "update-demo-nautilus-t5pf6 "
Nov 16 20:18:49.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods update-demo-nautilus-t5pf6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2553'
Nov 16 20:18:49.774: INFO: stderr: ""
Nov 16 20:18:49.774: INFO: stdout: "true"
Nov 16 20:18:49.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods update-demo-nautilus-t5pf6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2553'
Nov 16 20:18:49.882: INFO: stderr: ""
Nov 16 20:18:49.882: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 16 20:18:49.882: INFO: validating pod update-demo-nautilus-t5pf6
Nov 16 20:18:49.895: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 16 20:18:49.895: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 16 20:18:49.895: INFO: update-demo-nautilus-t5pf6 is verified up and running
STEP: scaling up the replication controller
Nov 16 20:18:49.899: INFO: scanned /root for discovery docs: <nil>
Nov 16 20:18:49.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-2553'
Nov 16 20:18:51.083: INFO: stderr: ""
Nov 16 20:18:51.083: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 16 20:18:51.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2553'
Nov 16 20:18:51.197: INFO: stderr: ""
Nov 16 20:18:51.197: INFO: stdout: "update-demo-nautilus-9pk7g update-demo-nautilus-t5pf6 "
Nov 16 20:18:51.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods update-demo-nautilus-9pk7g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2553'
Nov 16 20:18:51.305: INFO: stderr: ""
Nov 16 20:18:51.305: INFO: stdout: ""
Nov 16 20:18:51.306: INFO: update-demo-nautilus-9pk7g is created but not running
Nov 16 20:18:56.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2553'
Nov 16 20:18:56.415: INFO: stderr: ""
Nov 16 20:18:56.415: INFO: stdout: "update-demo-nautilus-9pk7g update-demo-nautilus-t5pf6 "
Nov 16 20:18:56.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods update-demo-nautilus-9pk7g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2553'
Nov 16 20:18:56.530: INFO: stderr: ""
Nov 16 20:18:56.530: INFO: stdout: ""
Nov 16 20:18:56.530: INFO: update-demo-nautilus-9pk7g is created but not running
Nov 16 20:19:01.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2553'
Nov 16 20:19:01.645: INFO: stderr: ""
Nov 16 20:19:01.645: INFO: stdout: "update-demo-nautilus-9pk7g update-demo-nautilus-t5pf6 "
Nov 16 20:19:01.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods update-demo-nautilus-9pk7g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2553'
Nov 16 20:19:01.766: INFO: stderr: ""
Nov 16 20:19:01.766: INFO: stdout: "true"
Nov 16 20:19:01.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods update-demo-nautilus-9pk7g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2553'
Nov 16 20:19:01.870: INFO: stderr: ""
Nov 16 20:19:01.870: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 16 20:19:01.871: INFO: validating pod update-demo-nautilus-9pk7g
Nov 16 20:19:01.887: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 16 20:19:01.887: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 16 20:19:01.887: INFO: update-demo-nautilus-9pk7g is verified up and running
Nov 16 20:19:01.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods update-demo-nautilus-t5pf6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2553'
Nov 16 20:19:01.991: INFO: stderr: ""
Nov 16 20:19:01.991: INFO: stdout: "true"
Nov 16 20:19:01.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods update-demo-nautilus-t5pf6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2553'
Nov 16 20:19:02.106: INFO: stderr: ""
Nov 16 20:19:02.106: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 16 20:19:02.106: INFO: validating pod update-demo-nautilus-t5pf6
Nov 16 20:19:02.119: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 16 20:19:02.119: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 16 20:19:02.119: INFO: update-demo-nautilus-t5pf6 is verified up and running
STEP: using delete to clean up resources
Nov 16 20:19:02.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 delete --grace-period=0 --force -f - --namespace=kubectl-2553'
Nov 16 20:19:02.261: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 16 20:19:02.261: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov 16 20:19:02.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2553'
Nov 16 20:19:02.403: INFO: stderr: "No resources found in kubectl-2553 namespace.\n"
Nov 16 20:19:02.403: INFO: stdout: ""
Nov 16 20:19:02.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods -l name=update-demo --namespace=kubectl-2553 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 16 20:19:02.531: INFO: stderr: ""
Nov 16 20:19:02.531: INFO: stdout: "update-demo-nautilus-9pk7g\nupdate-demo-nautilus-t5pf6\n"
Nov 16 20:19:03.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2553'
Nov 16 20:19:03.201: INFO: stderr: "No resources found in kubectl-2553 namespace.\n"
Nov 16 20:19:03.201: INFO: stdout: ""
Nov 16 20:19:03.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods -l name=update-demo --namespace=kubectl-2553 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 16 20:19:03.314: INFO: stderr: ""
Nov 16 20:19:03.314: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:19:03.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2553" for this suite.
Nov 16 20:19:17.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:19:19.262: INFO: namespace kubectl-2553 deletion completed in 15.934303299s

• [SLOW TEST:58.218 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:19:19.262: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Nov 16 20:19:19.409: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-121 /api/v1/namespaces/watch-121/configmaps/e2e-watch-test-watch-closed 3b6ab9a9-c69c-4e4c-b887-5ef86716b60b 70509 0 2020-11-16 20:19:19 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Nov 16 20:19:19.410: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-121 /api/v1/namespaces/watch-121/configmaps/e2e-watch-test-watch-closed 3b6ab9a9-c69c-4e4c-b887-5ef86716b60b 70513 0 2020-11-16 20:19:19 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Nov 16 20:19:19.454: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-121 /api/v1/namespaces/watch-121/configmaps/e2e-watch-test-watch-closed 3b6ab9a9-c69c-4e4c-b887-5ef86716b60b 70516 0 2020-11-16 20:19:19 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Nov 16 20:19:19.454: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-121 /api/v1/namespaces/watch-121/configmaps/e2e-watch-test-watch-closed 3b6ab9a9-c69c-4e4c-b887-5ef86716b60b 70517 0 2020-11-16 20:19:19 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:19:19.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-121" for this suite.
Nov 16 20:19:27.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:19:29.398: INFO: namespace watch-121 deletion completed in 9.932722012s

• [SLOW TEST:10.136 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:19:29.399: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service nodeport-test with type=NodePort in namespace services-9169
STEP: creating replication controller nodeport-test in namespace services-9169
I1116 20:19:29.563404      26 runners.go:184] Created replication controller with name: nodeport-test, namespace: services-9169, replica count: 2
Nov 16 20:19:32.614: INFO: Creating new exec pod
I1116 20:19:32.613935      26 runners.go:184] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 16 20:19:37.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=services-9169 execpodzgwlp -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Nov 16 20:19:37.975: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Nov 16 20:19:37.975: INFO: stdout: ""
Nov 16 20:19:37.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=services-9169 execpodzgwlp -- /bin/sh -x -c nc -zv -t -w 2 172.21.161.28 80'
Nov 16 20:19:38.264: INFO: stderr: "+ nc -zv -t -w 2 172.21.161.28 80\nConnection to 172.21.161.28 80 port [tcp/http] succeeded!\n"
Nov 16 20:19:38.264: INFO: stdout: ""
Nov 16 20:19:38.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=services-9169 execpodzgwlp -- /bin/sh -x -c nc -zv -t -w 2 10.240.167.206 31017'
Nov 16 20:19:38.542: INFO: stderr: "+ nc -zv -t -w 2 10.240.167.206 31017\nConnection to 10.240.167.206 31017 port [tcp/31017] succeeded!\n"
Nov 16 20:19:38.542: INFO: stdout: ""
Nov 16 20:19:38.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=services-9169 execpodzgwlp -- /bin/sh -x -c nc -zv -t -w 2 10.240.167.209 31017'
Nov 16 20:19:38.818: INFO: stderr: "+ nc -zv -t -w 2 10.240.167.209 31017\nConnection to 10.240.167.209 31017 port [tcp/31017] succeeded!\n"
Nov 16 20:19:38.818: INFO: stdout: ""
Nov 16 20:19:38.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=services-9169 execpodzgwlp -- /bin/sh -x -c nc -zv -t -w 2 161.156.102.242 31017'
Nov 16 20:19:39.094: INFO: stderr: "+ nc -zv -t -w 2 161.156.102.242 31017\nConnection to 161.156.102.242 31017 port [tcp/31017] succeeded!\n"
Nov 16 20:19:39.094: INFO: stdout: ""
Nov 16 20:19:39.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=services-9169 execpodzgwlp -- /bin/sh -x -c nc -zv -t -w 2 161.156.102.250 31017'
Nov 16 20:19:39.375: INFO: stderr: "+ nc -zv -t -w 2 161.156.102.250 31017\nConnection to 161.156.102.250 31017 port [tcp/31017] succeeded!\n"
Nov 16 20:19:39.375: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:19:39.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9169" for this suite.
Nov 16 20:19:47.416: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:19:49.328: INFO: namespace services-9169 deletion completed in 9.940528853s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:19.929 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:19:49.328: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Nov 16 20:19:49.461: INFO: Waiting up to 5m0s for pod "pod-12fc5ef1-338f-4572-8c87-c5160dc54b6f" in namespace "emptydir-1308" to be "success or failure"
Nov 16 20:19:49.469: INFO: Pod "pod-12fc5ef1-338f-4572-8c87-c5160dc54b6f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.63383ms
Nov 16 20:19:51.476: INFO: Pod "pod-12fc5ef1-338f-4572-8c87-c5160dc54b6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015248554s
STEP: Saw pod success
Nov 16 20:19:51.476: INFO: Pod "pod-12fc5ef1-338f-4572-8c87-c5160dc54b6f" satisfied condition "success or failure"
Nov 16 20:19:51.483: INFO: Trying to get logs from node 10.240.167.254 pod pod-12fc5ef1-338f-4572-8c87-c5160dc54b6f container test-container: <nil>
STEP: delete the pod
Nov 16 20:19:51.542: INFO: Waiting for pod pod-12fc5ef1-338f-4572-8c87-c5160dc54b6f to disappear
Nov 16 20:19:51.548: INFO: Pod pod-12fc5ef1-338f-4572-8c87-c5160dc54b6f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:19:51.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1308" for this suite.
Nov 16 20:19:59.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:20:01.492: INFO: namespace emptydir-1308 deletion completed in 9.933758468s

• [SLOW TEST:12.164 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:20:01.493: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Nov 16 20:20:06.223: INFO: Successfully updated pod "labelsupdate0e1832a9-61fa-41dc-a85e-d6678d5230f8"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:20:08.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1776" for this suite.
Nov 16 20:20:38.301: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:20:40.207: INFO: namespace downward-api-1776 deletion completed in 31.93355948s

• [SLOW TEST:38.714 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:20:40.208: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 20:20:42.415: INFO: Waiting up to 5m0s for pod "client-envvars-3122f1da-efd7-4745-adf5-3d32569688f2" in namespace "pods-3573" to be "success or failure"
Nov 16 20:20:42.424: INFO: Pod "client-envvars-3122f1da-efd7-4745-adf5-3d32569688f2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.388471ms
Nov 16 20:20:44.436: INFO: Pod "client-envvars-3122f1da-efd7-4745-adf5-3d32569688f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020954451s
Nov 16 20:20:46.443: INFO: Pod "client-envvars-3122f1da-efd7-4745-adf5-3d32569688f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028069931s
STEP: Saw pod success
Nov 16 20:20:46.443: INFO: Pod "client-envvars-3122f1da-efd7-4745-adf5-3d32569688f2" satisfied condition "success or failure"
Nov 16 20:20:46.449: INFO: Trying to get logs from node 10.240.167.206 pod client-envvars-3122f1da-efd7-4745-adf5-3d32569688f2 container env3cont: <nil>
STEP: delete the pod
Nov 16 20:20:46.507: INFO: Waiting for pod client-envvars-3122f1da-efd7-4745-adf5-3d32569688f2 to disappear
Nov 16 20:20:46.514: INFO: Pod client-envvars-3122f1da-efd7-4745-adf5-3d32569688f2 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:20:46.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3573" for this suite.
Nov 16 20:21:16.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:21:18.457: INFO: namespace pods-3573 deletion completed in 31.933239366s

• [SLOW TEST:38.249 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:21:18.458: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 20:21:18.587: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:21:19.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3409" for this suite.
Nov 16 20:21:27.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:21:29.589: INFO: namespace custom-resource-definition-3409 deletion completed in 9.93519714s

• [SLOW TEST:11.132 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:21:29.590: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's command
Nov 16 20:21:29.714: INFO: Waiting up to 5m0s for pod "var-expansion-3820700f-0453-4735-ac90-a77738c082d2" in namespace "var-expansion-9297" to be "success or failure"
Nov 16 20:21:29.721: INFO: Pod "var-expansion-3820700f-0453-4735-ac90-a77738c082d2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.176161ms
Nov 16 20:21:31.729: INFO: Pod "var-expansion-3820700f-0453-4735-ac90-a77738c082d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014946266s
STEP: Saw pod success
Nov 16 20:21:31.729: INFO: Pod "var-expansion-3820700f-0453-4735-ac90-a77738c082d2" satisfied condition "success or failure"
Nov 16 20:21:31.736: INFO: Trying to get logs from node 10.240.167.254 pod var-expansion-3820700f-0453-4735-ac90-a77738c082d2 container dapi-container: <nil>
STEP: delete the pod
Nov 16 20:21:31.782: INFO: Waiting for pod var-expansion-3820700f-0453-4735-ac90-a77738c082d2 to disappear
Nov 16 20:21:31.791: INFO: Pod var-expansion-3820700f-0453-4735-ac90-a77738c082d2 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:21:31.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9297" for this suite.
Nov 16 20:21:39.830: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:21:41.756: INFO: namespace var-expansion-9297 deletion completed in 9.95555753s

• [SLOW TEST:12.167 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:21:41.756: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-db4cf346-9940-4e6a-b332-060284557a82
STEP: Creating a pod to test consume secrets
Nov 16 20:21:41.920: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3c541caf-0cd5-4e2e-bc24-3d334846c1f3" in namespace "projected-789" to be "success or failure"
Nov 16 20:21:41.936: INFO: Pod "pod-projected-secrets-3c541caf-0cd5-4e2e-bc24-3d334846c1f3": Phase="Pending", Reason="", readiness=false. Elapsed: 15.65983ms
Nov 16 20:21:43.945: INFO: Pod "pod-projected-secrets-3c541caf-0cd5-4e2e-bc24-3d334846c1f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024198712s
STEP: Saw pod success
Nov 16 20:21:43.945: INFO: Pod "pod-projected-secrets-3c541caf-0cd5-4e2e-bc24-3d334846c1f3" satisfied condition "success or failure"
Nov 16 20:21:43.952: INFO: Trying to get logs from node 10.240.167.254 pod pod-projected-secrets-3c541caf-0cd5-4e2e-bc24-3d334846c1f3 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 16 20:21:43.994: INFO: Waiting for pod pod-projected-secrets-3c541caf-0cd5-4e2e-bc24-3d334846c1f3 to disappear
Nov 16 20:21:44.000: INFO: Pod pod-projected-secrets-3c541caf-0cd5-4e2e-bc24-3d334846c1f3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:21:44.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-789" for this suite.
Nov 16 20:21:52.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:21:53.944: INFO: namespace projected-789 deletion completed in 9.933289247s

• [SLOW TEST:12.188 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:21:53.944: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Nov 16 20:21:54.171: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 16 20:21:54.215: INFO: Waiting for terminating namespaces to be deleted...
Nov 16 20:21:54.227: INFO: 
Logging pods the kubelet thinks is on node 10.240.167.206 before test
Nov 16 20:21:54.295: INFO: tuned-9m9tq from openshift-cluster-node-tuning-operator started at 2020-11-16 17:40:38 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.295: INFO: 	Container tuned ready: true, restart count 0
Nov 16 20:21:54.295: INFO: node-ca-2pgxt from openshift-image-registry started at 2020-11-16 17:41:42 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.295: INFO: 	Container node-ca ready: true, restart count 0
Nov 16 20:21:54.295: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-11-16 17:47:07 +0000 UTC (7 container statuses recorded)
Nov 16 20:21:54.295: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 20:21:54.295: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 20:21:54.295: INFO: 	Container prometheus ready: true, restart count 1
Nov 16 20:21:54.295: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Nov 16 20:21:54.295: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov 16 20:21:54.295: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Nov 16 20:21:54.295: INFO: 	Container thanos-sidecar ready: true, restart count 0
Nov 16 20:21:54.295: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-11-16 17:46:19 +0000 UTC (3 container statuses recorded)
Nov 16 20:21:54.295: INFO: 	Container alertmanager ready: true, restart count 0
Nov 16 20:21:54.295: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 16 20:21:54.295: INFO: 	Container config-reloader ready: true, restart count 0
Nov 16 20:21:54.295: INFO: sonobuoy-systemd-logs-daemon-set-18804f2d5c884991-lmbvb from sonobuoy started at 2020-11-16 19:14:33 +0000 UTC (2 container statuses recorded)
Nov 16 20:21:54.295: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Nov 16 20:21:54.295: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 16 20:21:54.295: INFO: ibm-keepalived-watcher-2t6l4 from kube-system started at 2020-11-16 17:39:32 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.295: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 20:21:54.295: INFO: kube-state-metrics-776f8894df-p8xsw from openshift-monitoring started at 2020-11-16 17:41:15 +0000 UTC (3 container statuses recorded)
Nov 16 20:21:54.295: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Nov 16 20:21:54.295: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Nov 16 20:21:54.295: INFO: 	Container kube-state-metrics ready: true, restart count 0
Nov 16 20:21:54.295: INFO: packageserver-68c9b7ddbb-rqt2z from openshift-operator-lifecycle-manager started at 2020-11-16 17:44:04 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.295: INFO: 	Container packageserver ready: true, restart count 0
Nov 16 20:21:54.295: INFO: dns-default-zqnbq from openshift-dns started at 2020-11-16 17:42:09 +0000 UTC (2 container statuses recorded)
Nov 16 20:21:54.295: INFO: 	Container dns ready: true, restart count 0
Nov 16 20:21:54.295: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 16 20:21:54.295: INFO: calico-node-pk4fl from calico-system started at 2020-11-16 17:40:12 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.295: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 20:21:54.295: INFO: cluster-samples-operator-55cf746658-rtxxg from openshift-cluster-samples-operator started at 2020-11-16 17:42:11 +0000 UTC (2 container statuses recorded)
Nov 16 20:21:54.295: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Nov 16 20:21:54.295: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Nov 16 20:21:54.295: INFO: console-7f85d88fbc-4r5lv from openshift-console started at 2020-11-16 17:42:45 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.295: INFO: 	Container console ready: true, restart count 0
Nov 16 20:21:54.295: INFO: ibm-master-proxy-static-10.240.167.206 from kube-system started at 2020-11-16 17:39:29 +0000 UTC (2 container statuses recorded)
Nov 16 20:21:54.295: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 20:21:54.295: INFO: 	Container pause ready: true, restart count 0
Nov 16 20:21:54.295: INFO: service-serving-cert-signer-787695f6b4-s4fqz from openshift-service-ca started at 2020-11-16 17:41:13 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.295: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
Nov 16 20:21:54.295: INFO: node-exporter-jssfh from openshift-monitoring started at 2020-11-16 17:41:17 +0000 UTC (2 container statuses recorded)
Nov 16 20:21:54.295: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 20:21:54.295: INFO: 	Container node-exporter ready: true, restart count 0
Nov 16 20:21:54.295: INFO: router-default-589c4b7b87-7299r from openshift-ingress started at 2020-11-16 17:41:37 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.295: INFO: 	Container router ready: true, restart count 0
Nov 16 20:21:54.295: INFO: multus-gnk8t from openshift-multus started at 2020-11-16 17:39:32 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.295: INFO: 	Container kube-multus ready: true, restart count 0
Nov 16 20:21:54.295: INFO: ibmcloud-block-storage-driver-2s8xr from kube-system started at 2020-11-16 17:39:34 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.295: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 16 20:21:54.295: INFO: prometheus-adapter-554fc6c4db-6qn5b from openshift-monitoring started at 2020-11-16 17:45:54 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.295: INFO: 	Container prometheus-adapter ready: true, restart count 0
Nov 16 20:21:54.295: INFO: openshift-state-metrics-86c5b47587-nhfn8 from openshift-monitoring started at 2020-11-16 17:41:13 +0000 UTC (3 container statuses recorded)
Nov 16 20:21:54.295: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Nov 16 20:21:54.295: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Nov 16 20:21:54.295: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Nov 16 20:21:54.295: INFO: calico-typha-b5486f777-9gw72 from calico-system started at 2020-11-16 17:42:09 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.295: INFO: 	Container calico-typha ready: true, restart count 0
Nov 16 20:21:54.295: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-11-16 17:44:46 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.295: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Nov 16 20:21:54.295: INFO: thanos-querier-54b47b4fc4-rcx4j from openshift-monitoring started at 2020-11-16 17:46:47 +0000 UTC (4 container statuses recorded)
Nov 16 20:21:54.295: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 20:21:54.295: INFO: 	Container oauth-proxy ready: true, restart count 0
Nov 16 20:21:54.295: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 20:21:54.295: INFO: 	Container thanos-querier ready: true, restart count 0
Nov 16 20:21:54.295: INFO: openshift-kube-proxy-5kpn9 from openshift-kube-proxy started at 2020-11-16 17:39:32 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.295: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 16 20:21:54.295: INFO: multus-admission-controller-x6jqb from openshift-multus started at 2020-11-16 17:40:32 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.295: INFO: 	Container multus-admission-controller ready: true, restart count 0
Nov 16 20:21:54.295: INFO: 
Logging pods the kubelet thinks is on node 10.240.167.209 before test
Nov 16 20:21:54.372: INFO: image-registry-77d48f6786-gqts2 from openshift-image-registry started at 2020-11-16 17:44:28 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.372: INFO: 	Container registry ready: true, restart count 0
Nov 16 20:21:54.372: INFO: console-7f85d88fbc-fdc4m from openshift-console started at 2020-11-16 17:41:44 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.372: INFO: 	Container console ready: true, restart count 0
Nov 16 20:21:54.372: INFO: dns-default-m5gqp from openshift-dns started at 2020-11-16 17:42:09 +0000 UTC (2 container statuses recorded)
Nov 16 20:21:54.373: INFO: 	Container dns ready: true, restart count 0
Nov 16 20:21:54.373: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 16 20:21:54.373: INFO: ibm-cloud-provider-ip-161-156-99-92-596ff786f9-gqmvr from ibm-system started at 2020-11-16 17:46:26 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.373: INFO: 	Container ibm-cloud-provider-ip-161-156-99-92 ready: true, restart count 0
Nov 16 20:21:54.373: INFO: ibmcloud-block-storage-driver-6zlqm from kube-system started at 2020-11-16 17:39:27 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.373: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 16 20:21:54.373: INFO: calico-typha-b5486f777-7g7gg from calico-system started at 2020-11-16 17:40:11 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.373: INFO: 	Container calico-typha ready: true, restart count 0
Nov 16 20:21:54.373: INFO: tuned-cp7vt from openshift-cluster-node-tuning-operator started at 2020-11-16 17:40:38 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.373: INFO: 	Container tuned ready: true, restart count 0
Nov 16 20:21:54.373: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-11-16 17:46:06 +0000 UTC (3 container statuses recorded)
Nov 16 20:21:54.373: INFO: 	Container alertmanager ready: true, restart count 0
Nov 16 20:21:54.373: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 16 20:21:54.373: INFO: 	Container config-reloader ready: true, restart count 0
Nov 16 20:21:54.373: INFO: packageserver-68c9b7ddbb-phc28 from openshift-operator-lifecycle-manager started at 2020-11-16 17:44:00 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.374: INFO: 	Container packageserver ready: true, restart count 0
Nov 16 20:21:54.374: INFO: thanos-querier-54b47b4fc4-zvlf7 from openshift-monitoring started at 2020-11-16 17:46:57 +0000 UTC (4 container statuses recorded)
Nov 16 20:21:54.374: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 20:21:54.374: INFO: 	Container oauth-proxy ready: true, restart count 0
Nov 16 20:21:54.374: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 20:21:54.374: INFO: 	Container thanos-querier ready: true, restart count 0
Nov 16 20:21:54.374: INFO: calico-node-tc4r7 from calico-system started at 2020-11-16 17:40:12 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.374: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 20:21:54.374: INFO: redhat-operators-7f9c6c576-rs2f7 from openshift-marketplace started at 2020-11-16 17:42:33 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.374: INFO: 	Container redhat-operators ready: true, restart count 0
Nov 16 20:21:54.374: INFO: router-default-589c4b7b87-74cdn from openshift-ingress started at 2020-11-16 17:41:37 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.374: INFO: 	Container router ready: true, restart count 0
Nov 16 20:21:54.374: INFO: prometheus-operator-7646fdcb7b-tw6jj from openshift-monitoring started at 2020-11-16 17:45:42 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.374: INFO: 	Container prometheus-operator ready: true, restart count 0
Nov 16 20:21:54.374: INFO: grafana-58c46d5bb-hs8tj from openshift-monitoring started at 2020-11-16 17:46:00 +0000 UTC (2 container statuses recorded)
Nov 16 20:21:54.374: INFO: 	Container grafana ready: true, restart count 0
Nov 16 20:21:54.375: INFO: 	Container grafana-proxy ready: true, restart count 0
Nov 16 20:21:54.375: INFO: sonobuoy-systemd-logs-daemon-set-18804f2d5c884991-rfhks from sonobuoy started at 2020-11-16 19:14:33 +0000 UTC (2 container statuses recorded)
Nov 16 20:21:54.375: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Nov 16 20:21:54.375: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 16 20:21:54.375: INFO: node-exporter-lqwgh from openshift-monitoring started at 2020-11-16 17:41:17 +0000 UTC (2 container statuses recorded)
Nov 16 20:21:54.375: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 20:21:54.375: INFO: 	Container node-exporter ready: true, restart count 0
Nov 16 20:21:54.375: INFO: multus-admission-controller-tf7n7 from openshift-multus started at 2020-11-16 17:40:30 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.375: INFO: 	Container multus-admission-controller ready: true, restart count 0
Nov 16 20:21:54.375: INFO: configmap-cabundle-injector-5bd6fcf58-hlwpk from openshift-service-ca started at 2020-11-16 17:41:14 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.375: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
Nov 16 20:21:54.375: INFO: ibm-keepalived-watcher-x4nqc from kube-system started at 2020-11-16 17:39:19 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.375: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 20:21:54.375: INFO: openshift-kube-proxy-qfwg7 from openshift-kube-proxy started at 2020-11-16 17:39:31 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.375: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 16 20:21:54.375: INFO: apiservice-cabundle-injector-75dcd4f8db-sllzv from openshift-service-ca started at 2020-11-16 17:41:13 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.376: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
Nov 16 20:21:54.376: INFO: community-operators-587cdbd4fb-5mwwt from openshift-marketplace started at 2020-11-16 17:42:34 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.376: INFO: 	Container community-operators ready: true, restart count 0
Nov 16 20:21:54.376: INFO: vpn-679798bf87-s5h4z from kube-system started at 2020-11-16 17:44:16 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.376: INFO: 	Container vpn ready: true, restart count 0
Nov 16 20:21:54.376: INFO: prometheus-adapter-554fc6c4db-8f7cc from openshift-monitoring started at 2020-11-16 17:45:54 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.376: INFO: 	Container prometheus-adapter ready: true, restart count 0
Nov 16 20:21:54.376: INFO: ibm-master-proxy-static-10.240.167.209 from kube-system started at 2020-11-16 17:39:17 +0000 UTC (2 container statuses recorded)
Nov 16 20:21:54.376: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 20:21:54.376: INFO: 	Container pause ready: true, restart count 0
Nov 16 20:21:54.376: INFO: node-ca-mg4vg from openshift-image-registry started at 2020-11-16 17:41:42 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.376: INFO: 	Container node-ca ready: true, restart count 0
Nov 16 20:21:54.376: INFO: certified-operators-676df69595-kgtfx from openshift-marketplace started at 2020-11-16 17:42:33 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.376: INFO: 	Container certified-operators ready: true, restart count 0
Nov 16 20:21:54.377: INFO: registry-pvc-permissions-l4jgc from openshift-image-registry started at 2020-11-16 17:44:28 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.377: INFO: 	Container pvc-permissions ready: false, restart count 0
Nov 16 20:21:54.377: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-11-16 17:46:57 +0000 UTC (7 container statuses recorded)
Nov 16 20:21:54.377: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 20:21:54.377: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 20:21:54.377: INFO: 	Container prometheus ready: true, restart count 1
Nov 16 20:21:54.377: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Nov 16 20:21:54.377: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov 16 20:21:54.377: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Nov 16 20:21:54.377: INFO: 	Container thanos-sidecar ready: true, restart count 0
Nov 16 20:21:54.377: INFO: multus-dfpk4 from openshift-multus started at 2020-11-16 17:39:24 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.377: INFO: 	Container kube-multus ready: true, restart count 0
Nov 16 20:21:54.377: INFO: 
Logging pods the kubelet thinks is on node 10.240.167.254 before test
Nov 16 20:21:54.452: INFO: downloads-5dbb4f4ff7-mtjcb from openshift-console started at 2020-11-16 17:40:25 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.452: INFO: 	Container download-server ready: true, restart count 0
Nov 16 20:21:54.452: INFO: downloads-5dbb4f4ff7-krzbx from openshift-console started at 2020-11-16 17:40:25 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.453: INFO: 	Container download-server ready: true, restart count 0
Nov 16 20:21:54.453: INFO: node-exporter-4kz5x from openshift-monitoring started at 2020-11-16 17:41:17 +0000 UTC (2 container statuses recorded)
Nov 16 20:21:54.453: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 20:21:54.453: INFO: 	Container node-exporter ready: true, restart count 0
Nov 16 20:21:54.453: INFO: sonobuoy-systemd-logs-daemon-set-18804f2d5c884991-ffhv2 from sonobuoy started at 2020-11-16 19:14:33 +0000 UTC (2 container statuses recorded)
Nov 16 20:21:54.453: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Nov 16 20:21:54.453: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 16 20:21:54.453: INFO: network-operator-74fc599569-c45s6 from openshift-network-operator started at 2020-11-16 17:39:02 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.453: INFO: 	Container network-operator ready: true, restart count 0
Nov 16 20:21:54.453: INFO: ibmcloud-block-storage-driver-qdh9z from kube-system started at 2020-11-16 17:39:02 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.453: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 16 20:21:54.453: INFO: calico-node-s6qzg from calico-system started at 2020-11-16 17:40:12 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.453: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 20:21:54.453: INFO: cluster-monitoring-operator-7d44956445-lqkh9 from openshift-monitoring started at 2020-11-16 17:40:24 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.454: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Nov 16 20:21:54.454: INFO: cluster-image-registry-operator-8d47696c5-hpv5z from openshift-image-registry started at 2020-11-16 17:40:25 +0000 UTC (2 container statuses recorded)
Nov 16 20:21:54.454: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Nov 16 20:21:54.454: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
Nov 16 20:21:54.454: INFO: tuned-n7h6s from openshift-cluster-node-tuning-operator started at 2020-11-16 17:40:38 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.454: INFO: 	Container tuned ready: true, restart count 0
Nov 16 20:21:54.454: INFO: telemeter-client-95c48d495-ctf9b from openshift-monitoring started at 2020-11-16 17:45:52 +0000 UTC (3 container statuses recorded)
Nov 16 20:21:54.454: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 20:21:54.454: INFO: 	Container reload ready: true, restart count 0
Nov 16 20:21:54.454: INFO: 	Container telemeter-client ready: true, restart count 0
Nov 16 20:21:54.454: INFO: openshift-kube-proxy-smf7r from openshift-kube-proxy started at 2020-11-16 17:39:31 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.454: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 16 20:21:54.454: INFO: openshift-service-catalog-apiserver-operator-86b98d6fff-znhvn from openshift-service-catalog-apiserver-operator started at 2020-11-16 17:40:25 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.455: INFO: 	Container operator ready: true, restart count 1
Nov 16 20:21:54.455: INFO: sonobuoy from sonobuoy started at 2020-11-16 19:14:22 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.455: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 16 20:21:54.455: INFO: sonobuoy-e2e-job-b6bf7e56608b4535 from sonobuoy started at 2020-11-16 19:14:33 +0000 UTC (2 container statuses recorded)
Nov 16 20:21:54.455: INFO: 	Container e2e ready: true, restart count 0
Nov 16 20:21:54.455: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 20:21:54.455: INFO: multus-gf29b from openshift-multus started at 2020-11-16 17:39:24 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.455: INFO: 	Container kube-multus ready: true, restart count 0
Nov 16 20:21:54.455: INFO: marketplace-operator-54847664c-cbqsc from openshift-marketplace started at 2020-11-16 17:40:25 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.455: INFO: 	Container marketplace-operator ready: true, restart count 0
Nov 16 20:21:54.455: INFO: calico-kube-controllers-599969f895-4vf88 from calico-system started at 2020-11-16 17:40:25 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.455: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Nov 16 20:21:54.456: INFO: ibmcloud-block-storage-plugin-79495594d5-6wqx5 from kube-system started at 2020-11-16 17:40:25 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.456: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Nov 16 20:21:54.456: INFO: dns-default-rtvgx from openshift-dns started at 2020-11-16 17:42:09 +0000 UTC (2 container statuses recorded)
Nov 16 20:21:54.456: INFO: 	Container dns ready: true, restart count 0
Nov 16 20:21:54.456: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 16 20:21:54.456: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-11-16 17:45:54 +0000 UTC (3 container statuses recorded)
Nov 16 20:21:54.456: INFO: 	Container alertmanager ready: true, restart count 0
Nov 16 20:21:54.456: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 16 20:21:54.456: INFO: 	Container config-reloader ready: true, restart count 0
Nov 16 20:21:54.456: INFO: ibm-cloud-provider-ip-161-156-99-92-596ff786f9-8zlk8 from ibm-system started at 2020-11-16 17:46:19 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.457: INFO: 	Container ibm-cloud-provider-ip-161-156-99-92 ready: true, restart count 0
Nov 16 20:21:54.457: INFO: console-operator-c9844474-csp4h from openshift-console-operator started at 2020-11-16 17:40:25 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.457: INFO: 	Container console-operator ready: true, restart count 1
Nov 16 20:21:54.457: INFO: catalog-operator-86d68f4684-n8kk4 from openshift-operator-lifecycle-manager started at 2020-11-16 17:40:27 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.457: INFO: 	Container catalog-operator ready: true, restart count 0
Nov 16 20:21:54.457: INFO: calico-typha-b5486f777-km6dh from calico-system started at 2020-11-16 17:42:09 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.457: INFO: 	Container calico-typha ready: true, restart count 0
Nov 16 20:21:54.457: INFO: ibm-master-proxy-static-10.240.167.254 from kube-system started at 2020-11-16 17:38:51 +0000 UTC (2 container statuses recorded)
Nov 16 20:21:54.457: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 20:21:54.457: INFO: 	Container pause ready: true, restart count 0
Nov 16 20:21:54.457: INFO: ibm-keepalived-watcher-gw7ng from kube-system started at 2020-11-16 17:38:54 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.458: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 20:21:54.458: INFO: service-ca-operator-5db7bd4f-qmnrv from openshift-service-ca-operator started at 2020-11-16 17:40:25 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.458: INFO: 	Container operator ready: true, restart count 0
Nov 16 20:21:54.458: INFO: ingress-operator-57d688547-8j97m from openshift-ingress-operator started at 2020-11-16 17:40:25 +0000 UTC (2 container statuses recorded)
Nov 16 20:21:54.458: INFO: 	Container ingress-operator ready: true, restart count 0
Nov 16 20:21:54.458: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 20:21:54.458: INFO: ibm-file-plugin-75bbff878-w8grr from kube-system started at 2020-11-16 17:40:27 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.458: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Nov 16 20:21:54.458: INFO: tigera-operator-798cfbf7dd-6rmk9 from tigera-operator started at 2020-11-16 17:39:02 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.458: INFO: 	Container tigera-operator ready: true, restart count 2
Nov 16 20:21:54.459: INFO: openshift-service-catalog-controller-manager-operator-595fxjf2f from openshift-service-catalog-controller-manager-operator started at 2020-11-16 17:40:24 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.459: INFO: 	Container operator ready: true, restart count 1
Nov 16 20:21:54.459: INFO: olm-operator-88bfd79df-7mm7b from openshift-operator-lifecycle-manager started at 2020-11-16 17:40:24 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.459: INFO: 	Container olm-operator ready: true, restart count 0
Nov 16 20:21:54.459: INFO: ibm-storage-watcher-69d9c445b4-k5bvh from kube-system started at 2020-11-16 17:40:24 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.459: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Nov 16 20:21:54.459: INFO: cluster-storage-operator-6488c9f77b-wq8vv from openshift-cluster-storage-operator started at 2020-11-16 17:40:27 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.459: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Nov 16 20:21:54.459: INFO: dns-operator-847f45b78c-49hz4 from openshift-dns-operator started at 2020-11-16 17:40:24 +0000 UTC (2 container statuses recorded)
Nov 16 20:21:54.459: INFO: 	Container dns-operator ready: true, restart count 0
Nov 16 20:21:54.459: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 20:21:54.459: INFO: cluster-node-tuning-operator-5c44ccc99b-6g94b from openshift-cluster-node-tuning-operator started at 2020-11-16 17:40:25 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.459: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Nov 16 20:21:54.459: INFO: multus-admission-controller-xw5h7 from openshift-multus started at 2020-11-16 17:40:25 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.460: INFO: 	Container multus-admission-controller ready: true, restart count 0
Nov 16 20:21:54.460: INFO: node-ca-852zk from openshift-image-registry started at 2020-11-16 17:41:42 +0000 UTC (1 container statuses recorded)
Nov 16 20:21:54.460: INFO: 	Container node-ca ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-50a0e3af-3e46-44fa-821c-8426ee35d166 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-50a0e3af-3e46-44fa-821c-8426ee35d166 off the node 10.240.167.254
STEP: verifying the node doesn't have the label kubernetes.io/e2e-50a0e3af-3e46-44fa-821c-8426ee35d166
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:21:58.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5422" for this suite.
Nov 16 20:22:12.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:22:14.589: INFO: namespace sched-pred-5422 deletion completed in 15.936171915s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:20.645 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:22:14.603: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the initial replication controller
Nov 16 20:22:14.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 create -f - --namespace=kubectl-5085'
Nov 16 20:22:15.447: INFO: stderr: ""
Nov 16 20:22:15.447: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 16 20:22:15.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5085'
Nov 16 20:22:15.599: INFO: stderr: ""
Nov 16 20:22:15.599: INFO: stdout: "update-demo-nautilus-f9zv5 update-demo-nautilus-tg574 "
Nov 16 20:22:15.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods update-demo-nautilus-f9zv5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5085'
Nov 16 20:22:15.724: INFO: stderr: ""
Nov 16 20:22:15.724: INFO: stdout: ""
Nov 16 20:22:15.724: INFO: update-demo-nautilus-f9zv5 is created but not running
Nov 16 20:22:20.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5085'
Nov 16 20:22:20.843: INFO: stderr: ""
Nov 16 20:22:20.843: INFO: stdout: "update-demo-nautilus-f9zv5 update-demo-nautilus-tg574 "
Nov 16 20:22:20.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods update-demo-nautilus-f9zv5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5085'
Nov 16 20:22:20.954: INFO: stderr: ""
Nov 16 20:22:20.955: INFO: stdout: "true"
Nov 16 20:22:20.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods update-demo-nautilus-f9zv5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5085'
Nov 16 20:22:21.070: INFO: stderr: ""
Nov 16 20:22:21.070: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 16 20:22:21.070: INFO: validating pod update-demo-nautilus-f9zv5
Nov 16 20:22:21.088: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 16 20:22:21.088: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 16 20:22:21.088: INFO: update-demo-nautilus-f9zv5 is verified up and running
Nov 16 20:22:21.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods update-demo-nautilus-tg574 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5085'
Nov 16 20:22:21.200: INFO: stderr: ""
Nov 16 20:22:21.200: INFO: stdout: "true"
Nov 16 20:22:21.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods update-demo-nautilus-tg574 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5085'
Nov 16 20:22:21.300: INFO: stderr: ""
Nov 16 20:22:21.300: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 16 20:22:21.300: INFO: validating pod update-demo-nautilus-tg574
Nov 16 20:22:21.322: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 16 20:22:21.322: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 16 20:22:21.322: INFO: update-demo-nautilus-tg574 is verified up and running
STEP: rolling-update to new replication controller
Nov 16 20:22:21.326: INFO: scanned /root for discovery docs: <nil>
Nov 16 20:22:21.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-5085'
Nov 16 20:22:52.318: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Nov 16 20:22:52.318: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 16 20:22:52.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5085'
Nov 16 20:22:52.437: INFO: stderr: ""
Nov 16 20:22:52.437: INFO: stdout: "update-demo-kitten-q5cvp update-demo-kitten-xrj8x "
Nov 16 20:22:52.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods update-demo-kitten-q5cvp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5085'
Nov 16 20:22:52.535: INFO: stderr: ""
Nov 16 20:22:52.535: INFO: stdout: "true"
Nov 16 20:22:52.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods update-demo-kitten-q5cvp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5085'
Nov 16 20:22:52.658: INFO: stderr: ""
Nov 16 20:22:52.658: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Nov 16 20:22:52.658: INFO: validating pod update-demo-kitten-q5cvp
Nov 16 20:22:52.676: INFO: got data: {
  "image": "kitten.jpg"
}

Nov 16 20:22:52.676: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Nov 16 20:22:52.676: INFO: update-demo-kitten-q5cvp is verified up and running
Nov 16 20:22:52.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods update-demo-kitten-xrj8x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5085'
Nov 16 20:22:53.788: INFO: stderr: ""
Nov 16 20:22:53.788: INFO: stdout: "true"
Nov 16 20:22:53.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods update-demo-kitten-xrj8x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5085'
Nov 16 20:22:53.911: INFO: stderr: ""
Nov 16 20:22:53.911: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Nov 16 20:22:53.911: INFO: validating pod update-demo-kitten-xrj8x
Nov 16 20:22:53.937: INFO: got data: {
  "image": "kitten.jpg"
}

Nov 16 20:22:53.937: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Nov 16 20:22:53.937: INFO: update-demo-kitten-xrj8x is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:22:53.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5085" for this suite.
Nov 16 20:23:25.975: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:23:27.880: INFO: namespace kubectl-5085 deletion completed in 33.933257073s

• [SLOW TEST:73.277 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:23:27.882: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-58cd045a-fb46-4ce0-9571-49ac533e551a
STEP: Creating a pod to test consume secrets
Nov 16 20:23:28.046: INFO: Waiting up to 5m0s for pod "pod-secrets-75930325-1554-4dc8-baf7-55334c11829e" in namespace "secrets-3555" to be "success or failure"
Nov 16 20:23:28.054: INFO: Pod "pod-secrets-75930325-1554-4dc8-baf7-55334c11829e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.173172ms
Nov 16 20:23:30.066: INFO: Pod "pod-secrets-75930325-1554-4dc8-baf7-55334c11829e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02006949s
STEP: Saw pod success
Nov 16 20:23:30.066: INFO: Pod "pod-secrets-75930325-1554-4dc8-baf7-55334c11829e" satisfied condition "success or failure"
Nov 16 20:23:30.073: INFO: Trying to get logs from node 10.240.167.254 pod pod-secrets-75930325-1554-4dc8-baf7-55334c11829e container secret-volume-test: <nil>
STEP: delete the pod
Nov 16 20:23:30.145: INFO: Waiting for pod pod-secrets-75930325-1554-4dc8-baf7-55334c11829e to disappear
Nov 16 20:23:30.152: INFO: Pod pod-secrets-75930325-1554-4dc8-baf7-55334c11829e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:23:30.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3555" for this suite.
Nov 16 20:23:38.189: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:23:40.096: INFO: namespace secrets-3555 deletion completed in 9.933779594s

• [SLOW TEST:12.214 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:23:40.097: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-a2829663-47a4-48bb-97ef-dd06adda7c92
STEP: Creating a pod to test consume configMaps
Nov 16 20:23:40.259: INFO: Waiting up to 5m0s for pod "pod-configmaps-a39318d7-cad2-40d8-bf0b-1d4c44fad060" in namespace "configmap-260" to be "success or failure"
Nov 16 20:23:40.266: INFO: Pod "pod-configmaps-a39318d7-cad2-40d8-bf0b-1d4c44fad060": Phase="Pending", Reason="", readiness=false. Elapsed: 6.750328ms
Nov 16 20:23:42.274: INFO: Pod "pod-configmaps-a39318d7-cad2-40d8-bf0b-1d4c44fad060": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01481217s
STEP: Saw pod success
Nov 16 20:23:42.274: INFO: Pod "pod-configmaps-a39318d7-cad2-40d8-bf0b-1d4c44fad060" satisfied condition "success or failure"
Nov 16 20:23:42.281: INFO: Trying to get logs from node 10.240.167.254 pod pod-configmaps-a39318d7-cad2-40d8-bf0b-1d4c44fad060 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 16 20:23:42.320: INFO: Waiting for pod pod-configmaps-a39318d7-cad2-40d8-bf0b-1d4c44fad060 to disappear
Nov 16 20:23:42.326: INFO: Pod pod-configmaps-a39318d7-cad2-40d8-bf0b-1d4c44fad060 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:23:42.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-260" for this suite.
Nov 16 20:23:50.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:23:52.274: INFO: namespace configmap-260 deletion completed in 9.938961547s

• [SLOW TEST:12.178 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:23:52.275: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-downwardapi-xlxw
STEP: Creating a pod to test atomic-volume-subpath
Nov 16 20:23:52.433: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-xlxw" in namespace "subpath-874" to be "success or failure"
Nov 16 20:23:52.443: INFO: Pod "pod-subpath-test-downwardapi-xlxw": Phase="Pending", Reason="", readiness=false. Elapsed: 9.506112ms
Nov 16 20:23:54.451: INFO: Pod "pod-subpath-test-downwardapi-xlxw": Phase="Running", Reason="", readiness=true. Elapsed: 2.017769111s
Nov 16 20:23:56.460: INFO: Pod "pod-subpath-test-downwardapi-xlxw": Phase="Running", Reason="", readiness=true. Elapsed: 4.026712318s
Nov 16 20:23:58.469: INFO: Pod "pod-subpath-test-downwardapi-xlxw": Phase="Running", Reason="", readiness=true. Elapsed: 6.035399663s
Nov 16 20:24:00.477: INFO: Pod "pod-subpath-test-downwardapi-xlxw": Phase="Running", Reason="", readiness=true. Elapsed: 8.043399967s
Nov 16 20:24:02.485: INFO: Pod "pod-subpath-test-downwardapi-xlxw": Phase="Running", Reason="", readiness=true. Elapsed: 10.051649909s
Nov 16 20:24:04.493: INFO: Pod "pod-subpath-test-downwardapi-xlxw": Phase="Running", Reason="", readiness=true. Elapsed: 12.059630595s
Nov 16 20:24:06.501: INFO: Pod "pod-subpath-test-downwardapi-xlxw": Phase="Running", Reason="", readiness=true. Elapsed: 14.067923942s
Nov 16 20:24:08.511: INFO: Pod "pod-subpath-test-downwardapi-xlxw": Phase="Running", Reason="", readiness=true. Elapsed: 16.077397594s
Nov 16 20:24:10.519: INFO: Pod "pod-subpath-test-downwardapi-xlxw": Phase="Running", Reason="", readiness=true. Elapsed: 18.085706969s
Nov 16 20:24:12.528: INFO: Pod "pod-subpath-test-downwardapi-xlxw": Phase="Running", Reason="", readiness=true. Elapsed: 20.094047166s
Nov 16 20:24:14.536: INFO: Pod "pod-subpath-test-downwardapi-xlxw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.102803912s
STEP: Saw pod success
Nov 16 20:24:14.536: INFO: Pod "pod-subpath-test-downwardapi-xlxw" satisfied condition "success or failure"
Nov 16 20:24:14.543: INFO: Trying to get logs from node 10.240.167.254 pod pod-subpath-test-downwardapi-xlxw container test-container-subpath-downwardapi-xlxw: <nil>
STEP: delete the pod
Nov 16 20:24:14.586: INFO: Waiting for pod pod-subpath-test-downwardapi-xlxw to disappear
Nov 16 20:24:14.594: INFO: Pod pod-subpath-test-downwardapi-xlxw no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-xlxw
Nov 16 20:24:14.594: INFO: Deleting pod "pod-subpath-test-downwardapi-xlxw" in namespace "subpath-874"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:24:14.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-874" for this suite.
Nov 16 20:24:22.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:24:24.567: INFO: namespace subpath-874 deletion completed in 9.935487711s

• [SLOW TEST:32.292 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:24:24.568: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Nov 16 20:24:24.691: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:25:01.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2629" for this suite.
Nov 16 20:25:09.737: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:25:11.659: INFO: namespace crd-publish-openapi-2629 deletion completed in 9.952820256s

• [SLOW TEST:47.092 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:25:11.660: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Nov 16 20:25:11.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 create -f - --namespace=kubectl-9736'
Nov 16 20:25:12.296: INFO: stderr: ""
Nov 16 20:25:12.296: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 16 20:25:12.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9736'
Nov 16 20:25:12.425: INFO: stderr: ""
Nov 16 20:25:12.425: INFO: stdout: "update-demo-nautilus-89pl2 update-demo-nautilus-ftqq7 "
Nov 16 20:25:12.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods update-demo-nautilus-89pl2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9736'
Nov 16 20:25:12.534: INFO: stderr: ""
Nov 16 20:25:12.534: INFO: stdout: ""
Nov 16 20:25:12.534: INFO: update-demo-nautilus-89pl2 is created but not running
Nov 16 20:25:17.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9736'
Nov 16 20:25:17.653: INFO: stderr: ""
Nov 16 20:25:17.653: INFO: stdout: "update-demo-nautilus-89pl2 update-demo-nautilus-ftqq7 "
Nov 16 20:25:17.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods update-demo-nautilus-89pl2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9736'
Nov 16 20:25:17.773: INFO: stderr: ""
Nov 16 20:25:17.773: INFO: stdout: "true"
Nov 16 20:25:17.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods update-demo-nautilus-89pl2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9736'
Nov 16 20:25:17.880: INFO: stderr: ""
Nov 16 20:25:17.881: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 16 20:25:17.881: INFO: validating pod update-demo-nautilus-89pl2
Nov 16 20:25:17.899: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 16 20:25:17.899: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 16 20:25:17.899: INFO: update-demo-nautilus-89pl2 is verified up and running
Nov 16 20:25:17.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods update-demo-nautilus-ftqq7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9736'
Nov 16 20:25:18.009: INFO: stderr: ""
Nov 16 20:25:18.009: INFO: stdout: "true"
Nov 16 20:25:18.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods update-demo-nautilus-ftqq7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9736'
Nov 16 20:25:18.124: INFO: stderr: ""
Nov 16 20:25:18.124: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 16 20:25:18.124: INFO: validating pod update-demo-nautilus-ftqq7
Nov 16 20:25:18.153: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 16 20:25:18.153: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 16 20:25:18.153: INFO: update-demo-nautilus-ftqq7 is verified up and running
STEP: using delete to clean up resources
Nov 16 20:25:18.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 delete --grace-period=0 --force -f - --namespace=kubectl-9736'
Nov 16 20:25:18.275: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 16 20:25:18.275: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov 16 20:25:18.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9736'
Nov 16 20:25:18.411: INFO: stderr: "No resources found in kubectl-9736 namespace.\n"
Nov 16 20:25:18.411: INFO: stdout: ""
Nov 16 20:25:18.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods -l name=update-demo --namespace=kubectl-9736 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 16 20:25:18.533: INFO: stderr: ""
Nov 16 20:25:18.533: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:25:18.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9736" for this suite.
Nov 16 20:25:38.577: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:25:40.480: INFO: namespace kubectl-9736 deletion completed in 21.935314231s

• [SLOW TEST:28.819 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:25:40.480: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 20:25:40.587: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Nov 16 20:25:47.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 --namespace=crd-publish-openapi-2194 create -f -'
Nov 16 20:25:48.270: INFO: stderr: ""
Nov 16 20:25:48.270: INFO: stdout: "e2e-test-crd-publish-openapi-1807-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Nov 16 20:25:48.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 --namespace=crd-publish-openapi-2194 delete e2e-test-crd-publish-openapi-1807-crds test-cr'
Nov 16 20:25:48.471: INFO: stderr: ""
Nov 16 20:25:48.472: INFO: stdout: "e2e-test-crd-publish-openapi-1807-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Nov 16 20:25:48.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 --namespace=crd-publish-openapi-2194 apply -f -'
Nov 16 20:25:48.775: INFO: stderr: ""
Nov 16 20:25:48.776: INFO: stdout: "e2e-test-crd-publish-openapi-1807-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Nov 16 20:25:48.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 --namespace=crd-publish-openapi-2194 delete e2e-test-crd-publish-openapi-1807-crds test-cr'
Nov 16 20:25:48.889: INFO: stderr: ""
Nov 16 20:25:48.889: INFO: stdout: "e2e-test-crd-publish-openapi-1807-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Nov 16 20:25:48.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 explain e2e-test-crd-publish-openapi-1807-crds'
Nov 16 20:25:49.178: INFO: stderr: ""
Nov 16 20:25:49.178: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1807-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:25:55.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2194" for this suite.
Nov 16 20:26:03.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:26:05.674: INFO: namespace crd-publish-openapi-2194 deletion completed in 9.930962562s

• [SLOW TEST:25.194 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:26:05.675: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:26:08.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1125" for this suite.
Nov 16 20:26:38.927: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:26:40.858: INFO: namespace replication-controller-1125 deletion completed in 31.958921552s

• [SLOW TEST:35.183 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:26:40.858: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Nov 16 20:26:41.010: INFO: Waiting up to 5m0s for pod "pod-40d91b00-5154-4660-a7ed-af6da05a7175" in namespace "emptydir-9074" to be "success or failure"
Nov 16 20:26:41.017: INFO: Pod "pod-40d91b00-5154-4660-a7ed-af6da05a7175": Phase="Pending", Reason="", readiness=false. Elapsed: 7.404633ms
Nov 16 20:26:43.025: INFO: Pod "pod-40d91b00-5154-4660-a7ed-af6da05a7175": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015848929s
STEP: Saw pod success
Nov 16 20:26:43.026: INFO: Pod "pod-40d91b00-5154-4660-a7ed-af6da05a7175" satisfied condition "success or failure"
Nov 16 20:26:43.033: INFO: Trying to get logs from node 10.240.167.254 pod pod-40d91b00-5154-4660-a7ed-af6da05a7175 container test-container: <nil>
STEP: delete the pod
Nov 16 20:26:43.098: INFO: Waiting for pod pod-40d91b00-5154-4660-a7ed-af6da05a7175 to disappear
Nov 16 20:26:43.104: INFO: Pod pod-40d91b00-5154-4660-a7ed-af6da05a7175 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:26:43.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9074" for this suite.
Nov 16 20:26:51.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:26:53.047: INFO: namespace emptydir-9074 deletion completed in 9.932407716s

• [SLOW TEST:12.189 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:26:53.048: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override all
Nov 16 20:26:53.184: INFO: Waiting up to 5m0s for pod "client-containers-ab2dfb0f-aa5d-44b4-85c0-82900c95d1ba" in namespace "containers-538" to be "success or failure"
Nov 16 20:26:53.191: INFO: Pod "client-containers-ab2dfb0f-aa5d-44b4-85c0-82900c95d1ba": Phase="Pending", Reason="", readiness=false. Elapsed: 6.759753ms
Nov 16 20:26:55.199: INFO: Pod "client-containers-ab2dfb0f-aa5d-44b4-85c0-82900c95d1ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015578235s
STEP: Saw pod success
Nov 16 20:26:55.199: INFO: Pod "client-containers-ab2dfb0f-aa5d-44b4-85c0-82900c95d1ba" satisfied condition "success or failure"
Nov 16 20:26:55.207: INFO: Trying to get logs from node 10.240.167.254 pod client-containers-ab2dfb0f-aa5d-44b4-85c0-82900c95d1ba container test-container: <nil>
STEP: delete the pod
Nov 16 20:26:55.250: INFO: Waiting for pod client-containers-ab2dfb0f-aa5d-44b4-85c0-82900c95d1ba to disappear
Nov 16 20:26:55.257: INFO: Pod client-containers-ab2dfb0f-aa5d-44b4-85c0-82900c95d1ba no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:26:55.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-538" for this suite.
Nov 16 20:27:03.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:27:05.231: INFO: namespace containers-538 deletion completed in 9.958116773s

• [SLOW TEST:12.184 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:27:05.232: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-5493c639-1a51-4139-9f7f-ac239fe03936
STEP: Creating a pod to test consume configMaps
Nov 16 20:27:05.429: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3f4dde1b-bce6-4da9-80f0-490c37f74723" in namespace "projected-1385" to be "success or failure"
Nov 16 20:27:05.436: INFO: Pod "pod-projected-configmaps-3f4dde1b-bce6-4da9-80f0-490c37f74723": Phase="Pending", Reason="", readiness=false. Elapsed: 6.493893ms
Nov 16 20:27:07.444: INFO: Pod "pod-projected-configmaps-3f4dde1b-bce6-4da9-80f0-490c37f74723": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014953747s
Nov 16 20:27:09.452: INFO: Pod "pod-projected-configmaps-3f4dde1b-bce6-4da9-80f0-490c37f74723": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022606859s
STEP: Saw pod success
Nov 16 20:27:09.452: INFO: Pod "pod-projected-configmaps-3f4dde1b-bce6-4da9-80f0-490c37f74723" satisfied condition "success or failure"
Nov 16 20:27:09.459: INFO: Trying to get logs from node 10.240.167.254 pod pod-projected-configmaps-3f4dde1b-bce6-4da9-80f0-490c37f74723 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 16 20:27:09.507: INFO: Waiting for pod pod-projected-configmaps-3f4dde1b-bce6-4da9-80f0-490c37f74723 to disappear
Nov 16 20:27:09.515: INFO: Pod pod-projected-configmaps-3f4dde1b-bce6-4da9-80f0-490c37f74723 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:27:09.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1385" for this suite.
Nov 16 20:27:17.552: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:27:19.472: INFO: namespace projected-1385 deletion completed in 9.946670931s

• [SLOW TEST:14.240 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:27:19.472: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Nov 16 20:27:22.197: INFO: Successfully updated pod "pod-update-activedeadlineseconds-13e4f93e-542f-4c06-8e2b-79d2263c1e2e"
Nov 16 20:27:22.198: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-13e4f93e-542f-4c06-8e2b-79d2263c1e2e" in namespace "pods-1497" to be "terminated due to deadline exceeded"
Nov 16 20:27:22.204: INFO: Pod "pod-update-activedeadlineseconds-13e4f93e-542f-4c06-8e2b-79d2263c1e2e": Phase="Running", Reason="", readiness=true. Elapsed: 6.757428ms
Nov 16 20:27:24.213: INFO: Pod "pod-update-activedeadlineseconds-13e4f93e-542f-4c06-8e2b-79d2263c1e2e": Phase="Running", Reason="", readiness=true. Elapsed: 2.015469802s
Nov 16 20:27:26.222: INFO: Pod "pod-update-activedeadlineseconds-13e4f93e-542f-4c06-8e2b-79d2263c1e2e": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.023859979s
Nov 16 20:27:26.222: INFO: Pod "pod-update-activedeadlineseconds-13e4f93e-542f-4c06-8e2b-79d2263c1e2e" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:27:26.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1497" for this suite.
Nov 16 20:27:34.264: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:27:36.170: INFO: namespace pods-1497 deletion completed in 9.935782057s

• [SLOW TEST:16.698 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:27:36.170: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating replication controller svc-latency-rc in namespace svc-latency-8533
I1116 20:27:36.294947      26 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8533, replica count: 1
I1116 20:27:37.345497      26 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1116 20:27:38.345758      26 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 16 20:27:38.475: INFO: Created: latency-svc-pxjlr
Nov 16 20:27:38.489: INFO: Got endpoints: latency-svc-pxjlr [43.895415ms]
Nov 16 20:27:38.522: INFO: Created: latency-svc-vrkh4
Nov 16 20:27:38.533: INFO: Got endpoints: latency-svc-vrkh4 [43.664093ms]
Nov 16 20:27:38.534: INFO: Created: latency-svc-gckcm
Nov 16 20:27:38.542: INFO: Created: latency-svc-p942x
Nov 16 20:27:38.544: INFO: Got endpoints: latency-svc-gckcm [53.975785ms]
Nov 16 20:27:38.552: INFO: Got endpoints: latency-svc-p942x [62.30147ms]
Nov 16 20:27:38.557: INFO: Created: latency-svc-df4fl
Nov 16 20:27:38.565: INFO: Created: latency-svc-k4bpd
Nov 16 20:27:38.565: INFO: Got endpoints: latency-svc-df4fl [74.953448ms]
Nov 16 20:27:38.575: INFO: Got endpoints: latency-svc-k4bpd [84.942551ms]
Nov 16 20:27:38.577: INFO: Created: latency-svc-kczls
Nov 16 20:27:38.589: INFO: Got endpoints: latency-svc-kczls [98.677718ms]
Nov 16 20:27:38.589: INFO: Created: latency-svc-mfpf5
Nov 16 20:27:38.601: INFO: Created: latency-svc-qtgkt
Nov 16 20:27:38.601: INFO: Got endpoints: latency-svc-mfpf5 [111.110893ms]
Nov 16 20:27:38.611: INFO: Got endpoints: latency-svc-qtgkt [120.88322ms]
Nov 16 20:27:38.621: INFO: Created: latency-svc-7d6jv
Nov 16 20:27:38.644: INFO: Got endpoints: latency-svc-7d6jv [153.688941ms]
Nov 16 20:27:38.647: INFO: Created: latency-svc-6qr8n
Nov 16 20:27:38.659: INFO: Got endpoints: latency-svc-6qr8n [168.397266ms]
Nov 16 20:27:38.679: INFO: Created: latency-svc-dqkqn
Nov 16 20:27:38.692: INFO: Got endpoints: latency-svc-dqkqn [201.434019ms]
Nov 16 20:27:38.705: INFO: Created: latency-svc-svssp
Nov 16 20:27:38.716: INFO: Got endpoints: latency-svc-svssp [224.993895ms]
Nov 16 20:27:38.717: INFO: Created: latency-svc-5scv9
Nov 16 20:27:38.726: INFO: Created: latency-svc-2s66j
Nov 16 20:27:38.726: INFO: Got endpoints: latency-svc-5scv9 [235.360711ms]
Nov 16 20:27:38.732: INFO: Got endpoints: latency-svc-2s66j [241.545363ms]
Nov 16 20:27:38.738: INFO: Created: latency-svc-xg77m
Nov 16 20:27:38.751: INFO: Created: latency-svc-v4dmn
Nov 16 20:27:38.751: INFO: Got endpoints: latency-svc-xg77m [260.530917ms]
Nov 16 20:27:38.757: INFO: Created: latency-svc-8vvcm
Nov 16 20:27:38.760: INFO: Got endpoints: latency-svc-v4dmn [226.127538ms]
Nov 16 20:27:38.771: INFO: Got endpoints: latency-svc-8vvcm [227.445645ms]
Nov 16 20:27:38.772: INFO: Created: latency-svc-vkclm
Nov 16 20:27:38.783: INFO: Got endpoints: latency-svc-vkclm [230.064705ms]
Nov 16 20:27:38.783: INFO: Created: latency-svc-v5qtj
Nov 16 20:27:38.793: INFO: Got endpoints: latency-svc-v5qtj [227.35333ms]
Nov 16 20:27:38.793: INFO: Created: latency-svc-flwbz
Nov 16 20:27:38.802: INFO: Got endpoints: latency-svc-flwbz [226.889002ms]
Nov 16 20:27:38.804: INFO: Created: latency-svc-p6sf2
Nov 16 20:27:38.815: INFO: Got endpoints: latency-svc-p6sf2 [226.034377ms]
Nov 16 20:27:38.816: INFO: Created: latency-svc-6mm52
Nov 16 20:27:38.824: INFO: Got endpoints: latency-svc-6mm52 [222.244547ms]
Nov 16 20:27:38.825: INFO: Created: latency-svc-2qwxm
Nov 16 20:27:38.835: INFO: Created: latency-svc-8f4pj
Nov 16 20:27:38.836: INFO: Got endpoints: latency-svc-2qwxm [225.301629ms]
Nov 16 20:27:38.845: INFO: Got endpoints: latency-svc-8f4pj [201.030695ms]
Nov 16 20:27:38.847: INFO: Created: latency-svc-ndd72
Nov 16 20:27:38.857: INFO: Got endpoints: latency-svc-ndd72 [198.179745ms]
Nov 16 20:27:38.857: INFO: Created: latency-svc-cm5df
Nov 16 20:27:38.867: INFO: Got endpoints: latency-svc-cm5df [174.940602ms]
Nov 16 20:27:38.869: INFO: Created: latency-svc-f2p2h
Nov 16 20:27:38.882: INFO: Created: latency-svc-8dpx2
Nov 16 20:27:38.882: INFO: Got endpoints: latency-svc-f2p2h [166.284305ms]
Nov 16 20:27:38.893: INFO: Got endpoints: latency-svc-8dpx2 [166.906976ms]
Nov 16 20:27:38.897: INFO: Created: latency-svc-l6z24
Nov 16 20:27:38.908: INFO: Created: latency-svc-cgqff
Nov 16 20:27:38.908: INFO: Got endpoints: latency-svc-l6z24 [176.207947ms]
Nov 16 20:27:38.917: INFO: Got endpoints: latency-svc-cgqff [165.691865ms]
Nov 16 20:27:38.920: INFO: Created: latency-svc-rzc68
Nov 16 20:27:38.933: INFO: Got endpoints: latency-svc-rzc68 [173.414555ms]
Nov 16 20:27:38.933: INFO: Created: latency-svc-vmvlh
Nov 16 20:27:38.947: INFO: Created: latency-svc-h7pfl
Nov 16 20:27:38.947: INFO: Got endpoints: latency-svc-vmvlh [175.569081ms]
Nov 16 20:27:38.959: INFO: Got endpoints: latency-svc-h7pfl [175.906203ms]
Nov 16 20:27:38.959: INFO: Created: latency-svc-jvbff
Nov 16 20:27:38.969: INFO: Created: latency-svc-2gjbd
Nov 16 20:27:38.969: INFO: Got endpoints: latency-svc-jvbff [176.067012ms]
Nov 16 20:27:38.986: INFO: Created: latency-svc-dbkwb
Nov 16 20:27:38.986: INFO: Got endpoints: latency-svc-2gjbd [183.663236ms]
Nov 16 20:27:38.995: INFO: Got endpoints: latency-svc-dbkwb [179.680993ms]
Nov 16 20:27:38.997: INFO: Created: latency-svc-2nchl
Nov 16 20:27:39.010: INFO: Got endpoints: latency-svc-2nchl [186.206247ms]
Nov 16 20:27:39.014: INFO: Created: latency-svc-8gqnq
Nov 16 20:27:39.030: INFO: Got endpoints: latency-svc-8gqnq [193.780304ms]
Nov 16 20:27:39.034: INFO: Created: latency-svc-dlg2c
Nov 16 20:27:39.046: INFO: Created: latency-svc-6cfhp
Nov 16 20:27:39.050: INFO: Got endpoints: latency-svc-dlg2c [203.913497ms]
Nov 16 20:27:39.057: INFO: Created: latency-svc-hzrth
Nov 16 20:27:39.059: INFO: Got endpoints: latency-svc-6cfhp [201.226869ms]
Nov 16 20:27:39.070: INFO: Got endpoints: latency-svc-hzrth [202.777276ms]
Nov 16 20:27:39.070: INFO: Created: latency-svc-p2zwj
Nov 16 20:27:39.080: INFO: Got endpoints: latency-svc-p2zwj [197.621861ms]
Nov 16 20:27:39.084: INFO: Created: latency-svc-n9ddx
Nov 16 20:27:39.094: INFO: Got endpoints: latency-svc-n9ddx [201.259306ms]
Nov 16 20:27:39.104: INFO: Created: latency-svc-4s5wk
Nov 16 20:27:39.104: INFO: Got endpoints: latency-svc-4s5wk [195.40709ms]
Nov 16 20:27:39.110: INFO: Created: latency-svc-4ddl4
Nov 16 20:27:39.121: INFO: Got endpoints: latency-svc-4ddl4 [204.050884ms]
Nov 16 20:27:39.122: INFO: Created: latency-svc-lqxw9
Nov 16 20:27:39.131: INFO: Got endpoints: latency-svc-lqxw9 [197.993891ms]
Nov 16 20:27:39.134: INFO: Created: latency-svc-74v44
Nov 16 20:27:39.141: INFO: Got endpoints: latency-svc-74v44 [193.797121ms]
Nov 16 20:27:39.144: INFO: Created: latency-svc-xdln4
Nov 16 20:27:39.158: INFO: Created: latency-svc-gbjmv
Nov 16 20:27:39.158: INFO: Got endpoints: latency-svc-xdln4 [37.294218ms]
Nov 16 20:27:39.165: INFO: Created: latency-svc-ck6wc
Nov 16 20:27:39.168: INFO: Got endpoints: latency-svc-gbjmv [209.673606ms]
Nov 16 20:27:39.183: INFO: Created: latency-svc-8wtmx
Nov 16 20:27:39.183: INFO: Got endpoints: latency-svc-ck6wc [214.107362ms]
Nov 16 20:27:39.195: INFO: Got endpoints: latency-svc-8wtmx [208.619046ms]
Nov 16 20:27:39.201: INFO: Created: latency-svc-vxcv2
Nov 16 20:27:39.212: INFO: Got endpoints: latency-svc-vxcv2 [216.938567ms]
Nov 16 20:27:39.217: INFO: Created: latency-svc-tf7z6
Nov 16 20:27:39.227: INFO: Got endpoints: latency-svc-tf7z6 [216.629457ms]
Nov 16 20:27:39.230: INFO: Created: latency-svc-pnrfp
Nov 16 20:27:39.237: INFO: Got endpoints: latency-svc-pnrfp [206.618481ms]
Nov 16 20:27:39.239: INFO: Created: latency-svc-vjrm8
Nov 16 20:27:39.270: INFO: Got endpoints: latency-svc-vjrm8 [220.4233ms]
Nov 16 20:27:39.277: INFO: Created: latency-svc-jp8v4
Nov 16 20:27:39.287: INFO: Got endpoints: latency-svc-jp8v4 [228.086487ms]
Nov 16 20:27:39.290: INFO: Created: latency-svc-47t6j
Nov 16 20:27:39.299: INFO: Got endpoints: latency-svc-47t6j [229.203549ms]
Nov 16 20:27:39.300: INFO: Created: latency-svc-k44mx
Nov 16 20:27:39.311: INFO: Got endpoints: latency-svc-k44mx [231.132215ms]
Nov 16 20:27:39.312: INFO: Created: latency-svc-nzvpm
Nov 16 20:27:39.323: INFO: Got endpoints: latency-svc-nzvpm [227.721066ms]
Nov 16 20:27:39.332: INFO: Created: latency-svc-ljzxj
Nov 16 20:27:39.343: INFO: Created: latency-svc-hnhnj
Nov 16 20:27:39.344: INFO: Got endpoints: latency-svc-ljzxj [239.890284ms]
Nov 16 20:27:39.353: INFO: Got endpoints: latency-svc-hnhnj [219.526343ms]
Nov 16 20:27:39.356: INFO: Created: latency-svc-r9vjj
Nov 16 20:27:39.370: INFO: Got endpoints: latency-svc-r9vjj [228.40863ms]
Nov 16 20:27:39.370: INFO: Created: latency-svc-lssqk
Nov 16 20:27:39.376: INFO: Got endpoints: latency-svc-lssqk [216.946682ms]
Nov 16 20:27:39.379: INFO: Created: latency-svc-fc4sr
Nov 16 20:27:39.389: INFO: Got endpoints: latency-svc-fc4sr [220.140656ms]
Nov 16 20:27:39.391: INFO: Created: latency-svc-pfmrx
Nov 16 20:27:39.403: INFO: Got endpoints: latency-svc-pfmrx [215.802666ms]
Nov 16 20:27:39.405: INFO: Created: latency-svc-nj29f
Nov 16 20:27:39.418: INFO: Got endpoints: latency-svc-nj29f [222.575819ms]
Nov 16 20:27:39.418: INFO: Created: latency-svc-8b4jz
Nov 16 20:27:39.431: INFO: Got endpoints: latency-svc-8b4jz [218.075032ms]
Nov 16 20:27:39.435: INFO: Created: latency-svc-96hjk
Nov 16 20:27:39.446: INFO: Got endpoints: latency-svc-96hjk [218.449009ms]
Nov 16 20:27:39.451: INFO: Created: latency-svc-j5865
Nov 16 20:27:39.460: INFO: Created: latency-svc-ssfww
Nov 16 20:27:39.464: INFO: Got endpoints: latency-svc-j5865 [226.433594ms]
Nov 16 20:27:39.470: INFO: Created: latency-svc-q6bhl
Nov 16 20:27:39.476: INFO: Got endpoints: latency-svc-ssfww [205.968101ms]
Nov 16 20:27:39.484: INFO: Created: latency-svc-n74f2
Nov 16 20:27:39.490: INFO: Got endpoints: latency-svc-q6bhl [203.397044ms]
Nov 16 20:27:39.496: INFO: Got endpoints: latency-svc-n74f2 [197.222754ms]
Nov 16 20:27:39.501: INFO: Created: latency-svc-fvwbz
Nov 16 20:27:39.511: INFO: Got endpoints: latency-svc-fvwbz [199.216732ms]
Nov 16 20:27:39.513: INFO: Created: latency-svc-ctfl6
Nov 16 20:27:39.527: INFO: Got endpoints: latency-svc-ctfl6 [203.548404ms]
Nov 16 20:27:39.529: INFO: Created: latency-svc-pkvzw
Nov 16 20:27:39.546: INFO: Got endpoints: latency-svc-pkvzw [202.04351ms]
Nov 16 20:27:39.546: INFO: Created: latency-svc-wrh62
Nov 16 20:27:39.557: INFO: Created: latency-svc-lfl2n
Nov 16 20:27:39.560: INFO: Got endpoints: latency-svc-wrh62 [206.968538ms]
Nov 16 20:27:39.567: INFO: Got endpoints: latency-svc-lfl2n [196.9399ms]
Nov 16 20:27:39.568: INFO: Created: latency-svc-vp7xf
Nov 16 20:27:39.580: INFO: Got endpoints: latency-svc-vp7xf [204.346673ms]
Nov 16 20:27:39.584: INFO: Created: latency-svc-m8qgl
Nov 16 20:27:39.594: INFO: Got endpoints: latency-svc-m8qgl [203.377001ms]
Nov 16 20:27:39.596: INFO: Created: latency-svc-wssx7
Nov 16 20:27:39.608: INFO: Got endpoints: latency-svc-wssx7 [205.090752ms]
Nov 16 20:27:39.610: INFO: Created: latency-svc-wwkk7
Nov 16 20:27:39.625: INFO: Got endpoints: latency-svc-wwkk7 [207.218788ms]
Nov 16 20:27:39.634: INFO: Created: latency-svc-vlxq2
Nov 16 20:27:39.643: INFO: Got endpoints: latency-svc-vlxq2 [211.732049ms]
Nov 16 20:27:39.646: INFO: Created: latency-svc-867vp
Nov 16 20:27:39.663: INFO: Got endpoints: latency-svc-867vp [217.591794ms]
Nov 16 20:27:39.669: INFO: Created: latency-svc-fq9qt
Nov 16 20:27:39.681: INFO: Got endpoints: latency-svc-fq9qt [216.939255ms]
Nov 16 20:27:39.684: INFO: Created: latency-svc-brgxr
Nov 16 20:27:39.696: INFO: Got endpoints: latency-svc-brgxr [219.470542ms]
Nov 16 20:27:39.698: INFO: Created: latency-svc-8qwbc
Nov 16 20:27:39.709: INFO: Created: latency-svc-trxbv
Nov 16 20:27:39.712: INFO: Got endpoints: latency-svc-8qwbc [222.085718ms]
Nov 16 20:27:39.722: INFO: Created: latency-svc-llgvv
Nov 16 20:27:39.723: INFO: Got endpoints: latency-svc-trxbv [226.188364ms]
Nov 16 20:27:39.735: INFO: Got endpoints: latency-svc-llgvv [223.974112ms]
Nov 16 20:27:39.738: INFO: Created: latency-svc-z9rfh
Nov 16 20:27:39.746: INFO: Created: latency-svc-sxx4c
Nov 16 20:27:39.747: INFO: Got endpoints: latency-svc-z9rfh [220.024026ms]
Nov 16 20:27:39.753: INFO: Got endpoints: latency-svc-sxx4c [206.946609ms]
Nov 16 20:27:39.759: INFO: Created: latency-svc-lz2ld
Nov 16 20:27:39.776: INFO: Got endpoints: latency-svc-lz2ld [215.432ms]
Nov 16 20:27:39.783: INFO: Created: latency-svc-xncqr
Nov 16 20:27:39.794: INFO: Created: latency-svc-m4w4n
Nov 16 20:27:39.797: INFO: Got endpoints: latency-svc-xncqr [230.062193ms]
Nov 16 20:27:39.803: INFO: Got endpoints: latency-svc-m4w4n [223.142074ms]
Nov 16 20:27:39.805: INFO: Created: latency-svc-njlcw
Nov 16 20:27:39.816: INFO: Got endpoints: latency-svc-njlcw [221.864421ms]
Nov 16 20:27:39.820: INFO: Created: latency-svc-lk24t
Nov 16 20:27:39.832: INFO: Got endpoints: latency-svc-lk24t [223.830074ms]
Nov 16 20:27:39.834: INFO: Created: latency-svc-7ddfc
Nov 16 20:27:39.845: INFO: Got endpoints: latency-svc-7ddfc [220.465366ms]
Nov 16 20:27:39.847: INFO: Created: latency-svc-42bkw
Nov 16 20:27:39.860: INFO: Got endpoints: latency-svc-42bkw [217.273961ms]
Nov 16 20:27:39.861: INFO: Created: latency-svc-6wwmc
Nov 16 20:27:39.871: INFO: Got endpoints: latency-svc-6wwmc [207.57108ms]
Nov 16 20:27:39.872: INFO: Created: latency-svc-89czf
Nov 16 20:27:39.885: INFO: Got endpoints: latency-svc-89czf [203.636733ms]
Nov 16 20:27:39.886: INFO: Created: latency-svc-4ql5n
Nov 16 20:27:39.897: INFO: Created: latency-svc-nz72q
Nov 16 20:27:39.910: INFO: Got endpoints: latency-svc-nz72q [197.661872ms]
Nov 16 20:27:39.910: INFO: Got endpoints: latency-svc-4ql5n [214.499125ms]
Nov 16 20:27:39.915: INFO: Created: latency-svc-k777n
Nov 16 20:27:39.927: INFO: Got endpoints: latency-svc-k777n [204.413333ms]
Nov 16 20:27:39.930: INFO: Created: latency-svc-5hcwf
Nov 16 20:27:39.941: INFO: Got endpoints: latency-svc-5hcwf [205.723701ms]
Nov 16 20:27:39.941: INFO: Created: latency-svc-6t49h
Nov 16 20:27:39.951: INFO: Got endpoints: latency-svc-6t49h [198.109706ms]
Nov 16 20:27:39.955: INFO: Created: latency-svc-ttppj
Nov 16 20:27:39.964: INFO: Created: latency-svc-44blb
Nov 16 20:27:39.964: INFO: Got endpoints: latency-svc-ttppj [211.884037ms]
Nov 16 20:27:39.973: INFO: Created: latency-svc-wm5tj
Nov 16 20:27:39.978: INFO: Got endpoints: latency-svc-44blb [201.729139ms]
Nov 16 20:27:39.989: INFO: Got endpoints: latency-svc-wm5tj [192.305143ms]
Nov 16 20:27:39.990: INFO: Created: latency-svc-55mtq
Nov 16 20:27:39.999: INFO: Created: latency-svc-lvfhc
Nov 16 20:27:40.002: INFO: Got endpoints: latency-svc-55mtq [198.170779ms]
Nov 16 20:27:40.016: INFO: Got endpoints: latency-svc-lvfhc [199.528657ms]
Nov 16 20:27:40.017: INFO: Created: latency-svc-6l446
Nov 16 20:27:40.028: INFO: Created: latency-svc-7qb46
Nov 16 20:27:40.029: INFO: Got endpoints: latency-svc-6l446 [193.89114ms]
Nov 16 20:27:40.044: INFO: Got endpoints: latency-svc-7qb46 [198.159829ms]
Nov 16 20:27:40.048: INFO: Created: latency-svc-62qh8
Nov 16 20:27:40.060: INFO: Got endpoints: latency-svc-62qh8 [199.508328ms]
Nov 16 20:27:40.066: INFO: Created: latency-svc-ps7lt
Nov 16 20:27:40.075: INFO: Got endpoints: latency-svc-ps7lt [204.015123ms]
Nov 16 20:27:40.077: INFO: Created: latency-svc-cxdwf
Nov 16 20:27:40.090: INFO: Got endpoints: latency-svc-cxdwf [205.507532ms]
Nov 16 20:27:40.093: INFO: Created: latency-svc-n2hrk
Nov 16 20:27:40.104: INFO: Got endpoints: latency-svc-n2hrk [193.182683ms]
Nov 16 20:27:40.105: INFO: Created: latency-svc-5hzgn
Nov 16 20:27:40.115: INFO: Got endpoints: latency-svc-5hzgn [204.753114ms]
Nov 16 20:27:40.116: INFO: Created: latency-svc-xnp8p
Nov 16 20:27:40.128: INFO: Got endpoints: latency-svc-xnp8p [200.570643ms]
Nov 16 20:27:40.131: INFO: Created: latency-svc-dc9sq
Nov 16 20:27:40.143: INFO: Got endpoints: latency-svc-dc9sq [202.443568ms]
Nov 16 20:27:40.144: INFO: Created: latency-svc-z9qsm
Nov 16 20:27:40.155: INFO: Got endpoints: latency-svc-z9qsm [203.40959ms]
Nov 16 20:27:40.155: INFO: Created: latency-svc-tkx4n
Nov 16 20:27:40.164: INFO: Got endpoints: latency-svc-tkx4n [199.725037ms]
Nov 16 20:27:40.166: INFO: Created: latency-svc-789ct
Nov 16 20:27:40.176: INFO: Created: latency-svc-wsj9t
Nov 16 20:27:40.182: INFO: Got endpoints: latency-svc-789ct [203.876049ms]
Nov 16 20:27:40.190: INFO: Got endpoints: latency-svc-wsj9t [200.254834ms]
Nov 16 20:27:40.190: INFO: Created: latency-svc-fj8lq
Nov 16 20:27:40.204: INFO: Created: latency-svc-hbjxr
Nov 16 20:27:40.205: INFO: Got endpoints: latency-svc-fj8lq [203.373404ms]
Nov 16 20:27:40.213: INFO: Got endpoints: latency-svc-hbjxr [197.160718ms]
Nov 16 20:27:40.215: INFO: Created: latency-svc-2vz4p
Nov 16 20:27:40.224: INFO: Got endpoints: latency-svc-2vz4p [195.218173ms]
Nov 16 20:27:40.234: INFO: Created: latency-svc-g2cn5
Nov 16 20:27:40.241: INFO: Created: latency-svc-fvrkj
Nov 16 20:27:40.242: INFO: Got endpoints: latency-svc-g2cn5 [197.98025ms]
Nov 16 20:27:40.258: INFO: Created: latency-svc-c25sf
Nov 16 20:27:40.258: INFO: Got endpoints: latency-svc-fvrkj [198.652393ms]
Nov 16 20:27:40.268: INFO: Got endpoints: latency-svc-c25sf [193.1811ms]
Nov 16 20:27:40.270: INFO: Created: latency-svc-lz2fc
Nov 16 20:27:40.279: INFO: Got endpoints: latency-svc-lz2fc [188.68196ms]
Nov 16 20:27:40.280: INFO: Created: latency-svc-wqztm
Nov 16 20:27:40.290: INFO: Got endpoints: latency-svc-wqztm [185.815303ms]
Nov 16 20:27:40.297: INFO: Created: latency-svc-vgbll
Nov 16 20:27:40.311: INFO: Got endpoints: latency-svc-vgbll [195.765693ms]
Nov 16 20:27:40.312: INFO: Created: latency-svc-xfl4g
Nov 16 20:27:40.326: INFO: Got endpoints: latency-svc-xfl4g [198.37526ms]
Nov 16 20:27:40.332: INFO: Created: latency-svc-5glf7
Nov 16 20:27:40.347: INFO: Got endpoints: latency-svc-5glf7 [203.986296ms]
Nov 16 20:27:40.348: INFO: Created: latency-svc-gtgvk
Nov 16 20:27:40.356: INFO: Got endpoints: latency-svc-gtgvk [201.528323ms]
Nov 16 20:27:40.366: INFO: Created: latency-svc-9h525
Nov 16 20:27:40.374: INFO: Got endpoints: latency-svc-9h525 [209.681493ms]
Nov 16 20:27:40.378: INFO: Created: latency-svc-m2w8q
Nov 16 20:27:40.388: INFO: Got endpoints: latency-svc-m2w8q [206.427709ms]
Nov 16 20:27:40.391: INFO: Created: latency-svc-rmzs8
Nov 16 20:27:40.403: INFO: Got endpoints: latency-svc-rmzs8 [213.690786ms]
Nov 16 20:27:40.404: INFO: Created: latency-svc-76bsq
Nov 16 20:27:40.419: INFO: Got endpoints: latency-svc-76bsq [213.452569ms]
Nov 16 20:27:40.419: INFO: Created: latency-svc-cvlqk
Nov 16 20:27:40.429: INFO: Got endpoints: latency-svc-cvlqk [215.525325ms]
Nov 16 20:27:40.434: INFO: Created: latency-svc-8bv84
Nov 16 20:27:40.446: INFO: Got endpoints: latency-svc-8bv84 [222.148698ms]
Nov 16 20:27:40.448: INFO: Created: latency-svc-pv2kd
Nov 16 20:27:40.457: INFO: Got endpoints: latency-svc-pv2kd [214.914182ms]
Nov 16 20:27:40.460: INFO: Created: latency-svc-skvw2
Nov 16 20:27:40.472: INFO: Created: latency-svc-fkppk
Nov 16 20:27:40.472: INFO: Got endpoints: latency-svc-skvw2 [213.91497ms]
Nov 16 20:27:40.482: INFO: Created: latency-svc-h8d9p
Nov 16 20:27:40.482: INFO: Got endpoints: latency-svc-fkppk [213.098401ms]
Nov 16 20:27:40.493: INFO: Got endpoints: latency-svc-h8d9p [213.838088ms]
Nov 16 20:27:40.494: INFO: Created: latency-svc-vhgqv
Nov 16 20:27:40.504: INFO: Created: latency-svc-8tq5f
Nov 16 20:27:40.504: INFO: Got endpoints: latency-svc-vhgqv [213.649137ms]
Nov 16 20:27:40.516: INFO: Created: latency-svc-bmkb4
Nov 16 20:27:40.524: INFO: Got endpoints: latency-svc-8tq5f [212.155389ms]
Nov 16 20:27:40.531: INFO: Got endpoints: latency-svc-bmkb4 [204.740867ms]
Nov 16 20:27:40.532: INFO: Created: latency-svc-kh54l
Nov 16 20:27:40.542: INFO: Got endpoints: latency-svc-kh54l [194.649857ms]
Nov 16 20:27:40.548: INFO: Created: latency-svc-lrvxq
Nov 16 20:27:40.559: INFO: Got endpoints: latency-svc-lrvxq [202.907116ms]
Nov 16 20:27:40.561: INFO: Created: latency-svc-cg7zs
Nov 16 20:27:40.572: INFO: Got endpoints: latency-svc-cg7zs [197.977172ms]
Nov 16 20:27:40.573: INFO: Created: latency-svc-6nbfp
Nov 16 20:27:40.587: INFO: Got endpoints: latency-svc-6nbfp [198.979021ms]
Nov 16 20:27:40.588: INFO: Created: latency-svc-vscqk
Nov 16 20:27:40.596: INFO: Created: latency-svc-fn7w6
Nov 16 20:27:40.597: INFO: Got endpoints: latency-svc-vscqk [193.343648ms]
Nov 16 20:27:40.619: INFO: Got endpoints: latency-svc-fn7w6 [200.568097ms]
Nov 16 20:27:40.623: INFO: Created: latency-svc-frk4k
Nov 16 20:27:40.632: INFO: Created: latency-svc-f9qz2
Nov 16 20:27:40.638: INFO: Got endpoints: latency-svc-frk4k [209.032489ms]
Nov 16 20:27:40.646: INFO: Got endpoints: latency-svc-f9qz2 [198.600516ms]
Nov 16 20:27:40.651: INFO: Created: latency-svc-9q676
Nov 16 20:27:40.662: INFO: Got endpoints: latency-svc-9q676 [205.502908ms]
Nov 16 20:27:40.663: INFO: Created: latency-svc-9bsp9
Nov 16 20:27:40.672: INFO: Created: latency-svc-lprf9
Nov 16 20:27:40.681: INFO: Got endpoints: latency-svc-9bsp9 [208.155442ms]
Nov 16 20:27:40.688: INFO: Got endpoints: latency-svc-lprf9 [206.476028ms]
Nov 16 20:27:40.692: INFO: Created: latency-svc-c56tv
Nov 16 20:27:40.702: INFO: Created: latency-svc-crcb4
Nov 16 20:27:40.705: INFO: Got endpoints: latency-svc-c56tv [212.055107ms]
Nov 16 20:27:40.714: INFO: Got endpoints: latency-svc-crcb4 [209.833429ms]
Nov 16 20:27:40.720: INFO: Created: latency-svc-kftp7
Nov 16 20:27:40.733: INFO: Got endpoints: latency-svc-kftp7 [209.312633ms]
Nov 16 20:27:40.736: INFO: Created: latency-svc-75ld5
Nov 16 20:27:40.744: INFO: Created: latency-svc-5gjj7
Nov 16 20:27:40.748: INFO: Got endpoints: latency-svc-75ld5 [217.287212ms]
Nov 16 20:27:40.754: INFO: Got endpoints: latency-svc-5gjj7 [211.9523ms]
Nov 16 20:27:40.759: INFO: Created: latency-svc-cb5bd
Nov 16 20:27:40.769: INFO: Created: latency-svc-72v45
Nov 16 20:27:40.772: INFO: Got endpoints: latency-svc-cb5bd [212.830119ms]
Nov 16 20:27:40.779: INFO: Got endpoints: latency-svc-72v45 [206.281524ms]
Nov 16 20:27:40.780: INFO: Created: latency-svc-gk76k
Nov 16 20:27:40.790: INFO: Got endpoints: latency-svc-gk76k [202.256552ms]
Nov 16 20:27:40.792: INFO: Created: latency-svc-hs868
Nov 16 20:27:40.803: INFO: Created: latency-svc-9lgnq
Nov 16 20:27:40.809: INFO: Got endpoints: latency-svc-hs868 [211.970139ms]
Nov 16 20:27:40.817: INFO: Got endpoints: latency-svc-9lgnq [197.710878ms]
Nov 16 20:27:40.822: INFO: Created: latency-svc-xn8kw
Nov 16 20:27:40.832: INFO: Got endpoints: latency-svc-xn8kw [194.072496ms]
Nov 16 20:27:40.835: INFO: Created: latency-svc-qz8fw
Nov 16 20:27:40.845: INFO: Got endpoints: latency-svc-qz8fw [199.569275ms]
Nov 16 20:27:40.847: INFO: Created: latency-svc-7vbqx
Nov 16 20:27:40.860: INFO: Got endpoints: latency-svc-7vbqx [197.205755ms]
Nov 16 20:27:40.860: INFO: Created: latency-svc-j9r57
Nov 16 20:27:40.870: INFO: Got endpoints: latency-svc-j9r57 [189.465381ms]
Nov 16 20:27:40.871: INFO: Created: latency-svc-tgjcd
Nov 16 20:27:40.885: INFO: Created: latency-svc-78c5m
Nov 16 20:27:40.886: INFO: Got endpoints: latency-svc-tgjcd [197.470793ms]
Nov 16 20:27:40.896: INFO: Created: latency-svc-9djm6
Nov 16 20:27:40.899: INFO: Got endpoints: latency-svc-78c5m [193.746344ms]
Nov 16 20:27:40.909: INFO: Created: latency-svc-bnm5j
Nov 16 20:27:40.912: INFO: Got endpoints: latency-svc-9djm6 [197.349139ms]
Nov 16 20:27:40.924: INFO: Created: latency-svc-bc54g
Nov 16 20:27:40.924: INFO: Got endpoints: latency-svc-bnm5j [190.709584ms]
Nov 16 20:27:40.934: INFO: Got endpoints: latency-svc-bc54g [184.941439ms]
Nov 16 20:27:40.937: INFO: Created: latency-svc-2rjdp
Nov 16 20:27:40.949: INFO: Got endpoints: latency-svc-2rjdp [195.074909ms]
Nov 16 20:27:40.951: INFO: Created: latency-svc-88j4l
Nov 16 20:27:40.962: INFO: Got endpoints: latency-svc-88j4l [189.967232ms]
Nov 16 20:27:40.964: INFO: Created: latency-svc-84kqq
Nov 16 20:27:40.976: INFO: Got endpoints: latency-svc-84kqq [197.132295ms]
Nov 16 20:27:40.984: INFO: Created: latency-svc-mzdhq
Nov 16 20:27:40.993: INFO: Created: latency-svc-qqpvm
Nov 16 20:27:40.994: INFO: Got endpoints: latency-svc-mzdhq [204.094784ms]
Nov 16 20:27:41.005: INFO: Got endpoints: latency-svc-qqpvm [196.403052ms]
Nov 16 20:27:41.010: INFO: Created: latency-svc-khc55
Nov 16 20:27:41.023: INFO: Created: latency-svc-bp8k8
Nov 16 20:27:41.024: INFO: Got endpoints: latency-svc-khc55 [206.663695ms]
Nov 16 20:27:41.034: INFO: Got endpoints: latency-svc-bp8k8 [201.144264ms]
Nov 16 20:27:41.040: INFO: Created: latency-svc-fgvll
Nov 16 20:27:41.051: INFO: Created: latency-svc-tl6j7
Nov 16 20:27:41.051: INFO: Got endpoints: latency-svc-fgvll [205.234297ms]
Nov 16 20:27:41.061: INFO: Got endpoints: latency-svc-tl6j7 [200.712435ms]
Nov 16 20:27:41.067: INFO: Created: latency-svc-9f28r
Nov 16 20:27:41.073: INFO: Got endpoints: latency-svc-9f28r [202.720761ms]
Nov 16 20:27:41.074: INFO: Created: latency-svc-hckjl
Nov 16 20:27:41.084: INFO: Got endpoints: latency-svc-hckjl [198.003161ms]
Nov 16 20:27:41.085: INFO: Created: latency-svc-vq24c
Nov 16 20:27:41.097: INFO: Got endpoints: latency-svc-vq24c [197.189846ms]
Nov 16 20:27:41.097: INFO: Created: latency-svc-rgp9k
Nov 16 20:27:41.116: INFO: Created: latency-svc-bxs88
Nov 16 20:27:41.124: INFO: Got endpoints: latency-svc-rgp9k [211.467418ms]
Nov 16 20:27:41.128: INFO: Got endpoints: latency-svc-bxs88 [203.398135ms]
Nov 16 20:27:41.138: INFO: Created: latency-svc-599ck
Nov 16 20:27:41.147: INFO: Got endpoints: latency-svc-599ck [213.403409ms]
Nov 16 20:27:41.149: INFO: Created: latency-svc-r6s5q
Nov 16 20:27:41.158: INFO: Created: latency-svc-njwp4
Nov 16 20:27:41.159: INFO: Got endpoints: latency-svc-r6s5q [209.210032ms]
Nov 16 20:27:41.169: INFO: Got endpoints: latency-svc-njwp4 [206.690516ms]
Nov 16 20:27:41.172: INFO: Created: latency-svc-8vj6f
Nov 16 20:27:41.184: INFO: Got endpoints: latency-svc-8vj6f [207.931645ms]
Nov 16 20:27:41.187: INFO: Created: latency-svc-xzhdk
Nov 16 20:27:41.195: INFO: Created: latency-svc-tzh74
Nov 16 20:27:41.198: INFO: Got endpoints: latency-svc-xzhdk [204.162884ms]
Nov 16 20:27:41.207: INFO: Created: latency-svc-gw242
Nov 16 20:27:41.208: INFO: Got endpoints: latency-svc-tzh74 [201.988886ms]
Nov 16 20:27:41.223: INFO: Got endpoints: latency-svc-gw242 [198.11707ms]
Nov 16 20:27:41.226: INFO: Created: latency-svc-94knc
Nov 16 20:27:41.237: INFO: Got endpoints: latency-svc-94knc [203.380083ms]
Nov 16 20:27:41.237: INFO: Created: latency-svc-8vkxp
Nov 16 20:27:41.246: INFO: Got endpoints: latency-svc-8vkxp [194.680896ms]
Nov 16 20:27:41.246: INFO: Latencies: [37.294218ms 43.664093ms 53.975785ms 62.30147ms 74.953448ms 84.942551ms 98.677718ms 111.110893ms 120.88322ms 153.688941ms 165.691865ms 166.284305ms 166.906976ms 168.397266ms 173.414555ms 174.940602ms 175.569081ms 175.906203ms 176.067012ms 176.207947ms 179.680993ms 183.663236ms 184.941439ms 185.815303ms 186.206247ms 188.68196ms 189.465381ms 189.967232ms 190.709584ms 192.305143ms 193.1811ms 193.182683ms 193.343648ms 193.746344ms 193.780304ms 193.797121ms 193.89114ms 194.072496ms 194.649857ms 194.680896ms 195.074909ms 195.218173ms 195.40709ms 195.765693ms 196.403052ms 196.9399ms 197.132295ms 197.160718ms 197.189846ms 197.205755ms 197.222754ms 197.349139ms 197.470793ms 197.621861ms 197.661872ms 197.710878ms 197.977172ms 197.98025ms 197.993891ms 198.003161ms 198.109706ms 198.11707ms 198.159829ms 198.170779ms 198.179745ms 198.37526ms 198.600516ms 198.652393ms 198.979021ms 199.216732ms 199.508328ms 199.528657ms 199.569275ms 199.725037ms 200.254834ms 200.568097ms 200.570643ms 200.712435ms 201.030695ms 201.144264ms 201.226869ms 201.259306ms 201.434019ms 201.528323ms 201.729139ms 201.988886ms 202.04351ms 202.256552ms 202.443568ms 202.720761ms 202.777276ms 202.907116ms 203.373404ms 203.377001ms 203.380083ms 203.397044ms 203.398135ms 203.40959ms 203.548404ms 203.636733ms 203.876049ms 203.913497ms 203.986296ms 204.015123ms 204.050884ms 204.094784ms 204.162884ms 204.346673ms 204.413333ms 204.740867ms 204.753114ms 205.090752ms 205.234297ms 205.502908ms 205.507532ms 205.723701ms 205.968101ms 206.281524ms 206.427709ms 206.476028ms 206.618481ms 206.663695ms 206.690516ms 206.946609ms 206.968538ms 207.218788ms 207.57108ms 207.931645ms 208.155442ms 208.619046ms 209.032489ms 209.210032ms 209.312633ms 209.673606ms 209.681493ms 209.833429ms 211.467418ms 211.732049ms 211.884037ms 211.9523ms 211.970139ms 212.055107ms 212.155389ms 212.830119ms 213.098401ms 213.403409ms 213.452569ms 213.649137ms 213.690786ms 213.838088ms 213.91497ms 214.107362ms 214.499125ms 214.914182ms 215.432ms 215.525325ms 215.802666ms 216.629457ms 216.938567ms 216.939255ms 216.946682ms 217.273961ms 217.287212ms 217.591794ms 218.075032ms 218.449009ms 219.470542ms 219.526343ms 220.024026ms 220.140656ms 220.4233ms 220.465366ms 221.864421ms 222.085718ms 222.148698ms 222.244547ms 222.575819ms 223.142074ms 223.830074ms 223.974112ms 224.993895ms 225.301629ms 226.034377ms 226.127538ms 226.188364ms 226.433594ms 226.889002ms 227.35333ms 227.445645ms 227.721066ms 228.086487ms 228.40863ms 229.203549ms 230.062193ms 230.064705ms 231.132215ms 235.360711ms 239.890284ms 241.545363ms 260.530917ms]
Nov 16 20:27:41.247: INFO: 50 %ile: 203.876049ms
Nov 16 20:27:41.247: INFO: 90 %ile: 224.993895ms
Nov 16 20:27:41.247: INFO: 99 %ile: 241.545363ms
Nov 16 20:27:41.247: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:27:41.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-8533" for this suite.
Nov 16 20:28:03.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:28:05.194: INFO: namespace svc-latency-8533 deletion completed in 23.936269699s

• [SLOW TEST:29.024 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:28:05.197: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:173
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating server pod server in namespace prestop-3692
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-3692
STEP: Deleting pre-stop pod
Nov 16 20:28:14.447: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:28:14.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-3692" for this suite.
Nov 16 20:28:54.500: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:28:56.419: INFO: namespace prestop-3692 deletion completed in 41.9451377s

• [SLOW TEST:51.222 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:28:56.419: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-461
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 16 20:28:56.564: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Nov 16 20:29:18.816: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.158.248:8080/dial?request=hostName&protocol=http&host=172.30.158.247&port=8080&tries=1'] Namespace:pod-network-test-461 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 16 20:29:18.816: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
Nov 16 20:29:19.005: INFO: Waiting for endpoints: map[]
Nov 16 20:29:19.014: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.158.248:8080/dial?request=hostName&protocol=http&host=172.30.216.231&port=8080&tries=1'] Namespace:pod-network-test-461 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 16 20:29:19.014: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
Nov 16 20:29:19.198: INFO: Waiting for endpoints: map[]
Nov 16 20:29:19.206: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.158.248:8080/dial?request=hostName&protocol=http&host=172.30.194.246&port=8080&tries=1'] Namespace:pod-network-test-461 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 16 20:29:19.206: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
Nov 16 20:29:19.391: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:29:19.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-461" for this suite.
Nov 16 20:29:27.432: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:29:29.353: INFO: namespace pod-network-test-461 deletion completed in 9.951184024s

• [SLOW TEST:32.934 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:29:29.354: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:29:31.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2261" for this suite.
Nov 16 20:30:19.593: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:30:21.497: INFO: namespace kubelet-test-2261 deletion completed in 49.93600888s

• [SLOW TEST:52.144 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a read only busybox container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:30:21.498: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 20:30:21.654: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-d5d86d58-238f-406f-a538-7be944b84104
STEP: Creating secret with name s-test-opt-upd-ffb619a7-c8cf-4aa3-8c40-16e001b90827
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-d5d86d58-238f-406f-a538-7be944b84104
STEP: Updating secret s-test-opt-upd-ffb619a7-c8cf-4aa3-8c40-16e001b90827
STEP: Creating secret with name s-test-opt-create-e63ac40a-1bad-4ad1-95ef-3312fd3238c0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:31:42.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9306" for this suite.
Nov 16 20:32:12.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:32:14.781: INFO: namespace secrets-9306 deletion completed in 31.94636779s

• [SLOW TEST:113.283 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:32:14.782: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7237
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-7237
STEP: creating replication controller externalsvc in namespace services-7237
I1116 20:32:14.949061      26 runners.go:184] Created replication controller with name: externalsvc, namespace: services-7237, replica count: 2
I1116 20:32:17.999609      26 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Nov 16 20:32:18.045: INFO: Creating new exec pod
Nov 16 20:32:20.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=services-7237 execpod7v744 -- /bin/sh -x -c nslookup clusterip-service'
Nov 16 20:32:20.403: INFO: stderr: "+ nslookup clusterip-service\n"
Nov 16 20:32:20.403: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-7237.svc.cluster.local\tcanonical name = externalsvc.services-7237.svc.cluster.local.\nName:\texternalsvc.services-7237.svc.cluster.local\nAddress: 172.21.155.142\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7237, will wait for the garbage collector to delete the pods
Nov 16 20:32:20.478: INFO: Deleting ReplicationController externalsvc took: 17.841504ms
Nov 16 20:32:20.882: INFO: Terminating ReplicationController externalsvc pods took: 403.536549ms
Nov 16 20:32:31.723: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:32:31.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7237" for this suite.
Nov 16 20:32:39.793: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:32:41.700: INFO: namespace services-7237 deletion completed in 9.940725679s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:26.918 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:32:41.701: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Nov 16 20:32:41.812: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 16 20:32:41.845: INFO: Waiting for terminating namespaces to be deleted...
Nov 16 20:32:41.855: INFO: 
Logging pods the kubelet thinks is on node 10.240.167.206 before test
Nov 16 20:32:41.924: INFO: multus-gnk8t from openshift-multus started at 2020-11-16 17:39:32 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.924: INFO: 	Container kube-multus ready: true, restart count 0
Nov 16 20:32:41.924: INFO: ibmcloud-block-storage-driver-2s8xr from kube-system started at 2020-11-16 17:39:34 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.924: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 16 20:32:41.924: INFO: prometheus-adapter-554fc6c4db-6qn5b from openshift-monitoring started at 2020-11-16 17:45:54 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.924: INFO: 	Container prometheus-adapter ready: true, restart count 0
Nov 16 20:32:41.924: INFO: openshift-state-metrics-86c5b47587-nhfn8 from openshift-monitoring started at 2020-11-16 17:41:13 +0000 UTC (3 container statuses recorded)
Nov 16 20:32:41.924: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Nov 16 20:32:41.924: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Nov 16 20:32:41.924: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Nov 16 20:32:41.924: INFO: calico-typha-b5486f777-9gw72 from calico-system started at 2020-11-16 17:42:09 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.925: INFO: 	Container calico-typha ready: true, restart count 0
Nov 16 20:32:41.925: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-11-16 17:44:46 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.925: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Nov 16 20:32:41.925: INFO: thanos-querier-54b47b4fc4-rcx4j from openshift-monitoring started at 2020-11-16 17:46:47 +0000 UTC (4 container statuses recorded)
Nov 16 20:32:41.925: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 20:32:41.925: INFO: 	Container oauth-proxy ready: true, restart count 0
Nov 16 20:32:41.925: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 20:32:41.925: INFO: 	Container thanos-querier ready: true, restart count 0
Nov 16 20:32:41.925: INFO: openshift-kube-proxy-5kpn9 from openshift-kube-proxy started at 2020-11-16 17:39:32 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.925: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 16 20:32:41.925: INFO: multus-admission-controller-x6jqb from openshift-multus started at 2020-11-16 17:40:32 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.925: INFO: 	Container multus-admission-controller ready: true, restart count 0
Nov 16 20:32:41.925: INFO: tuned-9m9tq from openshift-cluster-node-tuning-operator started at 2020-11-16 17:40:38 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.925: INFO: 	Container tuned ready: true, restart count 0
Nov 16 20:32:41.925: INFO: node-ca-2pgxt from openshift-image-registry started at 2020-11-16 17:41:42 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.925: INFO: 	Container node-ca ready: true, restart count 0
Nov 16 20:32:41.925: INFO: sonobuoy-systemd-logs-daemon-set-18804f2d5c884991-lmbvb from sonobuoy started at 2020-11-16 19:14:33 +0000 UTC (2 container statuses recorded)
Nov 16 20:32:41.925: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Nov 16 20:32:41.925: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 16 20:32:41.925: INFO: ibm-keepalived-watcher-2t6l4 from kube-system started at 2020-11-16 17:39:32 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.925: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 20:32:41.925: INFO: kube-state-metrics-776f8894df-p8xsw from openshift-monitoring started at 2020-11-16 17:41:15 +0000 UTC (3 container statuses recorded)
Nov 16 20:32:41.925: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Nov 16 20:32:41.925: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Nov 16 20:32:41.925: INFO: 	Container kube-state-metrics ready: true, restart count 0
Nov 16 20:32:41.925: INFO: packageserver-68c9b7ddbb-rqt2z from openshift-operator-lifecycle-manager started at 2020-11-16 17:44:04 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.925: INFO: 	Container packageserver ready: true, restart count 0
Nov 16 20:32:41.925: INFO: dns-default-zqnbq from openshift-dns started at 2020-11-16 17:42:09 +0000 UTC (2 container statuses recorded)
Nov 16 20:32:41.925: INFO: 	Container dns ready: true, restart count 0
Nov 16 20:32:41.925: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 16 20:32:41.925: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-11-16 17:47:07 +0000 UTC (7 container statuses recorded)
Nov 16 20:32:41.925: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 20:32:41.925: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 20:32:41.925: INFO: 	Container prometheus ready: true, restart count 1
Nov 16 20:32:41.925: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Nov 16 20:32:41.925: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov 16 20:32:41.925: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Nov 16 20:32:41.925: INFO: 	Container thanos-sidecar ready: true, restart count 0
Nov 16 20:32:41.925: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-11-16 17:46:19 +0000 UTC (3 container statuses recorded)
Nov 16 20:32:41.925: INFO: 	Container alertmanager ready: true, restart count 0
Nov 16 20:32:41.925: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 16 20:32:41.925: INFO: 	Container config-reloader ready: true, restart count 0
Nov 16 20:32:41.925: INFO: calico-node-pk4fl from calico-system started at 2020-11-16 17:40:12 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.925: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 20:32:41.925: INFO: cluster-samples-operator-55cf746658-rtxxg from openshift-cluster-samples-operator started at 2020-11-16 17:42:11 +0000 UTC (2 container statuses recorded)
Nov 16 20:32:41.925: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Nov 16 20:32:41.925: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Nov 16 20:32:41.925: INFO: ibm-master-proxy-static-10.240.167.206 from kube-system started at 2020-11-16 17:39:29 +0000 UTC (2 container statuses recorded)
Nov 16 20:32:41.925: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 20:32:41.925: INFO: 	Container pause ready: true, restart count 0
Nov 16 20:32:41.925: INFO: service-serving-cert-signer-787695f6b4-s4fqz from openshift-service-ca started at 2020-11-16 17:41:13 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.925: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
Nov 16 20:32:41.925: INFO: node-exporter-jssfh from openshift-monitoring started at 2020-11-16 17:41:17 +0000 UTC (2 container statuses recorded)
Nov 16 20:32:41.925: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 20:32:41.925: INFO: 	Container node-exporter ready: true, restart count 0
Nov 16 20:32:41.925: INFO: router-default-589c4b7b87-7299r from openshift-ingress started at 2020-11-16 17:41:37 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.925: INFO: 	Container router ready: true, restart count 0
Nov 16 20:32:41.925: INFO: console-7f85d88fbc-4r5lv from openshift-console started at 2020-11-16 17:42:45 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.925: INFO: 	Container console ready: true, restart count 0
Nov 16 20:32:41.925: INFO: 
Logging pods the kubelet thinks is on node 10.240.167.209 before test
Nov 16 20:32:41.984: INFO: calico-typha-b5486f777-7g7gg from calico-system started at 2020-11-16 17:40:11 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.984: INFO: 	Container calico-typha ready: true, restart count 0
Nov 16 20:32:41.984: INFO: calico-node-tc4r7 from calico-system started at 2020-11-16 17:40:12 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.984: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 20:32:41.984: INFO: tuned-cp7vt from openshift-cluster-node-tuning-operator started at 2020-11-16 17:40:38 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.984: INFO: 	Container tuned ready: true, restart count 0
Nov 16 20:32:41.984: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-11-16 17:46:06 +0000 UTC (3 container statuses recorded)
Nov 16 20:32:41.985: INFO: 	Container alertmanager ready: true, restart count 0
Nov 16 20:32:41.985: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 16 20:32:41.985: INFO: 	Container config-reloader ready: true, restart count 0
Nov 16 20:32:41.985: INFO: packageserver-68c9b7ddbb-phc28 from openshift-operator-lifecycle-manager started at 2020-11-16 17:44:00 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.985: INFO: 	Container packageserver ready: true, restart count 0
Nov 16 20:32:41.985: INFO: thanos-querier-54b47b4fc4-zvlf7 from openshift-monitoring started at 2020-11-16 17:46:57 +0000 UTC (4 container statuses recorded)
Nov 16 20:32:41.985: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 20:32:41.985: INFO: 	Container oauth-proxy ready: true, restart count 0
Nov 16 20:32:41.985: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 20:32:41.985: INFO: 	Container thanos-querier ready: true, restart count 0
Nov 16 20:32:41.985: INFO: router-default-589c4b7b87-74cdn from openshift-ingress started at 2020-11-16 17:41:37 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.985: INFO: 	Container router ready: true, restart count 0
Nov 16 20:32:41.985: INFO: redhat-operators-7f9c6c576-rs2f7 from openshift-marketplace started at 2020-11-16 17:42:33 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.985: INFO: 	Container redhat-operators ready: true, restart count 0
Nov 16 20:32:41.985: INFO: node-exporter-lqwgh from openshift-monitoring started at 2020-11-16 17:41:17 +0000 UTC (2 container statuses recorded)
Nov 16 20:32:41.985: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 20:32:41.985: INFO: 	Container node-exporter ready: true, restart count 0
Nov 16 20:32:41.985: INFO: prometheus-operator-7646fdcb7b-tw6jj from openshift-monitoring started at 2020-11-16 17:45:42 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.985: INFO: 	Container prometheus-operator ready: true, restart count 0
Nov 16 20:32:41.985: INFO: grafana-58c46d5bb-hs8tj from openshift-monitoring started at 2020-11-16 17:46:00 +0000 UTC (2 container statuses recorded)
Nov 16 20:32:41.985: INFO: 	Container grafana ready: true, restart count 0
Nov 16 20:32:41.986: INFO: 	Container grafana-proxy ready: true, restart count 0
Nov 16 20:32:41.986: INFO: sonobuoy-systemd-logs-daemon-set-18804f2d5c884991-rfhks from sonobuoy started at 2020-11-16 19:14:33 +0000 UTC (2 container statuses recorded)
Nov 16 20:32:41.986: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Nov 16 20:32:41.986: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 16 20:32:41.986: INFO: ibm-keepalived-watcher-x4nqc from kube-system started at 2020-11-16 17:39:19 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.986: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 20:32:41.986: INFO: multus-admission-controller-tf7n7 from openshift-multus started at 2020-11-16 17:40:30 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.986: INFO: 	Container multus-admission-controller ready: true, restart count 0
Nov 16 20:32:41.986: INFO: configmap-cabundle-injector-5bd6fcf58-hlwpk from openshift-service-ca started at 2020-11-16 17:41:14 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.986: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
Nov 16 20:32:41.986: INFO: ibm-master-proxy-static-10.240.167.209 from kube-system started at 2020-11-16 17:39:17 +0000 UTC (2 container statuses recorded)
Nov 16 20:32:41.986: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 20:32:41.986: INFO: 	Container pause ready: true, restart count 0
Nov 16 20:32:41.986: INFO: openshift-kube-proxy-qfwg7 from openshift-kube-proxy started at 2020-11-16 17:39:31 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.986: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 16 20:32:41.986: INFO: apiservice-cabundle-injector-75dcd4f8db-sllzv from openshift-service-ca started at 2020-11-16 17:41:13 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.986: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
Nov 16 20:32:41.986: INFO: community-operators-587cdbd4fb-5mwwt from openshift-marketplace started at 2020-11-16 17:42:34 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.986: INFO: 	Container community-operators ready: true, restart count 0
Nov 16 20:32:41.986: INFO: vpn-679798bf87-s5h4z from kube-system started at 2020-11-16 17:44:16 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.986: INFO: 	Container vpn ready: true, restart count 0
Nov 16 20:32:41.986: INFO: prometheus-adapter-554fc6c4db-8f7cc from openshift-monitoring started at 2020-11-16 17:45:54 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.986: INFO: 	Container prometheus-adapter ready: true, restart count 0
Nov 16 20:32:41.986: INFO: multus-dfpk4 from openshift-multus started at 2020-11-16 17:39:24 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.986: INFO: 	Container kube-multus ready: true, restart count 0
Nov 16 20:32:41.986: INFO: node-ca-mg4vg from openshift-image-registry started at 2020-11-16 17:41:42 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.987: INFO: 	Container node-ca ready: true, restart count 0
Nov 16 20:32:41.987: INFO: certified-operators-676df69595-kgtfx from openshift-marketplace started at 2020-11-16 17:42:33 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.987: INFO: 	Container certified-operators ready: true, restart count 0
Nov 16 20:32:41.987: INFO: registry-pvc-permissions-l4jgc from openshift-image-registry started at 2020-11-16 17:44:28 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.987: INFO: 	Container pvc-permissions ready: false, restart count 0
Nov 16 20:32:41.987: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-11-16 17:46:57 +0000 UTC (7 container statuses recorded)
Nov 16 20:32:41.987: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 20:32:41.987: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 20:32:41.987: INFO: 	Container prometheus ready: true, restart count 1
Nov 16 20:32:41.987: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Nov 16 20:32:41.987: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov 16 20:32:41.987: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Nov 16 20:32:41.987: INFO: 	Container thanos-sidecar ready: true, restart count 0
Nov 16 20:32:41.987: INFO: ibmcloud-block-storage-driver-6zlqm from kube-system started at 2020-11-16 17:39:27 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.987: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 16 20:32:41.987: INFO: image-registry-77d48f6786-gqts2 from openshift-image-registry started at 2020-11-16 17:44:28 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.987: INFO: 	Container registry ready: true, restart count 0
Nov 16 20:32:41.987: INFO: console-7f85d88fbc-fdc4m from openshift-console started at 2020-11-16 17:41:44 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.987: INFO: 	Container console ready: true, restart count 0
Nov 16 20:32:41.987: INFO: dns-default-m5gqp from openshift-dns started at 2020-11-16 17:42:09 +0000 UTC (2 container statuses recorded)
Nov 16 20:32:41.987: INFO: 	Container dns ready: true, restart count 0
Nov 16 20:32:41.987: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 16 20:32:41.987: INFO: ibm-cloud-provider-ip-161-156-99-92-596ff786f9-gqmvr from ibm-system started at 2020-11-16 17:46:26 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.987: INFO: 	Container ibm-cloud-provider-ip-161-156-99-92 ready: true, restart count 0
Nov 16 20:32:41.987: INFO: 
Logging pods the kubelet thinks is on node 10.240.167.254 before test
Nov 16 20:32:42.082: INFO: ibm-master-proxy-static-10.240.167.254 from kube-system started at 2020-11-16 17:38:51 +0000 UTC (2 container statuses recorded)
Nov 16 20:32:42.082: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 20:32:42.082: INFO: 	Container pause ready: true, restart count 0
Nov 16 20:32:42.082: INFO: ibm-keepalived-watcher-gw7ng from kube-system started at 2020-11-16 17:38:54 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:42.082: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 20:32:42.082: INFO: service-ca-operator-5db7bd4f-qmnrv from openshift-service-ca-operator started at 2020-11-16 17:40:25 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:42.082: INFO: 	Container operator ready: true, restart count 0
Nov 16 20:32:42.082: INFO: ingress-operator-57d688547-8j97m from openshift-ingress-operator started at 2020-11-16 17:40:25 +0000 UTC (2 container statuses recorded)
Nov 16 20:32:42.082: INFO: 	Container ingress-operator ready: true, restart count 0
Nov 16 20:32:42.082: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 20:32:42.082: INFO: ibm-file-plugin-75bbff878-w8grr from kube-system started at 2020-11-16 17:40:27 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:42.082: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Nov 16 20:32:42.082: INFO: tigera-operator-798cfbf7dd-6rmk9 from tigera-operator started at 2020-11-16 17:39:02 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:42.082: INFO: 	Container tigera-operator ready: true, restart count 2
Nov 16 20:32:42.082: INFO: openshift-service-catalog-controller-manager-operator-595fxjf2f from openshift-service-catalog-controller-manager-operator started at 2020-11-16 17:40:24 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:42.082: INFO: 	Container operator ready: true, restart count 1
Nov 16 20:32:42.082: INFO: olm-operator-88bfd79df-7mm7b from openshift-operator-lifecycle-manager started at 2020-11-16 17:40:24 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:42.082: INFO: 	Container olm-operator ready: true, restart count 0
Nov 16 20:32:42.083: INFO: ibm-storage-watcher-69d9c445b4-k5bvh from kube-system started at 2020-11-16 17:40:24 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:42.083: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Nov 16 20:32:42.083: INFO: cluster-storage-operator-6488c9f77b-wq8vv from openshift-cluster-storage-operator started at 2020-11-16 17:40:27 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:42.083: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Nov 16 20:32:42.083: INFO: dns-operator-847f45b78c-49hz4 from openshift-dns-operator started at 2020-11-16 17:40:24 +0000 UTC (2 container statuses recorded)
Nov 16 20:32:42.083: INFO: 	Container dns-operator ready: true, restart count 0
Nov 16 20:32:42.083: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 20:32:42.083: INFO: cluster-node-tuning-operator-5c44ccc99b-6g94b from openshift-cluster-node-tuning-operator started at 2020-11-16 17:40:25 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:42.083: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Nov 16 20:32:42.083: INFO: multus-admission-controller-xw5h7 from openshift-multus started at 2020-11-16 17:40:25 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:42.083: INFO: 	Container multus-admission-controller ready: true, restart count 0
Nov 16 20:32:42.083: INFO: node-ca-852zk from openshift-image-registry started at 2020-11-16 17:41:42 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:42.083: INFO: 	Container node-ca ready: true, restart count 0
Nov 16 20:32:42.083: INFO: downloads-5dbb4f4ff7-mtjcb from openshift-console started at 2020-11-16 17:40:25 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:42.083: INFO: 	Container download-server ready: true, restart count 0
Nov 16 20:32:42.083: INFO: downloads-5dbb4f4ff7-krzbx from openshift-console started at 2020-11-16 17:40:25 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:42.083: INFO: 	Container download-server ready: true, restart count 0
Nov 16 20:32:42.083: INFO: node-exporter-4kz5x from openshift-monitoring started at 2020-11-16 17:41:17 +0000 UTC (2 container statuses recorded)
Nov 16 20:32:42.083: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 20:32:42.084: INFO: 	Container node-exporter ready: true, restart count 0
Nov 16 20:32:42.084: INFO: sonobuoy-systemd-logs-daemon-set-18804f2d5c884991-ffhv2 from sonobuoy started at 2020-11-16 19:14:33 +0000 UTC (2 container statuses recorded)
Nov 16 20:32:42.084: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Nov 16 20:32:42.084: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 16 20:32:42.084: INFO: network-operator-74fc599569-c45s6 from openshift-network-operator started at 2020-11-16 17:39:02 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:42.084: INFO: 	Container network-operator ready: true, restart count 0
Nov 16 20:32:42.084: INFO: ibmcloud-block-storage-driver-qdh9z from kube-system started at 2020-11-16 17:39:02 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:42.084: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 16 20:32:42.084: INFO: calico-node-s6qzg from calico-system started at 2020-11-16 17:40:12 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:42.084: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 20:32:42.084: INFO: cluster-monitoring-operator-7d44956445-lqkh9 from openshift-monitoring started at 2020-11-16 17:40:24 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:42.084: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Nov 16 20:32:42.084: INFO: cluster-image-registry-operator-8d47696c5-hpv5z from openshift-image-registry started at 2020-11-16 17:40:25 +0000 UTC (2 container statuses recorded)
Nov 16 20:32:42.084: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Nov 16 20:32:42.084: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
Nov 16 20:32:42.084: INFO: tuned-n7h6s from openshift-cluster-node-tuning-operator started at 2020-11-16 17:40:38 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:42.084: INFO: 	Container tuned ready: true, restart count 0
Nov 16 20:32:42.084: INFO: telemeter-client-95c48d495-ctf9b from openshift-monitoring started at 2020-11-16 17:45:52 +0000 UTC (3 container statuses recorded)
Nov 16 20:32:42.084: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 20:32:42.084: INFO: 	Container reload ready: true, restart count 0
Nov 16 20:32:42.084: INFO: 	Container telemeter-client ready: true, restart count 0
Nov 16 20:32:42.084: INFO: openshift-kube-proxy-smf7r from openshift-kube-proxy started at 2020-11-16 17:39:31 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:42.084: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 16 20:32:42.084: INFO: openshift-service-catalog-apiserver-operator-86b98d6fff-znhvn from openshift-service-catalog-apiserver-operator started at 2020-11-16 17:40:25 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:42.084: INFO: 	Container operator ready: true, restart count 1
Nov 16 20:32:42.085: INFO: sonobuoy from sonobuoy started at 2020-11-16 19:14:22 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:42.085: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 16 20:32:42.085: INFO: sonobuoy-e2e-job-b6bf7e56608b4535 from sonobuoy started at 2020-11-16 19:14:33 +0000 UTC (2 container statuses recorded)
Nov 16 20:32:42.085: INFO: 	Container e2e ready: true, restart count 0
Nov 16 20:32:42.085: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 20:32:42.085: INFO: multus-gf29b from openshift-multus started at 2020-11-16 17:39:24 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:42.085: INFO: 	Container kube-multus ready: true, restart count 0
Nov 16 20:32:42.085: INFO: marketplace-operator-54847664c-cbqsc from openshift-marketplace started at 2020-11-16 17:40:25 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:42.085: INFO: 	Container marketplace-operator ready: true, restart count 0
Nov 16 20:32:42.085: INFO: calico-kube-controllers-599969f895-4vf88 from calico-system started at 2020-11-16 17:40:25 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:42.085: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Nov 16 20:32:42.085: INFO: ibmcloud-block-storage-plugin-79495594d5-6wqx5 from kube-system started at 2020-11-16 17:40:25 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:42.085: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Nov 16 20:32:42.085: INFO: dns-default-rtvgx from openshift-dns started at 2020-11-16 17:42:09 +0000 UTC (2 container statuses recorded)
Nov 16 20:32:42.085: INFO: 	Container dns ready: true, restart count 0
Nov 16 20:32:42.085: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 16 20:32:42.085: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-11-16 17:45:54 +0000 UTC (3 container statuses recorded)
Nov 16 20:32:42.085: INFO: 	Container alertmanager ready: true, restart count 0
Nov 16 20:32:42.085: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 16 20:32:42.085: INFO: 	Container config-reloader ready: true, restart count 0
Nov 16 20:32:42.085: INFO: ibm-cloud-provider-ip-161-156-99-92-596ff786f9-8zlk8 from ibm-system started at 2020-11-16 17:46:19 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:42.085: INFO: 	Container ibm-cloud-provider-ip-161-156-99-92 ready: true, restart count 0
Nov 16 20:32:42.085: INFO: console-operator-c9844474-csp4h from openshift-console-operator started at 2020-11-16 17:40:25 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:42.085: INFO: 	Container console-operator ready: true, restart count 1
Nov 16 20:32:42.085: INFO: catalog-operator-86d68f4684-n8kk4 from openshift-operator-lifecycle-manager started at 2020-11-16 17:40:27 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:42.085: INFO: 	Container catalog-operator ready: true, restart count 0
Nov 16 20:32:42.085: INFO: calico-typha-b5486f777-km6dh from calico-system started at 2020-11-16 17:42:09 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:42.086: INFO: 	Container calico-typha ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: verifying the node has the label node 10.240.167.206
STEP: verifying the node has the label node 10.240.167.209
STEP: verifying the node has the label node 10.240.167.254
Nov 16 20:32:42.227: INFO: Pod calico-kube-controllers-599969f895-4vf88 requesting resource cpu=0m on Node 10.240.167.254
Nov 16 20:32:42.227: INFO: Pod calico-node-pk4fl requesting resource cpu=0m on Node 10.240.167.206
Nov 16 20:32:42.227: INFO: Pod calico-node-s6qzg requesting resource cpu=0m on Node 10.240.167.254
Nov 16 20:32:42.227: INFO: Pod calico-node-tc4r7 requesting resource cpu=0m on Node 10.240.167.209
Nov 16 20:32:42.227: INFO: Pod calico-typha-b5486f777-7g7gg requesting resource cpu=0m on Node 10.240.167.209
Nov 16 20:32:42.227: INFO: Pod calico-typha-b5486f777-9gw72 requesting resource cpu=0m on Node 10.240.167.206
Nov 16 20:32:42.227: INFO: Pod calico-typha-b5486f777-km6dh requesting resource cpu=0m on Node 10.240.167.254
Nov 16 20:32:42.227: INFO: Pod test-k8s-e2e-pvg-master-verification requesting resource cpu=0m on Node 10.240.167.206
Nov 16 20:32:42.227: INFO: Pod ibm-cloud-provider-ip-161-156-99-92-596ff786f9-8zlk8 requesting resource cpu=5m on Node 10.240.167.254
Nov 16 20:32:42.227: INFO: Pod ibm-cloud-provider-ip-161-156-99-92-596ff786f9-gqmvr requesting resource cpu=5m on Node 10.240.167.209
Nov 16 20:32:42.227: INFO: Pod ibm-file-plugin-75bbff878-w8grr requesting resource cpu=50m on Node 10.240.167.254
Nov 16 20:32:42.227: INFO: Pod ibm-keepalived-watcher-2t6l4 requesting resource cpu=5m on Node 10.240.167.206
Nov 16 20:32:42.228: INFO: Pod ibm-keepalived-watcher-gw7ng requesting resource cpu=5m on Node 10.240.167.254
Nov 16 20:32:42.228: INFO: Pod ibm-keepalived-watcher-x4nqc requesting resource cpu=5m on Node 10.240.167.209
Nov 16 20:32:42.228: INFO: Pod ibm-master-proxy-static-10.240.167.206 requesting resource cpu=25m on Node 10.240.167.206
Nov 16 20:32:42.228: INFO: Pod ibm-master-proxy-static-10.240.167.209 requesting resource cpu=25m on Node 10.240.167.209
Nov 16 20:32:42.228: INFO: Pod ibm-master-proxy-static-10.240.167.254 requesting resource cpu=25m on Node 10.240.167.254
Nov 16 20:32:42.228: INFO: Pod ibm-storage-watcher-69d9c445b4-k5bvh requesting resource cpu=50m on Node 10.240.167.254
Nov 16 20:32:42.228: INFO: Pod ibmcloud-block-storage-driver-2s8xr requesting resource cpu=50m on Node 10.240.167.206
Nov 16 20:32:42.228: INFO: Pod ibmcloud-block-storage-driver-6zlqm requesting resource cpu=50m on Node 10.240.167.209
Nov 16 20:32:42.228: INFO: Pod ibmcloud-block-storage-driver-qdh9z requesting resource cpu=50m on Node 10.240.167.254
Nov 16 20:32:42.228: INFO: Pod ibmcloud-block-storage-plugin-79495594d5-6wqx5 requesting resource cpu=50m on Node 10.240.167.254
Nov 16 20:32:42.228: INFO: Pod vpn-679798bf87-s5h4z requesting resource cpu=5m on Node 10.240.167.209
Nov 16 20:32:42.228: INFO: Pod cluster-node-tuning-operator-5c44ccc99b-6g94b requesting resource cpu=10m on Node 10.240.167.254
Nov 16 20:32:42.228: INFO: Pod tuned-9m9tq requesting resource cpu=10m on Node 10.240.167.206
Nov 16 20:32:42.228: INFO: Pod tuned-cp7vt requesting resource cpu=10m on Node 10.240.167.209
Nov 16 20:32:42.228: INFO: Pod tuned-n7h6s requesting resource cpu=10m on Node 10.240.167.254
Nov 16 20:32:42.228: INFO: Pod cluster-samples-operator-55cf746658-rtxxg requesting resource cpu=20m on Node 10.240.167.206
Nov 16 20:32:42.228: INFO: Pod cluster-storage-operator-6488c9f77b-wq8vv requesting resource cpu=10m on Node 10.240.167.254
Nov 16 20:32:42.228: INFO: Pod console-operator-c9844474-csp4h requesting resource cpu=10m on Node 10.240.167.254
Nov 16 20:32:42.228: INFO: Pod console-7f85d88fbc-4r5lv requesting resource cpu=10m on Node 10.240.167.206
Nov 16 20:32:42.228: INFO: Pod console-7f85d88fbc-fdc4m requesting resource cpu=10m on Node 10.240.167.209
Nov 16 20:32:42.228: INFO: Pod downloads-5dbb4f4ff7-krzbx requesting resource cpu=10m on Node 10.240.167.254
Nov 16 20:32:42.228: INFO: Pod downloads-5dbb4f4ff7-mtjcb requesting resource cpu=10m on Node 10.240.167.254
Nov 16 20:32:42.228: INFO: Pod dns-operator-847f45b78c-49hz4 requesting resource cpu=20m on Node 10.240.167.254
Nov 16 20:32:42.228: INFO: Pod dns-default-m5gqp requesting resource cpu=110m on Node 10.240.167.209
Nov 16 20:32:42.228: INFO: Pod dns-default-rtvgx requesting resource cpu=110m on Node 10.240.167.254
Nov 16 20:32:42.228: INFO: Pod dns-default-zqnbq requesting resource cpu=110m on Node 10.240.167.206
Nov 16 20:32:42.228: INFO: Pod cluster-image-registry-operator-8d47696c5-hpv5z requesting resource cpu=20m on Node 10.240.167.254
Nov 16 20:32:42.228: INFO: Pod image-registry-77d48f6786-gqts2 requesting resource cpu=100m on Node 10.240.167.209
Nov 16 20:32:42.228: INFO: Pod node-ca-2pgxt requesting resource cpu=10m on Node 10.240.167.206
Nov 16 20:32:42.228: INFO: Pod node-ca-852zk requesting resource cpu=10m on Node 10.240.167.254
Nov 16 20:32:42.228: INFO: Pod node-ca-mg4vg requesting resource cpu=10m on Node 10.240.167.209
Nov 16 20:32:42.228: INFO: Pod ingress-operator-57d688547-8j97m requesting resource cpu=20m on Node 10.240.167.254
Nov 16 20:32:42.228: INFO: Pod router-default-589c4b7b87-7299r requesting resource cpu=100m on Node 10.240.167.206
Nov 16 20:32:42.228: INFO: Pod router-default-589c4b7b87-74cdn requesting resource cpu=100m on Node 10.240.167.209
Nov 16 20:32:42.228: INFO: Pod openshift-kube-proxy-5kpn9 requesting resource cpu=100m on Node 10.240.167.206
Nov 16 20:32:42.228: INFO: Pod openshift-kube-proxy-qfwg7 requesting resource cpu=100m on Node 10.240.167.209
Nov 16 20:32:42.228: INFO: Pod openshift-kube-proxy-smf7r requesting resource cpu=100m on Node 10.240.167.254
Nov 16 20:32:42.228: INFO: Pod certified-operators-676df69595-kgtfx requesting resource cpu=10m on Node 10.240.167.209
Nov 16 20:32:42.228: INFO: Pod community-operators-587cdbd4fb-5mwwt requesting resource cpu=10m on Node 10.240.167.209
Nov 16 20:32:42.228: INFO: Pod marketplace-operator-54847664c-cbqsc requesting resource cpu=10m on Node 10.240.167.254
Nov 16 20:32:42.228: INFO: Pod redhat-operators-7f9c6c576-rs2f7 requesting resource cpu=10m on Node 10.240.167.209
Nov 16 20:32:42.228: INFO: Pod alertmanager-main-0 requesting resource cpu=6m on Node 10.240.167.206
Nov 16 20:32:42.228: INFO: Pod alertmanager-main-1 requesting resource cpu=6m on Node 10.240.167.209
Nov 16 20:32:42.228: INFO: Pod alertmanager-main-2 requesting resource cpu=6m on Node 10.240.167.254
Nov 16 20:32:42.228: INFO: Pod cluster-monitoring-operator-7d44956445-lqkh9 requesting resource cpu=10m on Node 10.240.167.254
Nov 16 20:32:42.228: INFO: Pod grafana-58c46d5bb-hs8tj requesting resource cpu=5m on Node 10.240.167.209
Nov 16 20:32:42.228: INFO: Pod kube-state-metrics-776f8894df-p8xsw requesting resource cpu=4m on Node 10.240.167.206
Nov 16 20:32:42.228: INFO: Pod node-exporter-4kz5x requesting resource cpu=9m on Node 10.240.167.254
Nov 16 20:32:42.228: INFO: Pod node-exporter-jssfh requesting resource cpu=9m on Node 10.240.167.206
Nov 16 20:32:42.228: INFO: Pod node-exporter-lqwgh requesting resource cpu=9m on Node 10.240.167.209
Nov 16 20:32:42.228: INFO: Pod openshift-state-metrics-86c5b47587-nhfn8 requesting resource cpu=3m on Node 10.240.167.206
Nov 16 20:32:42.228: INFO: Pod prometheus-adapter-554fc6c4db-6qn5b requesting resource cpu=1m on Node 10.240.167.206
Nov 16 20:32:42.228: INFO: Pod prometheus-adapter-554fc6c4db-8f7cc requesting resource cpu=1m on Node 10.240.167.209
Nov 16 20:32:42.228: INFO: Pod prometheus-k8s-0 requesting resource cpu=76m on Node 10.240.167.206
Nov 16 20:32:42.228: INFO: Pod prometheus-k8s-1 requesting resource cpu=76m on Node 10.240.167.209
Nov 16 20:32:42.228: INFO: Pod prometheus-operator-7646fdcb7b-tw6jj requesting resource cpu=5m on Node 10.240.167.209
Nov 16 20:32:42.228: INFO: Pod telemeter-client-95c48d495-ctf9b requesting resource cpu=3m on Node 10.240.167.254
Nov 16 20:32:42.228: INFO: Pod thanos-querier-54b47b4fc4-rcx4j requesting resource cpu=8m on Node 10.240.167.206
Nov 16 20:32:42.228: INFO: Pod thanos-querier-54b47b4fc4-zvlf7 requesting resource cpu=8m on Node 10.240.167.209
Nov 16 20:32:42.228: INFO: Pod multus-admission-controller-tf7n7 requesting resource cpu=10m on Node 10.240.167.209
Nov 16 20:32:42.228: INFO: Pod multus-admission-controller-x6jqb requesting resource cpu=10m on Node 10.240.167.206
Nov 16 20:32:42.229: INFO: Pod multus-admission-controller-xw5h7 requesting resource cpu=10m on Node 10.240.167.254
Nov 16 20:32:42.229: INFO: Pod multus-dfpk4 requesting resource cpu=10m on Node 10.240.167.209
Nov 16 20:32:42.229: INFO: Pod multus-gf29b requesting resource cpu=10m on Node 10.240.167.254
Nov 16 20:32:42.229: INFO: Pod multus-gnk8t requesting resource cpu=10m on Node 10.240.167.206
Nov 16 20:32:42.229: INFO: Pod network-operator-74fc599569-c45s6 requesting resource cpu=10m on Node 10.240.167.254
Nov 16 20:32:42.229: INFO: Pod catalog-operator-86d68f4684-n8kk4 requesting resource cpu=10m on Node 10.240.167.254
Nov 16 20:32:42.229: INFO: Pod olm-operator-88bfd79df-7mm7b requesting resource cpu=10m on Node 10.240.167.254
Nov 16 20:32:42.229: INFO: Pod packageserver-68c9b7ddbb-phc28 requesting resource cpu=10m on Node 10.240.167.209
Nov 16 20:32:42.229: INFO: Pod packageserver-68c9b7ddbb-rqt2z requesting resource cpu=10m on Node 10.240.167.206
Nov 16 20:32:42.229: INFO: Pod service-ca-operator-5db7bd4f-qmnrv requesting resource cpu=10m on Node 10.240.167.254
Nov 16 20:32:42.229: INFO: Pod apiservice-cabundle-injector-75dcd4f8db-sllzv requesting resource cpu=10m on Node 10.240.167.209
Nov 16 20:32:42.229: INFO: Pod configmap-cabundle-injector-5bd6fcf58-hlwpk requesting resource cpu=10m on Node 10.240.167.209
Nov 16 20:32:42.229: INFO: Pod service-serving-cert-signer-787695f6b4-s4fqz requesting resource cpu=10m on Node 10.240.167.206
Nov 16 20:32:42.229: INFO: Pod openshift-service-catalog-apiserver-operator-86b98d6fff-znhvn requesting resource cpu=0m on Node 10.240.167.254
Nov 16 20:32:42.229: INFO: Pod openshift-service-catalog-controller-manager-operator-595fxjf2f requesting resource cpu=10m on Node 10.240.167.254
Nov 16 20:32:42.229: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.240.167.254
Nov 16 20:32:42.229: INFO: Pod sonobuoy-e2e-job-b6bf7e56608b4535 requesting resource cpu=0m on Node 10.240.167.254
Nov 16 20:32:42.229: INFO: Pod sonobuoy-systemd-logs-daemon-set-18804f2d5c884991-ffhv2 requesting resource cpu=0m on Node 10.240.167.254
Nov 16 20:32:42.229: INFO: Pod sonobuoy-systemd-logs-daemon-set-18804f2d5c884991-lmbvb requesting resource cpu=0m on Node 10.240.167.206
Nov 16 20:32:42.229: INFO: Pod sonobuoy-systemd-logs-daemon-set-18804f2d5c884991-rfhks requesting resource cpu=0m on Node 10.240.167.209
Nov 16 20:32:42.229: INFO: Pod tigera-operator-798cfbf7dd-6rmk9 requesting resource cpu=100m on Node 10.240.167.254
STEP: Starting Pods to consume most of the cluster CPU.
Nov 16 20:32:42.229: INFO: Creating a pod which consumes cpu=2233m on Node 10.240.167.209
Nov 16 20:32:42.259: INFO: Creating a pod which consumes cpu=2188m on Node 10.240.167.254
Nov 16 20:32:42.283: INFO: Creating a pod which consumes cpu=2326m on Node 10.240.167.206
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-58a7d33f-e81c-4e99-99a1-1c7af7b982cb.164817300b23a59c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6906/filler-pod-58a7d33f-e81c-4e99-99a1-1c7af7b982cb to 10.240.167.209]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-58a7d33f-e81c-4e99-99a1-1c7af7b982cb.1648173043c8fa60], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-58a7d33f-e81c-4e99-99a1-1c7af7b982cb.164817319e614d4d], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-58a7d33f-e81c-4e99-99a1-1c7af7b982cb.16481731ab5528d4], Reason = [Created], Message = [Created container filler-pod-58a7d33f-e81c-4e99-99a1-1c7af7b982cb]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-58a7d33f-e81c-4e99-99a1-1c7af7b982cb.16481731ae3458f3], Reason = [Started], Message = [Started container filler-pod-58a7d33f-e81c-4e99-99a1-1c7af7b982cb]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-eb15bf76-faf9-41f9-b929-d8581b83033c.164817300e40dcd9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6906/filler-pod-eb15bf76-faf9-41f9-b929-d8581b83033c to 10.240.167.206]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-eb15bf76-faf9-41f9-b929-d8581b83033c.16481730494ba05e], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-eb15bf76-faf9-41f9-b929-d8581b83033c.164817305944b96a], Reason = [Created], Message = [Created container filler-pod-eb15bf76-faf9-41f9-b929-d8581b83033c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-eb15bf76-faf9-41f9-b929-d8581b83033c.164817305bfbfa69], Reason = [Started], Message = [Started container filler-pod-eb15bf76-faf9-41f9-b929-d8581b83033c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f8b2c3b1-1193-4b9c-a262-0faa1a37034e.164817300cc4b4e7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6906/filler-pod-f8b2c3b1-1193-4b9c-a262-0faa1a37034e to 10.240.167.254]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f8b2c3b1-1193-4b9c-a262-0faa1a37034e.16481730432eb4ed], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f8b2c3b1-1193-4b9c-a262-0faa1a37034e.16481730509f6a18], Reason = [Created], Message = [Created container filler-pod-f8b2c3b1-1193-4b9c-a262-0faa1a37034e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f8b2c3b1-1193-4b9c-a262-0faa1a37034e.16481730536c4547], Reason = [Started], Message = [Started container filler-pod-f8b2c3b1-1193-4b9c-a262-0faa1a37034e]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16481731ee449b00], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 10.240.167.206
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.240.167.209
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.240.167.254
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:32:51.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6906" for this suite.
Nov 16 20:32:59.512: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:33:01.418: INFO: namespace sched-pred-6906 deletion completed in 9.933501215s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:19.717 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:33:01.418: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating pod
Nov 16 20:33:05.582: INFO: Pod pod-hostip-fdea62a4-5c49-4a38-9b50-02c76d70d8c3 has hostIP: 10.240.167.254
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:33:05.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1005" for this suite.
Nov 16 20:33:19.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:33:21.540: INFO: namespace pods-1005 deletion completed in 15.948176632s

• [SLOW TEST:20.122 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:33:21.540: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1160
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-1160
I1116 20:33:21.719669      26 runners.go:184] Created replication controller with name: externalname-service, namespace: services-1160, replica count: 2
I1116 20:33:24.770585      26 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 16 20:33:24.770: INFO: Creating new exec pod
Nov 16 20:33:29.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=services-1160 execpodsp77c -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Nov 16 20:33:30.146: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Nov 16 20:33:30.146: INFO: stdout: ""
Nov 16 20:33:30.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=services-1160 execpodsp77c -- /bin/sh -x -c nc -zv -t -w 2 172.21.74.20 80'
Nov 16 20:33:30.445: INFO: stderr: "+ nc -zv -t -w 2 172.21.74.20 80\nConnection to 172.21.74.20 80 port [tcp/http] succeeded!\n"
Nov 16 20:33:30.445: INFO: stdout: ""
Nov 16 20:33:30.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=services-1160 execpodsp77c -- /bin/sh -x -c nc -zv -t -w 2 10.240.167.206 30026'
Nov 16 20:33:30.748: INFO: stderr: "+ nc -zv -t -w 2 10.240.167.206 30026\nConnection to 10.240.167.206 30026 port [tcp/30026] succeeded!\n"
Nov 16 20:33:30.748: INFO: stdout: ""
Nov 16 20:33:30.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=services-1160 execpodsp77c -- /bin/sh -x -c nc -zv -t -w 2 10.240.167.209 30026'
Nov 16 20:33:31.055: INFO: stderr: "+ nc -zv -t -w 2 10.240.167.209 30026\nConnection to 10.240.167.209 30026 port [tcp/30026] succeeded!\n"
Nov 16 20:33:31.055: INFO: stdout: ""
Nov 16 20:33:31.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=services-1160 execpodsp77c -- /bin/sh -x -c nc -zv -t -w 2 161.156.102.242 30026'
Nov 16 20:33:31.363: INFO: stderr: "+ nc -zv -t -w 2 161.156.102.242 30026\nConnection to 161.156.102.242 30026 port [tcp/30026] succeeded!\n"
Nov 16 20:33:31.363: INFO: stdout: ""
Nov 16 20:33:31.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=services-1160 execpodsp77c -- /bin/sh -x -c nc -zv -t -w 2 161.156.102.250 30026'
Nov 16 20:33:31.699: INFO: stderr: "+ nc -zv -t -w 2 161.156.102.250 30026\nConnection to 161.156.102.250 30026 port [tcp/30026] succeeded!\n"
Nov 16 20:33:31.699: INFO: stdout: ""
Nov 16 20:33:31.699: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:33:31.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1160" for this suite.
Nov 16 20:33:39.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:33:41.687: INFO: namespace services-1160 deletion completed in 9.932061793s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:20.147 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:33:41.688: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8260.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8260.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8260.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8260.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 16 20:33:45.879: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-95f7cdc2-9fa7-44bb-824a-a7d90e6860d4: the server could not find the requested resource (get pods dns-test-95f7cdc2-9fa7-44bb-824a-a7d90e6860d4)
Nov 16 20:33:45.891: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-95f7cdc2-9fa7-44bb-824a-a7d90e6860d4: the server could not find the requested resource (get pods dns-test-95f7cdc2-9fa7-44bb-824a-a7d90e6860d4)
Nov 16 20:33:45.903: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-95f7cdc2-9fa7-44bb-824a-a7d90e6860d4: the server could not find the requested resource (get pods dns-test-95f7cdc2-9fa7-44bb-824a-a7d90e6860d4)
Nov 16 20:33:45.976: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-95f7cdc2-9fa7-44bb-824a-a7d90e6860d4: the server could not find the requested resource (get pods dns-test-95f7cdc2-9fa7-44bb-824a-a7d90e6860d4)
Nov 16 20:33:45.988: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-95f7cdc2-9fa7-44bb-824a-a7d90e6860d4: the server could not find the requested resource (get pods dns-test-95f7cdc2-9fa7-44bb-824a-a7d90e6860d4)
Nov 16 20:33:46.012: INFO: Lookups using dns-8260/dns-test-95f7cdc2-9fa7-44bb-824a-a7d90e6860d4 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local jessie_udp@dns-test-service-2.dns-8260.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8260.svc.cluster.local]

Nov 16 20:33:51.162: INFO: DNS probes using dns-8260/dns-test-95f7cdc2-9fa7-44bb-824a-a7d90e6860d4 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:33:51.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8260" for this suite.
Nov 16 20:33:59.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:34:01.161: INFO: namespace dns-8260 deletion completed in 9.935310833s

• [SLOW TEST:19.473 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:34:01.165: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 20:34:01.272: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:34:08.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2028" for this suite.
Nov 16 20:34:16.078: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:34:17.983: INFO: namespace custom-resource-definition-2028 deletion completed in 9.932869643s

• [SLOW TEST:16.819 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:34:17.986: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1499
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Nov 16 20:34:18.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-5789'
Nov 16 20:34:18.240: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Nov 16 20:34:18.240: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Nov 16 20:34:18.250: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Nov 16 20:34:18.278: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Nov 16 20:34:18.310: INFO: scanned /root for discovery docs: <nil>
Nov 16 20:34:18.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-5789'
Nov 16 20:34:34.261: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Nov 16 20:34:34.261: INFO: stdout: "Created e2e-test-httpd-rc-23c4354d5574c41ffcb1e06b2c8c4c43\nScaling up e2e-test-httpd-rc-23c4354d5574c41ffcb1e06b2c8c4c43 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-23c4354d5574c41ffcb1e06b2c8c4c43 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-23c4354d5574c41ffcb1e06b2c8c4c43 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Nov 16 20:34:34.261: INFO: stdout: "Created e2e-test-httpd-rc-23c4354d5574c41ffcb1e06b2c8c4c43\nScaling up e2e-test-httpd-rc-23c4354d5574c41ffcb1e06b2c8c4c43 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-23c4354d5574c41ffcb1e06b2c8c4c43 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-23c4354d5574c41ffcb1e06b2c8c4c43 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Nov 16 20:34:34.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-5789'
Nov 16 20:34:34.375: INFO: stderr: ""
Nov 16 20:34:34.375: INFO: stdout: "e2e-test-httpd-rc-23c4354d5574c41ffcb1e06b2c8c4c43-rmwzm "
Nov 16 20:34:34.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods e2e-test-httpd-rc-23c4354d5574c41ffcb1e06b2c8c4c43-rmwzm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5789'
Nov 16 20:34:34.485: INFO: stderr: ""
Nov 16 20:34:34.485: INFO: stdout: "true"
Nov 16 20:34:34.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 get pods e2e-test-httpd-rc-23c4354d5574c41ffcb1e06b2c8c4c43-rmwzm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5789'
Nov 16 20:34:34.602: INFO: stderr: ""
Nov 16 20:34:34.602: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Nov 16 20:34:34.602: INFO: e2e-test-httpd-rc-23c4354d5574c41ffcb1e06b2c8c4c43-rmwzm is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
Nov 16 20:34:34.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 delete rc e2e-test-httpd-rc --namespace=kubectl-5789'
Nov 16 20:34:34.737: INFO: stderr: ""
Nov 16 20:34:34.737: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:34:34.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5789" for this suite.
Nov 16 20:34:48.774: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:34:50.681: INFO: namespace kubectl-5789 deletion completed in 15.933334128s

• [SLOW TEST:32.695 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1494
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:34:50.685: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 16 20:34:50.814: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b4573d9b-7dd6-4b5a-88b0-28fe2d8122c5" in namespace "projected-9846" to be "success or failure"
Nov 16 20:34:50.822: INFO: Pod "downwardapi-volume-b4573d9b-7dd6-4b5a-88b0-28fe2d8122c5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.783382ms
Nov 16 20:34:52.831: INFO: Pod "downwardapi-volume-b4573d9b-7dd6-4b5a-88b0-28fe2d8122c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017017926s
STEP: Saw pod success
Nov 16 20:34:52.831: INFO: Pod "downwardapi-volume-b4573d9b-7dd6-4b5a-88b0-28fe2d8122c5" satisfied condition "success or failure"
Nov 16 20:34:52.838: INFO: Trying to get logs from node 10.240.167.254 pod downwardapi-volume-b4573d9b-7dd6-4b5a-88b0-28fe2d8122c5 container client-container: <nil>
STEP: delete the pod
Nov 16 20:34:52.899: INFO: Waiting for pod downwardapi-volume-b4573d9b-7dd6-4b5a-88b0-28fe2d8122c5 to disappear
Nov 16 20:34:52.907: INFO: Pod downwardapi-volume-b4573d9b-7dd6-4b5a-88b0-28fe2d8122c5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:34:52.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9846" for this suite.
Nov 16 20:35:00.945: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:35:02.851: INFO: namespace projected-9846 deletion completed in 9.933618108s

• [SLOW TEST:12.166 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:35:02.853: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl logs
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1274
STEP: creating an pod
Nov 16 20:35:02.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.6 --namespace=kubectl-4214 -- logs-generator --log-lines-total 100 --run-duration 20s'
Nov 16 20:35:03.123: INFO: stderr: ""
Nov 16 20:35:03.123: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Waiting for log generator to start.
Nov 16 20:35:03.124: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Nov 16 20:35:03.124: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-4214" to be "running and ready, or succeeded"
Nov 16 20:35:03.132: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.720123ms
Nov 16 20:35:05.140: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01548332s
Nov 16 20:35:07.148: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.023935475s
Nov 16 20:35:07.148: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Nov 16 20:35:07.148: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Nov 16 20:35:07.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 logs logs-generator logs-generator --namespace=kubectl-4214'
Nov 16 20:35:07.290: INFO: stderr: ""
Nov 16 20:35:07.290: INFO: stdout: "I1116 20:35:04.263125       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/qhg 407\nI1116 20:35:04.463445       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/mqg 374\nI1116 20:35:04.663309       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/v4v5 476\nI1116 20:35:04.863497       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/zr5j 381\nI1116 20:35:05.063338       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/55r9 485\nI1116 20:35:05.263380       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/spd4 471\nI1116 20:35:05.463387       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/brt 556\nI1116 20:35:05.663382       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/mbhv 231\nI1116 20:35:05.863325       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/j8k 533\nI1116 20:35:06.063352       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/9rlt 412\nI1116 20:35:06.263378       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/c5hj 454\nI1116 20:35:06.463395       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/dtpm 207\nI1116 20:35:06.663426       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/m2cw 461\nI1116 20:35:06.863360       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/ftnr 432\nI1116 20:35:07.063362       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/g2h7 524\nI1116 20:35:07.263290       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/pdkt 284\n"
STEP: limiting log lines
Nov 16 20:35:07.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 logs logs-generator logs-generator --namespace=kubectl-4214 --tail=1'
Nov 16 20:35:07.421: INFO: stderr: ""
Nov 16 20:35:07.421: INFO: stdout: "I1116 20:35:07.263290       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/pdkt 284\n"
STEP: limiting log bytes
Nov 16 20:35:07.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 logs logs-generator logs-generator --namespace=kubectl-4214 --limit-bytes=1'
Nov 16 20:35:07.581: INFO: stderr: ""
Nov 16 20:35:07.581: INFO: stdout: "I"
STEP: exposing timestamps
Nov 16 20:35:07.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 logs logs-generator logs-generator --namespace=kubectl-4214 --tail=1 --timestamps'
Nov 16 20:35:07.721: INFO: stderr: ""
Nov 16 20:35:07.721: INFO: stdout: "2020-11-16T14:35:07.663494594-06:00 I1116 20:35:07.663430       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/cntl 302\n"
STEP: restricting to a time range
Nov 16 20:35:10.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 logs logs-generator logs-generator --namespace=kubectl-4214 --since=1s'
Nov 16 20:35:10.368: INFO: stderr: ""
Nov 16 20:35:10.368: INFO: stdout: "I1116 20:35:09.463347       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/f42 417\nI1116 20:35:09.663441       1 logs_generator.go:76] 27 GET /api/v1/namespaces/ns/pods/php 500\nI1116 20:35:09.863467       1 logs_generator.go:76] 28 GET /api/v1/namespaces/ns/pods/cvb 443\nI1116 20:35:10.063334       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/kube-system/pods/xcm7 201\nI1116 20:35:10.263409       1 logs_generator.go:76] 30 POST /api/v1/namespaces/ns/pods/7d4l 365\n"
Nov 16 20:35:10.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 logs logs-generator logs-generator --namespace=kubectl-4214 --since=24h'
Nov 16 20:35:10.522: INFO: stderr: ""
Nov 16 20:35:10.522: INFO: stdout: "I1116 20:35:04.263125       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/qhg 407\nI1116 20:35:04.463445       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/mqg 374\nI1116 20:35:04.663309       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/v4v5 476\nI1116 20:35:04.863497       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/zr5j 381\nI1116 20:35:05.063338       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/55r9 485\nI1116 20:35:05.263380       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/spd4 471\nI1116 20:35:05.463387       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/brt 556\nI1116 20:35:05.663382       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/mbhv 231\nI1116 20:35:05.863325       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/j8k 533\nI1116 20:35:06.063352       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/9rlt 412\nI1116 20:35:06.263378       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/c5hj 454\nI1116 20:35:06.463395       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/dtpm 207\nI1116 20:35:06.663426       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/m2cw 461\nI1116 20:35:06.863360       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/ftnr 432\nI1116 20:35:07.063362       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/g2h7 524\nI1116 20:35:07.263290       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/pdkt 284\nI1116 20:35:07.463322       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/kcp 502\nI1116 20:35:07.663430       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/cntl 302\nI1116 20:35:07.863324       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/286x 364\nI1116 20:35:08.063415       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/z249 219\nI1116 20:35:08.263363       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/vk7 251\nI1116 20:35:08.463345       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/c8bg 381\nI1116 20:35:08.663327       1 logs_generator.go:76] 22 POST /api/v1/namespaces/default/pods/bxvs 576\nI1116 20:35:08.863323       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/default/pods/zrj2 332\nI1116 20:35:09.063434       1 logs_generator.go:76] 24 GET /api/v1/namespaces/kube-system/pods/jsc 599\nI1116 20:35:09.263375       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/kube-system/pods/gzc 377\nI1116 20:35:09.463347       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/f42 417\nI1116 20:35:09.663441       1 logs_generator.go:76] 27 GET /api/v1/namespaces/ns/pods/php 500\nI1116 20:35:09.863467       1 logs_generator.go:76] 28 GET /api/v1/namespaces/ns/pods/cvb 443\nI1116 20:35:10.063334       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/kube-system/pods/xcm7 201\nI1116 20:35:10.263409       1 logs_generator.go:76] 30 POST /api/v1/namespaces/ns/pods/7d4l 365\nI1116 20:35:10.463399       1 logs_generator.go:76] 31 GET /api/v1/namespaces/ns/pods/wlt6 573\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1280
Nov 16 20:35:10.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 delete pod logs-generator --namespace=kubectl-4214'
Nov 16 20:35:21.674: INFO: stderr: ""
Nov 16 20:35:21.674: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:35:21.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4214" for this suite.
Nov 16 20:35:29.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:35:31.625: INFO: namespace kubectl-4214 deletion completed in 9.938005112s

• [SLOW TEST:28.773 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1270
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:35:31.627: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-998ef17c-95b8-4865-985c-a2f28f3833ee
STEP: Creating a pod to test consume secrets
Nov 16 20:35:31.769: INFO: Waiting up to 5m0s for pod "pod-secrets-b7607436-0fc3-404a-9bf0-6294cc020cae" in namespace "secrets-7973" to be "success or failure"
Nov 16 20:35:31.777: INFO: Pod "pod-secrets-b7607436-0fc3-404a-9bf0-6294cc020cae": Phase="Pending", Reason="", readiness=false. Elapsed: 8.040492ms
Nov 16 20:35:33.785: INFO: Pod "pod-secrets-b7607436-0fc3-404a-9bf0-6294cc020cae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015600183s
STEP: Saw pod success
Nov 16 20:35:33.785: INFO: Pod "pod-secrets-b7607436-0fc3-404a-9bf0-6294cc020cae" satisfied condition "success or failure"
Nov 16 20:35:33.792: INFO: Trying to get logs from node 10.240.167.254 pod pod-secrets-b7607436-0fc3-404a-9bf0-6294cc020cae container secret-volume-test: <nil>
STEP: delete the pod
Nov 16 20:35:33.832: INFO: Waiting for pod pod-secrets-b7607436-0fc3-404a-9bf0-6294cc020cae to disappear
Nov 16 20:35:33.840: INFO: Pod pod-secrets-b7607436-0fc3-404a-9bf0-6294cc020cae no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:35:33.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7973" for this suite.
Nov 16 20:35:41.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:35:43.784: INFO: namespace secrets-7973 deletion completed in 9.934120709s

• [SLOW TEST:12.158 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:35:43.786: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 20:35:43.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 version'
Nov 16 20:35:44.073: INFO: stderr: ""
Nov 16 20:35:44.073: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.2\", GitCommit:\"c97fe5036ef3df2967d086711e6c0c405941e14b\", GitTreeState:\"clean\", BuildDate:\"2019-10-15T19:18:23Z\", GoVersion:\"go1.12.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"16+\", GitVersion:\"v1.16.2+853223d\", GitCommit:\"853223d\", GitTreeState:\"clean\", BuildDate:\"2020-10-14T15:02:59Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:35:44.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5974" for this suite.
Nov 16 20:35:52.113: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:35:54.019: INFO: namespace kubectl-5974 deletion completed in 9.935954307s

• [SLOW TEST:10.234 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl version
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1380
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:35:54.021: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Nov 16 20:35:54.181: INFO: Waiting up to 5m0s for pod "pod-7815560a-9c59-4437-aeed-d8450446118e" in namespace "emptydir-7729" to be "success or failure"
Nov 16 20:35:54.188: INFO: Pod "pod-7815560a-9c59-4437-aeed-d8450446118e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.19246ms
Nov 16 20:35:56.197: INFO: Pod "pod-7815560a-9c59-4437-aeed-d8450446118e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015490436s
STEP: Saw pod success
Nov 16 20:35:56.197: INFO: Pod "pod-7815560a-9c59-4437-aeed-d8450446118e" satisfied condition "success or failure"
Nov 16 20:35:56.204: INFO: Trying to get logs from node 10.240.167.254 pod pod-7815560a-9c59-4437-aeed-d8450446118e container test-container: <nil>
STEP: delete the pod
Nov 16 20:35:56.245: INFO: Waiting for pod pod-7815560a-9c59-4437-aeed-d8450446118e to disappear
Nov 16 20:35:56.253: INFO: Pod pod-7815560a-9c59-4437-aeed-d8450446118e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:35:56.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7729" for this suite.
Nov 16 20:36:04.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:36:06.192: INFO: namespace emptydir-7729 deletion completed in 9.929627696s

• [SLOW TEST:12.171 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:36:06.192: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1116 20:36:12.355718      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Nov 16 20:36:12.355: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:36:12.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2798" for this suite.
Nov 16 20:36:20.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:36:22.304: INFO: namespace gc-2798 deletion completed in 9.938368034s

• [SLOW TEST:16.112 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:36:22.307: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Nov 16 20:36:22.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 create -f - --namespace=kubectl-1589'
Nov 16 20:36:23.120: INFO: stderr: ""
Nov 16 20:36:23.120: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Nov 16 20:36:24.131: INFO: Selector matched 1 pods for map[app:redis]
Nov 16 20:36:24.131: INFO: Found 0 / 1
Nov 16 20:36:25.132: INFO: Selector matched 1 pods for map[app:redis]
Nov 16 20:36:25.132: INFO: Found 1 / 1
Nov 16 20:36:25.132: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Nov 16 20:36:25.142: INFO: Selector matched 1 pods for map[app:redis]
Nov 16 20:36:25.142: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 16 20:36:25.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 patch pod redis-master-lpzfv --namespace=kubectl-1589 -p {"metadata":{"annotations":{"x":"y"}}}'
Nov 16 20:36:25.298: INFO: stderr: ""
Nov 16 20:36:25.298: INFO: stdout: "pod/redis-master-lpzfv patched\n"
STEP: checking annotations
Nov 16 20:36:25.306: INFO: Selector matched 1 pods for map[app:redis]
Nov 16 20:36:25.306: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:36:25.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1589" for this suite.
Nov 16 20:36:55.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:36:57.251: INFO: namespace kubectl-1589 deletion completed in 31.933567951s

• [SLOW TEST:34.944 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1346
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:36:57.253: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 20:36:57.353: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Nov 16 20:37:03.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 --namespace=crd-publish-openapi-8540 create -f -'
Nov 16 20:37:04.543: INFO: stderr: ""
Nov 16 20:37:04.543: INFO: stdout: "e2e-test-crd-publish-openapi-153-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Nov 16 20:37:04.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 --namespace=crd-publish-openapi-8540 delete e2e-test-crd-publish-openapi-153-crds test-foo'
Nov 16 20:37:04.670: INFO: stderr: ""
Nov 16 20:37:04.670: INFO: stdout: "e2e-test-crd-publish-openapi-153-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Nov 16 20:37:04.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 --namespace=crd-publish-openapi-8540 apply -f -'
Nov 16 20:37:05.200: INFO: stderr: ""
Nov 16 20:37:05.200: INFO: stdout: "e2e-test-crd-publish-openapi-153-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Nov 16 20:37:05.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 --namespace=crd-publish-openapi-8540 delete e2e-test-crd-publish-openapi-153-crds test-foo'
Nov 16 20:37:05.344: INFO: stderr: ""
Nov 16 20:37:05.344: INFO: stdout: "e2e-test-crd-publish-openapi-153-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Nov 16 20:37:05.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 --namespace=crd-publish-openapi-8540 create -f -'
Nov 16 20:37:05.832: INFO: rc: 1
Nov 16 20:37:05.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 --namespace=crd-publish-openapi-8540 apply -f -'
Nov 16 20:37:06.340: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Nov 16 20:37:06.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 --namespace=crd-publish-openapi-8540 create -f -'
Nov 16 20:37:06.766: INFO: rc: 1
Nov 16 20:37:06.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 --namespace=crd-publish-openapi-8540 apply -f -'
Nov 16 20:37:07.239: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Nov 16 20:37:07.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 explain e2e-test-crd-publish-openapi-153-crds'
Nov 16 20:37:07.694: INFO: stderr: ""
Nov 16 20:37:07.694: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-153-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Nov 16 20:37:07.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 explain e2e-test-crd-publish-openapi-153-crds.metadata'
Nov 16 20:37:08.215: INFO: stderr: ""
Nov 16 20:37:08.215: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-153-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Nov 16 20:37:08.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 explain e2e-test-crd-publish-openapi-153-crds.spec'
Nov 16 20:37:08.681: INFO: stderr: ""
Nov 16 20:37:08.682: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-153-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Nov 16 20:37:08.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 explain e2e-test-crd-publish-openapi-153-crds.spec.bars'
Nov 16 20:37:09.207: INFO: stderr: ""
Nov 16 20:37:09.207: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-153-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Nov 16 20:37:09.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 explain e2e-test-crd-publish-openapi-153-crds.spec.bars2'
Nov 16 20:37:09.497: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:37:16.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8540" for this suite.
Nov 16 20:37:24.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:37:26.481: INFO: namespace crd-publish-openapi-8540 deletion completed in 9.934414471s

• [SLOW TEST:29.228 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:37:26.482: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 20:37:26.967: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 16 20:37:28.996: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741155846, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741155846, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741155847, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741155846, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 20:37:32.031: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 20:37:32.039: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1054-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:37:33.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1101" for this suite.
Nov 16 20:37:41.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:37:43.221: INFO: namespace webhook-1101 deletion completed in 9.946946858s
STEP: Destroying namespace "webhook-1101-markers" for this suite.
Nov 16 20:37:51.247: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:37:53.165: INFO: namespace webhook-1101-markers deletion completed in 9.943785122s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:26.722 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:37:53.204: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-b262756d-b3f9-4bec-9061-2d5a27ba9506 in namespace container-probe-9799
Nov 16 20:37:55.354: INFO: Started pod busybox-b262756d-b3f9-4bec-9061-2d5a27ba9506 in namespace container-probe-9799
STEP: checking the pod's current state and verifying that restartCount is present
Nov 16 20:37:55.366: INFO: Initial restart count of pod busybox-b262756d-b3f9-4bec-9061-2d5a27ba9506 is 0
Nov 16 20:38:49.603: INFO: Restart count of pod container-probe-9799/busybox-b262756d-b3f9-4bec-9061-2d5a27ba9506 is now 1 (54.237076869s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:38:49.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9799" for this suite.
Nov 16 20:38:57.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:38:59.586: INFO: namespace container-probe-9799 deletion completed in 9.949271046s

• [SLOW TEST:66.382 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:38:59.588: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 20:38:59.705: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Nov 16 20:39:06.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 --namespace=crd-publish-openapi-1137 create -f -'
Nov 16 20:39:06.883: INFO: stderr: ""
Nov 16 20:39:06.883: INFO: stdout: "e2e-test-crd-publish-openapi-7596-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Nov 16 20:39:06.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 --namespace=crd-publish-openapi-1137 delete e2e-test-crd-publish-openapi-7596-crds test-cr'
Nov 16 20:39:07.050: INFO: stderr: ""
Nov 16 20:39:07.050: INFO: stdout: "e2e-test-crd-publish-openapi-7596-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Nov 16 20:39:07.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 --namespace=crd-publish-openapi-1137 apply -f -'
Nov 16 20:39:07.366: INFO: stderr: ""
Nov 16 20:39:07.366: INFO: stdout: "e2e-test-crd-publish-openapi-7596-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Nov 16 20:39:07.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 --namespace=crd-publish-openapi-1137 delete e2e-test-crd-publish-openapi-7596-crds test-cr'
Nov 16 20:39:07.500: INFO: stderr: ""
Nov 16 20:39:07.500: INFO: stdout: "e2e-test-crd-publish-openapi-7596-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Nov 16 20:39:07.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 explain e2e-test-crd-publish-openapi-7596-crds'
Nov 16 20:39:07.980: INFO: stderr: ""
Nov 16 20:39:07.980: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7596-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:39:14.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1137" for this suite.
Nov 16 20:39:22.965: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:39:24.870: INFO: namespace crd-publish-openapi-1137 deletion completed in 9.934731765s

• [SLOW TEST:25.283 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:39:24.870: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 20:39:24.996: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-81f9172e-6b9f-42c2-917f-9d4e7d124d1c
STEP: Creating secret with name s-test-opt-upd-7c76a0cb-1621-4c31-961e-9657bf6854ac
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-81f9172e-6b9f-42c2-917f-9d4e7d124d1c
STEP: Updating secret s-test-opt-upd-7c76a0cb-1621-4c31-961e-9657bf6854ac
STEP: Creating secret with name s-test-opt-create-3d50e130-9c13-4c75-b56a-7eaf85c2bf43
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:40:38.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4725" for this suite.
Nov 16 20:40:52.129: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:40:54.034: INFO: namespace projected-4725 deletion completed in 15.935933046s

• [SLOW TEST:89.164 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:40:54.035: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1439
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Nov 16 20:40:54.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-8551'
Nov 16 20:40:54.295: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Nov 16 20:40:54.295: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Nov 16 20:40:56.319: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-v2psx]
Nov 16 20:40:56.319: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-v2psx" in namespace "kubectl-8551" to be "running and ready"
Nov 16 20:40:56.327: INFO: Pod "e2e-test-httpd-rc-v2psx": Phase="Pending", Reason="", readiness=false. Elapsed: 8.603975ms
Nov 16 20:40:58.336: INFO: Pod "e2e-test-httpd-rc-v2psx": Phase="Running", Reason="", readiness=true. Elapsed: 2.017557512s
Nov 16 20:40:58.337: INFO: Pod "e2e-test-httpd-rc-v2psx" satisfied condition "running and ready"
Nov 16 20:40:58.337: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-v2psx]
Nov 16 20:40:58.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 logs rc/e2e-test-httpd-rc --namespace=kubectl-8551'
Nov 16 20:40:58.526: INFO: stderr: ""
Nov 16 20:40:58.526: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.30.216.249. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.30.216.249. Set the 'ServerName' directive globally to suppress this message\n[Mon Nov 16 20:40:55.548493 2020] [mpm_event:notice] [pid 1:tid 140109901482856] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Mon Nov 16 20:40:55.548560 2020] [core:notice] [pid 1:tid 140109901482856] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
Nov 16 20:40:58.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 delete rc e2e-test-httpd-rc --namespace=kubectl-8551'
Nov 16 20:40:58.674: INFO: stderr: ""
Nov 16 20:40:58.674: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:40:58.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8551" for this suite.
Nov 16 20:41:28.715: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:41:30.619: INFO: namespace kubectl-8551 deletion completed in 31.933102701s

• [SLOW TEST:36.584 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1435
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:41:30.619: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating cluster-info
Nov 16 20:41:30.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 cluster-info'
Nov 16 20:41:30.824: INFO: stderr: ""
Nov 16 20:41:30.824: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:41:30.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2800" for this suite.
Nov 16 20:41:38.864: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:41:40.816: INFO: namespace kubectl-2800 deletion completed in 9.981841811s

• [SLOW TEST:10.198 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:974
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:41:40.817: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-c988d2a0-8276-49ce-9d74-fd9806fe8abd
STEP: Creating a pod to test consume configMaps
Nov 16 20:41:40.976: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4b3fcf5f-122d-44c8-a39f-970a42b2dfac" in namespace "projected-3451" to be "success or failure"
Nov 16 20:41:40.990: INFO: Pod "pod-projected-configmaps-4b3fcf5f-122d-44c8-a39f-970a42b2dfac": Phase="Pending", Reason="", readiness=false. Elapsed: 7.102095ms
Nov 16 20:41:42.998: INFO: Pod "pod-projected-configmaps-4b3fcf5f-122d-44c8-a39f-970a42b2dfac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015278098s
STEP: Saw pod success
Nov 16 20:41:42.998: INFO: Pod "pod-projected-configmaps-4b3fcf5f-122d-44c8-a39f-970a42b2dfac" satisfied condition "success or failure"
Nov 16 20:41:43.005: INFO: Trying to get logs from node 10.240.167.254 pod pod-projected-configmaps-4b3fcf5f-122d-44c8-a39f-970a42b2dfac container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 16 20:41:43.063: INFO: Waiting for pod pod-projected-configmaps-4b3fcf5f-122d-44c8-a39f-970a42b2dfac to disappear
Nov 16 20:41:43.071: INFO: Pod pod-projected-configmaps-4b3fcf5f-122d-44c8-a39f-970a42b2dfac no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:41:43.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3451" for this suite.
Nov 16 20:41:51.112: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:41:53.018: INFO: namespace projected-3451 deletion completed in 9.936532983s

• [SLOW TEST:12.201 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:41:53.019: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 20:41:53.123: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Nov 16 20:41:53.142: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 16 20:41:58.151: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 16 20:41:58.151: INFO: Creating deployment "test-rolling-update-deployment"
Nov 16 20:41:58.166: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Nov 16 20:41:58.186: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Nov 16 20:42:00.208: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Nov 16 20:42:00.216: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Nov 16 20:42:00.243: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3155 /apis/apps/v1/namespaces/deployment-3155/deployments/test-rolling-update-deployment f86496e7-0ecf-468c-80f6-e3121f887ded 81655 1 2020-11-16 20:41:58 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00208b878 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-11-16 20:41:58 +0000 UTC,LastTransitionTime:2020-11-16 20:41:58 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-55d946486" has successfully progressed.,LastUpdateTime:2020-11-16 20:41:59 +0000 UTC,LastTransitionTime:2020-11-16 20:41:58 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 16 20:42:00.251: INFO: New ReplicaSet "test-rolling-update-deployment-55d946486" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-55d946486  deployment-3155 /apis/apps/v1/namespaces/deployment-3155/replicasets/test-rolling-update-deployment-55d946486 109fc5b0-c5e0-4653-b484-a6de894094ed 81644 1 2020-11-16 20:41:58 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment f86496e7-0ecf-468c-80f6-e3121f887ded 0xc0026d10c0 0xc0026d10c1}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 55d946486,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0026d1128 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 16 20:42:00.251: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Nov 16 20:42:00.251: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3155 /apis/apps/v1/namespaces/deployment-3155/replicasets/test-rolling-update-controller b9ad1cc1-ed94-492d-b315-7802018055d7 81654 2 2020-11-16 20:41:53 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment f86496e7-0ecf-468c-80f6-e3121f887ded 0xc0026d0ff7 0xc0026d0ff8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0026d1058 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 16 20:42:00.258: INFO: Pod "test-rolling-update-deployment-55d946486-tcrdn" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-55d946486-tcrdn test-rolling-update-deployment-55d946486- deployment-3155 /api/v1/namespaces/deployment-3155/pods/test-rolling-update-deployment-55d946486-tcrdn e741949b-4979-47ab-94f2-54f74cc11752 81643 0 2020-11-16 20:41:58 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[cni.projectcalico.org/podIP:172.30.216.252/32 cni.projectcalico.org/podIPs:172.30.216.252/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.216.252"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-rolling-update-deployment-55d946486 109fc5b0-c5e0-4653-b484-a6de894094ed 0xc00208bfd0 0xc00208bfd1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-drbhd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-drbhd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-drbhd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.167.254,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gtrfl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 20:41:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 20:41:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 20:41:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 20:41:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.167.254,PodIP:172.30.216.252,StartTime:2020-11-16 20:41:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-16 20:41:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:cri-o://0cfa19f46a3faa61ed6915e1470aa550829edfa3d7f0b2c997d4269f6632c432,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.216.252,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:42:00.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3155" for this suite.
Nov 16 20:42:08.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:42:10.228: INFO: namespace deployment-3155 deletion completed in 9.960677857s

• [SLOW TEST:17.210 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:42:10.229: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
STEP: reading a file in the container
Nov 16 20:42:12.919: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5447 pod-service-account-c82e4064-8a55-4c6f-a339-b038971a6f02 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Nov 16 20:42:13.241: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5447 pod-service-account-c82e4064-8a55-4c6f-a339-b038971a6f02 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Nov 16 20:42:13.545: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5447 pod-service-account-c82e4064-8a55-4c6f-a339-b038971a6f02 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:42:13.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5447" for this suite.
Nov 16 20:42:21.921: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:42:23.835: INFO: namespace svcaccounts-5447 deletion completed in 9.94514249s

• [SLOW TEST:13.606 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:42:23.835: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:164
Nov 16 20:42:23.935: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 16 20:43:24.021: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 20:43:24.029: INFO: Starting informer...
STEP: Starting pod...
Nov 16 20:43:24.267: INFO: Pod is running on 10.240.167.254. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Nov 16 20:43:24.299: INFO: Pod wasn't evicted. Proceeding
Nov 16 20:43:24.299: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Nov 16 20:44:39.389: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:44:39.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-4303" for this suite.
Nov 16 20:44:53.446: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:44:55.348: INFO: namespace taint-single-pod-4303 deletion completed in 15.941249895s

• [SLOW TEST:151.513 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:44:55.349: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 20:44:55.530: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Creating first CR 
Nov 16 20:44:55.681: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-16T20:44:55Z generation:1 name:name1 resourceVersion:84070 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:40fb929a-4dd2-400c-ac0d-7fcb19e6daae] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Nov 16 20:45:05.696: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-16T20:45:05Z generation:1 name:name2 resourceVersion:84132 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:e4e3ddef-12a9-46ee-ad93-b94fa68e266d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Nov 16 20:45:15.710: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-16T20:44:55Z generation:2 name:name1 resourceVersion:84169 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:40fb929a-4dd2-400c-ac0d-7fcb19e6daae] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Nov 16 20:45:25.724: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-16T20:45:05Z generation:2 name:name2 resourceVersion:84213 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:e4e3ddef-12a9-46ee-ad93-b94fa68e266d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Nov 16 20:45:35.749: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-16T20:44:55Z generation:2 name:name1 resourceVersion:84252 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:40fb929a-4dd2-400c-ac0d-7fcb19e6daae] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Nov 16 20:45:45.773: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-16T20:45:05Z generation:2 name:name2 resourceVersion:84293 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:e4e3ddef-12a9-46ee-ad93-b94fa68e266d] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:45:56.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-4039" for this suite.
Nov 16 20:46:04.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:46:06.255: INFO: namespace crd-watch-4039 deletion completed in 9.949507541s

• [SLOW TEST:70.906 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:46:06.256: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-5b8eb351-1fc1-40c4-82fa-e943222db293
STEP: Creating a pod to test consume secrets
Nov 16 20:46:06.398: INFO: Waiting up to 5m0s for pod "pod-secrets-f3bcba91-b22b-4d1e-adff-fb024d71a8d4" in namespace "secrets-9952" to be "success or failure"
Nov 16 20:46:06.405: INFO: Pod "pod-secrets-f3bcba91-b22b-4d1e-adff-fb024d71a8d4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.47626ms
Nov 16 20:46:08.413: INFO: Pod "pod-secrets-f3bcba91-b22b-4d1e-adff-fb024d71a8d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015300145s
STEP: Saw pod success
Nov 16 20:46:08.413: INFO: Pod "pod-secrets-f3bcba91-b22b-4d1e-adff-fb024d71a8d4" satisfied condition "success or failure"
Nov 16 20:46:08.420: INFO: Trying to get logs from node 10.240.167.254 pod pod-secrets-f3bcba91-b22b-4d1e-adff-fb024d71a8d4 container secret-volume-test: <nil>
STEP: delete the pod
Nov 16 20:46:08.484: INFO: Waiting for pod pod-secrets-f3bcba91-b22b-4d1e-adff-fb024d71a8d4 to disappear
Nov 16 20:46:08.490: INFO: Pod pod-secrets-f3bcba91-b22b-4d1e-adff-fb024d71a8d4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:46:08.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9952" for this suite.
Nov 16 20:46:16.529: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:46:18.435: INFO: namespace secrets-9952 deletion completed in 9.933600498s

• [SLOW TEST:12.179 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:46:18.435: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 20:46:18.573: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Nov 16 20:46:18.592: INFO: Number of nodes with available pods: 0
Nov 16 20:46:18.592: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Nov 16 20:46:18.634: INFO: Number of nodes with available pods: 0
Nov 16 20:46:18.634: INFO: Node 10.240.167.206 is running more than one daemon pod
Nov 16 20:46:19.642: INFO: Number of nodes with available pods: 0
Nov 16 20:46:19.642: INFO: Node 10.240.167.206 is running more than one daemon pod
Nov 16 20:46:20.641: INFO: Number of nodes with available pods: 0
Nov 16 20:46:20.641: INFO: Node 10.240.167.206 is running more than one daemon pod
Nov 16 20:46:21.642: INFO: Number of nodes with available pods: 1
Nov 16 20:46:21.642: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Nov 16 20:46:21.681: INFO: Number of nodes with available pods: 1
Nov 16 20:46:21.681: INFO: Number of running nodes: 0, number of available pods: 1
Nov 16 20:46:22.689: INFO: Number of nodes with available pods: 0
Nov 16 20:46:22.689: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Nov 16 20:46:22.715: INFO: Number of nodes with available pods: 0
Nov 16 20:46:22.715: INFO: Node 10.240.167.206 is running more than one daemon pod
Nov 16 20:46:23.724: INFO: Number of nodes with available pods: 0
Nov 16 20:46:23.724: INFO: Node 10.240.167.206 is running more than one daemon pod
Nov 16 20:46:24.723: INFO: Number of nodes with available pods: 0
Nov 16 20:46:24.723: INFO: Node 10.240.167.206 is running more than one daemon pod
Nov 16 20:46:25.725: INFO: Number of nodes with available pods: 0
Nov 16 20:46:25.725: INFO: Node 10.240.167.206 is running more than one daemon pod
Nov 16 20:46:26.724: INFO: Number of nodes with available pods: 0
Nov 16 20:46:26.724: INFO: Node 10.240.167.206 is running more than one daemon pod
Nov 16 20:46:27.724: INFO: Number of nodes with available pods: 1
Nov 16 20:46:27.724: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3933, will wait for the garbage collector to delete the pods
Nov 16 20:46:27.830: INFO: Deleting DaemonSet.extensions daemon-set took: 27.394595ms
Nov 16 20:46:28.331: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.293476ms
Nov 16 20:46:30.939: INFO: Number of nodes with available pods: 0
Nov 16 20:46:30.939: INFO: Number of running nodes: 0, number of available pods: 0
Nov 16 20:46:30.948: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3933/daemonsets","resourceVersion":"84663"},"items":null}

Nov 16 20:46:30.955: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3933/pods","resourceVersion":"84663"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:46:31.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3933" for this suite.
Nov 16 20:46:39.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:46:40.944: INFO: namespace daemonsets-3933 deletion completed in 9.932867695s

• [SLOW TEST:22.509 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:46:40.944: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:46:52.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1542" for this suite.
Nov 16 20:47:00.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:47:02.124: INFO: namespace resourcequota-1542 deletion completed in 9.930535146s

• [SLOW TEST:21.179 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:47:02.124: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap that has name configmap-test-emptyKey-cf0c5c72-21fa-464b-b9d8-b52fd2792681
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:47:02.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5698" for this suite.
Nov 16 20:47:10.281: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:47:12.199: INFO: namespace configmap-5698 deletion completed in 9.949917456s

• [SLOW TEST:10.075 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:47:12.199: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 16 20:47:14.359: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:47:14.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7482" for this suite.
Nov 16 20:47:22.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:47:24.329: INFO: namespace container-runtime-7482 deletion completed in 9.932167957s

• [SLOW TEST:12.130 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:47:24.331: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:47:40.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6520" for this suite.
Nov 16 20:47:48.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:47:50.635: INFO: namespace resourcequota-6520 deletion completed in 9.93673264s

• [SLOW TEST:26.304 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:47:50.637: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-020ba410-b027-4e67-9cab-bce2568f3683
STEP: Creating a pod to test consume secrets
Nov 16 20:47:50.791: INFO: Waiting up to 5m0s for pod "pod-secrets-3e09f65b-2d39-4581-b3d3-043947e76594" in namespace "secrets-1980" to be "success or failure"
Nov 16 20:47:50.798: INFO: Pod "pod-secrets-3e09f65b-2d39-4581-b3d3-043947e76594": Phase="Pending", Reason="", readiness=false. Elapsed: 6.655508ms
Nov 16 20:47:52.806: INFO: Pod "pod-secrets-3e09f65b-2d39-4581-b3d3-043947e76594": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014713308s
STEP: Saw pod success
Nov 16 20:47:52.806: INFO: Pod "pod-secrets-3e09f65b-2d39-4581-b3d3-043947e76594" satisfied condition "success or failure"
Nov 16 20:47:52.814: INFO: Trying to get logs from node 10.240.167.254 pod pod-secrets-3e09f65b-2d39-4581-b3d3-043947e76594 container secret-volume-test: <nil>
STEP: delete the pod
Nov 16 20:47:52.874: INFO: Waiting for pod pod-secrets-3e09f65b-2d39-4581-b3d3-043947e76594 to disappear
Nov 16 20:47:52.881: INFO: Pod pod-secrets-3e09f65b-2d39-4581-b3d3-043947e76594 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:47:52.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1980" for this suite.
Nov 16 20:48:00.916: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:48:02.838: INFO: namespace secrets-1980 deletion completed in 9.947589896s

• [SLOW TEST:12.201 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:48:02.838: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: executing a command with run --rm and attach with stdin
Nov 16 20:48:02.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 --namespace=kubectl-1702 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Nov 16 20:48:04.653: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Nov 16 20:48:04.653: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:48:06.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1702" for this suite.
Nov 16 20:48:14.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:48:16.616: INFO: namespace kubectl-1702 deletion completed in 9.932907977s

• [SLOW TEST:13.778 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1751
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:48:16.617: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Nov 16 20:48:16.714: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 16 20:48:16.750: INFO: Waiting for terminating namespaces to be deleted...
Nov 16 20:48:16.760: INFO: 
Logging pods the kubelet thinks is on node 10.240.167.206 before test
Nov 16 20:48:16.847: INFO: openshift-kube-proxy-5kpn9 from openshift-kube-proxy started at 2020-11-16 17:39:32 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.848: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 16 20:48:16.848: INFO: multus-admission-controller-x6jqb from openshift-multus started at 2020-11-16 17:40:32 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.848: INFO: 	Container multus-admission-controller ready: true, restart count 0
Nov 16 20:48:16.848: INFO: ibm-cloud-provider-ip-161-156-99-92-596ff786f9-bqp8j from ibm-system started at 2020-11-16 20:43:24 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.848: INFO: 	Container ibm-cloud-provider-ip-161-156-99-92 ready: true, restart count 0
Nov 16 20:48:16.848: INFO: node-ca-2pgxt from openshift-image-registry started at 2020-11-16 17:41:42 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.848: INFO: 	Container node-ca ready: true, restart count 0
Nov 16 20:48:16.848: INFO: ibm-keepalived-watcher-2t6l4 from kube-system started at 2020-11-16 17:39:32 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.848: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 20:48:16.848: INFO: kube-state-metrics-776f8894df-p8xsw from openshift-monitoring started at 2020-11-16 17:41:15 +0000 UTC (3 container statuses recorded)
Nov 16 20:48:16.848: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Nov 16 20:48:16.848: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Nov 16 20:48:16.848: INFO: 	Container kube-state-metrics ready: true, restart count 0
Nov 16 20:48:16.848: INFO: packageserver-68c9b7ddbb-rqt2z from openshift-operator-lifecycle-manager started at 2020-11-16 17:44:04 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.848: INFO: 	Container packageserver ready: true, restart count 0
Nov 16 20:48:16.848: INFO: dns-default-zqnbq from openshift-dns started at 2020-11-16 17:42:09 +0000 UTC (2 container statuses recorded)
Nov 16 20:48:16.849: INFO: 	Container dns ready: true, restart count 0
Nov 16 20:48:16.849: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 16 20:48:16.849: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-11-16 17:47:07 +0000 UTC (7 container statuses recorded)
Nov 16 20:48:16.849: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 20:48:16.849: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 20:48:16.849: INFO: 	Container prometheus ready: true, restart count 1
Nov 16 20:48:16.849: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Nov 16 20:48:16.849: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov 16 20:48:16.849: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Nov 16 20:48:16.849: INFO: 	Container thanos-sidecar ready: true, restart count 0
Nov 16 20:48:16.849: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-11-16 17:46:19 +0000 UTC (3 container statuses recorded)
Nov 16 20:48:16.849: INFO: 	Container alertmanager ready: true, restart count 0
Nov 16 20:48:16.849: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 16 20:48:16.849: INFO: 	Container config-reloader ready: true, restart count 0
Nov 16 20:48:16.849: INFO: sonobuoy-systemd-logs-daemon-set-18804f2d5c884991-lmbvb from sonobuoy started at 2020-11-16 19:14:33 +0000 UTC (2 container statuses recorded)
Nov 16 20:48:16.849: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Nov 16 20:48:16.849: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 16 20:48:16.849: INFO: calico-node-pk4fl from calico-system started at 2020-11-16 17:40:12 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.850: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 20:48:16.850: INFO: cluster-samples-operator-55cf746658-rtxxg from openshift-cluster-samples-operator started at 2020-11-16 17:42:11 +0000 UTC (2 container statuses recorded)
Nov 16 20:48:16.850: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Nov 16 20:48:16.850: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Nov 16 20:48:16.850: INFO: ibm-master-proxy-static-10.240.167.206 from kube-system started at 2020-11-16 17:39:29 +0000 UTC (2 container statuses recorded)
Nov 16 20:48:16.850: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 20:48:16.850: INFO: 	Container pause ready: true, restart count 0
Nov 16 20:48:16.850: INFO: service-serving-cert-signer-787695f6b4-s4fqz from openshift-service-ca started at 2020-11-16 17:41:13 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.850: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
Nov 16 20:48:16.850: INFO: node-exporter-jssfh from openshift-monitoring started at 2020-11-16 17:41:17 +0000 UTC (2 container statuses recorded)
Nov 16 20:48:16.850: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 20:48:16.850: INFO: 	Container node-exporter ready: true, restart count 0
Nov 16 20:48:16.850: INFO: router-default-589c4b7b87-7299r from openshift-ingress started at 2020-11-16 17:41:37 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.850: INFO: 	Container router ready: true, restart count 0
Nov 16 20:48:16.850: INFO: console-7f85d88fbc-4r5lv from openshift-console started at 2020-11-16 17:42:45 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.850: INFO: 	Container console ready: true, restart count 0
Nov 16 20:48:16.850: INFO: service-ca-operator-5db7bd4f-fqv7p from openshift-service-ca-operator started at 2020-11-16 20:43:24 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.851: INFO: 	Container operator ready: true, restart count 0
Nov 16 20:48:16.851: INFO: multus-gnk8t from openshift-multus started at 2020-11-16 17:39:32 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.851: INFO: 	Container kube-multus ready: true, restart count 0
Nov 16 20:48:16.851: INFO: ibmcloud-block-storage-driver-2s8xr from kube-system started at 2020-11-16 17:39:34 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.851: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 16 20:48:16.851: INFO: prometheus-adapter-554fc6c4db-6qn5b from openshift-monitoring started at 2020-11-16 17:45:54 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.851: INFO: 	Container prometheus-adapter ready: true, restart count 0
Nov 16 20:48:16.851: INFO: openshift-state-metrics-86c5b47587-nhfn8 from openshift-monitoring started at 2020-11-16 17:41:13 +0000 UTC (3 container statuses recorded)
Nov 16 20:48:16.851: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Nov 16 20:48:16.851: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Nov 16 20:48:16.851: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Nov 16 20:48:16.851: INFO: calico-typha-b5486f777-9gw72 from calico-system started at 2020-11-16 17:42:09 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.851: INFO: 	Container calico-typha ready: true, restart count 0
Nov 16 20:48:16.851: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-11-16 17:44:46 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.851: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Nov 16 20:48:16.851: INFO: thanos-querier-54b47b4fc4-rcx4j from openshift-monitoring started at 2020-11-16 17:46:47 +0000 UTC (4 container statuses recorded)
Nov 16 20:48:16.851: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 20:48:16.851: INFO: 	Container oauth-proxy ready: true, restart count 0
Nov 16 20:48:16.852: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 20:48:16.852: INFO: 	Container thanos-querier ready: true, restart count 0
Nov 16 20:48:16.852: INFO: tuned-lrr6v from openshift-cluster-node-tuning-operator started at 2020-11-16 20:43:46 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.852: INFO: 	Container tuned ready: true, restart count 0
Nov 16 20:48:16.852: INFO: 
Logging pods the kubelet thinks is on node 10.240.167.209 before test
Nov 16 20:48:16.985: INFO: thanos-querier-54b47b4fc4-zvlf7 from openshift-monitoring started at 2020-11-16 17:46:57 +0000 UTC (4 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 20:48:16.986: INFO: 	Container oauth-proxy ready: true, restart count 0
Nov 16 20:48:16.986: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 20:48:16.986: INFO: 	Container thanos-querier ready: true, restart count 0
Nov 16 20:48:16.986: INFO: cluster-node-tuning-operator-5c44ccc99b-bvkzv from openshift-cluster-node-tuning-operator started at 2020-11-16 20:43:24 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Nov 16 20:48:16.986: INFO: openshift-service-catalog-controller-manager-operator-595fxwfq6 from openshift-service-catalog-controller-manager-operator started at 2020-11-16 20:43:24 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container operator ready: true, restart count 0
Nov 16 20:48:16.986: INFO: calico-node-tc4r7 from calico-system started at 2020-11-16 17:40:12 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 20:48:16.986: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-11-16 17:46:06 +0000 UTC (3 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container alertmanager ready: true, restart count 0
Nov 16 20:48:16.986: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 16 20:48:16.986: INFO: 	Container config-reloader ready: true, restart count 0
Nov 16 20:48:16.986: INFO: packageserver-68c9b7ddbb-phc28 from openshift-operator-lifecycle-manager started at 2020-11-16 17:44:00 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container packageserver ready: true, restart count 0
Nov 16 20:48:16.986: INFO: console-operator-c9844474-sk7j2 from openshift-console-operator started at 2020-11-16 20:43:24 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container console-operator ready: true, restart count 0
Nov 16 20:48:16.986: INFO: router-default-589c4b7b87-74cdn from openshift-ingress started at 2020-11-16 17:41:37 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container router ready: true, restart count 0
Nov 16 20:48:16.986: INFO: node-exporter-lqwgh from openshift-monitoring started at 2020-11-16 17:41:17 +0000 UTC (2 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 20:48:16.986: INFO: 	Container node-exporter ready: true, restart count 0
Nov 16 20:48:16.986: INFO: prometheus-operator-7646fdcb7b-tw6jj from openshift-monitoring started at 2020-11-16 17:45:42 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container prometheus-operator ready: true, restart count 0
Nov 16 20:48:16.986: INFO: grafana-58c46d5bb-hs8tj from openshift-monitoring started at 2020-11-16 17:46:00 +0000 UTC (2 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container grafana ready: true, restart count 0
Nov 16 20:48:16.986: INFO: 	Container grafana-proxy ready: true, restart count 0
Nov 16 20:48:16.986: INFO: sonobuoy-systemd-logs-daemon-set-18804f2d5c884991-rfhks from sonobuoy started at 2020-11-16 19:14:33 +0000 UTC (2 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Nov 16 20:48:16.986: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 16 20:48:16.986: INFO: marketplace-operator-54847664c-zhpmb from openshift-marketplace started at 2020-11-16 20:43:24 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container marketplace-operator ready: true, restart count 0
Nov 16 20:48:16.986: INFO: network-operator-74fc599569-7k9c2 from openshift-network-operator started at 2020-11-16 20:43:24 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container network-operator ready: true, restart count 0
Nov 16 20:48:16.986: INFO: tigera-operator-798cfbf7dd-7jpjh from tigera-operator started at 2020-11-16 20:43:24 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container tigera-operator ready: true, restart count 0
Nov 16 20:48:16.986: INFO: catalog-operator-86d68f4684-97mn8 from openshift-operator-lifecycle-manager started at 2020-11-16 20:43:24 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container catalog-operator ready: true, restart count 0
Nov 16 20:48:16.986: INFO: ibm-keepalived-watcher-x4nqc from kube-system started at 2020-11-16 17:39:19 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 20:48:16.986: INFO: multus-admission-controller-tf7n7 from openshift-multus started at 2020-11-16 17:40:30 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container multus-admission-controller ready: true, restart count 0
Nov 16 20:48:16.986: INFO: configmap-cabundle-injector-5bd6fcf58-hlwpk from openshift-service-ca started at 2020-11-16 17:41:14 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
Nov 16 20:48:16.986: INFO: ibmcloud-block-storage-plugin-79495594d5-w5x25 from kube-system started at 2020-11-16 20:43:24 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Nov 16 20:48:16.986: INFO: cluster-storage-operator-6488c9f77b-9xhqh from openshift-cluster-storage-operator started at 2020-11-16 20:43:24 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Nov 16 20:48:16.986: INFO: prometheus-adapter-554fc6c4db-8f7cc from openshift-monitoring started at 2020-11-16 17:45:54 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container prometheus-adapter ready: true, restart count 0
Nov 16 20:48:16.986: INFO: tuned-xnc58 from openshift-cluster-node-tuning-operator started at 2020-11-16 20:43:46 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container tuned ready: true, restart count 0
Nov 16 20:48:16.986: INFO: certified-operators-d97ff5cf9-m7nq4 from openshift-marketplace started at 2020-11-16 20:43:49 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container certified-operators ready: true, restart count 0
Nov 16 20:48:16.986: INFO: ibm-master-proxy-static-10.240.167.209 from kube-system started at 2020-11-16 17:39:17 +0000 UTC (2 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 20:48:16.986: INFO: 	Container pause ready: true, restart count 0
Nov 16 20:48:16.986: INFO: openshift-kube-proxy-qfwg7 from openshift-kube-proxy started at 2020-11-16 17:39:31 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 16 20:48:16.986: INFO: apiservice-cabundle-injector-75dcd4f8db-sllzv from openshift-service-ca started at 2020-11-16 17:41:13 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
Nov 16 20:48:16.986: INFO: vpn-679798bf87-s5h4z from kube-system started at 2020-11-16 17:44:16 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container vpn ready: true, restart count 0
Nov 16 20:48:16.986: INFO: calico-kube-controllers-599969f895-dlw5z from calico-system started at 2020-11-16 20:43:24 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Nov 16 20:48:16.986: INFO: olm-operator-88bfd79df-9zx8c from openshift-operator-lifecycle-manager started at 2020-11-16 20:43:24 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container olm-operator ready: true, restart count 0
Nov 16 20:48:16.986: INFO: multus-dfpk4 from openshift-multus started at 2020-11-16 17:39:24 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container kube-multus ready: true, restart count 0
Nov 16 20:48:16.986: INFO: node-ca-mg4vg from openshift-image-registry started at 2020-11-16 17:41:42 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container node-ca ready: true, restart count 0
Nov 16 20:48:16.986: INFO: registry-pvc-permissions-l4jgc from openshift-image-registry started at 2020-11-16 17:44:28 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container pvc-permissions ready: false, restart count 0
Nov 16 20:48:16.986: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-11-16 17:46:57 +0000 UTC (7 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 20:48:16.986: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 20:48:16.986: INFO: 	Container prometheus ready: true, restart count 1
Nov 16 20:48:16.986: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Nov 16 20:48:16.986: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov 16 20:48:16.986: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Nov 16 20:48:16.986: INFO: 	Container thanos-sidecar ready: true, restart count 0
Nov 16 20:48:16.986: INFO: openshift-service-catalog-apiserver-operator-86b98d6fff-djdgm from openshift-service-catalog-apiserver-operator started at 2020-11-16 20:43:24 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container operator ready: true, restart count 0
Nov 16 20:48:16.986: INFO: cluster-monitoring-operator-7d44956445-csxjm from openshift-monitoring started at 2020-11-16 20:43:24 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Nov 16 20:48:16.986: INFO: ibmcloud-block-storage-driver-6zlqm from kube-system started at 2020-11-16 17:39:27 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 16 20:48:16.986: INFO: image-registry-77d48f6786-gqts2 from openshift-image-registry started at 2020-11-16 17:44:28 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container registry ready: true, restart count 0
Nov 16 20:48:16.986: INFO: console-7f85d88fbc-fdc4m from openshift-console started at 2020-11-16 17:41:44 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container console ready: true, restart count 0
Nov 16 20:48:16.986: INFO: dns-default-m5gqp from openshift-dns started at 2020-11-16 17:42:09 +0000 UTC (2 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container dns ready: true, restart count 0
Nov 16 20:48:16.986: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 16 20:48:16.986: INFO: ibm-cloud-provider-ip-161-156-99-92-596ff786f9-gqmvr from ibm-system started at 2020-11-16 17:46:26 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container ibm-cloud-provider-ip-161-156-99-92 ready: true, restart count 0
Nov 16 20:48:16.986: INFO: calico-typha-b5486f777-7g7gg from calico-system started at 2020-11-16 17:40:11 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:16.986: INFO: 	Container calico-typha ready: true, restart count 0
Nov 16 20:48:16.986: INFO: 
Logging pods the kubelet thinks is on node 10.240.167.254 before test
Nov 16 20:48:17.037: INFO: community-operators-67b5fc9c77-6ltdl from openshift-marketplace started at 2020-11-16 20:43:50 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:17.037: INFO: 	Container community-operators ready: true, restart count 0
Nov 16 20:48:17.037: INFO: node-exporter-4kz5x from openshift-monitoring started at 2020-11-16 17:41:17 +0000 UTC (2 container statuses recorded)
Nov 16 20:48:17.037: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 20:48:17.037: INFO: 	Container node-exporter ready: true, restart count 0
Nov 16 20:48:17.037: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-11-16 20:43:42 +0000 UTC (3 container statuses recorded)
Nov 16 20:48:17.037: INFO: 	Container alertmanager ready: true, restart count 0
Nov 16 20:48:17.037: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 16 20:48:17.037: INFO: 	Container config-reloader ready: true, restart count 0
Nov 16 20:48:17.037: INFO: ibmcloud-block-storage-driver-qdh9z from kube-system started at 2020-11-16 17:39:02 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:17.037: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 16 20:48:17.037: INFO: calico-node-s6qzg from calico-system started at 2020-11-16 17:40:12 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:17.037: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 20:48:17.037: INFO: sonobuoy-systemd-logs-daemon-set-18804f2d5c884991-ffhv2 from sonobuoy started at 2020-11-16 19:14:33 +0000 UTC (2 container statuses recorded)
Nov 16 20:48:17.038: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Nov 16 20:48:17.038: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 16 20:48:17.038: INFO: cluster-image-registry-operator-8d47696c5-7fl88 from openshift-image-registry started at 2020-11-16 20:43:24 +0000 UTC (2 container statuses recorded)
Nov 16 20:48:17.038: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Nov 16 20:48:17.038: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
Nov 16 20:48:17.038: INFO: openshift-kube-proxy-smf7r from openshift-kube-proxy started at 2020-11-16 17:39:31 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:17.038: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 16 20:48:17.038: INFO: multus-admission-controller-gxj7j from openshift-multus started at 2020-11-16 20:44:01 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:17.038: INFO: 	Container multus-admission-controller ready: true, restart count 0
Nov 16 20:48:17.038: INFO: sonobuoy from sonobuoy started at 2020-11-16 19:14:22 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:17.038: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 16 20:48:17.038: INFO: sonobuoy-e2e-job-b6bf7e56608b4535 from sonobuoy started at 2020-11-16 19:14:33 +0000 UTC (2 container statuses recorded)
Nov 16 20:48:17.038: INFO: 	Container e2e ready: true, restart count 0
Nov 16 20:48:17.038: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 20:48:17.038: INFO: ingress-operator-57d688547-q6jkw from openshift-ingress-operator started at 2020-11-16 20:43:24 +0000 UTC (2 container statuses recorded)
Nov 16 20:48:17.038: INFO: 	Container ingress-operator ready: true, restart count 0
Nov 16 20:48:17.038: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 20:48:17.038: INFO: multus-gf29b from openshift-multus started at 2020-11-16 17:39:24 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:17.038: INFO: 	Container kube-multus ready: true, restart count 0
Nov 16 20:48:17.038: INFO: dns-default-rtvgx from openshift-dns started at 2020-11-16 17:42:09 +0000 UTC (2 container statuses recorded)
Nov 16 20:48:17.038: INFO: 	Container dns ready: true, restart count 0
Nov 16 20:48:17.038: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 16 20:48:17.038: INFO: ibm-file-plugin-75bbff878-s7hbw from kube-system started at 2020-11-16 20:43:24 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:17.038: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Nov 16 20:48:17.038: INFO: downloads-5dbb4f4ff7-vpn4m from openshift-console started at 2020-11-16 20:43:24 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:17.038: INFO: 	Container download-server ready: true, restart count 0
Nov 16 20:48:17.038: INFO: tuned-9dnzp from openshift-cluster-node-tuning-operator started at 2020-11-16 20:43:46 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:17.038: INFO: 	Container tuned ready: true, restart count 0
Nov 16 20:48:17.038: INFO: ibm-storage-watcher-69d9c445b4-xts8p from kube-system started at 2020-11-16 20:43:24 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:17.038: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Nov 16 20:48:17.038: INFO: calico-typha-b5486f777-km6dh from calico-system started at 2020-11-16 17:42:09 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:17.038: INFO: 	Container calico-typha ready: true, restart count 0
Nov 16 20:48:17.038: INFO: dns-operator-847f45b78c-zmcrb from openshift-dns-operator started at 2020-11-16 20:43:24 +0000 UTC (2 container statuses recorded)
Nov 16 20:48:17.038: INFO: 	Container dns-operator ready: true, restart count 0
Nov 16 20:48:17.038: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 20:48:17.038: INFO: ibm-master-proxy-static-10.240.167.254 from kube-system started at 2020-11-16 17:38:51 +0000 UTC (2 container statuses recorded)
Nov 16 20:48:17.038: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 20:48:17.038: INFO: 	Container pause ready: true, restart count 0
Nov 16 20:48:17.038: INFO: ibm-keepalived-watcher-gw7ng from kube-system started at 2020-11-16 17:38:54 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:17.038: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 20:48:17.038: INFO: downloads-5dbb4f4ff7-mwsnd from openshift-console started at 2020-11-16 20:43:24 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:17.038: INFO: 	Container download-server ready: true, restart count 0
Nov 16 20:48:17.038: INFO: telemeter-client-95c48d495-98mwq from openshift-monitoring started at 2020-11-16 20:43:24 +0000 UTC (3 container statuses recorded)
Nov 16 20:48:17.038: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 20:48:17.038: INFO: 	Container reload ready: true, restart count 0
Nov 16 20:48:17.038: INFO: 	Container telemeter-client ready: true, restart count 0
Nov 16 20:48:17.038: INFO: redhat-operators-7854cbf749-cssd7 from openshift-marketplace started at 2020-11-16 20:43:49 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:17.038: INFO: 	Container redhat-operators ready: true, restart count 0
Nov 16 20:48:17.038: INFO: node-ca-852zk from openshift-image-registry started at 2020-11-16 17:41:42 +0000 UTC (1 container statuses recorded)
Nov 16 20:48:17.038: INFO: 	Container node-ca ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16481809b477d364], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:48:18.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4684" for this suite.
Nov 16 20:48:26.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:48:28.084: INFO: namespace sched-pred-4684 deletion completed in 9.946263536s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:11.468 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:48:28.084: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 20:48:28.217: INFO: Waiting up to 5m0s for pod "busybox-user-65534-57d408c9-de8f-4376-bd1e-2be71087effe" in namespace "security-context-test-2890" to be "success or failure"
Nov 16 20:48:28.225: INFO: Pod "busybox-user-65534-57d408c9-de8f-4376-bd1e-2be71087effe": Phase="Pending", Reason="", readiness=false. Elapsed: 7.372469ms
Nov 16 20:48:30.234: INFO: Pod "busybox-user-65534-57d408c9-de8f-4376-bd1e-2be71087effe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016620021s
Nov 16 20:48:30.234: INFO: Pod "busybox-user-65534-57d408c9-de8f-4376-bd1e-2be71087effe" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:48:30.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2890" for this suite.
Nov 16 20:48:38.275: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:48:40.199: INFO: namespace security-context-test-2890 deletion completed in 9.95458768s

• [SLOW TEST:12.115 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a container with runAsUser
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:44
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:48:40.202: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 20:48:40.881: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 16 20:48:42.906: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741156520, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741156520, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741156520, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741156520, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 20:48:45.940: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Nov 16 20:48:50.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 attach --namespace=webhook-9626 to-be-attached-pod -i -c=container1'
Nov 16 20:48:50.189: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:48:50.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9626" for this suite.
Nov 16 20:48:58.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:49:00.150: INFO: namespace webhook-9626 deletion completed in 9.933664169s
STEP: Destroying namespace "webhook-9626-markers" for this suite.
Nov 16 20:49:08.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:49:10.099: INFO: namespace webhook-9626-markers deletion completed in 9.949137856s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:29.937 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:49:10.139: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Nov 16 20:49:20.314: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W1116 20:49:20.314381      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Nov 16 20:49:20.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5724" for this suite.
Nov 16 20:49:28.349: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:49:30.254: INFO: namespace gc-5724 deletion completed in 9.930515467s

• [SLOW TEST:20.115 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:49:30.254: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6995.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6995.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6995.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6995.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6995.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6995.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 16 20:49:52.529: INFO: DNS probes using dns-6995/dns-test-09f60e61-603f-42ea-8077-0634b94aaf89 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:49:52.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6995" for this suite.
Nov 16 20:50:00.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:50:02.510: INFO: namespace dns-6995 deletion completed in 9.939771737s

• [SLOW TEST:32.256 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:50:02.510: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 20:50:02.639: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:50:06.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8889" for this suite.
Nov 16 20:50:52.863: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:50:54.778: INFO: namespace pods-8889 deletion completed in 47.944491507s

• [SLOW TEST:52.268 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:50:54.779: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test hostPath mode
Nov 16 20:50:54.920: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-417" to be "success or failure"
Nov 16 20:50:54.927: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 7.02128ms
Nov 16 20:50:56.935: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015290397s
Nov 16 20:50:58.945: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025082124s
STEP: Saw pod success
Nov 16 20:50:58.945: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Nov 16 20:50:58.953: INFO: Trying to get logs from node 10.240.167.254 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Nov 16 20:50:59.008: INFO: Waiting for pod pod-host-path-test to disappear
Nov 16 20:50:59.014: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:50:59.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-417" for this suite.
Nov 16 20:51:07.051: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:51:08.969: INFO: namespace hostpath-417 deletion completed in 9.944547515s

• [SLOW TEST:14.190 seconds]
[sig-storage] HostPath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:51:08.970: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 20:51:09.598: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 20:51:12.655: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 20:51:12.664: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:51:13.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4833" for this suite.
Nov 16 20:51:22.011: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:51:23.918: INFO: namespace webhook-4833 deletion completed in 9.933892342s
STEP: Destroying namespace "webhook-4833-markers" for this suite.
Nov 16 20:51:31.944: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:51:33.849: INFO: namespace webhook-4833-markers deletion completed in 9.930768058s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:24.918 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:51:33.888: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Nov 16 20:51:34.020: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2235 /api/v1/namespaces/watch-2235/configmaps/e2e-watch-test-configmap-a b0989da3-6192-4f37-9b5d-207338ef8cb4 87188 0 2020-11-16 20:51:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Nov 16 20:51:34.020: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2235 /api/v1/namespaces/watch-2235/configmaps/e2e-watch-test-configmap-a b0989da3-6192-4f37-9b5d-207338ef8cb4 87188 0 2020-11-16 20:51:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Nov 16 20:51:44.043: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2235 /api/v1/namespaces/watch-2235/configmaps/e2e-watch-test-configmap-a b0989da3-6192-4f37-9b5d-207338ef8cb4 87230 0 2020-11-16 20:51:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Nov 16 20:51:44.043: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2235 /api/v1/namespaces/watch-2235/configmaps/e2e-watch-test-configmap-a b0989da3-6192-4f37-9b5d-207338ef8cb4 87230 0 2020-11-16 20:51:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Nov 16 20:51:54.068: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2235 /api/v1/namespaces/watch-2235/configmaps/e2e-watch-test-configmap-a b0989da3-6192-4f37-9b5d-207338ef8cb4 87271 0 2020-11-16 20:51:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Nov 16 20:51:54.068: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2235 /api/v1/namespaces/watch-2235/configmaps/e2e-watch-test-configmap-a b0989da3-6192-4f37-9b5d-207338ef8cb4 87271 0 2020-11-16 20:51:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Nov 16 20:52:04.093: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2235 /api/v1/namespaces/watch-2235/configmaps/e2e-watch-test-configmap-a b0989da3-6192-4f37-9b5d-207338ef8cb4 87314 0 2020-11-16 20:51:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Nov 16 20:52:04.093: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2235 /api/v1/namespaces/watch-2235/configmaps/e2e-watch-test-configmap-a b0989da3-6192-4f37-9b5d-207338ef8cb4 87314 0 2020-11-16 20:51:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Nov 16 20:52:14.114: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2235 /api/v1/namespaces/watch-2235/configmaps/e2e-watch-test-configmap-b 4deabb3b-f0e9-47b0-a80e-2ecdbd936a43 87352 0 2020-11-16 20:52:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Nov 16 20:52:14.115: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2235 /api/v1/namespaces/watch-2235/configmaps/e2e-watch-test-configmap-b 4deabb3b-f0e9-47b0-a80e-2ecdbd936a43 87352 0 2020-11-16 20:52:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Nov 16 20:52:24.136: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2235 /api/v1/namespaces/watch-2235/configmaps/e2e-watch-test-configmap-b 4deabb3b-f0e9-47b0-a80e-2ecdbd936a43 87388 0 2020-11-16 20:52:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Nov 16 20:52:24.136: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2235 /api/v1/namespaces/watch-2235/configmaps/e2e-watch-test-configmap-b 4deabb3b-f0e9-47b0-a80e-2ecdbd936a43 87388 0 2020-11-16 20:52:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:52:34.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2235" for this suite.
Nov 16 20:52:42.184: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:52:44.089: INFO: namespace watch-2235 deletion completed in 9.942013155s

• [SLOW TEST:70.201 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:52:44.089: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1595
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Nov 16 20:52:44.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-2906'
Nov 16 20:52:44.588: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Nov 16 20:52:44.589: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1600
Nov 16 20:52:44.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 delete jobs e2e-test-httpd-job --namespace=kubectl-2906'
Nov 16 20:52:44.739: INFO: stderr: ""
Nov 16 20:52:44.739: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:52:44.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2906" for this suite.
Nov 16 20:52:52.774: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:52:54.677: INFO: namespace kubectl-2906 deletion completed in 9.928116215s

• [SLOW TEST:10.588 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:52:54.678: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 16 20:52:54.828: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d66b19bc-d655-43bf-8a3f-022a773de495" in namespace "downward-api-312" to be "success or failure"
Nov 16 20:52:54.835: INFO: Pod "downwardapi-volume-d66b19bc-d655-43bf-8a3f-022a773de495": Phase="Pending", Reason="", readiness=false. Elapsed: 7.295466ms
Nov 16 20:52:56.844: INFO: Pod "downwardapi-volume-d66b19bc-d655-43bf-8a3f-022a773de495": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015657132s
STEP: Saw pod success
Nov 16 20:52:56.844: INFO: Pod "downwardapi-volume-d66b19bc-d655-43bf-8a3f-022a773de495" satisfied condition "success or failure"
Nov 16 20:52:56.851: INFO: Trying to get logs from node 10.240.167.254 pod downwardapi-volume-d66b19bc-d655-43bf-8a3f-022a773de495 container client-container: <nil>
STEP: delete the pod
Nov 16 20:52:56.922: INFO: Waiting for pod downwardapi-volume-d66b19bc-d655-43bf-8a3f-022a773de495 to disappear
Nov 16 20:52:56.928: INFO: Pod downwardapi-volume-d66b19bc-d655-43bf-8a3f-022a773de495 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:52:56.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-312" for this suite.
Nov 16 20:53:04.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:53:06.868: INFO: namespace downward-api-312 deletion completed in 9.930946759s

• [SLOW TEST:12.191 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:53:06.868: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7965
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating statefulset ss in namespace statefulset-7965
Nov 16 20:53:07.028: INFO: Found 0 stateful pods, waiting for 1
Nov 16 20:53:17.037: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Nov 16 20:53:17.082: INFO: Deleting all statefulset in ns statefulset-7965
Nov 16 20:53:17.089: INFO: Scaling statefulset ss to 0
Nov 16 20:53:37.126: INFO: Waiting for statefulset status.replicas updated to 0
Nov 16 20:53:37.134: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:53:37.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7965" for this suite.
Nov 16 20:53:45.206: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:53:47.111: INFO: namespace statefulset-7965 deletion completed in 9.931967191s

• [SLOW TEST:40.243 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:53:47.112: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-2519
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 16 20:53:47.204: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Nov 16 20:54:09.466: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.216.246:8080/dial?request=hostName&protocol=udp&host=172.30.216.247&port=8081&tries=1'] Namespace:pod-network-test-2519 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 16 20:54:09.466: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
Nov 16 20:54:09.643: INFO: Waiting for endpoints: map[]
Nov 16 20:54:09.649: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.216.246:8080/dial?request=hostName&protocol=udp&host=172.30.158.225&port=8081&tries=1'] Namespace:pod-network-test-2519 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 16 20:54:09.649: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
Nov 16 20:54:09.810: INFO: Waiting for endpoints: map[]
Nov 16 20:54:09.818: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.216.246:8080/dial?request=hostName&protocol=udp&host=172.30.194.201&port=8081&tries=1'] Namespace:pod-network-test-2519 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 16 20:54:09.818: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
Nov 16 20:54:09.988: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:54:09.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2519" for this suite.
Nov 16 20:54:18.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:54:19.953: INFO: namespace pod-network-test-2519 deletion completed in 9.955101018s

• [SLOW TEST:32.842 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:54:19.956: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting the proxy server
Nov 16 20:54:20.089: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-420283327 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:54:20.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9481" for this suite.
Nov 16 20:54:28.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:54:30.139: INFO: namespace kubectl-9481 deletion completed in 9.948980299s

• [SLOW TEST:10.184 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:54:30.141: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 20:54:30.293: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-1848ae72-70b8-43bc-9d98-8c5a603940b6
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-1848ae72-70b8-43bc-9d98-8c5a603940b6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:54:35.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1135" for this suite.
Nov 16 20:54:53.479: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:54:55.385: INFO: namespace projected-1135 deletion completed in 19.93254122s

• [SLOW TEST:25.244 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:54:55.385: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-d9611443-ff5f-4db4-80b1-a4c2df225ab1
STEP: Creating a pod to test consume configMaps
Nov 16 20:54:55.628: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ff5e66fa-b592-478a-8efd-f82d6f470240" in namespace "projected-4199" to be "success or failure"
Nov 16 20:54:55.638: INFO: Pod "pod-projected-configmaps-ff5e66fa-b592-478a-8efd-f82d6f470240": Phase="Pending", Reason="", readiness=false. Elapsed: 9.746513ms
Nov 16 20:54:57.647: INFO: Pod "pod-projected-configmaps-ff5e66fa-b592-478a-8efd-f82d6f470240": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018816993s
STEP: Saw pod success
Nov 16 20:54:57.647: INFO: Pod "pod-projected-configmaps-ff5e66fa-b592-478a-8efd-f82d6f470240" satisfied condition "success or failure"
Nov 16 20:54:57.655: INFO: Trying to get logs from node 10.240.167.254 pod pod-projected-configmaps-ff5e66fa-b592-478a-8efd-f82d6f470240 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 16 20:54:57.710: INFO: Waiting for pod pod-projected-configmaps-ff5e66fa-b592-478a-8efd-f82d6f470240 to disappear
Nov 16 20:54:57.716: INFO: Pod pod-projected-configmaps-ff5e66fa-b592-478a-8efd-f82d6f470240 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:54:57.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4199" for this suite.
Nov 16 20:55:05.751: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:55:07.658: INFO: namespace projected-4199 deletion completed in 9.9317692s

• [SLOW TEST:12.273 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:55:07.658: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-8880f4f8-0eb5-48b5-a2a6-295385dbf594
STEP: Creating a pod to test consume secrets
Nov 16 20:55:07.848: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f490d472-11db-4f2f-9fee-1cbd7e70ae56" in namespace "projected-2526" to be "success or failure"
Nov 16 20:55:07.855: INFO: Pod "pod-projected-secrets-f490d472-11db-4f2f-9fee-1cbd7e70ae56": Phase="Pending", Reason="", readiness=false. Elapsed: 6.643179ms
Nov 16 20:55:09.863: INFO: Pod "pod-projected-secrets-f490d472-11db-4f2f-9fee-1cbd7e70ae56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014365512s
STEP: Saw pod success
Nov 16 20:55:09.863: INFO: Pod "pod-projected-secrets-f490d472-11db-4f2f-9fee-1cbd7e70ae56" satisfied condition "success or failure"
Nov 16 20:55:09.871: INFO: Trying to get logs from node 10.240.167.254 pod pod-projected-secrets-f490d472-11db-4f2f-9fee-1cbd7e70ae56 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 16 20:55:09.918: INFO: Waiting for pod pod-projected-secrets-f490d472-11db-4f2f-9fee-1cbd7e70ae56 to disappear
Nov 16 20:55:09.926: INFO: Pod pod-projected-secrets-f490d472-11db-4f2f-9fee-1cbd7e70ae56 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:55:09.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2526" for this suite.
Nov 16 20:55:17.964: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:55:19.870: INFO: namespace projected-2526 deletion completed in 9.933347165s

• [SLOW TEST:12.212 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:55:19.873: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 16 20:55:20.001: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1c0f8e26-4b22-4769-9ccb-88476d0dfef1" in namespace "projected-5344" to be "success or failure"
Nov 16 20:55:20.008: INFO: Pod "downwardapi-volume-1c0f8e26-4b22-4769-9ccb-88476d0dfef1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.65433ms
Nov 16 20:55:22.017: INFO: Pod "downwardapi-volume-1c0f8e26-4b22-4769-9ccb-88476d0dfef1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015172027s
STEP: Saw pod success
Nov 16 20:55:22.017: INFO: Pod "downwardapi-volume-1c0f8e26-4b22-4769-9ccb-88476d0dfef1" satisfied condition "success or failure"
Nov 16 20:55:22.024: INFO: Trying to get logs from node 10.240.167.254 pod downwardapi-volume-1c0f8e26-4b22-4769-9ccb-88476d0dfef1 container client-container: <nil>
STEP: delete the pod
Nov 16 20:55:22.062: INFO: Waiting for pod downwardapi-volume-1c0f8e26-4b22-4769-9ccb-88476d0dfef1 to disappear
Nov 16 20:55:22.069: INFO: Pod downwardapi-volume-1c0f8e26-4b22-4769-9ccb-88476d0dfef1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:55:22.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5344" for this suite.
Nov 16 20:55:30.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:55:32.011: INFO: namespace projected-5344 deletion completed in 9.93132347s

• [SLOW TEST:12.138 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:55:32.012: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 20:55:32.538: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 16 20:55:34.565: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741156932, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741156932, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741156932, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741156932, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 20:55:37.596: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:55:37.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1426" for this suite.
Nov 16 20:55:45.818: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:55:47.723: INFO: namespace webhook-1426 deletion completed in 9.932305563s
STEP: Destroying namespace "webhook-1426-markers" for this suite.
Nov 16 20:55:55.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:55:57.652: INFO: namespace webhook-1426-markers deletion completed in 9.929430766s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:25.677 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:55:57.689: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating secret secrets-8334/secret-test-e95784f1-c581-4260-a803-ffff0e2faa31
STEP: Creating a pod to test consume secrets
Nov 16 20:55:57.875: INFO: Waiting up to 5m0s for pod "pod-configmaps-2096e389-4cd0-44d1-aa73-2a226e742f18" in namespace "secrets-8334" to be "success or failure"
Nov 16 20:55:57.883: INFO: Pod "pod-configmaps-2096e389-4cd0-44d1-aa73-2a226e742f18": Phase="Pending", Reason="", readiness=false. Elapsed: 7.741658ms
Nov 16 20:55:59.891: INFO: Pod "pod-configmaps-2096e389-4cd0-44d1-aa73-2a226e742f18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015534883s
STEP: Saw pod success
Nov 16 20:55:59.891: INFO: Pod "pod-configmaps-2096e389-4cd0-44d1-aa73-2a226e742f18" satisfied condition "success or failure"
Nov 16 20:55:59.898: INFO: Trying to get logs from node 10.240.167.254 pod pod-configmaps-2096e389-4cd0-44d1-aa73-2a226e742f18 container env-test: <nil>
STEP: delete the pod
Nov 16 20:55:59.941: INFO: Waiting for pod pod-configmaps-2096e389-4cd0-44d1-aa73-2a226e742f18 to disappear
Nov 16 20:55:59.947: INFO: Pod pod-configmaps-2096e389-4cd0-44d1-aa73-2a226e742f18 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:55:59.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8334" for this suite.
Nov 16 20:56:07.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:56:09.901: INFO: namespace secrets-8334 deletion completed in 9.945872979s

• [SLOW TEST:12.213 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:56:09.902: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-zp2s
STEP: Creating a pod to test atomic-volume-subpath
Nov 16 20:56:10.068: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-zp2s" in namespace "subpath-3823" to be "success or failure"
Nov 16 20:56:10.075: INFO: Pod "pod-subpath-test-configmap-zp2s": Phase="Pending", Reason="", readiness=false. Elapsed: 6.159001ms
Nov 16 20:56:12.083: INFO: Pod "pod-subpath-test-configmap-zp2s": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014452823s
Nov 16 20:56:14.091: INFO: Pod "pod-subpath-test-configmap-zp2s": Phase="Running", Reason="", readiness=true. Elapsed: 4.02276952s
Nov 16 20:56:16.100: INFO: Pod "pod-subpath-test-configmap-zp2s": Phase="Running", Reason="", readiness=true. Elapsed: 6.031071385s
Nov 16 20:56:18.108: INFO: Pod "pod-subpath-test-configmap-zp2s": Phase="Running", Reason="", readiness=true. Elapsed: 8.039519239s
Nov 16 20:56:20.116: INFO: Pod "pod-subpath-test-configmap-zp2s": Phase="Running", Reason="", readiness=true. Elapsed: 10.047653517s
Nov 16 20:56:22.124: INFO: Pod "pod-subpath-test-configmap-zp2s": Phase="Running", Reason="", readiness=true. Elapsed: 12.055354074s
Nov 16 20:56:24.134: INFO: Pod "pod-subpath-test-configmap-zp2s": Phase="Running", Reason="", readiness=true. Elapsed: 14.065190513s
Nov 16 20:56:26.143: INFO: Pod "pod-subpath-test-configmap-zp2s": Phase="Running", Reason="", readiness=true. Elapsed: 16.074230054s
Nov 16 20:56:28.154: INFO: Pod "pod-subpath-test-configmap-zp2s": Phase="Running", Reason="", readiness=true. Elapsed: 18.085592023s
Nov 16 20:56:30.162: INFO: Pod "pod-subpath-test-configmap-zp2s": Phase="Running", Reason="", readiness=true. Elapsed: 20.093051585s
Nov 16 20:56:32.169: INFO: Pod "pod-subpath-test-configmap-zp2s": Phase="Running", Reason="", readiness=true. Elapsed: 22.100907974s
Nov 16 20:56:34.178: INFO: Pod "pod-subpath-test-configmap-zp2s": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.108981702s
STEP: Saw pod success
Nov 16 20:56:34.178: INFO: Pod "pod-subpath-test-configmap-zp2s" satisfied condition "success or failure"
Nov 16 20:56:34.184: INFO: Trying to get logs from node 10.240.167.254 pod pod-subpath-test-configmap-zp2s container test-container-subpath-configmap-zp2s: <nil>
STEP: delete the pod
Nov 16 20:56:34.225: INFO: Waiting for pod pod-subpath-test-configmap-zp2s to disappear
Nov 16 20:56:34.231: INFO: Pod pod-subpath-test-configmap-zp2s no longer exists
STEP: Deleting pod pod-subpath-test-configmap-zp2s
Nov 16 20:56:34.231: INFO: Deleting pod "pod-subpath-test-configmap-zp2s" in namespace "subpath-3823"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:56:34.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3823" for this suite.
Nov 16 20:56:42.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:56:44.194: INFO: namespace subpath-3823 deletion completed in 9.947075663s

• [SLOW TEST:34.293 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:56:44.198: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-44
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-44
I1116 20:56:44.347516      26 runners.go:184] Created replication controller with name: externalname-service, namespace: services-44, replica count: 2
Nov 16 20:56:47.399: INFO: Creating new exec pod
I1116 20:56:47.399649      26 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 16 20:56:50.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=services-44 execpodr4fxd -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Nov 16 20:56:50.742: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Nov 16 20:56:50.742: INFO: stdout: ""
Nov 16 20:56:50.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=services-44 execpodr4fxd -- /bin/sh -x -c nc -zv -t -w 2 172.21.19.148 80'
Nov 16 20:56:51.025: INFO: stderr: "+ nc -zv -t -w 2 172.21.19.148 80\nConnection to 172.21.19.148 80 port [tcp/http] succeeded!\n"
Nov 16 20:56:51.025: INFO: stdout: ""
Nov 16 20:56:51.025: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:56:51.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-44" for this suite.
Nov 16 20:56:59.101: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:57:01.005: INFO: namespace services-44 deletion completed in 9.931036387s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:16.808 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:57:01.006: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Nov 16 20:57:01.099: INFO: PodSpec: initContainers in spec.initContainers
Nov 16 20:57:43.438: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-d0077db0-95bd-4dab-ba0e-4048f6d9d5bf", GenerateName:"", Namespace:"init-container-902", SelfLink:"/api/v1/namespaces/init-container-902/pods/pod-init-d0077db0-95bd-4dab-ba0e-4048f6d9d5bf", UID:"cf3bd930-f07b-4d06-a75e-df6efc511d20", ResourceVersion:"89911", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63741157021, loc:(*time.Location)(0x84c02a0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"99832231"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"172.30.216.199/32", "cni.projectcalico.org/podIPs":"172.30.216.199/32", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.216.199\"\n    ],\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-6b27l", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc003841a40), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-6b27l", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0010a5ae0), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-6b27l", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0010a5c20), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-6b27l", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0010a5a40), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00a9b8f38), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.240.167.254", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002ccaae0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00a9b9070)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00a9b9090)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00a9b90ac), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00a9b90b0), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741157021, loc:(*time.Location)(0x84c02a0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741157021, loc:(*time.Location)(0x84c02a0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741157021, loc:(*time.Location)(0x84c02a0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741157021, loc:(*time.Location)(0x84c02a0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.240.167.254", PodIP:"172.30.216.199", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.216.199"}}, StartTime:(*v1.Time)(0xc001a1c480), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001cb28c0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001cb2930)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"cri-o://eea31a1ab3a984fc1ac26c2f1fbd1dbba1acbdbd987f4c77fbc4295b7f68635a", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001a1c500), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001a1c4c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc00a9b9124)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:57:43.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-902" for this suite.
Nov 16 20:57:57.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:57:59.382: INFO: namespace init-container-902 deletion completed in 15.933228239s

• [SLOW TEST:58.377 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:57:59.384: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-9nn6
STEP: Creating a pod to test atomic-volume-subpath
Nov 16 20:57:59.552: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-9nn6" in namespace "subpath-4611" to be "success or failure"
Nov 16 20:57:59.560: INFO: Pod "pod-subpath-test-configmap-9nn6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.536584ms
Nov 16 20:58:01.568: INFO: Pod "pod-subpath-test-configmap-9nn6": Phase="Running", Reason="", readiness=true. Elapsed: 2.016081993s
Nov 16 20:58:03.577: INFO: Pod "pod-subpath-test-configmap-9nn6": Phase="Running", Reason="", readiness=true. Elapsed: 4.024680203s
Nov 16 20:58:05.587: INFO: Pod "pod-subpath-test-configmap-9nn6": Phase="Running", Reason="", readiness=true. Elapsed: 6.034356799s
Nov 16 20:58:07.594: INFO: Pod "pod-subpath-test-configmap-9nn6": Phase="Running", Reason="", readiness=true. Elapsed: 8.041348415s
Nov 16 20:58:09.602: INFO: Pod "pod-subpath-test-configmap-9nn6": Phase="Running", Reason="", readiness=true. Elapsed: 10.049469363s
Nov 16 20:58:11.610: INFO: Pod "pod-subpath-test-configmap-9nn6": Phase="Running", Reason="", readiness=true. Elapsed: 12.057579635s
Nov 16 20:58:13.618: INFO: Pod "pod-subpath-test-configmap-9nn6": Phase="Running", Reason="", readiness=true. Elapsed: 14.065764584s
Nov 16 20:58:15.627: INFO: Pod "pod-subpath-test-configmap-9nn6": Phase="Running", Reason="", readiness=true. Elapsed: 16.074450046s
Nov 16 20:58:17.634: INFO: Pod "pod-subpath-test-configmap-9nn6": Phase="Running", Reason="", readiness=true. Elapsed: 18.082166231s
Nov 16 20:58:19.642: INFO: Pod "pod-subpath-test-configmap-9nn6": Phase="Running", Reason="", readiness=true. Elapsed: 20.089719372s
Nov 16 20:58:21.651: INFO: Pod "pod-subpath-test-configmap-9nn6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.098232949s
STEP: Saw pod success
Nov 16 20:58:21.651: INFO: Pod "pod-subpath-test-configmap-9nn6" satisfied condition "success or failure"
Nov 16 20:58:21.658: INFO: Trying to get logs from node 10.240.167.254 pod pod-subpath-test-configmap-9nn6 container test-container-subpath-configmap-9nn6: <nil>
STEP: delete the pod
Nov 16 20:58:21.733: INFO: Waiting for pod pod-subpath-test-configmap-9nn6 to disappear
Nov 16 20:58:21.740: INFO: Pod pod-subpath-test-configmap-9nn6 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-9nn6
Nov 16 20:58:21.740: INFO: Deleting pod "pod-subpath-test-configmap-9nn6" in namespace "subpath-4611"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:58:21.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4611" for this suite.
Nov 16 20:58:29.785: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:58:31.689: INFO: namespace subpath-4611 deletion completed in 9.93290432s

• [SLOW TEST:32.305 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:58:31.689: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 20:58:32.106: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 16 20:58:34.134: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741157112, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741157112, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741157112, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741157112, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 20:58:37.169: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Nov 16 20:58:37.225: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:58:37.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1177" for this suite.
Nov 16 20:58:45.296: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:58:47.200: INFO: namespace webhook-1177 deletion completed in 9.932632674s
STEP: Destroying namespace "webhook-1177-markers" for this suite.
Nov 16 20:58:55.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:58:57.151: INFO: namespace webhook-1177-markers deletion completed in 9.951101911s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:25.503 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:58:57.193: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 20:58:57.313: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 20:58:59.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-504" for this suite.
Nov 16 20:59:47.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 20:59:49.374: INFO: namespace pods-504 deletion completed in 49.932165339s

• [SLOW TEST:52.181 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 20:59:49.374: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Nov 16 20:59:49.495: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
Nov 16 20:59:56.901: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:00:23.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5697" for this suite.
Nov 16 21:00:31.160: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:00:33.066: INFO: namespace crd-publish-openapi-5697 deletion completed in 9.934019402s

• [SLOW TEST:43.692 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:00:33.066: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 21:00:33.229: INFO: Pod name rollover-pod: Found 0 pods out of 1
Nov 16 21:00:38.238: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 16 21:00:38.238: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Nov 16 21:00:40.255: INFO: Creating deployment "test-rollover-deployment"
Nov 16 21:00:40.279: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Nov 16 21:00:42.301: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Nov 16 21:00:42.318: INFO: Ensure that both replica sets have 1 created replica
Nov 16 21:00:42.334: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Nov 16 21:00:42.353: INFO: Updating deployment test-rollover-deployment
Nov 16 21:00:42.353: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Nov 16 21:00:44.371: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Nov 16 21:00:44.388: INFO: Make sure deployment "test-rollover-deployment" is complete
Nov 16 21:00:44.403: INFO: all replica sets need to contain the pod-template-hash label
Nov 16 21:00:44.403: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741157240, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741157240, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741157243, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741157240, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 21:00:46.422: INFO: all replica sets need to contain the pod-template-hash label
Nov 16 21:00:46.422: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741157240, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741157240, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741157243, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741157240, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 21:00:48.423: INFO: all replica sets need to contain the pod-template-hash label
Nov 16 21:00:48.423: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741157240, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741157240, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741157243, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741157240, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 21:00:50.421: INFO: all replica sets need to contain the pod-template-hash label
Nov 16 21:00:50.422: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741157240, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741157240, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741157243, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741157240, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 21:00:52.420: INFO: all replica sets need to contain the pod-template-hash label
Nov 16 21:00:52.420: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741157240, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741157240, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741157243, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741157240, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 21:00:54.422: INFO: 
Nov 16 21:00:54.423: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Nov 16 21:00:54.447: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-3539 /apis/apps/v1/namespaces/deployment-3539/deployments/test-rollover-deployment 7ad85401-f7fa-40d7-b48a-dd7fb1bf176c 91196 2 2020-11-16 21:00:40 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005728888 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-11-16 21:00:40 +0000 UTC,LastTransitionTime:2020-11-16 21:00:40 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-7d7dc6548c" has successfully progressed.,LastUpdateTime:2020-11-16 21:00:54 +0000 UTC,LastTransitionTime:2020-11-16 21:00:40 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 16 21:00:54.455: INFO: New ReplicaSet "test-rollover-deployment-7d7dc6548c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-7d7dc6548c  deployment-3539 /apis/apps/v1/namespaces/deployment-3539/replicasets/test-rollover-deployment-7d7dc6548c df7dd79b-c3c9-45b9-a482-686d41a6e54a 91185 2 2020-11-16 21:00:42 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 7ad85401-f7fa-40d7-b48a-dd7fb1bf176c 0xc0022235b7 0xc0022235b8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7d7dc6548c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002223648 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 16 21:00:54.455: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Nov 16 21:00:54.455: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3539 /apis/apps/v1/namespaces/deployment-3539/replicasets/test-rollover-controller b953f347-d154-44a4-8b97-52b71e134e6d 91194 2 2020-11-16 21:00:33 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 7ad85401-f7fa-40d7-b48a-dd7fb1bf176c 0xc002223417 0xc002223418}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0022234d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 16 21:00:54.455: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-3539 /apis/apps/v1/namespaces/deployment-3539/replicasets/test-rollover-deployment-f6c94f66c c8f4bc51-880d-4cf6-b585-492cfd0bb05a 91117 2 2020-11-16 21:00:40 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 7ad85401-f7fa-40d7-b48a-dd7fb1bf176c 0xc0022236e0 0xc0022236e1}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002223c18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 16 21:00:54.463: INFO: Pod "test-rollover-deployment-7d7dc6548c-g2j6n" is available:
&Pod{ObjectMeta:{test-rollover-deployment-7d7dc6548c-g2j6n test-rollover-deployment-7d7dc6548c- deployment-3539 /api/v1/namespaces/deployment-3539/pods/test-rollover-deployment-7d7dc6548c-g2j6n ea61d2f5-d80b-40e3-8cdd-c37b94abc477 91147 0 2020-11-16 21:00:42 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[cni.projectcalico.org/podIP:172.30.158.230/32 cni.projectcalico.org/podIPs:172.30.158.230/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.158.230"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-rollover-deployment-7d7dc6548c df7dd79b-c3c9-45b9-a482-686d41a6e54a 0xc006e5a947 0xc006e5a948}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8gs22,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8gs22,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8gs22,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.167.209,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-7dx6x,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 21:00:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 21:00:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 21:00:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-16 21:00:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.167.209,PodIP:172.30.158.230,StartTime:2020-11-16 21:00:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-16 21:00:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:cri-o://cf9b0b2b03d6cdb264581f98244be660104145540470221e996a64aeacdeac36,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.158.230,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:00:54.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3539" for this suite.
Nov 16 21:01:02.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:01:04.426: INFO: namespace deployment-3539 deletion completed in 9.951983167s

• [SLOW TEST:31.360 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:01:04.427: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating api versions
Nov 16 21:01:04.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 api-versions'
Nov 16 21:01:04.624: INFO: stderr: ""
Nov 16 21:01:04.624: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps.openshift.io/v1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nauthorization.openshift.io/v1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1beta1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nibm.com/v1alpha1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachineconfiguration.openshift.io/v1\nmetal3.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperator.tigera.io/v1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\npolicy/v1beta1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsecurity.openshift.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:01:04.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2013" for this suite.
Nov 16 21:01:12.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:01:14.595: INFO: namespace kubectl-2013 deletion completed in 9.950922917s

• [SLOW TEST:10.168 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:738
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:01:14.595: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 16 21:01:14.734: INFO: Waiting up to 5m0s for pod "downwardapi-volume-27a11c10-5373-4941-99e8-06cffa856733" in namespace "projected-1515" to be "success or failure"
Nov 16 21:01:14.742: INFO: Pod "downwardapi-volume-27a11c10-5373-4941-99e8-06cffa856733": Phase="Pending", Reason="", readiness=false. Elapsed: 8.108233ms
Nov 16 21:01:16.751: INFO: Pod "downwardapi-volume-27a11c10-5373-4941-99e8-06cffa856733": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016642118s
Nov 16 21:01:18.759: INFO: Pod "downwardapi-volume-27a11c10-5373-4941-99e8-06cffa856733": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024740362s
STEP: Saw pod success
Nov 16 21:01:18.759: INFO: Pod "downwardapi-volume-27a11c10-5373-4941-99e8-06cffa856733" satisfied condition "success or failure"
Nov 16 21:01:18.766: INFO: Trying to get logs from node 10.240.167.206 pod downwardapi-volume-27a11c10-5373-4941-99e8-06cffa856733 container client-container: <nil>
STEP: delete the pod
Nov 16 21:01:18.834: INFO: Waiting for pod downwardapi-volume-27a11c10-5373-4941-99e8-06cffa856733 to disappear
Nov 16 21:01:18.841: INFO: Pod downwardapi-volume-27a11c10-5373-4941-99e8-06cffa856733 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:01:18.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1515" for this suite.
Nov 16 21:01:26.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:01:28.797: INFO: namespace projected-1515 deletion completed in 9.945565623s

• [SLOW TEST:14.202 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:01:28.798: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-844013b0-b1ae-43d6-8fe3-c509e3b2a446 in namespace container-probe-9210
Nov 16 21:01:30.945: INFO: Started pod liveness-844013b0-b1ae-43d6-8fe3-c509e3b2a446 in namespace container-probe-9210
STEP: checking the pod's current state and verifying that restartCount is present
Nov 16 21:01:30.952: INFO: Initial restart count of pod liveness-844013b0-b1ae-43d6-8fe3-c509e3b2a446 is 0
Nov 16 21:01:51.069: INFO: Restart count of pod container-probe-9210/liveness-844013b0-b1ae-43d6-8fe3-c509e3b2a446 is now 1 (20.117736676s elapsed)
Nov 16 21:02:11.152: INFO: Restart count of pod container-probe-9210/liveness-844013b0-b1ae-43d6-8fe3-c509e3b2a446 is now 2 (40.200623338s elapsed)
Nov 16 21:02:31.242: INFO: Restart count of pod container-probe-9210/liveness-844013b0-b1ae-43d6-8fe3-c509e3b2a446 is now 3 (1m0.290668661s elapsed)
Nov 16 21:02:51.325: INFO: Restart count of pod container-probe-9210/liveness-844013b0-b1ae-43d6-8fe3-c509e3b2a446 is now 4 (1m20.373279667s elapsed)
Nov 16 21:03:53.583: INFO: Restart count of pod container-probe-9210/liveness-844013b0-b1ae-43d6-8fe3-c509e3b2a446 is now 5 (2m22.631219157s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:03:53.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9210" for this suite.
Nov 16 21:04:01.640: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:04:03.545: INFO: namespace container-probe-9210 deletion completed in 9.929293688s

• [SLOW TEST:154.748 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:04:03.546: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 16 21:04:03.684: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c49a34ff-c1e5-4cfc-84fa-a5b0d7ad4ab9" in namespace "projected-5331" to be "success or failure"
Nov 16 21:04:03.691: INFO: Pod "downwardapi-volume-c49a34ff-c1e5-4cfc-84fa-a5b0d7ad4ab9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.1664ms
Nov 16 21:04:05.699: INFO: Pod "downwardapi-volume-c49a34ff-c1e5-4cfc-84fa-a5b0d7ad4ab9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015030902s
STEP: Saw pod success
Nov 16 21:04:05.699: INFO: Pod "downwardapi-volume-c49a34ff-c1e5-4cfc-84fa-a5b0d7ad4ab9" satisfied condition "success or failure"
Nov 16 21:04:05.707: INFO: Trying to get logs from node 10.240.167.206 pod downwardapi-volume-c49a34ff-c1e5-4cfc-84fa-a5b0d7ad4ab9 container client-container: <nil>
STEP: delete the pod
Nov 16 21:04:05.776: INFO: Waiting for pod downwardapi-volume-c49a34ff-c1e5-4cfc-84fa-a5b0d7ad4ab9 to disappear
Nov 16 21:04:05.784: INFO: Pod downwardapi-volume-c49a34ff-c1e5-4cfc-84fa-a5b0d7ad4ab9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:04:05.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5331" for this suite.
Nov 16 21:04:13.821: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:04:15.725: INFO: namespace projected-5331 deletion completed in 9.931724589s

• [SLOW TEST:12.179 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:04:15.725: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:04:17.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4636" for this suite.
Nov 16 21:04:27.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:04:29.906: INFO: namespace containers-4636 deletion completed in 11.958018223s

• [SLOW TEST:14.180 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:04:29.906: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:04:32.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1095" for this suite.
Nov 16 21:05:20.136: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:05:22.055: INFO: namespace kubelet-test-1095 deletion completed in 49.944497036s

• [SLOW TEST:52.149 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:05:22.055: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 21:05:22.160: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-fcee418c-d1aa-484f-a385-59418f35c69f
STEP: Creating configMap with name cm-test-opt-upd-6a2d0970-2de5-4b7e-adff-2e9e175a84a5
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-fcee418c-d1aa-484f-a385-59418f35c69f
STEP: Updating configmap cm-test-opt-upd-6a2d0970-2de5-4b7e-adff-2e9e175a84a5
STEP: Creating configMap with name cm-test-opt-create-7caa2677-b464-4714-b0a8-877e3b808f6c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:05:26.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1866" for this suite.
Nov 16 21:05:56.455: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:05:58.357: INFO: namespace configmap-1866 deletion completed in 31.930501518s

• [SLOW TEST:36.302 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:05:58.358: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Nov 16 21:05:58.874: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Nov 16 21:06:00.901: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741157558, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741157558, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741157558, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741157558, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 21:06:03.937: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 21:06:03.947: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:06:05.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2985" for this suite.
Nov 16 21:06:13.243: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:06:15.146: INFO: namespace crd-webhook-2985 deletion completed in 9.929562614s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:16.832 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:06:15.190: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Nov 16 21:06:55.358: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W1116 21:06:55.358647      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Nov 16 21:06:55.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9404" for this suite.
Nov 16 21:07:03.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:07:05.305: INFO: namespace gc-9404 deletion completed in 9.93721036s

• [SLOW TEST:50.115 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:07:05.307: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-xf2bv in namespace proxy-5764
I1116 21:07:05.464904      26 runners.go:184] Created replication controller with name: proxy-service-xf2bv, namespace: proxy-5764, replica count: 1
I1116 21:07:06.515513      26 runners.go:184] proxy-service-xf2bv Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1116 21:07:07.515952      26 runners.go:184] proxy-service-xf2bv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1116 21:07:08.516618      26 runners.go:184] proxy-service-xf2bv Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 16 21:07:08.525: INFO: setup took 3.101014589s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Nov 16 21:07:08.543: INFO: (0) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">test<... (200; 17.057529ms)
Nov 16 21:07:08.543: INFO: (0) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/rewriteme">test</a> (200; 17.604869ms)
Nov 16 21:07:08.552: INFO: (0) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 26.271909ms)
Nov 16 21:07:08.552: INFO: (0) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">... (200; 26.422643ms)
Nov 16 21:07:08.552: INFO: (0) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 26.268084ms)
Nov 16 21:07:08.552: INFO: (0) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 26.691969ms)
Nov 16 21:07:08.552: INFO: (0) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 26.282601ms)
Nov 16 21:07:08.557: INFO: (0) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname1/proxy/: foo (200; 31.521467ms)
Nov 16 21:07:08.557: INFO: (0) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname1/proxy/: foo (200; 31.657493ms)
Nov 16 21:07:08.560: INFO: (0) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname2/proxy/: bar (200; 34.456049ms)
Nov 16 21:07:08.561: INFO: (0) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname2/proxy/: bar (200; 35.485289ms)
Nov 16 21:07:08.571: INFO: (0) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:460/proxy/: tls baz (200; 45.760973ms)
Nov 16 21:07:08.571: INFO: (0) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname1/proxy/: tls baz (200; 45.54703ms)
Nov 16 21:07:08.571: INFO: (0) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/tlsrewritem... (200; 45.933767ms)
Nov 16 21:07:08.574: INFO: (0) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname2/proxy/: tls qux (200; 48.331662ms)
Nov 16 21:07:08.578: INFO: (0) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:462/proxy/: tls qux (200; 52.074606ms)
Nov 16 21:07:08.590: INFO: (1) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/rewriteme">test</a> (200; 12.418323ms)
Nov 16 21:07:08.591: INFO: (1) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 12.457851ms)
Nov 16 21:07:08.591: INFO: (1) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 12.297228ms)
Nov 16 21:07:08.594: INFO: (1) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:462/proxy/: tls qux (200; 13.503677ms)
Nov 16 21:07:08.594: INFO: (1) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">... (200; 14.069987ms)
Nov 16 21:07:08.595: INFO: (1) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 14.092404ms)
Nov 16 21:07:08.595: INFO: (1) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">test<... (200; 15.499615ms)
Nov 16 21:07:08.595: INFO: (1) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 16.28227ms)
Nov 16 21:07:08.595: INFO: (1) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname2/proxy/: tls qux (200; 16.909594ms)
Nov 16 21:07:08.595: INFO: (1) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:460/proxy/: tls baz (200; 16.213668ms)
Nov 16 21:07:08.595: INFO: (1) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/tlsrewritem... (200; 15.337134ms)
Nov 16 21:07:08.596: INFO: (1) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname2/proxy/: bar (200; 17.989539ms)
Nov 16 21:07:08.598: INFO: (1) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname1/proxy/: tls baz (200; 17.385406ms)
Nov 16 21:07:08.598: INFO: (1) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname1/proxy/: foo (200; 17.691547ms)
Nov 16 21:07:08.599: INFO: (1) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname1/proxy/: foo (200; 20.23157ms)
Nov 16 21:07:08.599: INFO: (1) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname2/proxy/: bar (200; 20.099199ms)
Nov 16 21:07:08.612: INFO: (2) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:460/proxy/: tls baz (200; 12.508124ms)
Nov 16 21:07:08.616: INFO: (2) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/tlsrewritem... (200; 15.118831ms)
Nov 16 21:07:08.616: INFO: (2) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">test<... (200; 15.883573ms)
Nov 16 21:07:08.617: INFO: (2) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">... (200; 15.766155ms)
Nov 16 21:07:08.617: INFO: (2) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:462/proxy/: tls qux (200; 15.40977ms)
Nov 16 21:07:08.617: INFO: (2) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 15.254101ms)
Nov 16 21:07:08.618: INFO: (2) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 14.986582ms)
Nov 16 21:07:08.618: INFO: (2) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 14.807959ms)
Nov 16 21:07:08.618: INFO: (2) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/rewriteme">test</a> (200; 15.87443ms)
Nov 16 21:07:08.618: INFO: (2) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 15.358307ms)
Nov 16 21:07:08.625: INFO: (2) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname2/proxy/: bar (200; 22.743884ms)
Nov 16 21:07:08.625: INFO: (2) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname1/proxy/: tls baz (200; 23.601357ms)
Nov 16 21:07:08.625: INFO: (2) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname2/proxy/: tls qux (200; 22.844101ms)
Nov 16 21:07:08.626: INFO: (2) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname1/proxy/: foo (200; 24.415839ms)
Nov 16 21:07:08.626: INFO: (2) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname1/proxy/: foo (200; 26.100389ms)
Nov 16 21:07:08.626: INFO: (2) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname2/proxy/: bar (200; 25.932164ms)
Nov 16 21:07:08.637: INFO: (3) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:460/proxy/: tls baz (200; 10.982586ms)
Nov 16 21:07:08.640: INFO: (3) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 12.711481ms)
Nov 16 21:07:08.640: INFO: (3) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 12.812132ms)
Nov 16 21:07:08.640: INFO: (3) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 13.088971ms)
Nov 16 21:07:08.642: INFO: (3) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 15.49905ms)
Nov 16 21:07:08.643: INFO: (3) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/rewriteme">test</a> (200; 15.802214ms)
Nov 16 21:07:08.643: INFO: (3) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">... (200; 15.819261ms)
Nov 16 21:07:08.644: INFO: (3) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">test<... (200; 16.832645ms)
Nov 16 21:07:08.644: INFO: (3) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:462/proxy/: tls qux (200; 16.915629ms)
Nov 16 21:07:08.645: INFO: (3) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/tlsrewritem... (200; 17.490759ms)
Nov 16 21:07:08.645: INFO: (3) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname2/proxy/: bar (200; 17.788816ms)
Nov 16 21:07:08.645: INFO: (3) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname2/proxy/: tls qux (200; 18.553479ms)
Nov 16 21:07:08.646: INFO: (3) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname1/proxy/: tls baz (200; 19.156081ms)
Nov 16 21:07:08.647: INFO: (3) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname1/proxy/: foo (200; 19.948951ms)
Nov 16 21:07:08.647: INFO: (3) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname1/proxy/: foo (200; 20.005948ms)
Nov 16 21:07:08.647: INFO: (3) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname2/proxy/: bar (200; 20.486883ms)
Nov 16 21:07:08.663: INFO: (4) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 14.497792ms)
Nov 16 21:07:08.663: INFO: (4) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 14.592149ms)
Nov 16 21:07:08.663: INFO: (4) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 14.626316ms)
Nov 16 21:07:08.663: INFO: (4) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">test<... (200; 14.575945ms)
Nov 16 21:07:08.663: INFO: (4) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/rewriteme">test</a> (200; 14.65315ms)
Nov 16 21:07:08.663: INFO: (4) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 14.920122ms)
Nov 16 21:07:08.663: INFO: (4) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:462/proxy/: tls qux (200; 15.769376ms)
Nov 16 21:07:08.663: INFO: (4) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">... (200; 14.6136ms)
Nov 16 21:07:08.664: INFO: (4) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/tlsrewritem... (200; 14.670792ms)
Nov 16 21:07:08.664: INFO: (4) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:460/proxy/: tls baz (200; 15.253936ms)
Nov 16 21:07:08.670: INFO: (4) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname1/proxy/: foo (200; 21.765675ms)
Nov 16 21:07:08.670: INFO: (4) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname1/proxy/: foo (200; 21.873764ms)
Nov 16 21:07:08.671: INFO: (4) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname2/proxy/: tls qux (200; 21.711016ms)
Nov 16 21:07:08.671: INFO: (4) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname2/proxy/: bar (200; 21.860083ms)
Nov 16 21:07:08.670: INFO: (4) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname2/proxy/: bar (200; 21.725023ms)
Nov 16 21:07:08.670: INFO: (4) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname1/proxy/: tls baz (200; 21.601108ms)
Nov 16 21:07:08.682: INFO: (5) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 10.971936ms)
Nov 16 21:07:08.684: INFO: (5) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 11.834512ms)
Nov 16 21:07:08.685: INFO: (5) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/rewriteme">test</a> (200; 13.328073ms)
Nov 16 21:07:08.687: INFO: (5) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">... (200; 14.006419ms)
Nov 16 21:07:08.688: INFO: (5) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 15.791578ms)
Nov 16 21:07:08.688: INFO: (5) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">test<... (200; 15.041738ms)
Nov 16 21:07:08.689: INFO: (5) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname2/proxy/: tls qux (200; 16.678524ms)
Nov 16 21:07:08.689: INFO: (5) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:462/proxy/: tls qux (200; 16.044626ms)
Nov 16 21:07:08.689: INFO: (5) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 16.206669ms)
Nov 16 21:07:08.689: INFO: (5) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/tlsrewritem... (200; 16.267971ms)
Nov 16 21:07:08.689: INFO: (5) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:460/proxy/: tls baz (200; 16.552696ms)
Nov 16 21:07:08.690: INFO: (5) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname2/proxy/: bar (200; 18.058128ms)
Nov 16 21:07:08.697: INFO: (5) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname1/proxy/: foo (200; 24.453782ms)
Nov 16 21:07:08.697: INFO: (5) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname2/proxy/: bar (200; 24.699876ms)
Nov 16 21:07:08.698: INFO: (5) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname1/proxy/: tls baz (200; 24.820303ms)
Nov 16 21:07:08.698: INFO: (5) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname1/proxy/: foo (200; 25.245624ms)
Nov 16 21:07:08.709: INFO: (6) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 11.067752ms)
Nov 16 21:07:08.712: INFO: (6) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:462/proxy/: tls qux (200; 10.947716ms)
Nov 16 21:07:08.713: INFO: (6) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/rewriteme">test</a> (200; 14.285513ms)
Nov 16 21:07:08.713: INFO: (6) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">... (200; 11.910631ms)
Nov 16 21:07:08.713: INFO: (6) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/tlsrewritem... (200; 11.727374ms)
Nov 16 21:07:08.713: INFO: (6) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 13.254816ms)
Nov 16 21:07:08.713: INFO: (6) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 13.704708ms)
Nov 16 21:07:08.713: INFO: (6) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 13.492679ms)
Nov 16 21:07:08.713: INFO: (6) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">test<... (200; 12.263509ms)
Nov 16 21:07:08.713: INFO: (6) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:460/proxy/: tls baz (200; 13.117251ms)
Nov 16 21:07:08.715: INFO: (6) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname1/proxy/: foo (200; 14.889051ms)
Nov 16 21:07:08.718: INFO: (6) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname2/proxy/: bar (200; 18.834496ms)
Nov 16 21:07:08.718: INFO: (6) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname2/proxy/: tls qux (200; 18.815588ms)
Nov 16 21:07:08.718: INFO: (6) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname1/proxy/: tls baz (200; 16.313653ms)
Nov 16 21:07:08.718: INFO: (6) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname1/proxy/: foo (200; 16.468366ms)
Nov 16 21:07:08.718: INFO: (6) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname2/proxy/: bar (200; 17.626536ms)
Nov 16 21:07:08.733: INFO: (7) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 12.869197ms)
Nov 16 21:07:08.734: INFO: (7) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">... (200; 15.298748ms)
Nov 16 21:07:08.734: INFO: (7) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/tlsrewritem... (200; 15.385847ms)
Nov 16 21:07:08.734: INFO: (7) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:460/proxy/: tls baz (200; 15.913793ms)
Nov 16 21:07:08.734: INFO: (7) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 14.591857ms)
Nov 16 21:07:08.734: INFO: (7) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">test<... (200; 15.68797ms)
Nov 16 21:07:08.734: INFO: (7) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:462/proxy/: tls qux (200; 15.452071ms)
Nov 16 21:07:08.735: INFO: (7) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 15.137132ms)
Nov 16 21:07:08.735: INFO: (7) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 15.308702ms)
Nov 16 21:07:08.735: INFO: (7) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/rewriteme">test</a> (200; 15.310999ms)
Nov 16 21:07:08.736: INFO: (7) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname2/proxy/: tls qux (200; 16.505093ms)
Nov 16 21:07:08.737: INFO: (7) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname1/proxy/: foo (200; 17.98073ms)
Nov 16 21:07:08.737: INFO: (7) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname1/proxy/: tls baz (200; 18.036753ms)
Nov 16 21:07:08.738: INFO: (7) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname2/proxy/: bar (200; 18.497466ms)
Nov 16 21:07:08.738: INFO: (7) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname2/proxy/: bar (200; 19.735689ms)
Nov 16 21:07:08.739: INFO: (7) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname1/proxy/: foo (200; 19.96082ms)
Nov 16 21:07:08.751: INFO: (8) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/rewriteme">test</a> (200; 12.406875ms)
Nov 16 21:07:08.753: INFO: (8) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 13.629276ms)
Nov 16 21:07:08.753: INFO: (8) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 14.226934ms)
Nov 16 21:07:08.754: INFO: (8) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/tlsrewritem... (200; 14.336301ms)
Nov 16 21:07:08.754: INFO: (8) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:460/proxy/: tls baz (200; 14.959489ms)
Nov 16 21:07:08.754: INFO: (8) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:462/proxy/: tls qux (200; 14.489709ms)
Nov 16 21:07:08.754: INFO: (8) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 15.28321ms)
Nov 16 21:07:08.754: INFO: (8) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">... (200; 14.823356ms)
Nov 16 21:07:08.754: INFO: (8) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 14.114935ms)
Nov 16 21:07:08.757: INFO: (8) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname1/proxy/: tls baz (200; 16.374736ms)
Nov 16 21:07:08.758: INFO: (8) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname1/proxy/: foo (200; 18.090553ms)
Nov 16 21:07:08.758: INFO: (8) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname1/proxy/: foo (200; 17.881698ms)
Nov 16 21:07:08.758: INFO: (8) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname2/proxy/: bar (200; 18.195567ms)
Nov 16 21:07:08.759: INFO: (8) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname2/proxy/: bar (200; 19.616676ms)
Nov 16 21:07:08.759: INFO: (8) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname2/proxy/: tls qux (200; 19.626802ms)
Nov 16 21:07:08.763: INFO: (8) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">test<... (200; 22.652872ms)
Nov 16 21:07:08.776: INFO: (9) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:462/proxy/: tls qux (200; 12.373936ms)
Nov 16 21:07:08.778: INFO: (9) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">test<... (200; 14.686657ms)
Nov 16 21:07:08.779: INFO: (9) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/tlsrewritem... (200; 16.130275ms)
Nov 16 21:07:08.779: INFO: (9) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 14.891113ms)
Nov 16 21:07:08.779: INFO: (9) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 15.150983ms)
Nov 16 21:07:08.779: INFO: (9) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:460/proxy/: tls baz (200; 15.898095ms)
Nov 16 21:07:08.779: INFO: (9) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">... (200; 15.972257ms)
Nov 16 21:07:08.780: INFO: (9) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 15.426509ms)
Nov 16 21:07:08.780: INFO: (9) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 17.003517ms)
Nov 16 21:07:08.780: INFO: (9) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/rewriteme">test</a> (200; 16.17225ms)
Nov 16 21:07:08.783: INFO: (9) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname1/proxy/: tls baz (200; 19.612564ms)
Nov 16 21:07:08.783: INFO: (9) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname2/proxy/: tls qux (200; 18.380429ms)
Nov 16 21:07:08.783: INFO: (9) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname1/proxy/: foo (200; 19.886ms)
Nov 16 21:07:08.783: INFO: (9) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname1/proxy/: foo (200; 19.566989ms)
Nov 16 21:07:08.783: INFO: (9) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname2/proxy/: bar (200; 19.629811ms)
Nov 16 21:07:08.784: INFO: (9) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname2/proxy/: bar (200; 19.556064ms)
Nov 16 21:07:08.794: INFO: (10) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:462/proxy/: tls qux (200; 10.073799ms)
Nov 16 21:07:08.796: INFO: (10) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">... (200; 11.795813ms)
Nov 16 21:07:08.797: INFO: (10) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">test<... (200; 12.607982ms)
Nov 16 21:07:08.797: INFO: (10) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 12.424217ms)
Nov 16 21:07:08.797: INFO: (10) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/rewriteme">test</a> (200; 12.315646ms)
Nov 16 21:07:08.797: INFO: (10) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 12.006794ms)
Nov 16 21:07:08.797: INFO: (10) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 13.888103ms)
Nov 16 21:07:08.797: INFO: (10) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:460/proxy/: tls baz (200; 12.802149ms)
Nov 16 21:07:08.797: INFO: (10) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 12.90039ms)
Nov 16 21:07:08.798: INFO: (10) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/tlsrewritem... (200; 14.17474ms)
Nov 16 21:07:08.800: INFO: (10) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname2/proxy/: tls qux (200; 14.782278ms)
Nov 16 21:07:08.807: INFO: (10) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname2/proxy/: bar (200; 22.005323ms)
Nov 16 21:07:08.807: INFO: (10) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname1/proxy/: foo (200; 22.727984ms)
Nov 16 21:07:08.807: INFO: (10) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname2/proxy/: bar (200; 22.98483ms)
Nov 16 21:07:08.807: INFO: (10) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname1/proxy/: tls baz (200; 22.846618ms)
Nov 16 21:07:08.807: INFO: (10) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname1/proxy/: foo (200; 23.276798ms)
Nov 16 21:07:08.819: INFO: (11) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">test<... (200; 11.115181ms)
Nov 16 21:07:08.821: INFO: (11) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/rewriteme">test</a> (200; 13.857243ms)
Nov 16 21:07:08.821: INFO: (11) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/tlsrewritem... (200; 13.853987ms)
Nov 16 21:07:08.822: INFO: (11) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 13.921145ms)
Nov 16 21:07:08.822: INFO: (11) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">... (200; 14.497992ms)
Nov 16 21:07:08.822: INFO: (11) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 14.068515ms)
Nov 16 21:07:08.822: INFO: (11) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 14.805443ms)
Nov 16 21:07:08.822: INFO: (11) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:460/proxy/: tls baz (200; 14.327842ms)
Nov 16 21:07:08.822: INFO: (11) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:462/proxy/: tls qux (200; 14.609646ms)
Nov 16 21:07:08.822: INFO: (11) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 14.608475ms)
Nov 16 21:07:08.825: INFO: (11) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname2/proxy/: bar (200; 17.205152ms)
Nov 16 21:07:08.826: INFO: (11) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname1/proxy/: foo (200; 17.720388ms)
Nov 16 21:07:08.826: INFO: (11) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname2/proxy/: tls qux (200; 17.940865ms)
Nov 16 21:07:08.827: INFO: (11) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname1/proxy/: tls baz (200; 18.648557ms)
Nov 16 21:07:08.827: INFO: (11) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname1/proxy/: foo (200; 18.86819ms)
Nov 16 21:07:08.827: INFO: (11) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname2/proxy/: bar (200; 19.755923ms)
Nov 16 21:07:08.839: INFO: (12) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 11.928641ms)
Nov 16 21:07:08.840: INFO: (12) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">test<... (200; 12.282513ms)
Nov 16 21:07:08.840: INFO: (12) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 12.389583ms)
Nov 16 21:07:08.840: INFO: (12) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:462/proxy/: tls qux (200; 12.798754ms)
Nov 16 21:07:08.844: INFO: (12) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 16.104599ms)
Nov 16 21:07:08.844: INFO: (12) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 16.149589ms)
Nov 16 21:07:08.845: INFO: (12) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname2/proxy/: tls qux (200; 16.98385ms)
Nov 16 21:07:08.845: INFO: (12) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/rewriteme">test</a> (200; 17.252276ms)
Nov 16 21:07:08.846: INFO: (12) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/tlsrewritem... (200; 18.702361ms)
Nov 16 21:07:08.846: INFO: (12) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:460/proxy/: tls baz (200; 18.503213ms)
Nov 16 21:07:08.846: INFO: (12) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname2/proxy/: bar (200; 18.368139ms)
Nov 16 21:07:08.846: INFO: (12) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">... (200; 18.558143ms)
Nov 16 21:07:08.846: INFO: (12) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname1/proxy/: foo (200; 19.057096ms)
Nov 16 21:07:08.849: INFO: (12) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname1/proxy/: foo (200; 21.159399ms)
Nov 16 21:07:08.849: INFO: (12) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname2/proxy/: bar (200; 21.180337ms)
Nov 16 21:07:08.849: INFO: (12) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname1/proxy/: tls baz (200; 21.469233ms)
Nov 16 21:07:08.863: INFO: (13) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">... (200; 13.871382ms)
Nov 16 21:07:08.864: INFO: (13) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 13.564693ms)
Nov 16 21:07:08.864: INFO: (13) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">test<... (200; 14.539729ms)
Nov 16 21:07:08.864: INFO: (13) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:462/proxy/: tls qux (200; 14.23039ms)
Nov 16 21:07:08.864: INFO: (13) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/tlsrewritem... (200; 14.63369ms)
Nov 16 21:07:08.865: INFO: (13) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 14.768859ms)
Nov 16 21:07:08.865: INFO: (13) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:460/proxy/: tls baz (200; 14.845064ms)
Nov 16 21:07:08.865: INFO: (13) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/rewriteme">test</a> (200; 15.246237ms)
Nov 16 21:07:08.865: INFO: (13) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 14.791354ms)
Nov 16 21:07:08.866: INFO: (13) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 15.34663ms)
Nov 16 21:07:08.866: INFO: (13) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname2/proxy/: tls qux (200; 17.153888ms)
Nov 16 21:07:08.867: INFO: (13) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname1/proxy/: foo (200; 17.16698ms)
Nov 16 21:07:08.870: INFO: (13) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname1/proxy/: tls baz (200; 19.439145ms)
Nov 16 21:07:08.870: INFO: (13) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname2/proxy/: bar (200; 19.601529ms)
Nov 16 21:07:08.870: INFO: (13) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname1/proxy/: foo (200; 19.687225ms)
Nov 16 21:07:08.870: INFO: (13) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname2/proxy/: bar (200; 19.793527ms)
Nov 16 21:07:08.885: INFO: (14) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 13.928818ms)
Nov 16 21:07:08.887: INFO: (14) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 16.610965ms)
Nov 16 21:07:08.887: INFO: (14) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 16.015798ms)
Nov 16 21:07:08.887: INFO: (14) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">... (200; 15.144805ms)
Nov 16 21:07:08.887: INFO: (14) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 15.377243ms)
Nov 16 21:07:08.887: INFO: (14) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/tlsrewritem... (200; 15.318609ms)
Nov 16 21:07:08.887: INFO: (14) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/rewriteme">test</a> (200; 16.841911ms)
Nov 16 21:07:08.889: INFO: (14) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname2/proxy/: bar (200; 17.454649ms)
Nov 16 21:07:08.889: INFO: (14) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:460/proxy/: tls baz (200; 17.775949ms)
Nov 16 21:07:08.889: INFO: (14) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:462/proxy/: tls qux (200; 16.871014ms)
Nov 16 21:07:08.889: INFO: (14) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">test<... (200; 16.472856ms)
Nov 16 21:07:08.889: INFO: (14) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname2/proxy/: tls qux (200; 17.860965ms)
Nov 16 21:07:08.892: INFO: (14) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname2/proxy/: bar (200; 19.153623ms)
Nov 16 21:07:08.892: INFO: (14) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname1/proxy/: foo (200; 19.485517ms)
Nov 16 21:07:08.892: INFO: (14) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname1/proxy/: tls baz (200; 19.505039ms)
Nov 16 21:07:08.893: INFO: (14) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname1/proxy/: foo (200; 20.339755ms)
Nov 16 21:07:08.908: INFO: (15) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 14.811632ms)
Nov 16 21:07:08.909: INFO: (15) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">test<... (200; 15.37733ms)
Nov 16 21:07:08.910: INFO: (15) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/rewriteme">test</a> (200; 16.060428ms)
Nov 16 21:07:08.910: INFO: (15) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">... (200; 16.286558ms)
Nov 16 21:07:08.910: INFO: (15) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 17.020147ms)
Nov 16 21:07:08.910: INFO: (15) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:460/proxy/: tls baz (200; 16.909974ms)
Nov 16 21:07:08.912: INFO: (15) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 17.916807ms)
Nov 16 21:07:08.917: INFO: (15) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:462/proxy/: tls qux (200; 22.762381ms)
Nov 16 21:07:08.917: INFO: (15) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname1/proxy/: foo (200; 23.7366ms)
Nov 16 21:07:08.917: INFO: (15) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname2/proxy/: tls qux (200; 23.426442ms)
Nov 16 21:07:08.917: INFO: (15) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 23.378655ms)
Nov 16 21:07:08.917: INFO: (15) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname2/proxy/: bar (200; 24.062878ms)
Nov 16 21:07:08.917: INFO: (15) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/tlsrewritem... (200; 23.576384ms)
Nov 16 21:07:08.919: INFO: (15) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname1/proxy/: foo (200; 25.491316ms)
Nov 16 21:07:08.919: INFO: (15) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname1/proxy/: tls baz (200; 25.464922ms)
Nov 16 21:07:08.920: INFO: (15) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname2/proxy/: bar (200; 26.809557ms)
Nov 16 21:07:08.937: INFO: (16) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/tlsrewritem... (200; 15.068404ms)
Nov 16 21:07:08.937: INFO: (16) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:460/proxy/: tls baz (200; 15.651211ms)
Nov 16 21:07:08.937: INFO: (16) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 16.103458ms)
Nov 16 21:07:08.937: INFO: (16) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">... (200; 15.594168ms)
Nov 16 21:07:08.937: INFO: (16) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/rewriteme">test</a> (200; 16.592676ms)
Nov 16 21:07:08.937: INFO: (16) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:462/proxy/: tls qux (200; 15.364519ms)
Nov 16 21:07:08.937: INFO: (16) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 16.217636ms)
Nov 16 21:07:08.937: INFO: (16) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 16.456707ms)
Nov 16 21:07:08.937: INFO: (16) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">test<... (200; 15.935533ms)
Nov 16 21:07:08.937: INFO: (16) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 17.027708ms)
Nov 16 21:07:08.939: INFO: (16) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname1/proxy/: tls baz (200; 17.298126ms)
Nov 16 21:07:08.940: INFO: (16) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname2/proxy/: tls qux (200; 19.146428ms)
Nov 16 21:07:08.941: INFO: (16) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname2/proxy/: bar (200; 19.917118ms)
Nov 16 21:07:08.941: INFO: (16) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname1/proxy/: foo (200; 19.37238ms)
Nov 16 21:07:08.942: INFO: (16) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname1/proxy/: foo (200; 19.516346ms)
Nov 16 21:07:08.942: INFO: (16) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname2/proxy/: bar (200; 20.174335ms)
Nov 16 21:07:08.956: INFO: (17) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 14.127846ms)
Nov 16 21:07:08.956: INFO: (17) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:460/proxy/: tls baz (200; 14.368982ms)
Nov 16 21:07:08.956: INFO: (17) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/tlsrewritem... (200; 14.176989ms)
Nov 16 21:07:08.956: INFO: (17) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/rewriteme">test</a> (200; 14.297105ms)
Nov 16 21:07:08.957: INFO: (17) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 14.841741ms)
Nov 16 21:07:08.957: INFO: (17) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 15.054306ms)
Nov 16 21:07:08.958: INFO: (17) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:462/proxy/: tls qux (200; 15.698474ms)
Nov 16 21:07:08.958: INFO: (17) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">... (200; 15.780753ms)
Nov 16 21:07:08.958: INFO: (17) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">test<... (200; 15.910046ms)
Nov 16 21:07:08.959: INFO: (17) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 16.565504ms)
Nov 16 21:07:08.960: INFO: (17) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname2/proxy/: bar (200; 18.680976ms)
Nov 16 21:07:08.962: INFO: (17) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname1/proxy/: foo (200; 19.70185ms)
Nov 16 21:07:08.962: INFO: (17) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname2/proxy/: bar (200; 19.994026ms)
Nov 16 21:07:08.962: INFO: (17) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname1/proxy/: tls baz (200; 20.254851ms)
Nov 16 21:07:08.962: INFO: (17) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname2/proxy/: tls qux (200; 20.313989ms)
Nov 16 21:07:08.963: INFO: (17) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname1/proxy/: foo (200; 20.298835ms)
Nov 16 21:07:08.974: INFO: (18) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:462/proxy/: tls qux (200; 10.976776ms)
Nov 16 21:07:08.976: INFO: (18) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 12.996711ms)
Nov 16 21:07:08.977: INFO: (18) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">test<... (200; 13.225004ms)
Nov 16 21:07:08.977: INFO: (18) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/rewriteme">test</a> (200; 13.556996ms)
Nov 16 21:07:08.978: INFO: (18) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">... (200; 14.96169ms)
Nov 16 21:07:08.978: INFO: (18) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:460/proxy/: tls baz (200; 15.007446ms)
Nov 16 21:07:08.978: INFO: (18) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/tlsrewritem... (200; 15.098314ms)
Nov 16 21:07:08.979: INFO: (18) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 15.348054ms)
Nov 16 21:07:08.979: INFO: (18) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 15.350383ms)
Nov 16 21:07:08.982: INFO: (18) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname2/proxy/: bar (200; 18.638305ms)
Nov 16 21:07:08.982: INFO: (18) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname2/proxy/: tls qux (200; 18.699726ms)
Nov 16 21:07:08.981: INFO: (18) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 18.084976ms)
Nov 16 21:07:08.983: INFO: (18) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname1/proxy/: tls baz (200; 19.445782ms)
Nov 16 21:07:08.986: INFO: (18) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname1/proxy/: foo (200; 22.625211ms)
Nov 16 21:07:08.986: INFO: (18) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname1/proxy/: foo (200; 23.136032ms)
Nov 16 21:07:08.987: INFO: (18) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname2/proxy/: bar (200; 23.654555ms)
Nov 16 21:07:08.999: INFO: (19) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x/proxy/rewriteme">test</a> (200; 11.653272ms)
Nov 16 21:07:09.000: INFO: (19) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">... (200; 12.71457ms)
Nov 16 21:07:09.000: INFO: (19) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:443/proxy/tlsrewritem... (200; 12.727092ms)
Nov 16 21:07:09.001: INFO: (19) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/: <a href="/api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:1080/proxy/rewriteme">test<... (200; 13.677863ms)
Nov 16 21:07:09.001: INFO: (19) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 13.612495ms)
Nov 16 21:07:09.002: INFO: (19) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:162/proxy/: bar (200; 14.328464ms)
Nov 16 21:07:09.002: INFO: (19) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:460/proxy/: tls baz (200; 14.340021ms)
Nov 16 21:07:09.002: INFO: (19) /api/v1/namespaces/proxy-5764/pods/https:proxy-service-xf2bv-kfl4x:462/proxy/: tls qux (200; 14.511203ms)
Nov 16 21:07:09.002: INFO: (19) /api/v1/namespaces/proxy-5764/pods/http:proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 15.089745ms)
Nov 16 21:07:09.003: INFO: (19) /api/v1/namespaces/proxy-5764/pods/proxy-service-xf2bv-kfl4x:160/proxy/: foo (200; 15.087536ms)
Nov 16 21:07:09.007: INFO: (19) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname1/proxy/: foo (200; 19.578498ms)
Nov 16 21:07:09.008: INFO: (19) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname1/proxy/: tls baz (200; 20.243471ms)
Nov 16 21:07:09.008: INFO: (19) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname2/proxy/: bar (200; 20.693175ms)
Nov 16 21:07:09.008: INFO: (19) /api/v1/namespaces/proxy-5764/services/proxy-service-xf2bv:portname1/proxy/: foo (200; 20.838046ms)
Nov 16 21:07:09.009: INFO: (19) /api/v1/namespaces/proxy-5764/services/https:proxy-service-xf2bv:tlsportname2/proxy/: tls qux (200; 21.392832ms)
Nov 16 21:07:09.009: INFO: (19) /api/v1/namespaces/proxy-5764/services/http:proxy-service-xf2bv:portname2/proxy/: bar (200; 21.514057ms)
STEP: deleting ReplicationController proxy-service-xf2bv in namespace proxy-5764, will wait for the garbage collector to delete the pods
Nov 16 21:07:09.083: INFO: Deleting ReplicationController proxy-service-xf2bv took: 15.92142ms
Nov 16 21:07:09.484: INFO: Terminating ReplicationController proxy-service-xf2bv pods took: 400.411471ms
[AfterEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:07:21.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5764" for this suite.
Nov 16 21:07:29.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:07:30.487: INFO: namespace proxy-5764 deletion completed in 8.79317872s

• [SLOW TEST:25.181 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:07:30.490: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service nodeport-service with the type=NodePort in namespace services-5001
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-5001
STEP: creating replication controller externalsvc in namespace services-5001
I1116 21:07:30.680603      26 runners.go:184] Created replication controller with name: externalsvc, namespace: services-5001, replica count: 2
I1116 21:07:33.731158      26 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Nov 16 21:07:33.795: INFO: Creating new exec pod
Nov 16 21:07:35.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=services-5001 execpodqv8rb -- /bin/sh -x -c nslookup nodeport-service'
Nov 16 21:07:36.325: INFO: stderr: "+ nslookup nodeport-service\n"
Nov 16 21:07:36.325: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-5001.svc.cluster.local\tcanonical name = externalsvc.services-5001.svc.cluster.local.\nName:\texternalsvc.services-5001.svc.cluster.local\nAddress: 172.21.165.152\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5001, will wait for the garbage collector to delete the pods
Nov 16 21:07:36.399: INFO: Deleting ReplicationController externalsvc took: 16.297337ms
Nov 16 21:07:36.500: INFO: Terminating ReplicationController externalsvc pods took: 100.728066ms
Nov 16 21:07:46.940: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:07:46.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5001" for this suite.
Nov 16 21:07:55.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:07:56.909: INFO: namespace services-5001 deletion completed in 9.93122049s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:26.419 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:07:56.910: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-806
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Nov 16 21:07:57.058: INFO: Found 0 stateful pods, waiting for 3
Nov 16 21:08:07.067: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 21:08:07.067: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 21:08:07.067: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 21:08:07.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-806 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 16 21:08:07.406: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 16 21:08:07.406: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 16 21:08:07.406: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Nov 16 21:08:17.469: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Nov 16 21:08:27.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-806 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 21:08:27.803: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 16 21:08:27.803: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 16 21:08:27.803: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 16 21:08:37.851: INFO: Waiting for StatefulSet statefulset-806/ss2 to complete update
Nov 16 21:08:37.851: INFO: Waiting for Pod statefulset-806/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 16 21:08:37.851: INFO: Waiting for Pod statefulset-806/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 16 21:08:37.851: INFO: Waiting for Pod statefulset-806/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 16 21:08:47.868: INFO: Waiting for StatefulSet statefulset-806/ss2 to complete update
Nov 16 21:08:47.868: INFO: Waiting for Pod statefulset-806/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 16 21:08:57.868: INFO: Waiting for StatefulSet statefulset-806/ss2 to complete update
Nov 16 21:08:57.868: INFO: Waiting for Pod statefulset-806/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Nov 16 21:09:07.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-806 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 16 21:09:08.179: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 16 21:09:08.179: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 16 21:09:08.179: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 16 21:09:18.258: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Nov 16 21:09:28.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec --namespace=statefulset-806 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 21:09:28.607: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 16 21:09:28.607: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 16 21:09:28.607: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 16 21:09:58.664: INFO: Waiting for StatefulSet statefulset-806/ss2 to complete update
Nov 16 21:09:58.664: INFO: Waiting for Pod statefulset-806/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Nov 16 21:10:08.679: INFO: Deleting all statefulset in ns statefulset-806
Nov 16 21:10:08.687: INFO: Scaling statefulset ss2 to 0
Nov 16 21:10:38.723: INFO: Waiting for statefulset status.replicas updated to 0
Nov 16 21:10:38.731: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:10:38.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-806" for this suite.
Nov 16 21:10:46.802: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:10:48.720: INFO: namespace statefulset-806 deletion completed in 9.945100398s

• [SLOW TEST:171.811 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:10:48.723: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override arguments
Nov 16 21:10:48.855: INFO: Waiting up to 5m0s for pod "client-containers-10f407b1-ff4b-4c34-a3d2-cb6f5d92af5e" in namespace "containers-8650" to be "success or failure"
Nov 16 21:10:48.862: INFO: Pod "client-containers-10f407b1-ff4b-4c34-a3d2-cb6f5d92af5e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.315658ms
Nov 16 21:10:50.869: INFO: Pod "client-containers-10f407b1-ff4b-4c34-a3d2-cb6f5d92af5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01417315s
Nov 16 21:10:52.878: INFO: Pod "client-containers-10f407b1-ff4b-4c34-a3d2-cb6f5d92af5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023050982s
STEP: Saw pod success
Nov 16 21:10:52.879: INFO: Pod "client-containers-10f407b1-ff4b-4c34-a3d2-cb6f5d92af5e" satisfied condition "success or failure"
Nov 16 21:10:52.886: INFO: Trying to get logs from node 10.240.167.254 pod client-containers-10f407b1-ff4b-4c34-a3d2-cb6f5d92af5e container test-container: <nil>
STEP: delete the pod
Nov 16 21:10:52.956: INFO: Waiting for pod client-containers-10f407b1-ff4b-4c34-a3d2-cb6f5d92af5e to disappear
Nov 16 21:10:52.963: INFO: Pod client-containers-10f407b1-ff4b-4c34-a3d2-cb6f5d92af5e no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:10:52.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8650" for this suite.
Nov 16 21:11:01.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:11:02.903: INFO: namespace containers-8650 deletion completed in 9.930335532s

• [SLOW TEST:14.180 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:11:02.903: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-projected-fclb
STEP: Creating a pod to test atomic-volume-subpath
Nov 16 21:11:04.095: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-fclb" in namespace "subpath-1776" to be "success or failure"
Nov 16 21:11:04.102: INFO: Pod "pod-subpath-test-projected-fclb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.839254ms
Nov 16 21:11:06.110: INFO: Pod "pod-subpath-test-projected-fclb": Phase="Running", Reason="", readiness=true. Elapsed: 2.015573596s
Nov 16 21:11:08.118: INFO: Pod "pod-subpath-test-projected-fclb": Phase="Running", Reason="", readiness=true. Elapsed: 4.023620434s
Nov 16 21:11:10.127: INFO: Pod "pod-subpath-test-projected-fclb": Phase="Running", Reason="", readiness=true. Elapsed: 6.032316658s
Nov 16 21:11:12.135: INFO: Pod "pod-subpath-test-projected-fclb": Phase="Running", Reason="", readiness=true. Elapsed: 8.040532247s
Nov 16 21:11:14.144: INFO: Pod "pod-subpath-test-projected-fclb": Phase="Running", Reason="", readiness=true. Elapsed: 10.048936309s
Nov 16 21:11:16.152: INFO: Pod "pod-subpath-test-projected-fclb": Phase="Running", Reason="", readiness=true. Elapsed: 12.05666282s
Nov 16 21:11:18.159: INFO: Pod "pod-subpath-test-projected-fclb": Phase="Running", Reason="", readiness=true. Elapsed: 14.064504693s
Nov 16 21:11:20.167: INFO: Pod "pod-subpath-test-projected-fclb": Phase="Running", Reason="", readiness=true. Elapsed: 16.072502487s
Nov 16 21:11:22.175: INFO: Pod "pod-subpath-test-projected-fclb": Phase="Running", Reason="", readiness=true. Elapsed: 18.08027288s
Nov 16 21:11:24.183: INFO: Pod "pod-subpath-test-projected-fclb": Phase="Running", Reason="", readiness=true. Elapsed: 20.08825901s
Nov 16 21:11:26.191: INFO: Pod "pod-subpath-test-projected-fclb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.096567153s
STEP: Saw pod success
Nov 16 21:11:26.191: INFO: Pod "pod-subpath-test-projected-fclb" satisfied condition "success or failure"
Nov 16 21:11:26.198: INFO: Trying to get logs from node 10.240.167.254 pod pod-subpath-test-projected-fclb container test-container-subpath-projected-fclb: <nil>
STEP: delete the pod
Nov 16 21:11:26.240: INFO: Waiting for pod pod-subpath-test-projected-fclb to disappear
Nov 16 21:11:26.247: INFO: Pod pod-subpath-test-projected-fclb no longer exists
STEP: Deleting pod pod-subpath-test-projected-fclb
Nov 16 21:11:26.247: INFO: Deleting pod "pod-subpath-test-projected-fclb" in namespace "subpath-1776"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:11:26.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1776" for this suite.
Nov 16 21:11:34.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:11:36.193: INFO: namespace subpath-1776 deletion completed in 9.929228811s

• [SLOW TEST:33.290 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:11:36.193: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 16 21:11:36.342: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5af0f435-a84c-407d-8aca-214e5e4ec4cb" in namespace "projected-9781" to be "success or failure"
Nov 16 21:11:36.349: INFO: Pod "downwardapi-volume-5af0f435-a84c-407d-8aca-214e5e4ec4cb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.034606ms
Nov 16 21:11:38.356: INFO: Pod "downwardapi-volume-5af0f435-a84c-407d-8aca-214e5e4ec4cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014368779s
STEP: Saw pod success
Nov 16 21:11:38.356: INFO: Pod "downwardapi-volume-5af0f435-a84c-407d-8aca-214e5e4ec4cb" satisfied condition "success or failure"
Nov 16 21:11:38.363: INFO: Trying to get logs from node 10.240.167.254 pod downwardapi-volume-5af0f435-a84c-407d-8aca-214e5e4ec4cb container client-container: <nil>
STEP: delete the pod
Nov 16 21:11:38.403: INFO: Waiting for pod downwardapi-volume-5af0f435-a84c-407d-8aca-214e5e4ec4cb to disappear
Nov 16 21:11:38.409: INFO: Pod downwardapi-volume-5af0f435-a84c-407d-8aca-214e5e4ec4cb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:11:38.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9781" for this suite.
Nov 16 21:11:46.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:11:48.351: INFO: namespace projected-9781 deletion completed in 9.933122235s

• [SLOW TEST:12.158 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:11:48.352: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:345
Nov 16 21:11:48.461: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 16 21:12:48.552: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 21:12:48.562: INFO: Starting informer...
STEP: Starting pods...
Nov 16 21:12:48.814: INFO: Pod1 is running on 10.240.167.254. Tainting Node
Nov 16 21:12:51.071: INFO: Pod2 is running on 10.240.167.254. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Nov 16 21:12:58.453: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Nov 16 21:13:21.670: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:13:21.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-8866" for this suite.
Nov 16 21:13:29.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:13:31.642: INFO: namespace taint-multiple-pods-8866 deletion completed in 9.934360038s

• [SLOW TEST:103.291 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:13:31.642: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Nov 16 21:13:31.895: INFO: Waiting up to 5m0s for pod "pod-fa7e33d9-8dfc-4f40-9024-87dc0f6fe067" in namespace "emptydir-6839" to be "success or failure"
Nov 16 21:13:31.906: INFO: Pod "pod-fa7e33d9-8dfc-4f40-9024-87dc0f6fe067": Phase="Pending", Reason="", readiness=false. Elapsed: 11.117239ms
Nov 16 21:13:33.917: INFO: Pod "pod-fa7e33d9-8dfc-4f40-9024-87dc0f6fe067": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022085207s
STEP: Saw pod success
Nov 16 21:13:33.917: INFO: Pod "pod-fa7e33d9-8dfc-4f40-9024-87dc0f6fe067" satisfied condition "success or failure"
Nov 16 21:13:33.925: INFO: Trying to get logs from node 10.240.167.254 pod pod-fa7e33d9-8dfc-4f40-9024-87dc0f6fe067 container test-container: <nil>
STEP: delete the pod
Nov 16 21:13:34.008: INFO: Waiting for pod pod-fa7e33d9-8dfc-4f40-9024-87dc0f6fe067 to disappear
Nov 16 21:13:34.016: INFO: Pod pod-fa7e33d9-8dfc-4f40-9024-87dc0f6fe067 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:13:34.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6839" for this suite.
Nov 16 21:13:42.059: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:13:43.999: INFO: namespace emptydir-6839 deletion completed in 9.967895403s

• [SLOW TEST:12.357 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:13:43.999: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Nov 16 21:13:44.137: INFO: Pod name pod-release: Found 0 pods out of 1
Nov 16 21:13:49.146: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:13:50.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4775" for this suite.
Nov 16 21:13:58.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:14:00.136: INFO: namespace replication-controller-4775 deletion completed in 9.933168711s

• [SLOW TEST:16.137 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:14:00.136: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-secret-fkx6
STEP: Creating a pod to test atomic-volume-subpath
Nov 16 21:14:00.314: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-fkx6" in namespace "subpath-8598" to be "success or failure"
Nov 16 21:14:00.322: INFO: Pod "pod-subpath-test-secret-fkx6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.78987ms
Nov 16 21:14:02.333: INFO: Pod "pod-subpath-test-secret-fkx6": Phase="Running", Reason="", readiness=true. Elapsed: 2.018411197s
Nov 16 21:14:04.341: INFO: Pod "pod-subpath-test-secret-fkx6": Phase="Running", Reason="", readiness=true. Elapsed: 4.027004672s
Nov 16 21:14:06.349: INFO: Pod "pod-subpath-test-secret-fkx6": Phase="Running", Reason="", readiness=true. Elapsed: 6.034158607s
Nov 16 21:14:08.358: INFO: Pod "pod-subpath-test-secret-fkx6": Phase="Running", Reason="", readiness=true. Elapsed: 8.043105206s
Nov 16 21:14:10.366: INFO: Pod "pod-subpath-test-secret-fkx6": Phase="Running", Reason="", readiness=true. Elapsed: 10.051255499s
Nov 16 21:14:12.373: INFO: Pod "pod-subpath-test-secret-fkx6": Phase="Running", Reason="", readiness=true. Elapsed: 12.058882417s
Nov 16 21:14:14.381: INFO: Pod "pod-subpath-test-secret-fkx6": Phase="Running", Reason="", readiness=true. Elapsed: 14.067054037s
Nov 16 21:14:16.390: INFO: Pod "pod-subpath-test-secret-fkx6": Phase="Running", Reason="", readiness=true. Elapsed: 16.075316014s
Nov 16 21:14:18.397: INFO: Pod "pod-subpath-test-secret-fkx6": Phase="Running", Reason="", readiness=true. Elapsed: 18.08291547s
Nov 16 21:14:20.405: INFO: Pod "pod-subpath-test-secret-fkx6": Phase="Running", Reason="", readiness=true. Elapsed: 20.090956476s
Nov 16 21:14:22.414: INFO: Pod "pod-subpath-test-secret-fkx6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.099378346s
STEP: Saw pod success
Nov 16 21:14:22.414: INFO: Pod "pod-subpath-test-secret-fkx6" satisfied condition "success or failure"
Nov 16 21:14:22.421: INFO: Trying to get logs from node 10.240.167.254 pod pod-subpath-test-secret-fkx6 container test-container-subpath-secret-fkx6: <nil>
STEP: delete the pod
Nov 16 21:14:22.466: INFO: Waiting for pod pod-subpath-test-secret-fkx6 to disappear
Nov 16 21:14:22.473: INFO: Pod pod-subpath-test-secret-fkx6 no longer exists
STEP: Deleting pod pod-subpath-test-secret-fkx6
Nov 16 21:14:22.473: INFO: Deleting pod "pod-subpath-test-secret-fkx6" in namespace "subpath-8598"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:14:22.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8598" for this suite.
Nov 16 21:14:30.517: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:14:32.421: INFO: namespace subpath-8598 deletion completed in 9.930700656s

• [SLOW TEST:32.284 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:14:32.422: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 21:14:32.903: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 16 21:14:34.932: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741158072, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741158072, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741158072, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741158072, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 21:14:37.965: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 21:14:37.973: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4737-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:14:39.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8721" for this suite.
Nov 16 21:14:47.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:14:49.140: INFO: namespace webhook-8721 deletion completed in 9.931007946s
STEP: Destroying namespace "webhook-8721-markers" for this suite.
Nov 16 21:14:57.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:14:59.069: INFO: namespace webhook-8721-markers deletion completed in 9.928969018s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:26.684 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:14:59.106: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 16 21:15:01.264: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:15:01.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7946" for this suite.
Nov 16 21:15:09.329: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:15:11.233: INFO: namespace container-runtime-7946 deletion completed in 9.930917976s

• [SLOW TEST:12.127 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:15:11.233: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Nov 16 21:15:11.367: INFO: Waiting up to 5m0s for pod "downward-api-c96eb6e3-cb9d-4912-8a6d-77aec8f9e884" in namespace "downward-api-6747" to be "success or failure"
Nov 16 21:15:11.373: INFO: Pod "downward-api-c96eb6e3-cb9d-4912-8a6d-77aec8f9e884": Phase="Pending", Reason="", readiness=false. Elapsed: 5.720287ms
Nov 16 21:15:13.381: INFO: Pod "downward-api-c96eb6e3-cb9d-4912-8a6d-77aec8f9e884": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013674176s
STEP: Saw pod success
Nov 16 21:15:13.381: INFO: Pod "downward-api-c96eb6e3-cb9d-4912-8a6d-77aec8f9e884" satisfied condition "success or failure"
Nov 16 21:15:13.387: INFO: Trying to get logs from node 10.240.167.254 pod downward-api-c96eb6e3-cb9d-4912-8a6d-77aec8f9e884 container dapi-container: <nil>
STEP: delete the pod
Nov 16 21:15:13.423: INFO: Waiting for pod downward-api-c96eb6e3-cb9d-4912-8a6d-77aec8f9e884 to disappear
Nov 16 21:15:13.430: INFO: Pod downward-api-c96eb6e3-cb9d-4912-8a6d-77aec8f9e884 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:15:13.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6747" for this suite.
Nov 16 21:15:21.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:15:23.388: INFO: namespace downward-api-6747 deletion completed in 9.948420437s

• [SLOW TEST:12.155 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:15:23.388: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Nov 16 21:15:23.523: INFO: Waiting up to 5m0s for pod "downward-api-749933cb-f907-459f-8f3c-c0b0e30bd954" in namespace "downward-api-9739" to be "success or failure"
Nov 16 21:15:23.531: INFO: Pod "downward-api-749933cb-f907-459f-8f3c-c0b0e30bd954": Phase="Pending", Reason="", readiness=false. Elapsed: 7.789306ms
Nov 16 21:15:25.539: INFO: Pod "downward-api-749933cb-f907-459f-8f3c-c0b0e30bd954": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015916863s
STEP: Saw pod success
Nov 16 21:15:25.539: INFO: Pod "downward-api-749933cb-f907-459f-8f3c-c0b0e30bd954" satisfied condition "success or failure"
Nov 16 21:15:25.547: INFO: Trying to get logs from node 10.240.167.254 pod downward-api-749933cb-f907-459f-8f3c-c0b0e30bd954 container dapi-container: <nil>
STEP: delete the pod
Nov 16 21:15:25.586: INFO: Waiting for pod downward-api-749933cb-f907-459f-8f3c-c0b0e30bd954 to disappear
Nov 16 21:15:25.592: INFO: Pod downward-api-749933cb-f907-459f-8f3c-c0b0e30bd954 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:15:25.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9739" for this suite.
Nov 16 21:15:33.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:15:35.535: INFO: namespace downward-api-9739 deletion completed in 9.934113053s

• [SLOW TEST:12.147 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:15:35.535: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod test-webserver-22e7ed92-98f8-4dcc-9e2e-5d4b510a1b16 in namespace container-probe-6509
Nov 16 21:15:37.698: INFO: Started pod test-webserver-22e7ed92-98f8-4dcc-9e2e-5d4b510a1b16 in namespace container-probe-6509
STEP: checking the pod's current state and verifying that restartCount is present
Nov 16 21:15:37.705: INFO: Initial restart count of pod test-webserver-22e7ed92-98f8-4dcc-9e2e-5d4b510a1b16 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:19:38.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6509" for this suite.
Nov 16 21:19:46.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:19:48.695: INFO: namespace container-probe-6509 deletion completed in 9.935028661s

• [SLOW TEST:253.160 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:19:48.696: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-f9c5abec-22a7-4dd5-91f2-0c1aacb9284b
STEP: Creating a pod to test consume configMaps
Nov 16 21:19:48.857: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1c472875-3074-4f67-99e8-682dda44b6ae" in namespace "projected-3901" to be "success or failure"
Nov 16 21:19:48.865: INFO: Pod "pod-projected-configmaps-1c472875-3074-4f67-99e8-682dda44b6ae": Phase="Pending", Reason="", readiness=false. Elapsed: 7.795537ms
Nov 16 21:19:50.872: INFO: Pod "pod-projected-configmaps-1c472875-3074-4f67-99e8-682dda44b6ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015401855s
STEP: Saw pod success
Nov 16 21:19:50.872: INFO: Pod "pod-projected-configmaps-1c472875-3074-4f67-99e8-682dda44b6ae" satisfied condition "success or failure"
Nov 16 21:19:50.880: INFO: Trying to get logs from node 10.240.167.254 pod pod-projected-configmaps-1c472875-3074-4f67-99e8-682dda44b6ae container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 16 21:19:50.953: INFO: Waiting for pod pod-projected-configmaps-1c472875-3074-4f67-99e8-682dda44b6ae to disappear
Nov 16 21:19:50.960: INFO: Pod pod-projected-configmaps-1c472875-3074-4f67-99e8-682dda44b6ae no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:19:50.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3901" for this suite.
Nov 16 21:19:58.999: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:20:00.920: INFO: namespace projected-3901 deletion completed in 9.949490752s

• [SLOW TEST:12.224 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:20:00.920: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Nov 16 21:20:01.083: INFO: Waiting up to 5m0s for pod "pod-0a7cf31d-f728-4d70-8830-2834415120ed" in namespace "emptydir-3764" to be "success or failure"
Nov 16 21:20:01.089: INFO: Pod "pod-0a7cf31d-f728-4d70-8830-2834415120ed": Phase="Pending", Reason="", readiness=false. Elapsed: 6.608764ms
Nov 16 21:20:03.098: INFO: Pod "pod-0a7cf31d-f728-4d70-8830-2834415120ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015277669s
STEP: Saw pod success
Nov 16 21:20:03.098: INFO: Pod "pod-0a7cf31d-f728-4d70-8830-2834415120ed" satisfied condition "success or failure"
Nov 16 21:20:03.105: INFO: Trying to get logs from node 10.240.167.254 pod pod-0a7cf31d-f728-4d70-8830-2834415120ed container test-container: <nil>
STEP: delete the pod
Nov 16 21:20:03.147: INFO: Waiting for pod pod-0a7cf31d-f728-4d70-8830-2834415120ed to disappear
Nov 16 21:20:03.154: INFO: Pod pod-0a7cf31d-f728-4d70-8830-2834415120ed no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:20:03.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3764" for this suite.
Nov 16 21:20:11.190: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:20:13.096: INFO: namespace emptydir-3764 deletion completed in 9.933091278s

• [SLOW TEST:12.175 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:20:13.096: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-d1bcca52-cfc6-47db-b27a-eb74ec385db7 in namespace container-probe-3978
Nov 16 21:20:15.273: INFO: Started pod busybox-d1bcca52-cfc6-47db-b27a-eb74ec385db7 in namespace container-probe-3978
STEP: checking the pod's current state and verifying that restartCount is present
Nov 16 21:20:15.281: INFO: Initial restart count of pod busybox-d1bcca52-cfc6-47db-b27a-eb74ec385db7 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:24:16.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3978" for this suite.
Nov 16 21:24:24.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:24:26.266: INFO: namespace container-probe-3978 deletion completed in 9.944180794s

• [SLOW TEST:253.170 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:24:26.267: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-50cf5b6f-30e0-455f-8cbb-a27381e55478
STEP: Creating a pod to test consume secrets
Nov 16 21:24:26.429: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-08d2a7c1-91dd-481b-b678-b1e47e9244cd" in namespace "projected-9484" to be "success or failure"
Nov 16 21:24:26.435: INFO: Pod "pod-projected-secrets-08d2a7c1-91dd-481b-b678-b1e47e9244cd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.185133ms
Nov 16 21:24:28.444: INFO: Pod "pod-projected-secrets-08d2a7c1-91dd-481b-b678-b1e47e9244cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015634392s
STEP: Saw pod success
Nov 16 21:24:28.444: INFO: Pod "pod-projected-secrets-08d2a7c1-91dd-481b-b678-b1e47e9244cd" satisfied condition "success or failure"
Nov 16 21:24:28.453: INFO: Trying to get logs from node 10.240.167.254 pod pod-projected-secrets-08d2a7c1-91dd-481b-b678-b1e47e9244cd container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 16 21:24:28.510: INFO: Waiting for pod pod-projected-secrets-08d2a7c1-91dd-481b-b678-b1e47e9244cd to disappear
Nov 16 21:24:28.517: INFO: Pod pod-projected-secrets-08d2a7c1-91dd-481b-b678-b1e47e9244cd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:24:28.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9484" for this suite.
Nov 16 21:24:36.556: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:24:38.458: INFO: namespace projected-9484 deletion completed in 9.931655316s

• [SLOW TEST:12.192 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:24:38.459: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-864e425f-e0c2-4da5-9fdf-95766464f9b6
STEP: Creating a pod to test consume secrets
Nov 16 21:24:39.624: INFO: Waiting up to 5m0s for pod "pod-secrets-b8211d3e-f7b1-4424-8a33-98cc9e0d16f0" in namespace "secrets-6516" to be "success or failure"
Nov 16 21:24:39.632: INFO: Pod "pod-secrets-b8211d3e-f7b1-4424-8a33-98cc9e0d16f0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.297771ms
Nov 16 21:24:41.639: INFO: Pod "pod-secrets-b8211d3e-f7b1-4424-8a33-98cc9e0d16f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014098344s
STEP: Saw pod success
Nov 16 21:24:41.639: INFO: Pod "pod-secrets-b8211d3e-f7b1-4424-8a33-98cc9e0d16f0" satisfied condition "success or failure"
Nov 16 21:24:41.645: INFO: Trying to get logs from node 10.240.167.254 pod pod-secrets-b8211d3e-f7b1-4424-8a33-98cc9e0d16f0 container secret-volume-test: <nil>
STEP: delete the pod
Nov 16 21:24:41.684: INFO: Waiting for pod pod-secrets-b8211d3e-f7b1-4424-8a33-98cc9e0d16f0 to disappear
Nov 16 21:24:41.691: INFO: Pod pod-secrets-b8211d3e-f7b1-4424-8a33-98cc9e0d16f0 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:24:41.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6516" for this suite.
Nov 16 21:24:49.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:24:51.647: INFO: namespace secrets-6516 deletion completed in 9.94647981s

• [SLOW TEST:13.189 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:24:51.650: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:24:51.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4862" for this suite.
Nov 16 21:24:59.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:25:01.753: INFO: namespace resourcequota-4862 deletion completed in 9.934838501s

• [SLOW TEST:10.104 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:25:01.757: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 16 21:25:01.887: INFO: Waiting up to 5m0s for pod "downwardapi-volume-315ad1dc-78a4-42bd-bcb7-e43a84e09d72" in namespace "downward-api-6089" to be "success or failure"
Nov 16 21:25:01.893: INFO: Pod "downwardapi-volume-315ad1dc-78a4-42bd-bcb7-e43a84e09d72": Phase="Pending", Reason="", readiness=false. Elapsed: 6.198755ms
Nov 16 21:25:03.901: INFO: Pod "downwardapi-volume-315ad1dc-78a4-42bd-bcb7-e43a84e09d72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014310084s
STEP: Saw pod success
Nov 16 21:25:03.902: INFO: Pod "downwardapi-volume-315ad1dc-78a4-42bd-bcb7-e43a84e09d72" satisfied condition "success or failure"
Nov 16 21:25:03.909: INFO: Trying to get logs from node 10.240.167.254 pod downwardapi-volume-315ad1dc-78a4-42bd-bcb7-e43a84e09d72 container client-container: <nil>
STEP: delete the pod
Nov 16 21:25:03.954: INFO: Waiting for pod downwardapi-volume-315ad1dc-78a4-42bd-bcb7-e43a84e09d72 to disappear
Nov 16 21:25:03.960: INFO: Pod downwardapi-volume-315ad1dc-78a4-42bd-bcb7-e43a84e09d72 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:25:03.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6089" for this suite.
Nov 16 21:25:12.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:25:13.914: INFO: namespace downward-api-6089 deletion completed in 9.943115012s

• [SLOW TEST:12.157 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:25:13.915: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 16 21:25:14.041: INFO: Waiting up to 5m0s for pod "downwardapi-volume-41fd8eca-afff-441e-85cb-80bb4f5b8ec4" in namespace "downward-api-7297" to be "success or failure"
Nov 16 21:25:14.048: INFO: Pod "downwardapi-volume-41fd8eca-afff-441e-85cb-80bb4f5b8ec4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.463574ms
Nov 16 21:25:16.056: INFO: Pod "downwardapi-volume-41fd8eca-afff-441e-85cb-80bb4f5b8ec4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014955166s
STEP: Saw pod success
Nov 16 21:25:16.056: INFO: Pod "downwardapi-volume-41fd8eca-afff-441e-85cb-80bb4f5b8ec4" satisfied condition "success or failure"
Nov 16 21:25:16.064: INFO: Trying to get logs from node 10.240.167.254 pod downwardapi-volume-41fd8eca-afff-441e-85cb-80bb4f5b8ec4 container client-container: <nil>
STEP: delete the pod
Nov 16 21:25:16.106: INFO: Waiting for pod downwardapi-volume-41fd8eca-afff-441e-85cb-80bb4f5b8ec4 to disappear
Nov 16 21:25:16.113: INFO: Pod downwardapi-volume-41fd8eca-afff-441e-85cb-80bb4f5b8ec4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:25:16.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7297" for this suite.
Nov 16 21:25:24.150: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:25:26.068: INFO: namespace downward-api-7297 deletion completed in 9.945619069s

• [SLOW TEST:12.154 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:25:26.069: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-4a902454-3a75-4d24-bce0-2647eb7947d8
STEP: Creating a pod to test consume secrets
Nov 16 21:25:26.228: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-817ddbb6-9701-478c-9d36-a5894c890d61" in namespace "projected-4694" to be "success or failure"
Nov 16 21:25:26.236: INFO: Pod "pod-projected-secrets-817ddbb6-9701-478c-9d36-a5894c890d61": Phase="Pending", Reason="", readiness=false. Elapsed: 8.146623ms
Nov 16 21:25:28.244: INFO: Pod "pod-projected-secrets-817ddbb6-9701-478c-9d36-a5894c890d61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015420921s
STEP: Saw pod success
Nov 16 21:25:28.244: INFO: Pod "pod-projected-secrets-817ddbb6-9701-478c-9d36-a5894c890d61" satisfied condition "success or failure"
Nov 16 21:25:28.251: INFO: Trying to get logs from node 10.240.167.254 pod pod-projected-secrets-817ddbb6-9701-478c-9d36-a5894c890d61 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 16 21:25:28.290: INFO: Waiting for pod pod-projected-secrets-817ddbb6-9701-478c-9d36-a5894c890d61 to disappear
Nov 16 21:25:28.297: INFO: Pod pod-projected-secrets-817ddbb6-9701-478c-9d36-a5894c890d61 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:25:28.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4694" for this suite.
Nov 16 21:25:36.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:25:38.239: INFO: namespace projected-4694 deletion completed in 9.932792578s

• [SLOW TEST:12.171 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:25:38.241: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Nov 16 21:25:38.347: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 16 21:25:38.393: INFO: Waiting for terminating namespaces to be deleted...
Nov 16 21:25:38.404: INFO: 
Logging pods the kubelet thinks is on node 10.240.167.206 before test
Nov 16 21:25:38.478: INFO: openshift-state-metrics-86c5b47587-nhfn8 from openshift-monitoring started at 2020-11-16 17:41:13 +0000 UTC (3 container statuses recorded)
Nov 16 21:25:38.478: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Nov 16 21:25:38.478: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Nov 16 21:25:38.478: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Nov 16 21:25:38.478: INFO: calico-typha-b5486f777-9gw72 from calico-system started at 2020-11-16 17:42:09 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.478: INFO: 	Container calico-typha ready: true, restart count 0
Nov 16 21:25:38.478: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-11-16 17:44:46 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.478: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Nov 16 21:25:38.478: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-11-16 21:13:02 +0000 UTC (3 container statuses recorded)
Nov 16 21:25:38.478: INFO: 	Container alertmanager ready: true, restart count 0
Nov 16 21:25:38.478: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 16 21:25:38.478: INFO: 	Container config-reloader ready: true, restart count 0
Nov 16 21:25:38.478: INFO: thanos-querier-54b47b4fc4-rcx4j from openshift-monitoring started at 2020-11-16 17:46:47 +0000 UTC (4 container statuses recorded)
Nov 16 21:25:38.478: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 21:25:38.478: INFO: 	Container oauth-proxy ready: true, restart count 0
Nov 16 21:25:38.478: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 21:25:38.478: INFO: 	Container thanos-querier ready: true, restart count 0
Nov 16 21:25:38.478: INFO: tuned-lrr6v from openshift-cluster-node-tuning-operator started at 2020-11-16 20:43:46 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.478: INFO: 	Container tuned ready: true, restart count 0
Nov 16 21:25:38.478: INFO: ingress-operator-57d688547-2zxhf from openshift-ingress-operator started at 2020-11-16 21:12:51 +0000 UTC (2 container statuses recorded)
Nov 16 21:25:38.478: INFO: 	Container ingress-operator ready: true, restart count 0
Nov 16 21:25:38.478: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 21:25:38.478: INFO: telemeter-client-95c48d495-wcsvw from openshift-monitoring started at 2020-11-16 21:12:51 +0000 UTC (3 container statuses recorded)
Nov 16 21:25:38.478: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 21:25:38.478: INFO: 	Container reload ready: true, restart count 0
Nov 16 21:25:38.478: INFO: 	Container telemeter-client ready: true, restart count 0
Nov 16 21:25:38.478: INFO: openshift-kube-proxy-5kpn9 from openshift-kube-proxy started at 2020-11-16 17:39:32 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.478: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 16 21:25:38.478: INFO: multus-admission-controller-x6jqb from openshift-multus started at 2020-11-16 17:40:32 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.478: INFO: 	Container multus-admission-controller ready: true, restart count 0
Nov 16 21:25:38.478: INFO: ibm-cloud-provider-ip-161-156-99-92-596ff786f9-bqp8j from ibm-system started at 2020-11-16 20:43:24 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.478: INFO: 	Container ibm-cloud-provider-ip-161-156-99-92 ready: true, restart count 0
Nov 16 21:25:38.479: INFO: node-ca-2pgxt from openshift-image-registry started at 2020-11-16 17:41:42 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.479: INFO: 	Container node-ca ready: true, restart count 0
Nov 16 21:25:38.479: INFO: dns-default-zqnbq from openshift-dns started at 2020-11-16 17:42:09 +0000 UTC (2 container statuses recorded)
Nov 16 21:25:38.479: INFO: 	Container dns ready: true, restart count 0
Nov 16 21:25:38.479: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 16 21:25:38.479: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-11-16 17:47:07 +0000 UTC (7 container statuses recorded)
Nov 16 21:25:38.479: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 21:25:38.479: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 21:25:38.479: INFO: 	Container prometheus ready: true, restart count 1
Nov 16 21:25:38.479: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Nov 16 21:25:38.479: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov 16 21:25:38.479: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Nov 16 21:25:38.479: INFO: 	Container thanos-sidecar ready: true, restart count 0
Nov 16 21:25:38.479: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-11-16 17:46:19 +0000 UTC (3 container statuses recorded)
Nov 16 21:25:38.479: INFO: 	Container alertmanager ready: true, restart count 0
Nov 16 21:25:38.479: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 16 21:25:38.479: INFO: 	Container config-reloader ready: true, restart count 0
Nov 16 21:25:38.479: INFO: sonobuoy-systemd-logs-daemon-set-18804f2d5c884991-lmbvb from sonobuoy started at 2020-11-16 19:14:33 +0000 UTC (2 container statuses recorded)
Nov 16 21:25:38.479: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Nov 16 21:25:38.479: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 16 21:25:38.479: INFO: downloads-5dbb4f4ff7-2bl72 from openshift-console started at 2020-11-16 21:12:51 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.479: INFO: 	Container download-server ready: true, restart count 0
Nov 16 21:25:38.479: INFO: ibm-keepalived-watcher-2t6l4 from kube-system started at 2020-11-16 17:39:32 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.479: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 21:25:38.479: INFO: kube-state-metrics-776f8894df-p8xsw from openshift-monitoring started at 2020-11-16 17:41:15 +0000 UTC (3 container statuses recorded)
Nov 16 21:25:38.479: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Nov 16 21:25:38.479: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Nov 16 21:25:38.479: INFO: 	Container kube-state-metrics ready: true, restart count 0
Nov 16 21:25:38.479: INFO: packageserver-68c9b7ddbb-rqt2z from openshift-operator-lifecycle-manager started at 2020-11-16 17:44:04 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.479: INFO: 	Container packageserver ready: true, restart count 0
Nov 16 21:25:38.479: INFO: calico-node-pk4fl from calico-system started at 2020-11-16 17:40:12 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.479: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 21:25:38.479: INFO: cluster-samples-operator-55cf746658-rtxxg from openshift-cluster-samples-operator started at 2020-11-16 17:42:11 +0000 UTC (2 container statuses recorded)
Nov 16 21:25:38.479: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Nov 16 21:25:38.479: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Nov 16 21:25:38.479: INFO: router-default-589c4b7b87-7299r from openshift-ingress started at 2020-11-16 17:41:37 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.479: INFO: 	Container router ready: true, restart count 0
Nov 16 21:25:38.479: INFO: console-7f85d88fbc-4r5lv from openshift-console started at 2020-11-16 17:42:45 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.479: INFO: 	Container console ready: true, restart count 0
Nov 16 21:25:38.479: INFO: service-ca-operator-5db7bd4f-fqv7p from openshift-service-ca-operator started at 2020-11-16 20:43:24 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.479: INFO: 	Container operator ready: true, restart count 0
Nov 16 21:25:38.479: INFO: ibm-master-proxy-static-10.240.167.206 from kube-system started at 2020-11-16 17:39:29 +0000 UTC (2 container statuses recorded)
Nov 16 21:25:38.479: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 21:25:38.479: INFO: 	Container pause ready: true, restart count 0
Nov 16 21:25:38.479: INFO: service-serving-cert-signer-787695f6b4-s4fqz from openshift-service-ca started at 2020-11-16 17:41:13 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.479: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
Nov 16 21:25:38.479: INFO: node-exporter-jssfh from openshift-monitoring started at 2020-11-16 17:41:17 +0000 UTC (2 container statuses recorded)
Nov 16 21:25:38.479: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 21:25:38.479: INFO: 	Container node-exporter ready: true, restart count 0
Nov 16 21:25:38.479: INFO: dns-operator-847f45b78c-kkw5j from openshift-dns-operator started at 2020-11-16 21:12:51 +0000 UTC (2 container statuses recorded)
Nov 16 21:25:38.479: INFO: 	Container dns-operator ready: true, restart count 0
Nov 16 21:25:38.479: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 21:25:38.479: INFO: multus-gnk8t from openshift-multus started at 2020-11-16 17:39:32 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.479: INFO: 	Container kube-multus ready: true, restart count 0
Nov 16 21:25:38.479: INFO: ibmcloud-block-storage-driver-2s8xr from kube-system started at 2020-11-16 17:39:34 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.479: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 16 21:25:38.479: INFO: prometheus-adapter-554fc6c4db-6qn5b from openshift-monitoring started at 2020-11-16 17:45:54 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.479: INFO: 	Container prometheus-adapter ready: true, restart count 0
Nov 16 21:25:38.479: INFO: 
Logging pods the kubelet thinks is on node 10.240.167.209 before test
Nov 16 21:25:38.569: INFO: registry-pvc-permissions-l4jgc from openshift-image-registry started at 2020-11-16 17:44:28 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.569: INFO: 	Container pvc-permissions ready: false, restart count 0
Nov 16 21:25:38.569: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-11-16 17:46:57 +0000 UTC (7 container statuses recorded)
Nov 16 21:25:38.569: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 21:25:38.569: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 21:25:38.570: INFO: 	Container prometheus ready: true, restart count 1
Nov 16 21:25:38.570: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Nov 16 21:25:38.570: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov 16 21:25:38.570: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Nov 16 21:25:38.570: INFO: 	Container thanos-sidecar ready: true, restart count 0
Nov 16 21:25:38.570: INFO: calico-kube-controllers-599969f895-dlw5z from calico-system started at 2020-11-16 20:43:24 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.570: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Nov 16 21:25:38.570: INFO: olm-operator-88bfd79df-9zx8c from openshift-operator-lifecycle-manager started at 2020-11-16 20:43:24 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.570: INFO: 	Container olm-operator ready: true, restart count 0
Nov 16 21:25:38.571: INFO: multus-dfpk4 from openshift-multus started at 2020-11-16 17:39:24 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.571: INFO: 	Container kube-multus ready: true, restart count 0
Nov 16 21:25:38.571: INFO: node-ca-mg4vg from openshift-image-registry started at 2020-11-16 17:41:42 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.571: INFO: 	Container node-ca ready: true, restart count 0
Nov 16 21:25:38.571: INFO: console-7f85d88fbc-fdc4m from openshift-console started at 2020-11-16 17:41:44 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.571: INFO: 	Container console ready: true, restart count 0
Nov 16 21:25:38.571: INFO: dns-default-m5gqp from openshift-dns started at 2020-11-16 17:42:09 +0000 UTC (2 container statuses recorded)
Nov 16 21:25:38.571: INFO: 	Container dns ready: true, restart count 0
Nov 16 21:25:38.571: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 16 21:25:38.571: INFO: ibm-cloud-provider-ip-161-156-99-92-596ff786f9-gqmvr from ibm-system started at 2020-11-16 17:46:26 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.571: INFO: 	Container ibm-cloud-provider-ip-161-156-99-92 ready: true, restart count 0
Nov 16 21:25:38.571: INFO: openshift-service-catalog-apiserver-operator-86b98d6fff-djdgm from openshift-service-catalog-apiserver-operator started at 2020-11-16 20:43:24 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.572: INFO: 	Container operator ready: true, restart count 0
Nov 16 21:25:38.572: INFO: cluster-monitoring-operator-7d44956445-csxjm from openshift-monitoring started at 2020-11-16 20:43:24 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.572: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Nov 16 21:25:38.572: INFO: ibm-storage-watcher-69d9c445b4-gznxn from kube-system started at 2020-11-16 21:12:51 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.572: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Nov 16 21:25:38.572: INFO: ibmcloud-block-storage-driver-6zlqm from kube-system started at 2020-11-16 17:39:27 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.572: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 16 21:25:38.572: INFO: image-registry-77d48f6786-gqts2 from openshift-image-registry started at 2020-11-16 17:44:28 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.572: INFO: 	Container registry ready: true, restart count 0
Nov 16 21:25:38.572: INFO: cluster-image-registry-operator-8d47696c5-cxwwk from openshift-image-registry started at 2020-11-16 21:12:51 +0000 UTC (2 container statuses recorded)
Nov 16 21:25:38.572: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Nov 16 21:25:38.572: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
Nov 16 21:25:38.572: INFO: calico-typha-b5486f777-7g7gg from calico-system started at 2020-11-16 17:40:11 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.572: INFO: 	Container calico-typha ready: true, restart count 0
Nov 16 21:25:38.573: INFO: community-operators-67b5fc9c77-vdp65 from openshift-marketplace started at 2020-11-16 21:12:51 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.573: INFO: 	Container community-operators ready: true, restart count 0
Nov 16 21:25:38.573: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-11-16 17:46:06 +0000 UTC (3 container statuses recorded)
Nov 16 21:25:38.573: INFO: 	Container alertmanager ready: true, restart count 0
Nov 16 21:25:38.573: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 16 21:25:38.573: INFO: 	Container config-reloader ready: true, restart count 0
Nov 16 21:25:38.573: INFO: packageserver-68c9b7ddbb-phc28 from openshift-operator-lifecycle-manager started at 2020-11-16 17:44:00 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.573: INFO: 	Container packageserver ready: true, restart count 0
Nov 16 21:25:38.573: INFO: console-operator-c9844474-sk7j2 from openshift-console-operator started at 2020-11-16 20:43:24 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.573: INFO: 	Container console-operator ready: true, restart count 0
Nov 16 21:25:38.573: INFO: thanos-querier-54b47b4fc4-zvlf7 from openshift-monitoring started at 2020-11-16 17:46:57 +0000 UTC (4 container statuses recorded)
Nov 16 21:25:38.573: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 21:25:38.573: INFO: 	Container oauth-proxy ready: true, restart count 0
Nov 16 21:25:38.573: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 16 21:25:38.574: INFO: 	Container thanos-querier ready: true, restart count 0
Nov 16 21:25:38.574: INFO: cluster-node-tuning-operator-5c44ccc99b-bvkzv from openshift-cluster-node-tuning-operator started at 2020-11-16 20:43:24 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.574: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Nov 16 21:25:38.574: INFO: openshift-service-catalog-controller-manager-operator-595fxwfq6 from openshift-service-catalog-controller-manager-operator started at 2020-11-16 20:43:24 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.574: INFO: 	Container operator ready: true, restart count 0
Nov 16 21:25:38.574: INFO: calico-node-tc4r7 from calico-system started at 2020-11-16 17:40:12 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.574: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 21:25:38.574: INFO: ibm-file-plugin-75bbff878-v6g4f from kube-system started at 2020-11-16 21:12:51 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.574: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Nov 16 21:25:38.574: INFO: downloads-5dbb4f4ff7-cvpn9 from openshift-console started at 2020-11-16 21:12:51 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.574: INFO: 	Container download-server ready: true, restart count 0
Nov 16 21:25:38.574: INFO: router-default-589c4b7b87-74cdn from openshift-ingress started at 2020-11-16 17:41:37 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.574: INFO: 	Container router ready: true, restart count 0
Nov 16 21:25:38.574: INFO: grafana-58c46d5bb-hs8tj from openshift-monitoring started at 2020-11-16 17:46:00 +0000 UTC (2 container statuses recorded)
Nov 16 21:25:38.574: INFO: 	Container grafana ready: true, restart count 0
Nov 16 21:25:38.575: INFO: 	Container grafana-proxy ready: true, restart count 0
Nov 16 21:25:38.575: INFO: sonobuoy-systemd-logs-daemon-set-18804f2d5c884991-rfhks from sonobuoy started at 2020-11-16 19:14:33 +0000 UTC (2 container statuses recorded)
Nov 16 21:25:38.575: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Nov 16 21:25:38.575: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 16 21:25:38.575: INFO: marketplace-operator-54847664c-zhpmb from openshift-marketplace started at 2020-11-16 20:43:24 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.575: INFO: 	Container marketplace-operator ready: true, restart count 0
Nov 16 21:25:38.575: INFO: redhat-operators-7854cbf749-dqt2k from openshift-marketplace started at 2020-11-16 21:12:51 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.575: INFO: 	Container redhat-operators ready: true, restart count 0
Nov 16 21:25:38.575: INFO: node-exporter-lqwgh from openshift-monitoring started at 2020-11-16 17:41:17 +0000 UTC (2 container statuses recorded)
Nov 16 21:25:38.575: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 21:25:38.575: INFO: 	Container node-exporter ready: true, restart count 0
Nov 16 21:25:38.576: INFO: prometheus-operator-7646fdcb7b-tw6jj from openshift-monitoring started at 2020-11-16 17:45:42 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.576: INFO: 	Container prometheus-operator ready: true, restart count 0
Nov 16 21:25:38.576: INFO: configmap-cabundle-injector-5bd6fcf58-hlwpk from openshift-service-ca started at 2020-11-16 17:41:14 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.576: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
Nov 16 21:25:38.576: INFO: ibmcloud-block-storage-plugin-79495594d5-w5x25 from kube-system started at 2020-11-16 20:43:24 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.576: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Nov 16 21:25:38.576: INFO: cluster-storage-operator-6488c9f77b-9xhqh from openshift-cluster-storage-operator started at 2020-11-16 20:43:24 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.576: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Nov 16 21:25:38.576: INFO: network-operator-74fc599569-7k9c2 from openshift-network-operator started at 2020-11-16 20:43:24 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.576: INFO: 	Container network-operator ready: true, restart count 0
Nov 16 21:25:38.576: INFO: tigera-operator-798cfbf7dd-7jpjh from tigera-operator started at 2020-11-16 20:43:24 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.576: INFO: 	Container tigera-operator ready: true, restart count 0
Nov 16 21:25:38.576: INFO: catalog-operator-86d68f4684-97mn8 from openshift-operator-lifecycle-manager started at 2020-11-16 20:43:24 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.577: INFO: 	Container catalog-operator ready: true, restart count 0
Nov 16 21:25:38.577: INFO: ibm-keepalived-watcher-x4nqc from kube-system started at 2020-11-16 17:39:19 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.577: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 21:25:38.577: INFO: multus-admission-controller-tf7n7 from openshift-multus started at 2020-11-16 17:40:30 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.577: INFO: 	Container multus-admission-controller ready: true, restart count 0
Nov 16 21:25:38.577: INFO: apiservice-cabundle-injector-75dcd4f8db-sllzv from openshift-service-ca started at 2020-11-16 17:41:13 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.577: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
Nov 16 21:25:38.577: INFO: vpn-679798bf87-s5h4z from kube-system started at 2020-11-16 17:44:16 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.577: INFO: 	Container vpn ready: true, restart count 0
Nov 16 21:25:38.577: INFO: prometheus-adapter-554fc6c4db-8f7cc from openshift-monitoring started at 2020-11-16 17:45:54 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.577: INFO: 	Container prometheus-adapter ready: true, restart count 0
Nov 16 21:25:38.577: INFO: tuned-xnc58 from openshift-cluster-node-tuning-operator started at 2020-11-16 20:43:46 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.578: INFO: 	Container tuned ready: true, restart count 0
Nov 16 21:25:38.578: INFO: certified-operators-d97ff5cf9-m7nq4 from openshift-marketplace started at 2020-11-16 20:43:49 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.578: INFO: 	Container certified-operators ready: true, restart count 0
Nov 16 21:25:38.578: INFO: ibm-master-proxy-static-10.240.167.209 from kube-system started at 2020-11-16 17:39:17 +0000 UTC (2 container statuses recorded)
Nov 16 21:25:38.578: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 21:25:38.578: INFO: 	Container pause ready: true, restart count 0
Nov 16 21:25:38.578: INFO: openshift-kube-proxy-qfwg7 from openshift-kube-proxy started at 2020-11-16 17:39:31 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.578: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 16 21:25:38.578: INFO: 
Logging pods the kubelet thinks is on node 10.240.167.254 before test
Nov 16 21:25:38.634: INFO: multus-gf29b from openshift-multus started at 2020-11-16 17:39:24 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.634: INFO: 	Container kube-multus ready: true, restart count 0
Nov 16 21:25:38.634: INFO: dns-default-rtvgx from openshift-dns started at 2020-11-16 17:42:09 +0000 UTC (2 container statuses recorded)
Nov 16 21:25:38.634: INFO: 	Container dns ready: true, restart count 0
Nov 16 21:25:38.634: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 16 21:25:38.635: INFO: tuned-9dnzp from openshift-cluster-node-tuning-operator started at 2020-11-16 20:43:46 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.635: INFO: 	Container tuned ready: true, restart count 0
Nov 16 21:25:38.635: INFO: multus-admission-controller-7s2k9 from openshift-multus started at 2020-11-16 21:13:31 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.635: INFO: 	Container multus-admission-controller ready: true, restart count 0
Nov 16 21:25:38.635: INFO: calico-typha-b5486f777-km6dh from calico-system started at 2020-11-16 17:42:09 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.635: INFO: 	Container calico-typha ready: true, restart count 0
Nov 16 21:25:38.635: INFO: ibm-master-proxy-static-10.240.167.254 from kube-system started at 2020-11-16 17:38:51 +0000 UTC (2 container statuses recorded)
Nov 16 21:25:38.635: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 21:25:38.635: INFO: 	Container pause ready: true, restart count 0
Nov 16 21:25:38.635: INFO: ibm-keepalived-watcher-gw7ng from kube-system started at 2020-11-16 17:38:54 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.635: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 21:25:38.635: INFO: node-ca-852zk from openshift-image-registry started at 2020-11-16 17:41:42 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.635: INFO: 	Container node-ca ready: true, restart count 0
Nov 16 21:25:38.635: INFO: node-exporter-4kz5x from openshift-monitoring started at 2020-11-16 17:41:17 +0000 UTC (2 container statuses recorded)
Nov 16 21:25:38.636: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 16 21:25:38.636: INFO: 	Container node-exporter ready: true, restart count 0
Nov 16 21:25:38.636: INFO: ibmcloud-block-storage-driver-qdh9z from kube-system started at 2020-11-16 17:39:02 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.636: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 16 21:25:38.636: INFO: calico-node-s6qzg from calico-system started at 2020-11-16 17:40:12 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.636: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 21:25:38.636: INFO: sonobuoy-systemd-logs-daemon-set-18804f2d5c884991-ffhv2 from sonobuoy started at 2020-11-16 19:14:33 +0000 UTC (2 container statuses recorded)
Nov 16 21:25:38.636: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Nov 16 21:25:38.636: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 16 21:25:38.636: INFO: openshift-kube-proxy-smf7r from openshift-kube-proxy started at 2020-11-16 17:39:31 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.636: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 16 21:25:38.636: INFO: sonobuoy from sonobuoy started at 2020-11-16 19:14:22 +0000 UTC (1 container statuses recorded)
Nov 16 21:25:38.636: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 16 21:25:38.636: INFO: sonobuoy-e2e-job-b6bf7e56608b4535 from sonobuoy started at 2020-11-16 19:14:33 +0000 UTC (2 container statuses recorded)
Nov 16 21:25:38.637: INFO: 	Container e2e ready: true, restart count 0
Nov 16 21:25:38.637: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-3b61f2ad-8234-439c-8f86-f1c4c38c311c 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-3b61f2ad-8234-439c-8f86-f1c4c38c311c off the node 10.240.167.254
STEP: verifying the node doesn't have the label kubernetes.io/e2e-3b61f2ad-8234-439c-8f86-f1c4c38c311c
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:30:42.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6471" for this suite.
Nov 16 21:30:52.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:30:54.786: INFO: namespace sched-pred-6471 deletion completed in 11.930712043s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:316.545 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:30:54.786: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name secret-emptykey-test-5e41e338-b2d9-4332-9a98-d64701a08fe5
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:30:54.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4050" for this suite.
Nov 16 21:31:02.923: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:31:04.828: INFO: namespace secrets-4050 deletion completed in 9.931759479s

• [SLOW TEST:10.042 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:31:04.829: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Nov 16 21:31:04.959: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dd7f19da-ae54-473c-8658-59a4195fe9bb" in namespace "downward-api-7686" to be "success or failure"
Nov 16 21:31:04.966: INFO: Pod "downwardapi-volume-dd7f19da-ae54-473c-8658-59a4195fe9bb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.286496ms
Nov 16 21:31:06.975: INFO: Pod "downwardapi-volume-dd7f19da-ae54-473c-8658-59a4195fe9bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015691126s
STEP: Saw pod success
Nov 16 21:31:06.975: INFO: Pod "downwardapi-volume-dd7f19da-ae54-473c-8658-59a4195fe9bb" satisfied condition "success or failure"
Nov 16 21:31:06.981: INFO: Trying to get logs from node 10.240.167.254 pod downwardapi-volume-dd7f19da-ae54-473c-8658-59a4195fe9bb container client-container: <nil>
STEP: delete the pod
Nov 16 21:31:07.042: INFO: Waiting for pod downwardapi-volume-dd7f19da-ae54-473c-8658-59a4195fe9bb to disappear
Nov 16 21:31:07.048: INFO: Pod downwardapi-volume-dd7f19da-ae54-473c-8658-59a4195fe9bb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:31:07.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7686" for this suite.
Nov 16 21:31:15.082: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:31:16.986: INFO: namespace downward-api-7686 deletion completed in 9.929984525s

• [SLOW TEST:12.158 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:31:16.987: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-7443
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 16 21:31:17.083: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Nov 16 21:31:37.334: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.158.244 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7443 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 16 21:31:37.334: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
Nov 16 21:31:38.501: INFO: Found all expected endpoints: [netserver-0]
Nov 16 21:31:38.511: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.194.223 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7443 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 16 21:31:38.511: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
Nov 16 21:31:39.675: INFO: Found all expected endpoints: [netserver-1]
Nov 16 21:31:39.682: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.216.199 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7443 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 16 21:31:39.682: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
Nov 16 21:31:40.846: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:31:40.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7443" for this suite.
Nov 16 21:31:48.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:31:50.787: INFO: namespace pod-network-test-7443 deletion completed in 9.932387937s

• [SLOW TEST:33.800 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:31:50.792: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:31:54.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4908" for this suite.
Nov 16 21:32:02.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:32:04.877: INFO: namespace kubelet-test-4908 deletion completed in 9.93170058s

• [SLOW TEST:14.085 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:32:04.878: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 21:32:04.986: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Nov 16 21:32:10.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 --namespace=crd-publish-openapi-9571 create -f -'
Nov 16 21:32:11.500: INFO: stderr: ""
Nov 16 21:32:11.500: INFO: stdout: "e2e-test-crd-publish-openapi-1606-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Nov 16 21:32:11.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 --namespace=crd-publish-openapi-9571 delete e2e-test-crd-publish-openapi-1606-crds test-cr'
Nov 16 21:32:11.742: INFO: stderr: ""
Nov 16 21:32:11.742: INFO: stdout: "e2e-test-crd-publish-openapi-1606-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Nov 16 21:32:11.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 --namespace=crd-publish-openapi-9571 apply -f -'
Nov 16 21:32:12.039: INFO: stderr: ""
Nov 16 21:32:12.040: INFO: stdout: "e2e-test-crd-publish-openapi-1606-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Nov 16 21:32:12.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 --namespace=crd-publish-openapi-9571 delete e2e-test-crd-publish-openapi-1606-crds test-cr'
Nov 16 21:32:12.164: INFO: stderr: ""
Nov 16 21:32:12.164: INFO: stdout: "e2e-test-crd-publish-openapi-1606-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Nov 16 21:32:12.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 explain e2e-test-crd-publish-openapi-1606-crds'
Nov 16 21:32:12.690: INFO: stderr: ""
Nov 16 21:32:12.690: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1606-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:32:18.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9571" for this suite.
Nov 16 21:32:26.541: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:32:28.445: INFO: namespace crd-publish-openapi-9571 deletion completed in 9.931448092s

• [SLOW TEST:23.567 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:32:28.447: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Nov 16 21:32:28.578: INFO: Waiting up to 5m0s for pod "pod-1109137a-d700-4ce7-8d70-ea312e2fa7a0" in namespace "emptydir-7887" to be "success or failure"
Nov 16 21:32:28.585: INFO: Pod "pod-1109137a-d700-4ce7-8d70-ea312e2fa7a0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.863143ms
Nov 16 21:32:30.594: INFO: Pod "pod-1109137a-d700-4ce7-8d70-ea312e2fa7a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016137331s
STEP: Saw pod success
Nov 16 21:32:30.594: INFO: Pod "pod-1109137a-d700-4ce7-8d70-ea312e2fa7a0" satisfied condition "success or failure"
Nov 16 21:32:30.602: INFO: Trying to get logs from node 10.240.167.254 pod pod-1109137a-d700-4ce7-8d70-ea312e2fa7a0 container test-container: <nil>
STEP: delete the pod
Nov 16 21:32:30.647: INFO: Waiting for pod pod-1109137a-d700-4ce7-8d70-ea312e2fa7a0 to disappear
Nov 16 21:32:30.653: INFO: Pod pod-1109137a-d700-4ce7-8d70-ea312e2fa7a0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:32:30.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7887" for this suite.
Nov 16 21:32:38.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:32:40.604: INFO: namespace emptydir-7887 deletion completed in 9.939132309s

• [SLOW TEST:12.158 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:32:40.605: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 21:32:41.594: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 16 21:32:43.623: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741159161, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741159161, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741159161, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741159161, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 21:32:46.657: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:32:46.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-982" for this suite.
Nov 16 21:32:54.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:32:56.801: INFO: namespace webhook-982 deletion completed in 9.938766899s
STEP: Destroying namespace "webhook-982-markers" for this suite.
Nov 16 21:33:04.830: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:33:06.750: INFO: namespace webhook-982-markers deletion completed in 9.94959044s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:26.183 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:33:06.789: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:33:17.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8274" for this suite.
Nov 16 21:33:26.034: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:33:27.939: INFO: namespace resourcequota-8274 deletion completed in 9.931308902s

• [SLOW TEST:21.151 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:33:27.941: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Nov 16 21:33:28.068: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Nov 16 21:33:35.186: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:33:35.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6329" for this suite.
Nov 16 21:33:43.231: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:33:45.138: INFO: namespace pods-6329 deletion completed in 9.934672909s

• [SLOW TEST:17.198 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:33:45.140: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 21:33:45.259: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-f0fc6994-7a52-4ab2-9946-493126b172fa
STEP: Creating configMap with name cm-test-opt-upd-8c2e352a-6b07-4f60-9456-5b56297c9324
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-f0fc6994-7a52-4ab2-9946-493126b172fa
STEP: Updating configmap cm-test-opt-upd-8c2e352a-6b07-4f60-9456-5b56297c9324
STEP: Creating configMap with name cm-test-opt-create-71e659e6-9e6e-45ae-a81b-6f40d22c72b0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:35:16.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3529" for this suite.
Nov 16 21:35:46.403: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:35:48.306: INFO: namespace projected-3529 deletion completed in 31.930363307s

• [SLOW TEST:123.166 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:35:48.306: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Nov 16 21:35:48.475: INFO: Create a RollingUpdate DaemonSet
Nov 16 21:35:48.493: INFO: Check that daemon pods launch on every node of the cluster
Nov 16 21:35:48.508: INFO: Number of nodes with available pods: 0
Nov 16 21:35:48.509: INFO: Node 10.240.167.206 is running more than one daemon pod
Nov 16 21:35:49.527: INFO: Number of nodes with available pods: 0
Nov 16 21:35:49.527: INFO: Node 10.240.167.206 is running more than one daemon pod
Nov 16 21:35:50.527: INFO: Number of nodes with available pods: 3
Nov 16 21:35:50.527: INFO: Number of running nodes: 3, number of available pods: 3
Nov 16 21:35:50.527: INFO: Update the DaemonSet to trigger a rollout
Nov 16 21:35:50.610: INFO: Updating DaemonSet daemon-set
Nov 16 21:35:57.644: INFO: Roll back the DaemonSet before rollout is complete
Nov 16 21:35:57.675: INFO: Updating DaemonSet daemon-set
Nov 16 21:35:57.675: INFO: Make sure DaemonSet rollback is complete
Nov 16 21:35:57.683: INFO: Wrong image for pod: daemon-set-7jpgg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Nov 16 21:35:57.683: INFO: Pod daemon-set-7jpgg is not available
Nov 16 21:35:58.701: INFO: Wrong image for pod: daemon-set-7jpgg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Nov 16 21:35:58.701: INFO: Pod daemon-set-7jpgg is not available
Nov 16 21:35:59.701: INFO: Pod daemon-set-pw48j is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5237, will wait for the garbage collector to delete the pods
Nov 16 21:35:59.812: INFO: Deleting DaemonSet.extensions daemon-set took: 23.167459ms
Nov 16 21:36:00.313: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.284938ms
Nov 16 21:36:08.920: INFO: Number of nodes with available pods: 0
Nov 16 21:36:08.921: INFO: Number of running nodes: 0, number of available pods: 0
Nov 16 21:36:08.930: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5237/daemonsets","resourceVersion":"104438"},"items":null}

Nov 16 21:36:08.938: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5237/pods","resourceVersion":"104438"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:36:08.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5237" for this suite.
Nov 16 21:36:17.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:36:18.930: INFO: namespace daemonsets-5237 deletion completed in 9.953409912s

• [SLOW TEST:30.624 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:36:18.930: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Nov 16 21:36:25.170: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 16 21:36:25.177: INFO: Pod pod-with-poststart-http-hook still exists
Nov 16 21:36:27.178: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 16 21:36:27.186: INFO: Pod pod-with-poststart-http-hook still exists
Nov 16 21:36:29.178: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 16 21:36:29.197: INFO: Pod pod-with-poststart-http-hook still exists
Nov 16 21:36:31.178: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 16 21:36:31.187: INFO: Pod pod-with-poststart-http-hook still exists
Nov 16 21:36:33.178: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 16 21:36:33.186: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:36:33.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-661" for this suite.
Nov 16 21:37:03.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:37:05.144: INFO: namespace container-lifecycle-hook-661 deletion completed in 31.946511208s

• [SLOW TEST:46.214 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:37:05.145: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Nov 16 21:37:07.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-420283327 exec pod-sharedvolume-560437f2-78c3-4da3-8470-eabdb316c14c -c busybox-main-container --namespace=emptydir-4314 -- cat /usr/share/volumeshare/shareddata.txt'
Nov 16 21:37:07.595: INFO: stderr: ""
Nov 16 21:37:07.595: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:37:07.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4314" for this suite.
Nov 16 21:37:15.631: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:37:17.550: INFO: namespace emptydir-4314 deletion completed in 9.945737757s

• [SLOW TEST:12.406 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:37:17.552: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:37:18.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6411" for this suite.
Nov 16 21:37:48.752: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:37:50.656: INFO: namespace pods-6411 deletion completed in 31.929516081s

• [SLOW TEST:33.105 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:37:50.656: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-4852, will wait for the garbage collector to delete the pods
Nov 16 21:37:52.865: INFO: Deleting Job.batch foo took: 28.934094ms
Nov 16 21:37:53.366: INFO: Terminating Job.batch foo pods took: 500.241403ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:38:31.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4852" for this suite.
Nov 16 21:38:39.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:38:41.720: INFO: namespace job-4852 deletion completed in 9.931795196s

• [SLOW TEST:51.064 seconds]
[sig-apps] Job
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:38:41.721: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-5847/configmap-test-0e3a5272-7a0e-4048-a6a0-1597912f83f7
STEP: Creating a pod to test consume configMaps
Nov 16 21:38:41.883: INFO: Waiting up to 5m0s for pod "pod-configmaps-a5589131-0c71-43bd-a258-5d3fba33296d" in namespace "configmap-5847" to be "success or failure"
Nov 16 21:38:41.891: INFO: Pod "pod-configmaps-a5589131-0c71-43bd-a258-5d3fba33296d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.431308ms
Nov 16 21:38:43.908: INFO: Pod "pod-configmaps-a5589131-0c71-43bd-a258-5d3fba33296d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024983716s
STEP: Saw pod success
Nov 16 21:38:43.909: INFO: Pod "pod-configmaps-a5589131-0c71-43bd-a258-5d3fba33296d" satisfied condition "success or failure"
Nov 16 21:38:43.916: INFO: Trying to get logs from node 10.240.167.254 pod pod-configmaps-a5589131-0c71-43bd-a258-5d3fba33296d container env-test: <nil>
STEP: delete the pod
Nov 16 21:38:43.978: INFO: Waiting for pod pod-configmaps-a5589131-0c71-43bd-a258-5d3fba33296d to disappear
Nov 16 21:38:43.985: INFO: Pod pod-configmaps-a5589131-0c71-43bd-a258-5d3fba33296d no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:38:43.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5847" for this suite.
Nov 16 21:38:52.021: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:38:53.944: INFO: namespace configmap-5847 deletion completed in 9.950545675s

• [SLOW TEST:12.223 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:38:53.947: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test env composition
Nov 16 21:38:54.089: INFO: Waiting up to 5m0s for pod "var-expansion-6ec82e11-fce9-40ac-ad95-f9b5eb66e730" in namespace "var-expansion-8473" to be "success or failure"
Nov 16 21:38:54.095: INFO: Pod "var-expansion-6ec82e11-fce9-40ac-ad95-f9b5eb66e730": Phase="Pending", Reason="", readiness=false. Elapsed: 5.992196ms
Nov 16 21:38:56.103: INFO: Pod "var-expansion-6ec82e11-fce9-40ac-ad95-f9b5eb66e730": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013782346s
STEP: Saw pod success
Nov 16 21:38:56.103: INFO: Pod "var-expansion-6ec82e11-fce9-40ac-ad95-f9b5eb66e730" satisfied condition "success or failure"
Nov 16 21:38:56.110: INFO: Trying to get logs from node 10.240.167.254 pod var-expansion-6ec82e11-fce9-40ac-ad95-f9b5eb66e730 container dapi-container: <nil>
STEP: delete the pod
Nov 16 21:38:56.149: INFO: Waiting for pod var-expansion-6ec82e11-fce9-40ac-ad95-f9b5eb66e730 to disappear
Nov 16 21:38:56.155: INFO: Pod var-expansion-6ec82e11-fce9-40ac-ad95-f9b5eb66e730 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:38:56.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8473" for this suite.
Nov 16 21:39:04.189: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:39:06.097: INFO: namespace var-expansion-8473 deletion completed in 9.933243484s

• [SLOW TEST:12.150 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:39:06.098: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Nov 16 21:39:06.261: INFO: Waiting up to 5m0s for pod "pod-2d603beb-ffa3-45f1-ad08-79cb33a82ba2" in namespace "emptydir-2113" to be "success or failure"
Nov 16 21:39:06.272: INFO: Pod "pod-2d603beb-ffa3-45f1-ad08-79cb33a82ba2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.132044ms
Nov 16 21:39:08.279: INFO: Pod "pod-2d603beb-ffa3-45f1-ad08-79cb33a82ba2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018919873s
STEP: Saw pod success
Nov 16 21:39:08.280: INFO: Pod "pod-2d603beb-ffa3-45f1-ad08-79cb33a82ba2" satisfied condition "success or failure"
Nov 16 21:39:08.286: INFO: Trying to get logs from node 10.240.167.254 pod pod-2d603beb-ffa3-45f1-ad08-79cb33a82ba2 container test-container: <nil>
STEP: delete the pod
Nov 16 21:39:08.324: INFO: Waiting for pod pod-2d603beb-ffa3-45f1-ad08-79cb33a82ba2 to disappear
Nov 16 21:39:08.332: INFO: Pod pod-2d603beb-ffa3-45f1-ad08-79cb33a82ba2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:39:08.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2113" for this suite.
Nov 16 21:39:16.371: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:39:18.287: INFO: namespace emptydir-2113 deletion completed in 9.945297933s

• [SLOW TEST:12.189 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:39:18.291: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's args
Nov 16 21:39:18.423: INFO: Waiting up to 5m0s for pod "var-expansion-a71f32b1-6c8f-431f-a00b-dbb90e9f03bc" in namespace "var-expansion-4409" to be "success or failure"
Nov 16 21:39:18.430: INFO: Pod "var-expansion-a71f32b1-6c8f-431f-a00b-dbb90e9f03bc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.157129ms
Nov 16 21:39:20.437: INFO: Pod "var-expansion-a71f32b1-6c8f-431f-a00b-dbb90e9f03bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014165298s
STEP: Saw pod success
Nov 16 21:39:20.437: INFO: Pod "var-expansion-a71f32b1-6c8f-431f-a00b-dbb90e9f03bc" satisfied condition "success or failure"
Nov 16 21:39:20.444: INFO: Trying to get logs from node 10.240.167.254 pod var-expansion-a71f32b1-6c8f-431f-a00b-dbb90e9f03bc container dapi-container: <nil>
STEP: delete the pod
Nov 16 21:39:20.486: INFO: Waiting for pod var-expansion-a71f32b1-6c8f-431f-a00b-dbb90e9f03bc to disappear
Nov 16 21:39:20.493: INFO: Pod var-expansion-a71f32b1-6c8f-431f-a00b-dbb90e9f03bc no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:39:20.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4409" for this suite.
Nov 16 21:39:28.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:39:30.451: INFO: namespace var-expansion-4409 deletion completed in 9.94973627s

• [SLOW TEST:12.160 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:39:30.452: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on tmpfs
Nov 16 21:39:30.581: INFO: Waiting up to 5m0s for pod "pod-b4797e7e-383c-4c8c-8d7d-36e22153ce6b" in namespace "emptydir-8621" to be "success or failure"
Nov 16 21:39:30.588: INFO: Pod "pod-b4797e7e-383c-4c8c-8d7d-36e22153ce6b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.834739ms
Nov 16 21:39:32.596: INFO: Pod "pod-b4797e7e-383c-4c8c-8d7d-36e22153ce6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015167043s
STEP: Saw pod success
Nov 16 21:39:32.596: INFO: Pod "pod-b4797e7e-383c-4c8c-8d7d-36e22153ce6b" satisfied condition "success or failure"
Nov 16 21:39:32.604: INFO: Trying to get logs from node 10.240.167.254 pod pod-b4797e7e-383c-4c8c-8d7d-36e22153ce6b container test-container: <nil>
STEP: delete the pod
Nov 16 21:39:32.643: INFO: Waiting for pod pod-b4797e7e-383c-4c8c-8d7d-36e22153ce6b to disappear
Nov 16 21:39:32.649: INFO: Pod pod-b4797e7e-383c-4c8c-8d7d-36e22153ce6b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:39:32.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8621" for this suite.
Nov 16 21:39:40.686: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:39:42.590: INFO: namespace emptydir-8621 deletion completed in 9.932009533s

• [SLOW TEST:12.139 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Nov 16 21:39:42.591: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-1901
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 16 21:39:42.699: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Nov 16 21:40:06.948: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.216.255:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1901 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 16 21:40:06.949: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
Nov 16 21:40:07.114: INFO: Found all expected endpoints: [netserver-0]
Nov 16 21:40:07.134: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.158.248:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1901 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 16 21:40:07.134: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
Nov 16 21:40:07.315: INFO: Found all expected endpoints: [netserver-1]
Nov 16 21:40:07.323: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.194.227:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1901 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 16 21:40:07.323: INFO: >>> kubeConfig: /tmp/kubeconfig-420283327
Nov 16 21:40:07.483: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Nov 16 21:40:07.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1901" for this suite.
Nov 16 21:40:15.521: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 16 21:40:17.428: INFO: namespace pod-network-test-1901 deletion completed in 9.935802029s

• [SLOW TEST:34.838 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSNov 16 21:40:17.429: INFO: Running AfterSuite actions on all nodes
Nov 16 21:40:17.429: INFO: Running AfterSuite actions on node 1
Nov 16 21:40:17.429: INFO: Skipping dumping logs from cluster

Ran 276 of 4897 Specs in 8708.131 seconds
SUCCESS! -- 276 Passed | 0 Failed | 0 Pending | 4621 Skipped
PASS

Ginkgo ran 1 suite in 2h25m9.737541685s
Test Suite Passed
