I1129 01:15:36.545883      22 test_context.go:410] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-195255472
I1129 01:15:36.546046      22 test_context.go:423] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I1129 01:15:36.546208      22 e2e.go:124] Starting e2e run "182ce59e-d243-406e-8b8f-ff5a8834b4a3" on Ginkgo node 1
{"msg":"Test Suite starting","total":277,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1606612534 - Will randomize all specs
Will run 277 of 4993 specs

Nov 29 01:15:36.575: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
Nov 29 01:15:36.584: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Nov 29 01:15:36.609: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Nov 29 01:15:36.661: INFO: 18 / 18 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Nov 29 01:15:36.661: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Nov 29 01:15:36.661: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Nov 29 01:15:36.686: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Nov 29 01:15:36.686: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Nov 29 01:15:36.686: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'nvidia-device-plugin-daemonset' (0 seconds elapsed)
Nov 29 01:15:36.686: INFO: e2e test version: v1.18.12
Nov 29 01:15:36.688: INFO: kube-apiserver version: v1.18.12
Nov 29 01:15:36.688: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
Nov 29 01:15:36.694: INFO: Cluster IP family: ipv4
S
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:15:36.696: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename kubectl
Nov 29 01:15:36.732: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 01:15:36.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 version'
Nov 29 01:15:36.852: INFO: stderr: ""
Nov 29 01:15:36.852: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"18\", GitVersion:\"v1.18.12\", GitCommit:\"7cd5e9086de8ae25d6a1514d0c87bac67ca4a481\", GitTreeState:\"clean\", BuildDate:\"2020-11-12T09:18:55Z\", GoVersion:\"go1.13.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"18\", GitVersion:\"v1.18.12\", GitCommit:\"7cd5e9086de8ae25d6a1514d0c87bac67ca4a481\", GitTreeState:\"clean\", BuildDate:\"2020-11-12T09:11:15Z\", GoVersion:\"go1.13.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:15:36.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6701" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":277,"completed":1,"skipped":1,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:15:36.863: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-map-f968a5b4-e5a1-45d3-abc7-feed76059c60
STEP: Creating a pod to test consume secrets
Nov 29 01:15:36.909: INFO: Waiting up to 5m0s for pod "pod-secrets-cff99c41-4e57-410a-9b36-dc6bb36d4e8b" in namespace "secrets-2832" to be "Succeeded or Failed"
Nov 29 01:15:36.912: INFO: Pod "pod-secrets-cff99c41-4e57-410a-9b36-dc6bb36d4e8b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.387705ms
Nov 29 01:15:38.915: INFO: Pod "pod-secrets-cff99c41-4e57-410a-9b36-dc6bb36d4e8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006556707s
Nov 29 01:15:40.920: INFO: Pod "pod-secrets-cff99c41-4e57-410a-9b36-dc6bb36d4e8b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011214671s
Nov 29 01:15:42.924: INFO: Pod "pod-secrets-cff99c41-4e57-410a-9b36-dc6bb36d4e8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01525529s
STEP: Saw pod success
Nov 29 01:15:42.924: INFO: Pod "pod-secrets-cff99c41-4e57-410a-9b36-dc6bb36d4e8b" satisfied condition "Succeeded or Failed"
Nov 29 01:15:42.927: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod pod-secrets-cff99c41-4e57-410a-9b36-dc6bb36d4e8b container secret-volume-test: <nil>
STEP: delete the pod
Nov 29 01:15:42.958: INFO: Waiting for pod pod-secrets-cff99c41-4e57-410a-9b36-dc6bb36d4e8b to disappear
Nov 29 01:15:42.965: INFO: Pod pod-secrets-cff99c41-4e57-410a-9b36-dc6bb36d4e8b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:15:42.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2832" for this suite.

• [SLOW TEST:6.112 seconds]
[sig-storage] Secrets
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":2,"skipped":28,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:15:42.978: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating service multi-endpoint-test in namespace services-1888
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1888 to expose endpoints map[]
Nov 29 01:15:43.061: INFO: successfully validated that service multi-endpoint-test in namespace services-1888 exposes endpoints map[] (24.408654ms elapsed)
STEP: Creating pod pod1 in namespace services-1888
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1888 to expose endpoints map[pod1:[100]]
Nov 29 01:15:47.121: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]] (4.045411911s elapsed, will retry)
Nov 29 01:15:52.177: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]] (9.102193661s elapsed, will retry)
Nov 29 01:15:57.207: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]] (14.13167484s elapsed, will retry)
Nov 29 01:16:00.225: INFO: successfully validated that service multi-endpoint-test in namespace services-1888 exposes endpoints map[pod1:[100]] (17.149754224s elapsed)
STEP: Creating pod pod2 in namespace services-1888
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1888 to expose endpoints map[pod1:[100] pod2:[101]]
Nov 29 01:16:04.292: INFO: Unexpected endpoints: found map[b13eacfa-3671-4f06-b9cd-1974d3c50d8e:[100]], expected map[pod1:[100] pod2:[101]] (4.06308975s elapsed, will retry)
Nov 29 01:16:08.329: INFO: successfully validated that service multi-endpoint-test in namespace services-1888 exposes endpoints map[pod1:[100] pod2:[101]] (8.100033097s elapsed)
STEP: Deleting pod pod1 in namespace services-1888
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1888 to expose endpoints map[pod2:[101]]
Nov 29 01:16:08.364: INFO: successfully validated that service multi-endpoint-test in namespace services-1888 exposes endpoints map[pod2:[101]] (28.418007ms elapsed)
STEP: Deleting pod pod2 in namespace services-1888
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1888 to expose endpoints map[]
Nov 29 01:16:09.377: INFO: successfully validated that service multi-endpoint-test in namespace services-1888 exposes endpoints map[] (1.008188545s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:16:09.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1888" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:26.447 seconds]
[sig-network] Services
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":277,"completed":3,"skipped":41,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:16:09.425: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test substitution in container's command
Nov 29 01:16:09.501: INFO: Waiting up to 5m0s for pod "var-expansion-76bbe8cf-a574-43eb-b27c-123684bcbf8e" in namespace "var-expansion-8234" to be "Succeeded or Failed"
Nov 29 01:16:09.507: INFO: Pod "var-expansion-76bbe8cf-a574-43eb-b27c-123684bcbf8e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.155444ms
Nov 29 01:16:11.511: INFO: Pod "var-expansion-76bbe8cf-a574-43eb-b27c-123684bcbf8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00970734s
Nov 29 01:16:13.514: INFO: Pod "var-expansion-76bbe8cf-a574-43eb-b27c-123684bcbf8e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013408892s
Nov 29 01:16:15.518: INFO: Pod "var-expansion-76bbe8cf-a574-43eb-b27c-123684bcbf8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01688976s
STEP: Saw pod success
Nov 29 01:16:15.518: INFO: Pod "var-expansion-76bbe8cf-a574-43eb-b27c-123684bcbf8e" satisfied condition "Succeeded or Failed"
Nov 29 01:16:15.520: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod var-expansion-76bbe8cf-a574-43eb-b27c-123684bcbf8e container dapi-container: <nil>
STEP: delete the pod
Nov 29 01:16:15.540: INFO: Waiting for pod var-expansion-76bbe8cf-a574-43eb-b27c-123684bcbf8e to disappear
Nov 29 01:16:15.547: INFO: Pod var-expansion-76bbe8cf-a574-43eb-b27c-123684bcbf8e no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:16:15.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8234" for this suite.

• [SLOW TEST:6.133 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":277,"completed":4,"skipped":50,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:16:15.560: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Nov 29 01:16:55.633: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1129 01:16:55.633526      22 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:16:55.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2507" for this suite.

• [SLOW TEST:40.088 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":277,"completed":5,"skipped":58,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:16:55.649: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-b717358e-3e35-41f0-b5cc-97fc2bce37ce
STEP: Creating a pod to test consume secrets
Nov 29 01:16:55.708: INFO: Waiting up to 5m0s for pod "pod-secrets-66baec38-b413-4fce-b2d7-698d6a8ebcc4" in namespace "secrets-9917" to be "Succeeded or Failed"
Nov 29 01:16:55.721: INFO: Pod "pod-secrets-66baec38-b413-4fce-b2d7-698d6a8ebcc4": Phase="Pending", Reason="", readiness=false. Elapsed: 13.460083ms
Nov 29 01:16:57.725: INFO: Pod "pod-secrets-66baec38-b413-4fce-b2d7-698d6a8ebcc4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017233341s
Nov 29 01:16:59.728: INFO: Pod "pod-secrets-66baec38-b413-4fce-b2d7-698d6a8ebcc4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020469757s
STEP: Saw pod success
Nov 29 01:16:59.728: INFO: Pod "pod-secrets-66baec38-b413-4fce-b2d7-698d6a8ebcc4" satisfied condition "Succeeded or Failed"
Nov 29 01:16:59.730: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod pod-secrets-66baec38-b413-4fce-b2d7-698d6a8ebcc4 container secret-volume-test: <nil>
STEP: delete the pod
Nov 29 01:16:59.766: INFO: Waiting for pod pod-secrets-66baec38-b413-4fce-b2d7-698d6a8ebcc4 to disappear
Nov 29 01:16:59.775: INFO: Pod pod-secrets-66baec38-b413-4fce-b2d7-698d6a8ebcc4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:16:59.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9917" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":277,"completed":6,"skipped":64,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:16:59.791: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 01:16:59.856: INFO: Pod name rollover-pod: Found 0 pods out of 1
Nov 29 01:17:04.863: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 29 01:17:08.909: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Nov 29 01:17:10.913: INFO: Creating deployment "test-rollover-deployment"
Nov 29 01:17:10.924: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Nov 29 01:17:12.937: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Nov 29 01:17:12.943: INFO: Ensure that both replica sets have 1 created replica
Nov 29 01:17:12.947: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Nov 29 01:17:12.956: INFO: Updating deployment test-rollover-deployment
Nov 29 01:17:12.957: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Nov 29 01:17:14.966: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Nov 29 01:17:14.971: INFO: Make sure deployment "test-rollover-deployment" is complete
Nov 29 01:17:14.975: INFO: all replica sets need to contain the pod-template-hash label
Nov 29 01:17:14.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209430, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209430, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209432, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209430, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 29 01:17:16.981: INFO: all replica sets need to contain the pod-template-hash label
Nov 29 01:17:16.981: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209430, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209430, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209434, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209430, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 29 01:17:18.982: INFO: all replica sets need to contain the pod-template-hash label
Nov 29 01:17:18.982: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209430, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209430, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209434, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209430, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 29 01:17:20.983: INFO: all replica sets need to contain the pod-template-hash label
Nov 29 01:17:20.983: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209430, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209430, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209434, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209430, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 29 01:17:22.983: INFO: all replica sets need to contain the pod-template-hash label
Nov 29 01:17:22.983: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209430, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209430, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209434, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209430, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 29 01:17:24.982: INFO: all replica sets need to contain the pod-template-hash label
Nov 29 01:17:24.982: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209430, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209430, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209434, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209430, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 29 01:17:26.982: INFO: all replica sets need to contain the pod-template-hash label
Nov 29 01:17:26.982: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209430, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209430, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209434, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209430, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 29 01:17:28.996: INFO: all replica sets need to contain the pod-template-hash label
Nov 29 01:17:28.996: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209430, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209430, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209434, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209430, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 29 01:17:30.981: INFO: all replica sets need to contain the pod-template-hash label
Nov 29 01:17:30.981: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209430, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209430, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209434, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209430, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 29 01:17:32.981: INFO: all replica sets need to contain the pod-template-hash label
Nov 29 01:17:32.981: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209430, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209430, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209434, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209430, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 29 01:17:34.981: INFO: all replica sets need to contain the pod-template-hash label
Nov 29 01:17:34.982: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209430, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209430, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209434, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742209430, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 29 01:17:36.982: INFO: 
Nov 29 01:17:36.982: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Nov 29 01:17:36.989: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-4766 /apis/apps/v1/namespaces/deployment-4766/deployments/test-rollover-deployment 03cc32e6-42e4-44a3-9547-0f30daab4e90 4613 2 2020-11-29 01:17:10 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-11-29 01:17:12 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 109 105 110 82 101 97 100 121 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-11-29 01:17:34 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003512388 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-11-29 01:17:10 +0000 UTC,LastTransitionTime:2020-11-29 01:17:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-84f7f6f64b" has successfully progressed.,LastUpdateTime:2020-11-29 01:17:34 +0000 UTC,LastTransitionTime:2020-11-29 01:17:10 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 29 01:17:36.993: INFO: New ReplicaSet "test-rollover-deployment-84f7f6f64b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-84f7f6f64b  deployment-4766 /apis/apps/v1/namespaces/deployment-4766/replicasets/test-rollover-deployment-84f7f6f64b 6b858f2c-a146-4e70-b524-6c73fca44195 4602 2 2020-11-29 01:17:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:84f7f6f64b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 03cc32e6-42e4-44a3-9547-0f30daab4e90 0xc00301ec07 0xc00301ec08}] []  [{kube-controller-manager Update apps/v1 2020-11-29 01:17:34 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 48 51 99 99 51 50 101 54 45 52 50 101 52 45 52 52 97 51 45 57 53 52 55 45 48 102 51 48 100 97 97 98 52 101 57 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 109 105 110 82 101 97 100 121 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 84f7f6f64b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:84f7f6f64b] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00301ec98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 29 01:17:36.993: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Nov 29 01:17:36.994: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4766 /apis/apps/v1/namespaces/deployment-4766/replicasets/test-rollover-controller eb0565df-1415-46e7-bdbe-60b7d2b4ef7b 4611 2 2020-11-29 01:16:59 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 03cc32e6-42e4-44a3-9547-0f30daab4e90 0xc00301ea0f 0xc00301ea20}] []  [{e2e.test Update apps/v1 2020-11-29 01:16:59 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-11-29 01:17:34 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 48 51 99 99 51 50 101 54 45 52 50 101 52 45 52 52 97 51 45 57 53 52 55 45 48 102 51 48 100 97 97 98 52 101 57 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00301eab8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 29 01:17:36.994: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-5686c4cfd5  deployment-4766 /apis/apps/v1/namespaces/deployment-4766/replicasets/test-rollover-deployment-5686c4cfd5 8f2f0a5d-739e-4940-a9bb-ba40d0136dde 4466 2 2020-11-29 01:17:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5686c4cfd5] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 03cc32e6-42e4-44a3-9547-0f30daab4e90 0xc00301eb17 0xc00301eb18}] []  [{kube-controller-manager Update apps/v1 2020-11-29 01:17:12 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 48 51 99 99 51 50 101 54 45 52 50 101 52 45 52 52 97 51 45 57 53 52 55 45 48 102 51 48 100 97 97 98 52 101 57 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 109 105 110 82 101 97 100 121 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 114 101 100 105 115 45 115 108 97 118 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5686c4cfd5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5686c4cfd5] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00301eba8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 29 01:17:36.997: INFO: Pod "test-rollover-deployment-84f7f6f64b-j2spd" is available:
&Pod{ObjectMeta:{test-rollover-deployment-84f7f6f64b-j2spd test-rollover-deployment-84f7f6f64b- deployment-4766 /api/v1/namespaces/deployment-4766/pods/test-rollover-deployment-84f7f6f64b-j2spd ec710d2a-18ad-45c2-a43d-75e22e90e331 4494 0 2020-11-29 01:17:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:84f7f6f64b] map[cni.projectcalico.org/podIP:192.168.36.204/32 cni.projectcalico.org/podIPs:192.168.36.204/32] [{apps/v1 ReplicaSet test-rollover-deployment-84f7f6f64b 6b858f2c-a146-4e70-b524-6c73fca44195 0xc00301f247 0xc00301f248}] []  [{kube-controller-manager Update v1 2020-11-29 01:17:12 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 54 98 56 53 56 102 50 99 45 97 49 52 54 45 52 101 55 48 45 98 53 50 52 45 54 99 55 51 102 99 97 52 52 49 57 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-11-29 01:17:13 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-11-29 01:17:14 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 57 50 46 49 54 56 46 51 54 46 50 48 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hjt99,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hjt99,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hjt99,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-ea4104c57e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 01:17:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 01:17:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 01:17:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 01:17:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.164,PodIP:192.168.36.204,StartTime:2020-11-29 01:17:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-29 01:17:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,ImageID:docker-pullable://us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c,ContainerID:docker://77082e6c2f2c3927f5f99b69218e0d7b025f7f1f06a82efb6d2f95c58ddab092,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.36.204,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:17:36.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4766" for this suite.

• [SLOW TEST:37.214 seconds]
[sig-apps] Deployment
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":277,"completed":7,"skipped":73,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:17:37.008: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:17:43.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4347" for this suite.

• [SLOW TEST:6.075 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when scheduling a read only busybox container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:188
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":8,"skipped":87,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:17:43.084: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Nov 29 01:17:43.135: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ca3c67fd-25f2-47ee-8a33-902a3cec526f" in namespace "downward-api-2240" to be "Succeeded or Failed"
Nov 29 01:17:43.144: INFO: Pod "downwardapi-volume-ca3c67fd-25f2-47ee-8a33-902a3cec526f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.866329ms
Nov 29 01:17:45.147: INFO: Pod "downwardapi-volume-ca3c67fd-25f2-47ee-8a33-902a3cec526f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012611739s
Nov 29 01:17:47.164: INFO: Pod "downwardapi-volume-ca3c67fd-25f2-47ee-8a33-902a3cec526f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02962989s
STEP: Saw pod success
Nov 29 01:17:47.164: INFO: Pod "downwardapi-volume-ca3c67fd-25f2-47ee-8a33-902a3cec526f" satisfied condition "Succeeded or Failed"
Nov 29 01:17:47.170: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod downwardapi-volume-ca3c67fd-25f2-47ee-8a33-902a3cec526f container client-container: <nil>
STEP: delete the pod
Nov 29 01:17:47.203: INFO: Waiting for pod downwardapi-volume-ca3c67fd-25f2-47ee-8a33-902a3cec526f to disappear
Nov 29 01:17:47.212: INFO: Pod downwardapi-volume-ca3c67fd-25f2-47ee-8a33-902a3cec526f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:17:47.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2240" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":277,"completed":9,"skipped":120,"failed":0}
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:17:47.225: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-5182
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating stateful set ss in namespace statefulset-5182
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5182
Nov 29 01:17:47.312: INFO: Found 0 stateful pods, waiting for 1
Nov 29 01:17:57.316: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Nov 29 01:17:57.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 29 01:17:58.919: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 29 01:17:58.919: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 29 01:17:58.919: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 29 01:17:58.923: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov 29 01:18:08.930: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 29 01:18:08.930: INFO: Waiting for statefulset status.replicas updated to 0
Nov 29 01:18:08.945: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Nov 29 01:18:08.945: INFO: ss-0  alex-cp1516-v3-vsp2-node-group-ea4104c57e  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:17:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:17:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:17:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:17:46 +0000 UTC  }]
Nov 29 01:18:08.945: INFO: 
Nov 29 01:18:08.945: INFO: StatefulSet ss has not reached scale 3, at 1
Nov 29 01:18:09.948: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994114697s
Nov 29 01:18:10.952: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990835431s
Nov 29 01:18:11.955: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987339696s
Nov 29 01:18:12.960: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.983835512s
Nov 29 01:18:13.964: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.978914146s
Nov 29 01:18:14.968: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.975332392s
Nov 29 01:18:15.971: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.97148497s
Nov 29 01:18:16.975: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.967949818s
Nov 29 01:18:17.979: INFO: Verifying statefulset ss doesn't scale past 3 for another 964.138693ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5182
Nov 29 01:18:18.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:18:19.200: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 29 01:18:19.200: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 29 01:18:19.200: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 29 01:18:19.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:18:19.424: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Nov 29 01:18:19.424: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 29 01:18:19.424: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 29 01:18:19.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:18:19.548: INFO: rc: 1
Nov 29 01:18:19.548: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 29 01:18:29.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:18:29.674: INFO: rc: 1
Nov 29 01:18:29.674: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 29 01:18:39.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:18:39.804: INFO: rc: 1
Nov 29 01:18:39.804: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 29 01:18:49.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:18:49.919: INFO: rc: 1
Nov 29 01:18:49.919: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 29 01:18:59.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:19:00.056: INFO: rc: 1
Nov 29 01:19:00.056: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 29 01:19:10.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:19:10.194: INFO: rc: 1
Nov 29 01:19:10.194: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 29 01:19:20.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:19:20.306: INFO: rc: 1
Nov 29 01:19:20.307: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 29 01:19:30.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:19:30.422: INFO: rc: 1
Nov 29 01:19:30.422: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 29 01:19:40.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:19:40.544: INFO: rc: 1
Nov 29 01:19:40.544: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 29 01:19:50.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:19:50.660: INFO: rc: 1
Nov 29 01:19:50.660: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 29 01:20:00.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:20:00.785: INFO: rc: 1
Nov 29 01:20:00.785: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 29 01:20:10.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:20:10.911: INFO: rc: 1
Nov 29 01:20:10.911: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 29 01:20:20.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:20:21.043: INFO: rc: 1
Nov 29 01:20:21.043: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 29 01:20:31.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:20:31.178: INFO: rc: 1
Nov 29 01:20:31.178: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 29 01:20:41.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:20:41.311: INFO: rc: 1
Nov 29 01:20:41.311: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 29 01:20:51.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:20:51.436: INFO: rc: 1
Nov 29 01:20:51.436: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 29 01:21:01.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:21:01.566: INFO: rc: 1
Nov 29 01:21:01.566: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 29 01:21:11.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:21:11.691: INFO: rc: 1
Nov 29 01:21:11.691: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 29 01:21:21.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:21:21.814: INFO: rc: 1
Nov 29 01:21:21.814: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 29 01:21:31.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:21:31.934: INFO: rc: 1
Nov 29 01:21:31.934: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 29 01:21:41.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:21:42.062: INFO: rc: 1
Nov 29 01:21:42.062: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 29 01:21:52.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:21:52.187: INFO: rc: 1
Nov 29 01:21:52.187: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 29 01:22:02.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:22:02.305: INFO: rc: 1
Nov 29 01:22:02.305: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 29 01:22:12.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:22:12.427: INFO: rc: 1
Nov 29 01:22:12.427: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 29 01:22:22.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:22:22.544: INFO: rc: 1
Nov 29 01:22:22.545: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 29 01:22:32.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:22:32.675: INFO: rc: 1
Nov 29 01:22:32.675: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 29 01:22:42.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:22:42.790: INFO: rc: 1
Nov 29 01:22:42.790: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 29 01:22:52.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:22:52.903: INFO: rc: 1
Nov 29 01:22:52.903: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 29 01:23:02.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:23:03.024: INFO: rc: 1
Nov 29 01:23:03.024: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 29 01:23:13.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:23:13.137: INFO: rc: 1
Nov 29 01:23:13.137: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 29 01:23:23.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:23:23.253: INFO: rc: 1
Nov 29 01:23:23.253: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Nov 29 01:23:23.261: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:23:23.261: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:23:23.261: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:23:33.265: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:23:33.265: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:23:33.265: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:23:43.265: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:23:43.265: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:23:43.265: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:23:53.265: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:23:53.265: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:23:53.265: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:24:03.265: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:24:03.265: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:24:03.265: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:24:13.273: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:24:13.274: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:24:13.274: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:24:23.265: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:24:23.265: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:24:23.265: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:24:33.267: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:24:33.267: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:24:33.267: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:24:43.266: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:24:43.266: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:24:43.266: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:24:53.265: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:24:53.265: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:24:53.265: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:25:03.267: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:25:03.267: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:25:03.267: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:25:13.273: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:25:13.273: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:25:13.273: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:25:23.265: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:25:23.265: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:25:23.265: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:25:33.265: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:25:33.265: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:25:33.265: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:25:43.264: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:25:43.264: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:25:43.265: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:25:53.266: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:25:53.266: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:25:53.266: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:26:03.265: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:26:03.265: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:26:03.265: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:26:13.266: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:26:13.266: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:26:13.266: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:26:23.266: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:26:23.266: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:26:23.266: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:26:33.266: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:26:33.266: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:26:33.266: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:26:43.266: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:26:43.266: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:26:43.266: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:26:53.266: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:26:53.266: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:26:53.266: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:27:03.265: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:27:03.265: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:27:03.265: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:27:13.265: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:27:13.265: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:27:13.265: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:27:23.265: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:27:23.265: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:27:23.265: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:27:33.265: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:27:33.265: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:27:33.265: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:27:43.264: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:27:43.265: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:27:43.265: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:27:53.265: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:27:53.265: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:27:53.265: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:28:03.266: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:28:03.266: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:28:03.266: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:28:13.266: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:28:13.266: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:28:13.266: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:28:23.264: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:28:23.264: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:28:23.264: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:28:33.265: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:28:33.265: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:28:33.265: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:28:43.265: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:28:43.265: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:28:43.265: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:28:53.265: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:28:53.265: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:28:53.265: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:29:03.267: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:29:03.267: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:29:03.267: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:29:13.279: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:29:13.279: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:29:13.279: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Nov 29 01:29:13.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 29 01:29:14.885: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 29 01:29:14.885: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 29 01:29:14.885: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 29 01:29:14.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 29 01:29:15.127: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 29 01:29:15.127: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 29 01:29:15.127: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 29 01:29:15.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 29 01:29:15.382: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 29 01:29:15.382: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 29 01:29:15.382: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 29 01:29:15.382: INFO: Waiting for statefulset status.replicas updated to 0
Nov 29 01:29:15.385: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Nov 29 01:29:25.391: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 29 01:29:25.391: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov 29 01:29:25.391: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov 29 01:29:25.399: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Nov 29 01:29:25.399: INFO: ss-0  alex-cp1516-v3-vsp2-node-group-ea4104c57e  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:17:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:17:46 +0000 UTC  }]
Nov 29 01:29:25.399: INFO: ss-1  alex-cp1516-v3-vsp2-node-group-bc6233c429  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:18:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:18:08 +0000 UTC  }]
Nov 29 01:29:25.399: INFO: ss-2  alex-cp1516-v3-vsp2-node-group-55fe8f9f1d  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:18:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:18:08 +0000 UTC  }]
Nov 29 01:29:25.399: INFO: 
Nov 29 01:29:25.399: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 29 01:29:26.402: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Nov 29 01:29:26.402: INFO: ss-0  alex-cp1516-v3-vsp2-node-group-ea4104c57e  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:17:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:17:46 +0000 UTC  }]
Nov 29 01:29:26.402: INFO: ss-1  alex-cp1516-v3-vsp2-node-group-bc6233c429  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:18:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:18:08 +0000 UTC  }]
Nov 29 01:29:26.402: INFO: ss-2  alex-cp1516-v3-vsp2-node-group-55fe8f9f1d  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:18:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:18:08 +0000 UTC  }]
Nov 29 01:29:26.402: INFO: 
Nov 29 01:29:26.402: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 29 01:29:27.406: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Nov 29 01:29:27.406: INFO: ss-0  alex-cp1516-v3-vsp2-node-group-ea4104c57e  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:17:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:17:46 +0000 UTC  }]
Nov 29 01:29:27.406: INFO: ss-1  alex-cp1516-v3-vsp2-node-group-bc6233c429  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:18:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:18:08 +0000 UTC  }]
Nov 29 01:29:27.406: INFO: ss-2  alex-cp1516-v3-vsp2-node-group-55fe8f9f1d  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:18:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:18:08 +0000 UTC  }]
Nov 29 01:29:27.406: INFO: 
Nov 29 01:29:27.406: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 29 01:29:28.410: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Nov 29 01:29:28.410: INFO: ss-0  alex-cp1516-v3-vsp2-node-group-ea4104c57e  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:17:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:17:46 +0000 UTC  }]
Nov 29 01:29:28.410: INFO: ss-1  alex-cp1516-v3-vsp2-node-group-bc6233c429  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:18:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:18:08 +0000 UTC  }]
Nov 29 01:29:28.410: INFO: ss-2  alex-cp1516-v3-vsp2-node-group-55fe8f9f1d  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:18:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:18:08 +0000 UTC  }]
Nov 29 01:29:28.410: INFO: 
Nov 29 01:29:28.410: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 29 01:29:29.415: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Nov 29 01:29:29.415: INFO: ss-0  alex-cp1516-v3-vsp2-node-group-ea4104c57e  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:17:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:17:46 +0000 UTC  }]
Nov 29 01:29:29.415: INFO: ss-1  alex-cp1516-v3-vsp2-node-group-bc6233c429  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:18:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:18:08 +0000 UTC  }]
Nov 29 01:29:29.415: INFO: ss-2  alex-cp1516-v3-vsp2-node-group-55fe8f9f1d  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:18:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:18:08 +0000 UTC  }]
Nov 29 01:29:29.415: INFO: 
Nov 29 01:29:29.415: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 29 01:29:30.418: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Nov 29 01:29:30.418: INFO: ss-1  alex-cp1516-v3-vsp2-node-group-bc6233c429  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:18:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:18:08 +0000 UTC  }]
Nov 29 01:29:30.419: INFO: ss-2  alex-cp1516-v3-vsp2-node-group-55fe8f9f1d  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:18:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:18:08 +0000 UTC  }]
Nov 29 01:29:30.419: INFO: 
Nov 29 01:29:30.419: INFO: StatefulSet ss has not reached scale 0, at 2
Nov 29 01:29:31.422: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Nov 29 01:29:31.422: INFO: ss-1  alex-cp1516-v3-vsp2-node-group-bc6233c429  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:18:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:18:08 +0000 UTC  }]
Nov 29 01:29:31.422: INFO: 
Nov 29 01:29:31.422: INFO: StatefulSet ss has not reached scale 0, at 1
Nov 29 01:29:32.426: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Nov 29 01:29:32.426: INFO: ss-1  alex-cp1516-v3-vsp2-node-group-bc6233c429  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:18:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:18:08 +0000 UTC  }]
Nov 29 01:29:32.426: INFO: 
Nov 29 01:29:32.426: INFO: StatefulSet ss has not reached scale 0, at 1
Nov 29 01:29:33.429: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Nov 29 01:29:33.429: INFO: ss-1  alex-cp1516-v3-vsp2-node-group-bc6233c429  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:18:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:18:08 +0000 UTC  }]
Nov 29 01:29:33.429: INFO: 
Nov 29 01:29:33.429: INFO: StatefulSet ss has not reached scale 0, at 1
Nov 29 01:29:34.434: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Nov 29 01:29:34.434: INFO: ss-1  alex-cp1516-v3-vsp2-node-group-bc6233c429  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:18:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:29:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-29 01:18:08 +0000 UTC  }]
Nov 29 01:29:34.434: INFO: 
Nov 29 01:29:34.434: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5182
Nov 29 01:29:35.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:29:35.556: INFO: rc: 1
Nov 29 01:29:35.556: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 29 01:29:45.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:29:45.660: INFO: rc: 1
Nov 29 01:29:45.660: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 29 01:29:55.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:29:55.756: INFO: rc: 1
Nov 29 01:29:55.756: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 29 01:30:05.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:30:05.860: INFO: rc: 1
Nov 29 01:30:05.860: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 29 01:30:15.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:30:15.948: INFO: rc: 1
Nov 29 01:30:15.948: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 29 01:30:25.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:30:26.038: INFO: rc: 1
Nov 29 01:30:26.039: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 29 01:30:36.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:30:36.130: INFO: rc: 1
Nov 29 01:30:36.130: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 29 01:30:46.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:30:46.224: INFO: rc: 1
Nov 29 01:30:46.224: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 29 01:30:56.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:30:56.314: INFO: rc: 1
Nov 29 01:30:56.315: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 29 01:31:06.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:31:06.403: INFO: rc: 1
Nov 29 01:31:06.404: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 29 01:31:16.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:31:16.500: INFO: rc: 1
Nov 29 01:31:16.500: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 29 01:31:26.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:31:26.584: INFO: rc: 1
Nov 29 01:31:26.584: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 29 01:31:36.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:31:36.684: INFO: rc: 1
Nov 29 01:31:36.684: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 29 01:31:46.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:31:46.781: INFO: rc: 1
Nov 29 01:31:46.781: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 29 01:31:56.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:31:56.884: INFO: rc: 1
Nov 29 01:31:56.884: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 29 01:32:06.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:32:06.983: INFO: rc: 1
Nov 29 01:32:06.983: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 29 01:32:16.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:32:17.075: INFO: rc: 1
Nov 29 01:32:17.075: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 29 01:32:27.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:32:27.162: INFO: rc: 1
Nov 29 01:32:27.162: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 29 01:32:37.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:32:37.257: INFO: rc: 1
Nov 29 01:32:37.257: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 29 01:32:47.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:32:47.355: INFO: rc: 1
Nov 29 01:32:47.355: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 29 01:32:57.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:32:57.440: INFO: rc: 1
Nov 29 01:32:57.441: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 29 01:33:07.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:33:07.534: INFO: rc: 1
Nov 29 01:33:07.534: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 29 01:33:17.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:33:17.619: INFO: rc: 1
Nov 29 01:33:17.619: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 29 01:33:27.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:33:27.704: INFO: rc: 1
Nov 29 01:33:27.704: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 29 01:33:37.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:33:37.797: INFO: rc: 1
Nov 29 01:33:37.797: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 29 01:33:47.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:33:47.907: INFO: rc: 1
Nov 29 01:33:47.907: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 29 01:33:57.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:33:58.002: INFO: rc: 1
Nov 29 01:33:58.002: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 29 01:34:08.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:34:08.105: INFO: rc: 1
Nov 29 01:34:08.105: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 29 01:34:18.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:34:18.190: INFO: rc: 1
Nov 29 01:34:18.190: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 29 01:34:28.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:34:28.273: INFO: rc: 1
Nov 29 01:34:28.274: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 29 01:34:38.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5182 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:34:38.359: INFO: rc: 1
Nov 29 01:34:38.360: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: 
Nov 29 01:34:38.360: INFO: Scaling statefulset ss to 0
Nov 29 01:34:38.373: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Nov 29 01:34:38.376: INFO: Deleting all statefulset in ns statefulset-5182
Nov 29 01:34:38.378: INFO: Scaling statefulset ss to 0
Nov 29 01:34:38.385: INFO: Waiting for statefulset status.replicas updated to 0
Nov 29 01:34:38.387: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:34:38.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5182" for this suite.

• [SLOW TEST:1011.188 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":277,"completed":10,"skipped":121,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:34:38.413: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6089.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6089.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6089.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6089.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6089.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6089.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6089.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6089.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6089.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6089.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6089.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 152.144.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.144.152_udp@PTR;check="$$(dig +tcp +noall +answer +search 152.144.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.144.152_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6089.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6089.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6089.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6089.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6089.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6089.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6089.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6089.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6089.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6089.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6089.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 152.144.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.144.152_udp@PTR;check="$$(dig +tcp +noall +answer +search 152.144.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.144.152_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 29 01:34:48.521: INFO: Unable to read wheezy_udp@dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:34:48.525: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:34:48.532: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:34:48.536: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:34:48.558: INFO: Unable to read jessie_udp@dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:34:48.560: INFO: Unable to read jessie_tcp@dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:34:48.563: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:34:48.565: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:34:48.579: INFO: Lookups using dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf failed for: [wheezy_udp@dns-test-service.dns-6089.svc.cluster.local wheezy_tcp@dns-test-service.dns-6089.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local jessie_udp@dns-test-service.dns-6089.svc.cluster.local jessie_tcp@dns-test-service.dns-6089.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local]

Nov 29 01:34:53.585: INFO: Unable to read wheezy_udp@dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:34:53.588: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:34:53.591: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:34:53.593: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:34:53.610: INFO: Unable to read jessie_udp@dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:34:53.612: INFO: Unable to read jessie_tcp@dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:34:53.614: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:34:53.616: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:34:53.631: INFO: Lookups using dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf failed for: [wheezy_udp@dns-test-service.dns-6089.svc.cluster.local wheezy_tcp@dns-test-service.dns-6089.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local jessie_udp@dns-test-service.dns-6089.svc.cluster.local jessie_tcp@dns-test-service.dns-6089.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local]

Nov 29 01:34:58.585: INFO: Unable to read wheezy_udp@dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:34:58.589: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:34:58.601: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:34:58.618: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:34:58.648: INFO: Unable to read jessie_udp@dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:34:58.651: INFO: Unable to read jessie_tcp@dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:34:58.654: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:34:58.658: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:34:58.686: INFO: Lookups using dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf failed for: [wheezy_udp@dns-test-service.dns-6089.svc.cluster.local wheezy_tcp@dns-test-service.dns-6089.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local jessie_udp@dns-test-service.dns-6089.svc.cluster.local jessie_tcp@dns-test-service.dns-6089.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local]

Nov 29 01:35:03.585: INFO: Unable to read wheezy_udp@dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:35:03.590: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:35:03.593: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:35:03.597: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:35:03.627: INFO: Unable to read jessie_udp@dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:35:03.631: INFO: Unable to read jessie_tcp@dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:35:03.635: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:35:03.645: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:35:03.685: INFO: Lookups using dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf failed for: [wheezy_udp@dns-test-service.dns-6089.svc.cluster.local wheezy_tcp@dns-test-service.dns-6089.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local jessie_udp@dns-test-service.dns-6089.svc.cluster.local jessie_tcp@dns-test-service.dns-6089.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local]

Nov 29 01:35:08.584: INFO: Unable to read wheezy_udp@dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:35:08.587: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:35:08.589: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:35:08.592: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:35:08.618: INFO: Unable to read jessie_udp@dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:35:08.621: INFO: Unable to read jessie_tcp@dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:35:08.624: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:35:08.627: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local from pod dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf: the server could not find the requested resource (get pods dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf)
Nov 29 01:35:08.644: INFO: Lookups using dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf failed for: [wheezy_udp@dns-test-service.dns-6089.svc.cluster.local wheezy_tcp@dns-test-service.dns-6089.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local jessie_udp@dns-test-service.dns-6089.svc.cluster.local jessie_tcp@dns-test-service.dns-6089.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6089.svc.cluster.local]

Nov 29 01:35:13.641: INFO: DNS probes using dns-6089/dns-test-36f3a3a5-16b4-4e0f-8d42-083f9ff110bf succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:35:13.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6089" for this suite.

• [SLOW TEST:35.358 seconds]
[sig-network] DNS
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":277,"completed":11,"skipped":124,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:35:13.772: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:35:30.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9328" for this suite.

• [SLOW TEST:17.093 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":277,"completed":12,"skipped":128,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:35:30.865: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl logs
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1288
STEP: creating an pod
Nov 29 01:35:30.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 run logs-generator --image=us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 --namespace=kubectl-1168 -- logs-generator --log-lines-total 100 --run-duration 20s'
Nov 29 01:35:31.006: INFO: stderr: ""
Nov 29 01:35:31.006: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Waiting for log generator to start.
Nov 29 01:35:31.006: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Nov 29 01:35:31.006: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-1168" to be "running and ready, or succeeded"
Nov 29 01:35:31.017: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 10.747345ms
Nov 29 01:35:33.021: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01431178s
Nov 29 01:35:35.026: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.019587219s
Nov 29 01:35:35.026: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Nov 29 01:35:35.026: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Nov 29 01:35:35.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 logs logs-generator logs-generator --namespace=kubectl-1168'
Nov 29 01:35:35.146: INFO: stderr: ""
Nov 29 01:35:35.146: INFO: stdout: "I1129 01:35:32.872758       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/2l7p 464\nI1129 01:35:33.072905       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/6bxz 356\nI1129 01:35:33.272941       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/gfzj 493\nI1129 01:35:33.472937       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/5l4 597\nI1129 01:35:33.672940       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/zgt 277\nI1129 01:35:33.872976       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/ccz4 593\nI1129 01:35:34.072976       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/sdt 408\nI1129 01:35:34.272899       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/9nvm 419\nI1129 01:35:34.472939       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/drx 578\nI1129 01:35:34.672920       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/x2b 597\nI1129 01:35:34.872926       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/8nl 517\nI1129 01:35:35.072942       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/hb62 478\nI1129 01:35:35.272923       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/9wb8 486\n"
STEP: limiting log lines
Nov 29 01:35:35.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 logs logs-generator logs-generator --namespace=kubectl-1168 --tail=1'
Nov 29 01:35:35.245: INFO: stderr: ""
Nov 29 01:35:35.245: INFO: stdout: "I1129 01:35:35.472909       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/t8x9 438\n"
Nov 29 01:35:35.245: INFO: got output "I1129 01:35:35.472909       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/t8x9 438\n"
STEP: limiting log bytes
Nov 29 01:35:35.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 logs logs-generator logs-generator --namespace=kubectl-1168 --limit-bytes=1'
Nov 29 01:35:35.356: INFO: stderr: ""
Nov 29 01:35:35.356: INFO: stdout: "I"
Nov 29 01:35:35.356: INFO: got output "I"
STEP: exposing timestamps
Nov 29 01:35:35.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 logs logs-generator logs-generator --namespace=kubectl-1168 --tail=1 --timestamps'
Nov 29 01:35:35.459: INFO: stderr: ""
Nov 29 01:35:35.459: INFO: stdout: "2020-11-29T01:35:35.673149602Z I1129 01:35:35.672973       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/psb6 208\n"
Nov 29 01:35:35.459: INFO: got output "2020-11-29T01:35:35.673149602Z I1129 01:35:35.672973       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/psb6 208\n"
STEP: restricting to a time range
Nov 29 01:35:37.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 logs logs-generator logs-generator --namespace=kubectl-1168 --since=1s'
Nov 29 01:35:38.079: INFO: stderr: ""
Nov 29 01:35:38.079: INFO: stdout: "I1129 01:35:37.472940       1 logs_generator.go:76] 23 GET /api/v1/namespaces/kube-system/pods/cnk 280\nI1129 01:35:37.672953       1 logs_generator.go:76] 24 GET /api/v1/namespaces/kube-system/pods/2f6 503\nI1129 01:35:37.873018       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/ns/pods/trg 309\nI1129 01:35:38.073015       1 logs_generator.go:76] 26 GET /api/v1/namespaces/ns/pods/c4s 508\nI1129 01:35:38.273036       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/g6fn 569\n"
Nov 29 01:35:38.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 logs logs-generator logs-generator --namespace=kubectl-1168 --since=24h'
Nov 29 01:35:38.187: INFO: stderr: ""
Nov 29 01:35:38.187: INFO: stdout: "I1129 01:35:32.872758       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/2l7p 464\nI1129 01:35:33.072905       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/6bxz 356\nI1129 01:35:33.272941       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/gfzj 493\nI1129 01:35:33.472937       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/5l4 597\nI1129 01:35:33.672940       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/zgt 277\nI1129 01:35:33.872976       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/ccz4 593\nI1129 01:35:34.072976       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/sdt 408\nI1129 01:35:34.272899       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/9nvm 419\nI1129 01:35:34.472939       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/drx 578\nI1129 01:35:34.672920       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/x2b 597\nI1129 01:35:34.872926       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/8nl 517\nI1129 01:35:35.072942       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/hb62 478\nI1129 01:35:35.272923       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/9wb8 486\nI1129 01:35:35.472909       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/t8x9 438\nI1129 01:35:35.672973       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/psb6 208\nI1129 01:35:35.873101       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/8bh 545\nI1129 01:35:36.072971       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/lxvx 464\nI1129 01:35:36.272933       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/qjg 225\nI1129 01:35:36.472992       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/tfsp 379\nI1129 01:35:36.672939       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/ftd 594\nI1129 01:35:36.872991       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/4bf4 403\nI1129 01:35:37.073012       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/lpn 245\nI1129 01:35:37.272977       1 logs_generator.go:76] 22 GET /api/v1/namespaces/default/pods/548 309\nI1129 01:35:37.472940       1 logs_generator.go:76] 23 GET /api/v1/namespaces/kube-system/pods/cnk 280\nI1129 01:35:37.672953       1 logs_generator.go:76] 24 GET /api/v1/namespaces/kube-system/pods/2f6 503\nI1129 01:35:37.873018       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/ns/pods/trg 309\nI1129 01:35:38.073015       1 logs_generator.go:76] 26 GET /api/v1/namespaces/ns/pods/c4s 508\nI1129 01:35:38.273036       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/g6fn 569\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1294
Nov 29 01:35:38.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 delete pod logs-generator --namespace=kubectl-1168'
Nov 29 01:35:40.174: INFO: stderr: ""
Nov 29 01:35:40.174: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:35:40.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1168" for this suite.

• [SLOW TEST:9.319 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1284
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":277,"completed":13,"skipped":132,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:35:40.185: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod test-webserver-1357b75f-ff8b-4e20-b54e-c2443be7e934 in namespace container-probe-4746
Nov 29 01:35:44.234: INFO: Started pod test-webserver-1357b75f-ff8b-4e20-b54e-c2443be7e934 in namespace container-probe-4746
STEP: checking the pod's current state and verifying that restartCount is present
Nov 29 01:35:44.236: INFO: Initial restart count of pod test-webserver-1357b75f-ff8b-4e20-b54e-c2443be7e934 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:39:44.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4746" for this suite.

• [SLOW TEST:244.515 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":277,"completed":14,"skipped":162,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:39:44.700: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Nov 29 01:39:44.736: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4d7d8650-48ed-40c2-a984-8a5be38e1d7c" in namespace "projected-6740" to be "Succeeded or Failed"
Nov 29 01:39:44.743: INFO: Pod "downwardapi-volume-4d7d8650-48ed-40c2-a984-8a5be38e1d7c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.895282ms
Nov 29 01:39:46.747: INFO: Pod "downwardapi-volume-4d7d8650-48ed-40c2-a984-8a5be38e1d7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010149437s
Nov 29 01:39:48.750: INFO: Pod "downwardapi-volume-4d7d8650-48ed-40c2-a984-8a5be38e1d7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013137578s
STEP: Saw pod success
Nov 29 01:39:48.750: INFO: Pod "downwardapi-volume-4d7d8650-48ed-40c2-a984-8a5be38e1d7c" satisfied condition "Succeeded or Failed"
Nov 29 01:39:48.752: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod downwardapi-volume-4d7d8650-48ed-40c2-a984-8a5be38e1d7c container client-container: <nil>
STEP: delete the pod
Nov 29 01:39:48.790: INFO: Waiting for pod downwardapi-volume-4d7d8650-48ed-40c2-a984-8a5be38e1d7c to disappear
Nov 29 01:39:48.796: INFO: Pod downwardapi-volume-4d7d8650-48ed-40c2-a984-8a5be38e1d7c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:39:48.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6740" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":277,"completed":15,"skipped":168,"failed":0}
S
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:39:48.811: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 01:39:48.860: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:39:53.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8361" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":277,"completed":16,"skipped":169,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:39:53.017: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:40:09.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1319" for this suite.

• [SLOW TEST:16.122 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":277,"completed":17,"skipped":172,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:40:09.140: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1555.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1555.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1555.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1555.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1555.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1555.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 29 01:40:13.237: INFO: DNS probes using dns-1555/dns-test-8592d93c-cb8d-4262-a00e-1f8af130f713 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:40:13.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1555" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":277,"completed":18,"skipped":194,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:40:13.307: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:40:17.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2608" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":277,"completed":19,"skipped":204,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:40:17.474: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Nov 29 01:40:17.524: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5f7da47c-332e-4dc1-a538-9ab21477d802" in namespace "downward-api-8229" to be "Succeeded or Failed"
Nov 29 01:40:17.533: INFO: Pod "downwardapi-volume-5f7da47c-332e-4dc1-a538-9ab21477d802": Phase="Pending", Reason="", readiness=false. Elapsed: 8.105329ms
Nov 29 01:40:19.536: INFO: Pod "downwardapi-volume-5f7da47c-332e-4dc1-a538-9ab21477d802": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011490134s
Nov 29 01:40:21.539: INFO: Pod "downwardapi-volume-5f7da47c-332e-4dc1-a538-9ab21477d802": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014596236s
STEP: Saw pod success
Nov 29 01:40:21.539: INFO: Pod "downwardapi-volume-5f7da47c-332e-4dc1-a538-9ab21477d802" satisfied condition "Succeeded or Failed"
Nov 29 01:40:21.541: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod downwardapi-volume-5f7da47c-332e-4dc1-a538-9ab21477d802 container client-container: <nil>
STEP: delete the pod
Nov 29 01:40:21.557: INFO: Waiting for pod downwardapi-volume-5f7da47c-332e-4dc1-a538-9ab21477d802 to disappear
Nov 29 01:40:21.567: INFO: Pod downwardapi-volume-5f7da47c-332e-4dc1-a538-9ab21477d802 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:40:21.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8229" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":277,"completed":20,"skipped":210,"failed":0}
SS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:40:21.579: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:40:21.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-363" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":277,"completed":21,"skipped":212,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:40:21.628: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W1129 01:40:31.698886      22 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Nov 29 01:40:31.699: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:40:31.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2627" for this suite.

• [SLOW TEST:10.079 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":277,"completed":22,"skipped":234,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:40:31.707: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-3288dc8a-4173-4564-8087-41f72a159642
STEP: Creating a pod to test consume secrets
Nov 29 01:40:31.751: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-eea411b9-126b-4072-a6d6-891da0e7f07b" in namespace "projected-6960" to be "Succeeded or Failed"
Nov 29 01:40:31.763: INFO: Pod "pod-projected-secrets-eea411b9-126b-4072-a6d6-891da0e7f07b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.616326ms
Nov 29 01:40:33.767: INFO: Pod "pod-projected-secrets-eea411b9-126b-4072-a6d6-891da0e7f07b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015904587s
Nov 29 01:40:35.769: INFO: Pod "pod-projected-secrets-eea411b9-126b-4072-a6d6-891da0e7f07b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01868085s
STEP: Saw pod success
Nov 29 01:40:35.769: INFO: Pod "pod-projected-secrets-eea411b9-126b-4072-a6d6-891da0e7f07b" satisfied condition "Succeeded or Failed"
Nov 29 01:40:35.772: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod pod-projected-secrets-eea411b9-126b-4072-a6d6-891da0e7f07b container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 29 01:40:35.799: INFO: Waiting for pod pod-projected-secrets-eea411b9-126b-4072-a6d6-891da0e7f07b to disappear
Nov 29 01:40:35.807: INFO: Pod pod-projected-secrets-eea411b9-126b-4072-a6d6-891da0e7f07b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:40:35.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6960" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":23,"skipped":237,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:40:35.819: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:40:35.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2832" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":277,"completed":24,"skipped":244,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:40:35.912: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-map-ed2bb811-1795-4791-a978-21c265cf2aff
STEP: Creating a pod to test consume configMaps
Nov 29 01:40:35.962: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-10ba594c-a2c8-46ca-acbd-3eba0047104d" in namespace "projected-5072" to be "Succeeded or Failed"
Nov 29 01:40:35.967: INFO: Pod "pod-projected-configmaps-10ba594c-a2c8-46ca-acbd-3eba0047104d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.63331ms
Nov 29 01:40:37.970: INFO: Pod "pod-projected-configmaps-10ba594c-a2c8-46ca-acbd-3eba0047104d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007636897s
Nov 29 01:40:39.974: INFO: Pod "pod-projected-configmaps-10ba594c-a2c8-46ca-acbd-3eba0047104d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012019637s
STEP: Saw pod success
Nov 29 01:40:39.975: INFO: Pod "pod-projected-configmaps-10ba594c-a2c8-46ca-acbd-3eba0047104d" satisfied condition "Succeeded or Failed"
Nov 29 01:40:39.977: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod pod-projected-configmaps-10ba594c-a2c8-46ca-acbd-3eba0047104d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 01:40:40.017: INFO: Waiting for pod pod-projected-configmaps-10ba594c-a2c8-46ca-acbd-3eba0047104d to disappear
Nov 29 01:40:40.024: INFO: Pod pod-projected-configmaps-10ba594c-a2c8-46ca-acbd-3eba0047104d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:40:40.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5072" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":277,"completed":25,"skipped":248,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:40:40.031: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on node default medium
Nov 29 01:40:40.091: INFO: Waiting up to 5m0s for pod "pod-4406b181-d6fb-4674-b27c-5e76065ebd34" in namespace "emptydir-4232" to be "Succeeded or Failed"
Nov 29 01:40:40.094: INFO: Pod "pod-4406b181-d6fb-4674-b27c-5e76065ebd34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.373229ms
Nov 29 01:40:42.097: INFO: Pod "pod-4406b181-d6fb-4674-b27c-5e76065ebd34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005520749s
Nov 29 01:40:44.099: INFO: Pod "pod-4406b181-d6fb-4674-b27c-5e76065ebd34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008303574s
STEP: Saw pod success
Nov 29 01:40:44.100: INFO: Pod "pod-4406b181-d6fb-4674-b27c-5e76065ebd34" satisfied condition "Succeeded or Failed"
Nov 29 01:40:44.102: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod pod-4406b181-d6fb-4674-b27c-5e76065ebd34 container test-container: <nil>
STEP: delete the pod
Nov 29 01:40:44.121: INFO: Waiting for pod pod-4406b181-d6fb-4674-b27c-5e76065ebd34 to disappear
Nov 29 01:40:44.131: INFO: Pod pod-4406b181-d6fb-4674-b27c-5e76065ebd34 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:40:44.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4232" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":26,"skipped":254,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:40:44.145: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-2101, will wait for the garbage collector to delete the pods
Nov 29 01:40:48.252: INFO: Deleting Job.batch foo took: 4.364179ms
Nov 29 01:40:49.053: INFO: Terminating Job.batch foo pods took: 800.361026ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:41:29.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2101" for this suite.

• [SLOW TEST:45.818 seconds]
[sig-apps] Job
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":277,"completed":27,"skipped":273,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:41:29.963: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-558dc91c-364e-4c3a-a174-2b1291820c09
STEP: Creating a pod to test consume configMaps
Nov 29 01:41:30.033: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2f9c6c62-3028-4a30-b6be-970bad30929c" in namespace "projected-9943" to be "Succeeded or Failed"
Nov 29 01:41:30.036: INFO: Pod "pod-projected-configmaps-2f9c6c62-3028-4a30-b6be-970bad30929c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.505193ms
Nov 29 01:41:32.039: INFO: Pod "pod-projected-configmaps-2f9c6c62-3028-4a30-b6be-970bad30929c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005704281s
Nov 29 01:41:34.042: INFO: Pod "pod-projected-configmaps-2f9c6c62-3028-4a30-b6be-970bad30929c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008721459s
STEP: Saw pod success
Nov 29 01:41:34.042: INFO: Pod "pod-projected-configmaps-2f9c6c62-3028-4a30-b6be-970bad30929c" satisfied condition "Succeeded or Failed"
Nov 29 01:41:34.044: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod pod-projected-configmaps-2f9c6c62-3028-4a30-b6be-970bad30929c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 01:41:34.063: INFO: Waiting for pod pod-projected-configmaps-2f9c6c62-3028-4a30-b6be-970bad30929c to disappear
Nov 29 01:41:34.071: INFO: Pod pod-projected-configmaps-2f9c6c62-3028-4a30-b6be-970bad30929c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:41:34.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9943" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":277,"completed":28,"skipped":296,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:41:34.079: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Nov 29 01:41:34.129: INFO: Waiting up to 5m0s for pod "downwardapi-volume-713239a1-b665-4515-8f9f-6add1bb53d9d" in namespace "projected-4080" to be "Succeeded or Failed"
Nov 29 01:41:34.137: INFO: Pod "downwardapi-volume-713239a1-b665-4515-8f9f-6add1bb53d9d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.294263ms
Nov 29 01:41:36.141: INFO: Pod "downwardapi-volume-713239a1-b665-4515-8f9f-6add1bb53d9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012088251s
Nov 29 01:41:38.145: INFO: Pod "downwardapi-volume-713239a1-b665-4515-8f9f-6add1bb53d9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015934401s
STEP: Saw pod success
Nov 29 01:41:38.145: INFO: Pod "downwardapi-volume-713239a1-b665-4515-8f9f-6add1bb53d9d" satisfied condition "Succeeded or Failed"
Nov 29 01:41:38.156: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod downwardapi-volume-713239a1-b665-4515-8f9f-6add1bb53d9d container client-container: <nil>
STEP: delete the pod
Nov 29 01:41:38.194: INFO: Waiting for pod downwardapi-volume-713239a1-b665-4515-8f9f-6add1bb53d9d to disappear
Nov 29 01:41:38.199: INFO: Pod downwardapi-volume-713239a1-b665-4515-8f9f-6add1bb53d9d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:41:38.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4080" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":277,"completed":29,"skipped":298,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:41:38.216: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 29 01:41:38.941: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 29 01:41:40.958: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742210898, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742210898, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742210898, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742210898, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 01:41:43.982: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Nov 29 01:41:44.000: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:41:44.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2578" for this suite.
STEP: Destroying namespace "webhook-2578-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.886 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":277,"completed":30,"skipped":312,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:41:44.102: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Nov 29 01:41:48.662: INFO: Successfully updated pod "labelsupdate33e16992-4bd6-40c1-b311-89d901cb31c6"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:41:50.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6438" for this suite.

• [SLOW TEST:6.595 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":277,"completed":31,"skipped":328,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:41:50.697: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Nov 29 01:41:58.756: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 29 01:41:58.765: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 29 01:42:00.765: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 29 01:42:00.769: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 29 01:42:02.765: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 29 01:42:02.768: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 29 01:42:04.765: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 29 01:42:04.769: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 29 01:42:06.765: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 29 01:42:06.769: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 29 01:42:08.765: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 29 01:42:08.768: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:42:08.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6028" for this suite.

• [SLOW TEST:18.088 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":277,"completed":32,"skipped":336,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:42:08.786: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 01:42:08.843: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Nov 29 01:42:08.849: INFO: Number of nodes with available pods: 0
Nov 29 01:42:08.849: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Nov 29 01:42:08.891: INFO: Number of nodes with available pods: 0
Nov 29 01:42:08.891: INFO: Node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d is running more than one daemon pod
Nov 29 01:42:09.895: INFO: Number of nodes with available pods: 0
Nov 29 01:42:09.895: INFO: Node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d is running more than one daemon pod
Nov 29 01:42:10.896: INFO: Number of nodes with available pods: 0
Nov 29 01:42:10.896: INFO: Node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d is running more than one daemon pod
Nov 29 01:42:11.934: INFO: Number of nodes with available pods: 1
Nov 29 01:42:11.934: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Nov 29 01:42:11.953: INFO: Number of nodes with available pods: 1
Nov 29 01:42:11.953: INFO: Number of running nodes: 0, number of available pods: 1
Nov 29 01:42:12.956: INFO: Number of nodes with available pods: 0
Nov 29 01:42:12.956: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Nov 29 01:42:12.996: INFO: Number of nodes with available pods: 0
Nov 29 01:42:12.996: INFO: Node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d is running more than one daemon pod
Nov 29 01:42:14.003: INFO: Number of nodes with available pods: 0
Nov 29 01:42:14.003: INFO: Node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d is running more than one daemon pod
Nov 29 01:42:15.000: INFO: Number of nodes with available pods: 0
Nov 29 01:42:15.000: INFO: Node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d is running more than one daemon pod
Nov 29 01:42:15.999: INFO: Number of nodes with available pods: 0
Nov 29 01:42:15.999: INFO: Node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d is running more than one daemon pod
Nov 29 01:42:17.000: INFO: Number of nodes with available pods: 0
Nov 29 01:42:17.000: INFO: Node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d is running more than one daemon pod
Nov 29 01:42:18.001: INFO: Number of nodes with available pods: 1
Nov 29 01:42:18.001: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2170, will wait for the garbage collector to delete the pods
Nov 29 01:42:18.068: INFO: Deleting DaemonSet.extensions daemon-set took: 8.75489ms
Nov 29 01:42:18.869: INFO: Terminating DaemonSet.extensions daemon-set pods took: 800.279377ms
Nov 29 01:42:21.473: INFO: Number of nodes with available pods: 0
Nov 29 01:42:21.473: INFO: Number of running nodes: 0, number of available pods: 0
Nov 29 01:42:21.478: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2170/daemonsets","resourceVersion":"14203"},"items":null}

Nov 29 01:42:21.482: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2170/pods","resourceVersion":"14203"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:42:21.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2170" for this suite.

• [SLOW TEST:12.725 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":277,"completed":33,"skipped":344,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:42:21.512: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Nov 29 01:42:21.539: INFO: PodSpec: initContainers in spec.initContainers
Nov 29 01:43:04.215: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-f99f6a9e-65bb-44b0-895a-5f13fb42a49a", GenerateName:"", Namespace:"init-container-1421", SelfLink:"/api/v1/namespaces/init-container-1421/pods/pod-init-f99f6a9e-65bb-44b0-895a-5f13fb42a49a", UID:"e431d55d-0215-46c7-8f11-af3818d221bb", ResourceVersion:"14491", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63742210940, loc:(*time.Location)(0x7b675e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"539542992"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"192.168.191.152/32", "cni.projectcalico.org/podIPs":"192.168.191.152/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0016e8da0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0016e8dc0)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0016e8de0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0016e8e00)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0016e8e20), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0016e8e40)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-5m97q", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc00277bb40), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-5m97q", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-5m97q", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-5m97q", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc000806f48), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"alex-cp1516-v3-vsp2-node-group-bc6233c429", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002735960), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000806fe0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000807000)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc000807008), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00080700c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742210941, loc:(*time.Location)(0x7b675e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742210941, loc:(*time.Location)(0x7b675e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742210941, loc:(*time.Location)(0x7b675e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742210940, loc:(*time.Location)(0x7b675e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.10.103.165", PodIP:"192.168.191.152", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.191.152"}}, StartTime:(*v1.Time)(0xc0016e8e60), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002735a40)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002735ab0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://a812eae1577af5c9de637b9a9e5d33f07ca730d065e6dddd778403c2b3c7b95b", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0016e8ea0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0016e8e80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc0008070a4)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:43:04.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1421" for this suite.

• [SLOW TEST:42.715 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":277,"completed":34,"skipped":361,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:43:04.228: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:43:08.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6562" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":277,"completed":35,"skipped":371,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:43:08.305: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-configmap-k2l7
STEP: Creating a pod to test atomic-volume-subpath
Nov 29 01:43:08.361: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-k2l7" in namespace "subpath-1758" to be "Succeeded or Failed"
Nov 29 01:43:08.363: INFO: Pod "pod-subpath-test-configmap-k2l7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.679038ms
Nov 29 01:43:10.370: INFO: Pod "pod-subpath-test-configmap-k2l7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008949565s
Nov 29 01:43:12.373: INFO: Pod "pod-subpath-test-configmap-k2l7": Phase="Running", Reason="", readiness=true. Elapsed: 4.012367757s
Nov 29 01:43:14.377: INFO: Pod "pod-subpath-test-configmap-k2l7": Phase="Running", Reason="", readiness=true. Elapsed: 6.015932504s
Nov 29 01:43:16.380: INFO: Pod "pod-subpath-test-configmap-k2l7": Phase="Running", Reason="", readiness=true. Elapsed: 8.019426314s
Nov 29 01:43:18.383: INFO: Pod "pod-subpath-test-configmap-k2l7": Phase="Running", Reason="", readiness=true. Elapsed: 10.022258226s
Nov 29 01:43:20.386: INFO: Pod "pod-subpath-test-configmap-k2l7": Phase="Running", Reason="", readiness=true. Elapsed: 12.02565152s
Nov 29 01:43:22.389: INFO: Pod "pod-subpath-test-configmap-k2l7": Phase="Running", Reason="", readiness=true. Elapsed: 14.028780066s
Nov 29 01:43:24.392: INFO: Pod "pod-subpath-test-configmap-k2l7": Phase="Running", Reason="", readiness=true. Elapsed: 16.031831844s
Nov 29 01:43:26.396: INFO: Pod "pod-subpath-test-configmap-k2l7": Phase="Running", Reason="", readiness=true. Elapsed: 18.035299388s
Nov 29 01:43:28.405: INFO: Pod "pod-subpath-test-configmap-k2l7": Phase="Running", Reason="", readiness=true. Elapsed: 20.044091782s
Nov 29 01:43:30.408: INFO: Pod "pod-subpath-test-configmap-k2l7": Phase="Running", Reason="", readiness=true. Elapsed: 22.047420825s
Nov 29 01:43:32.411: INFO: Pod "pod-subpath-test-configmap-k2l7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.050577702s
STEP: Saw pod success
Nov 29 01:43:32.411: INFO: Pod "pod-subpath-test-configmap-k2l7" satisfied condition "Succeeded or Failed"
Nov 29 01:43:32.414: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod pod-subpath-test-configmap-k2l7 container test-container-subpath-configmap-k2l7: <nil>
STEP: delete the pod
Nov 29 01:43:32.433: INFO: Waiting for pod pod-subpath-test-configmap-k2l7 to disappear
Nov 29 01:43:32.444: INFO: Pod pod-subpath-test-configmap-k2l7 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-k2l7
Nov 29 01:43:32.444: INFO: Deleting pod "pod-subpath-test-configmap-k2l7" in namespace "subpath-1758"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:43:32.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1758" for this suite.

• [SLOW TEST:24.150 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":277,"completed":36,"skipped":419,"failed":0}
S
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:43:32.455: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Nov 29 01:43:36.516: INFO: &Pod{ObjectMeta:{send-events-47b67f58-cf86-4809-9d9d-dc7403a6e3d2  events-7053 /api/v1/namespaces/events-7053/pods/send-events-47b67f58-cf86-4809-9d9d-dc7403a6e3d2 f6915940-c4a8-4349-a667-a59e6df98eec 14745 0 2020-11-29 01:43:31 +0000 UTC <nil> <nil> map[name:foo time:491281493] map[cni.projectcalico.org/podIP:192.168.191.153/32 cni.projectcalico.org/podIPs:192.168.191.153/32] [] []  [{e2e.test Update v1 2020-11-29 01:43:31 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 116 105 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 112 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 114 103 115 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 114 116 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 99 111 110 116 97 105 110 101 114 80 111 114 116 92 34 58 56 48 44 92 34 112 114 111 116 111 99 111 108 92 34 58 92 34 84 67 80 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 99 111 110 116 97 105 110 101 114 80 111 114 116 34 58 123 125 44 34 102 58 112 114 111 116 111 99 111 108 34 58 123 125 125 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-11-29 01:43:33 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-11-29 01:43:33 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 57 50 46 49 54 56 46 49 57 49 46 49 53 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7mm7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7mm7t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7mm7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-bc6233c429,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 01:43:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 01:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 01:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 01:43:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.165,PodIP:192.168.191.153,StartTime:2020-11-29 01:43:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-29 01:43:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,ImageID:docker-pullable://us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c,ContainerID:docker://fe5bd1b950e29daf1976b8d36fd69dacc9fb1ef17806b236d58d772277b9d1e3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.191.153,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Nov 29 01:43:38.521: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Nov 29 01:43:40.525: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:43:40.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7053" for this suite.

• [SLOW TEST:8.090 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":277,"completed":37,"skipped":420,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:43:40.548: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-map-9e867f93-b787-4e35-b4ef-4cad3c01c02d
STEP: Creating a pod to test consume secrets
Nov 29 01:43:40.595: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-40f081ac-f8ab-47dc-9551-20128af3384c" in namespace "projected-5508" to be "Succeeded or Failed"
Nov 29 01:43:40.598: INFO: Pod "pod-projected-secrets-40f081ac-f8ab-47dc-9551-20128af3384c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.397938ms
Nov 29 01:43:42.601: INFO: Pod "pod-projected-secrets-40f081ac-f8ab-47dc-9551-20128af3384c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006390959s
Nov 29 01:43:44.607: INFO: Pod "pod-projected-secrets-40f081ac-f8ab-47dc-9551-20128af3384c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011849261s
STEP: Saw pod success
Nov 29 01:43:44.607: INFO: Pod "pod-projected-secrets-40f081ac-f8ab-47dc-9551-20128af3384c" satisfied condition "Succeeded or Failed"
Nov 29 01:43:44.611: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod pod-projected-secrets-40f081ac-f8ab-47dc-9551-20128af3384c container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 29 01:43:44.644: INFO: Waiting for pod pod-projected-secrets-40f081ac-f8ab-47dc-9551-20128af3384c to disappear
Nov 29 01:43:44.650: INFO: Pod pod-projected-secrets-40f081ac-f8ab-47dc-9551-20128af3384c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:43:44.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5508" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":38,"skipped":459,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:43:44.660: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:43:48.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6641" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":39,"skipped":472,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:43:48.746: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 29 01:43:49.500: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 29 01:43:51.509: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211028, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211028, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211028, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211028, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 01:43:54.531: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 01:43:54.534: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7013-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:43:55.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-248" for this suite.
STEP: Destroying namespace "webhook-248-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.189 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":277,"completed":40,"skipped":486,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:43:55.936: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 29 01:43:56.353: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 29 01:43:58.362: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211035, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211035, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211035, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211035, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 01:44:01.386: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 01:44:01.389: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:44:02.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4560" for this suite.
STEP: Destroying namespace "webhook-4560-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.674 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":277,"completed":41,"skipped":507,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:44:02.610: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 29 01:44:03.422: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 29 01:44:05.431: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211042, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211042, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211042, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211042, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 01:44:08.479: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 01:44:08.482: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2465-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:44:09.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2486" for this suite.
STEP: Destroying namespace "webhook-2486-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.104 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":277,"completed":42,"skipped":551,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:44:09.715: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 29 01:44:10.436: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 29 01:44:12.446: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211049, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211049, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211049, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211049, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 01:44:15.482: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:44:15.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-934" for this suite.
STEP: Destroying namespace "webhook-934-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.976 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":277,"completed":43,"skipped":604,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:44:15.692: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Nov 29 01:44:15.739: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4199 /api/v1/namespaces/watch-4199/configmaps/e2e-watch-test-configmap-a 3a42b362-df92-4794-81e7-a80c060d79c5 15375 0 2020-11-29 01:44:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-29 01:44:14 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 29 01:44:15.739: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4199 /api/v1/namespaces/watch-4199/configmaps/e2e-watch-test-configmap-a 3a42b362-df92-4794-81e7-a80c060d79c5 15375 0 2020-11-29 01:44:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-29 01:44:14 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Nov 29 01:44:25.752: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4199 /api/v1/namespaces/watch-4199/configmaps/e2e-watch-test-configmap-a 3a42b362-df92-4794-81e7-a80c060d79c5 15460 0 2020-11-29 01:44:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-29 01:44:24 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 29 01:44:25.752: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4199 /api/v1/namespaces/watch-4199/configmaps/e2e-watch-test-configmap-a 3a42b362-df92-4794-81e7-a80c060d79c5 15460 0 2020-11-29 01:44:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-29 01:44:24 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Nov 29 01:44:35.758: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4199 /api/v1/namespaces/watch-4199/configmaps/e2e-watch-test-configmap-a 3a42b362-df92-4794-81e7-a80c060d79c5 15517 0 2020-11-29 01:44:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-29 01:44:34 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 29 01:44:35.759: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4199 /api/v1/namespaces/watch-4199/configmaps/e2e-watch-test-configmap-a 3a42b362-df92-4794-81e7-a80c060d79c5 15517 0 2020-11-29 01:44:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-29 01:44:34 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Nov 29 01:44:45.765: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4199 /api/v1/namespaces/watch-4199/configmaps/e2e-watch-test-configmap-a 3a42b362-df92-4794-81e7-a80c060d79c5 15573 0 2020-11-29 01:44:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-29 01:44:34 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 29 01:44:45.765: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4199 /api/v1/namespaces/watch-4199/configmaps/e2e-watch-test-configmap-a 3a42b362-df92-4794-81e7-a80c060d79c5 15573 0 2020-11-29 01:44:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-29 01:44:34 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Nov 29 01:44:55.803: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4199 /api/v1/namespaces/watch-4199/configmaps/e2e-watch-test-configmap-b 809aa125-8f93-4972-9426-bf39cb1f2b6d 15632 0 2020-11-29 01:44:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-11-29 01:44:55 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 29 01:44:55.803: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4199 /api/v1/namespaces/watch-4199/configmaps/e2e-watch-test-configmap-b 809aa125-8f93-4972-9426-bf39cb1f2b6d 15632 0 2020-11-29 01:44:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-11-29 01:44:55 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Nov 29 01:45:05.808: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4199 /api/v1/namespaces/watch-4199/configmaps/e2e-watch-test-configmap-b 809aa125-8f93-4972-9426-bf39cb1f2b6d 15689 0 2020-11-29 01:44:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-11-29 01:44:55 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 29 01:45:05.808: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4199 /api/v1/namespaces/watch-4199/configmaps/e2e-watch-test-configmap-b 809aa125-8f93-4972-9426-bf39cb1f2b6d 15689 0 2020-11-29 01:44:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-11-29 01:44:55 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:45:15.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4199" for this suite.

• [SLOW TEST:60.124 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":277,"completed":44,"skipped":664,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:45:15.819: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Nov 29 01:45:15.854: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the sample API server.
Nov 29 01:45:16.229: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Nov 29 01:45:18.297: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211115, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211115, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211115, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211115, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 29 01:45:20.300: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211115, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211115, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211115, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211115, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 29 01:45:22.301: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211115, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211115, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211115, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211115, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 29 01:45:24.301: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211115, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211115, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211115, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211115, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 29 01:45:26.301: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211115, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211115, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211115, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211115, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 29 01:45:29.624: INFO: Waited 1.318335186s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:45:30.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-7656" for this suite.

• [SLOW TEST:14.867 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":277,"completed":45,"skipped":677,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:45:30.687: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on tmpfs
Nov 29 01:45:30.731: INFO: Waiting up to 5m0s for pod "pod-cd657067-bc17-4b5d-9685-dbe86d76ec77" in namespace "emptydir-7164" to be "Succeeded or Failed"
Nov 29 01:45:30.743: INFO: Pod "pod-cd657067-bc17-4b5d-9685-dbe86d76ec77": Phase="Pending", Reason="", readiness=false. Elapsed: 12.264916ms
Nov 29 01:45:32.747: INFO: Pod "pod-cd657067-bc17-4b5d-9685-dbe86d76ec77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016097006s
Nov 29 01:45:34.751: INFO: Pod "pod-cd657067-bc17-4b5d-9685-dbe86d76ec77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019809539s
STEP: Saw pod success
Nov 29 01:45:34.751: INFO: Pod "pod-cd657067-bc17-4b5d-9685-dbe86d76ec77" satisfied condition "Succeeded or Failed"
Nov 29 01:45:34.753: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod pod-cd657067-bc17-4b5d-9685-dbe86d76ec77 container test-container: <nil>
STEP: delete the pod
Nov 29 01:45:34.782: INFO: Waiting for pod pod-cd657067-bc17-4b5d-9685-dbe86d76ec77 to disappear
Nov 29 01:45:34.785: INFO: Pod pod-cd657067-bc17-4b5d-9685-dbe86d76ec77 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:45:34.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7164" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":46,"skipped":697,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:45:34.795: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 01:45:34.835: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:45:36.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3261" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":277,"completed":47,"skipped":707,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:45:36.101: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Nov 29 01:45:41.265: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:45:42.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9573" for this suite.

• [SLOW TEST:6.195 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":277,"completed":48,"skipped":720,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:45:42.299: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Nov 29 01:45:42.372: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9934 /api/v1/namespaces/watch-9934/configmaps/e2e-watch-test-label-changed e7468d53-b214-4685-b9ed-44ec3c03b004 16080 0 2020-11-29 01:45:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-29 01:45:41 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 29 01:45:42.372: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9934 /api/v1/namespaces/watch-9934/configmaps/e2e-watch-test-label-changed e7468d53-b214-4685-b9ed-44ec3c03b004 16081 0 2020-11-29 01:45:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-29 01:45:41 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 29 01:45:42.372: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9934 /api/v1/namespaces/watch-9934/configmaps/e2e-watch-test-label-changed e7468d53-b214-4685-b9ed-44ec3c03b004 16082 0 2020-11-29 01:45:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-29 01:45:41 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Nov 29 01:45:52.401: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9934 /api/v1/namespaces/watch-9934/configmaps/e2e-watch-test-label-changed e7468d53-b214-4685-b9ed-44ec3c03b004 16171 0 2020-11-29 01:45:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-29 01:45:51 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 29 01:45:52.401: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9934 /api/v1/namespaces/watch-9934/configmaps/e2e-watch-test-label-changed e7468d53-b214-4685-b9ed-44ec3c03b004 16172 0 2020-11-29 01:45:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-29 01:45:51 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 29 01:45:52.401: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9934 /api/v1/namespaces/watch-9934/configmaps/e2e-watch-test-label-changed e7468d53-b214-4685-b9ed-44ec3c03b004 16173 0 2020-11-29 01:45:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-29 01:45:51 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:45:52.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9934" for this suite.

• [SLOW TEST:10.120 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":277,"completed":49,"skipped":729,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:45:52.419: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:45:59.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9685" for this suite.

• [SLOW TEST:7.049 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":277,"completed":50,"skipped":749,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:45:59.469: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-2c7e5324-f3b1-446d-be4b-bae43f74692b
STEP: Creating a pod to test consume secrets
Nov 29 01:45:59.535: INFO: Waiting up to 5m0s for pod "pod-secrets-c63ad621-61c2-49ca-9f4b-1f8721630573" in namespace "secrets-5992" to be "Succeeded or Failed"
Nov 29 01:45:59.542: INFO: Pod "pod-secrets-c63ad621-61c2-49ca-9f4b-1f8721630573": Phase="Pending", Reason="", readiness=false. Elapsed: 6.776978ms
Nov 29 01:46:01.546: INFO: Pod "pod-secrets-c63ad621-61c2-49ca-9f4b-1f8721630573": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010302567s
Nov 29 01:46:03.549: INFO: Pod "pod-secrets-c63ad621-61c2-49ca-9f4b-1f8721630573": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013489796s
STEP: Saw pod success
Nov 29 01:46:03.549: INFO: Pod "pod-secrets-c63ad621-61c2-49ca-9f4b-1f8721630573" satisfied condition "Succeeded or Failed"
Nov 29 01:46:03.552: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod pod-secrets-c63ad621-61c2-49ca-9f4b-1f8721630573 container secret-env-test: <nil>
STEP: delete the pod
Nov 29 01:46:03.572: INFO: Waiting for pod pod-secrets-c63ad621-61c2-49ca-9f4b-1f8721630573 to disappear
Nov 29 01:46:03.576: INFO: Pod pod-secrets-c63ad621-61c2-49ca-9f4b-1f8721630573 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:46:03.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5992" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":277,"completed":51,"skipped":765,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:46:03.587: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test hostPath mode
Nov 29 01:46:03.632: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-7777" to be "Succeeded or Failed"
Nov 29 01:46:03.661: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 29.62288ms
Nov 29 01:46:05.666: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03414078s
Nov 29 01:46:07.670: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038142267s
STEP: Saw pod success
Nov 29 01:46:07.670: INFO: Pod "pod-host-path-test" satisfied condition "Succeeded or Failed"
Nov 29 01:46:07.674: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Nov 29 01:46:07.702: INFO: Waiting for pod pod-host-path-test to disappear
Nov 29 01:46:07.711: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:46:07.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-7777" for this suite.
•{"msg":"PASSED [sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":52,"skipped":774,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:46:07.724: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 01:46:07.759: INFO: Creating ReplicaSet my-hostname-basic-9295e75d-3c34-40ed-9b61-ddb8eef09263
Nov 29 01:46:07.768: INFO: Pod name my-hostname-basic-9295e75d-3c34-40ed-9b61-ddb8eef09263: Found 0 pods out of 1
Nov 29 01:46:12.773: INFO: Pod name my-hostname-basic-9295e75d-3c34-40ed-9b61-ddb8eef09263: Found 1 pods out of 1
Nov 29 01:46:12.773: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-9295e75d-3c34-40ed-9b61-ddb8eef09263" is running
Nov 29 01:46:12.775: INFO: Pod "my-hostname-basic-9295e75d-3c34-40ed-9b61-ddb8eef09263-mg9q8" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-29 01:46:07 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-29 01:46:09 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-29 01:46:09 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-29 01:46:07 +0000 UTC Reason: Message:}])
Nov 29 01:46:12.775: INFO: Trying to dial the pod
Nov 29 01:46:17.786: INFO: Controller my-hostname-basic-9295e75d-3c34-40ed-9b61-ddb8eef09263: Got expected result from replica 1 [my-hostname-basic-9295e75d-3c34-40ed-9b61-ddb8eef09263-mg9q8]: "my-hostname-basic-9295e75d-3c34-40ed-9b61-ddb8eef09263-mg9q8", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:46:17.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-723" for this suite.

• [SLOW TEST:10.070 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":277,"completed":53,"skipped":778,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:46:17.795: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name s-test-opt-del-97c74d62-a039-47e5-b1f0-427ef6c6a883
STEP: Creating secret with name s-test-opt-upd-c10a0e7d-5bde-45ec-93bc-ea49a40904d0
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-97c74d62-a039-47e5-b1f0-427ef6c6a883
STEP: Updating secret s-test-opt-upd-c10a0e7d-5bde-45ec-93bc-ea49a40904d0
STEP: Creating secret with name s-test-opt-create-17688108-c92c-4d9e-a4c6-bc5f2c6244be
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:46:25.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-877" for this suite.

• [SLOW TEST:8.149 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":54,"skipped":790,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:46:25.945: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-5022
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-5022
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5022
Nov 29 01:46:26.078: INFO: Found 0 stateful pods, waiting for 1
Nov 29 01:46:36.082: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Nov 29 01:46:36.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5022 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 29 01:46:37.751: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 29 01:46:37.751: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 29 01:46:37.751: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 29 01:46:37.754: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov 29 01:46:47.757: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 29 01:46:47.757: INFO: Waiting for statefulset status.replicas updated to 0
Nov 29 01:46:47.775: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999072s
Nov 29 01:46:48.779: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996006295s
Nov 29 01:46:49.783: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992510543s
Nov 29 01:46:50.786: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.988929933s
Nov 29 01:46:51.790: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.985598031s
Nov 29 01:46:52.794: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.981765106s
Nov 29 01:46:53.797: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.97780038s
Nov 29 01:46:54.801: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.974062061s
Nov 29 01:46:55.805: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.970344063s
Nov 29 01:46:56.809: INFO: Verifying statefulset ss doesn't scale past 1 for another 966.694754ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5022
Nov 29 01:46:57.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5022 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:46:58.026: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 29 01:46:58.026: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 29 01:46:58.026: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 29 01:46:58.031: INFO: Found 1 stateful pods, waiting for 3
Nov 29 01:47:08.037: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:47:08.037: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:47:08.037: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Nov 29 01:47:08.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5022 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 29 01:47:08.256: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 29 01:47:08.256: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 29 01:47:08.256: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 29 01:47:08.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5022 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 29 01:47:08.491: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 29 01:47:08.491: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 29 01:47:08.491: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 29 01:47:08.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5022 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 29 01:47:08.753: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 29 01:47:08.753: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 29 01:47:08.753: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 29 01:47:08.753: INFO: Waiting for statefulset status.replicas updated to 0
Nov 29 01:47:08.756: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Nov 29 01:47:18.761: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 29 01:47:18.761: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov 29 01:47:18.761: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov 29 01:47:18.771: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999428s
Nov 29 01:47:19.775: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996230687s
Nov 29 01:47:20.779: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.99208533s
Nov 29 01:47:21.785: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988433274s
Nov 29 01:47:22.789: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.982120199s
Nov 29 01:47:23.793: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.978035286s
Nov 29 01:47:24.798: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.974097476s
Nov 29 01:47:25.802: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.96933325s
Nov 29 01:47:26.805: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.965732654s
Nov 29 01:47:27.809: INFO: Verifying statefulset ss doesn't scale past 3 for another 962.268194ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5022
Nov 29 01:47:28.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5022 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:47:29.016: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 29 01:47:29.016: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 29 01:47:29.016: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 29 01:47:29.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5022 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:47:29.240: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 29 01:47:29.240: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 29 01:47:29.240: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 29 01:47:29.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-5022 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 01:47:29.475: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 29 01:47:29.475: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 29 01:47:29.475: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 29 01:47:29.475: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Nov 29 01:48:09.557: INFO: Deleting all statefulset in ns statefulset-5022
Nov 29 01:48:09.560: INFO: Scaling statefulset ss to 0
Nov 29 01:48:09.570: INFO: Waiting for statefulset status.replicas updated to 0
Nov 29 01:48:09.572: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:48:09.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5022" for this suite.

• [SLOW TEST:103.651 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":277,"completed":55,"skipped":814,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:48:09.597: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Nov 29 01:48:09.673: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ce7a52ee-6fa6-46ea-92e1-992974b48f29" in namespace "downward-api-2004" to be "Succeeded or Failed"
Nov 29 01:48:09.685: INFO: Pod "downwardapi-volume-ce7a52ee-6fa6-46ea-92e1-992974b48f29": Phase="Pending", Reason="", readiness=false. Elapsed: 11.47805ms
Nov 29 01:48:11.688: INFO: Pod "downwardapi-volume-ce7a52ee-6fa6-46ea-92e1-992974b48f29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014341632s
Nov 29 01:48:13.692: INFO: Pod "downwardapi-volume-ce7a52ee-6fa6-46ea-92e1-992974b48f29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018124558s
STEP: Saw pod success
Nov 29 01:48:13.692: INFO: Pod "downwardapi-volume-ce7a52ee-6fa6-46ea-92e1-992974b48f29" satisfied condition "Succeeded or Failed"
Nov 29 01:48:13.694: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod downwardapi-volume-ce7a52ee-6fa6-46ea-92e1-992974b48f29 container client-container: <nil>
STEP: delete the pod
Nov 29 01:48:13.716: INFO: Waiting for pod downwardapi-volume-ce7a52ee-6fa6-46ea-92e1-992974b48f29 to disappear
Nov 29 01:48:13.719: INFO: Pod downwardapi-volume-ce7a52ee-6fa6-46ea-92e1-992974b48f29 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:48:13.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2004" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":56,"skipped":816,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:48:13.727: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-downwardapi-c8xf
STEP: Creating a pod to test atomic-volume-subpath
Nov 29 01:48:13.800: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-c8xf" in namespace "subpath-5856" to be "Succeeded or Failed"
Nov 29 01:48:13.807: INFO: Pod "pod-subpath-test-downwardapi-c8xf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.280733ms
Nov 29 01:48:15.827: INFO: Pod "pod-subpath-test-downwardapi-c8xf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026375954s
Nov 29 01:48:17.831: INFO: Pod "pod-subpath-test-downwardapi-c8xf": Phase="Running", Reason="", readiness=true. Elapsed: 4.030730527s
Nov 29 01:48:19.835: INFO: Pod "pod-subpath-test-downwardapi-c8xf": Phase="Running", Reason="", readiness=true. Elapsed: 6.034271062s
Nov 29 01:48:21.838: INFO: Pod "pod-subpath-test-downwardapi-c8xf": Phase="Running", Reason="", readiness=true. Elapsed: 8.038090515s
Nov 29 01:48:23.841: INFO: Pod "pod-subpath-test-downwardapi-c8xf": Phase="Running", Reason="", readiness=true. Elapsed: 10.040915966s
Nov 29 01:48:25.845: INFO: Pod "pod-subpath-test-downwardapi-c8xf": Phase="Running", Reason="", readiness=true. Elapsed: 12.044295097s
Nov 29 01:48:27.848: INFO: Pod "pod-subpath-test-downwardapi-c8xf": Phase="Running", Reason="", readiness=true. Elapsed: 14.047816572s
Nov 29 01:48:29.851: INFO: Pod "pod-subpath-test-downwardapi-c8xf": Phase="Running", Reason="", readiness=true. Elapsed: 16.050897401s
Nov 29 01:48:31.855: INFO: Pod "pod-subpath-test-downwardapi-c8xf": Phase="Running", Reason="", readiness=true. Elapsed: 18.054301421s
Nov 29 01:48:33.858: INFO: Pod "pod-subpath-test-downwardapi-c8xf": Phase="Running", Reason="", readiness=true. Elapsed: 20.057606555s
Nov 29 01:48:35.861: INFO: Pod "pod-subpath-test-downwardapi-c8xf": Phase="Running", Reason="", readiness=true. Elapsed: 22.061074188s
Nov 29 01:48:37.865: INFO: Pod "pod-subpath-test-downwardapi-c8xf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.064805662s
STEP: Saw pod success
Nov 29 01:48:37.865: INFO: Pod "pod-subpath-test-downwardapi-c8xf" satisfied condition "Succeeded or Failed"
Nov 29 01:48:37.868: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod pod-subpath-test-downwardapi-c8xf container test-container-subpath-downwardapi-c8xf: <nil>
STEP: delete the pod
Nov 29 01:48:37.910: INFO: Waiting for pod pod-subpath-test-downwardapi-c8xf to disappear
Nov 29 01:48:37.911: INFO: Pod pod-subpath-test-downwardapi-c8xf no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-c8xf
Nov 29 01:48:37.911: INFO: Deleting pod "pod-subpath-test-downwardapi-c8xf" in namespace "subpath-5856"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:48:37.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5856" for this suite.

• [SLOW TEST:24.194 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":277,"completed":57,"skipped":839,"failed":0}
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:48:37.922: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Nov 29 01:48:38.744: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Nov 29 01:48:40.752: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211317, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211317, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211318, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211317, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-65c6cd5fdf\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 01:48:43.790: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 01:48:43.793: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:48:44.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5216" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.053 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":277,"completed":58,"skipped":839,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:48:44.975: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:48:49.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9294" for this suite.

• [SLOW TEST:5.121 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":277,"completed":59,"skipped":856,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:48:50.096: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Nov 29 01:48:50.134: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Nov 29 01:48:50.142: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Nov 29 01:48:50.143: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Nov 29 01:48:50.157: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Nov 29 01:48:50.157: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Nov 29 01:48:50.181: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Nov 29 01:48:50.181: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Nov 29 01:48:57.230: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:48:57.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-2967" for this suite.

• [SLOW TEST:7.162 seconds]
[sig-scheduling] LimitRange
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":277,"completed":60,"skipped":868,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:48:57.259: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service externalname-service with the type=ExternalName in namespace services-39
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-39
I1129 01:48:57.356027      22 runners.go:190] Created replication controller with name: externalname-service, namespace: services-39, replica count: 2
I1129 01:49:00.406502      22 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 29 01:49:00.406: INFO: Creating new exec pod
Nov 29 01:49:05.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=services-39 execpod46hmt -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Nov 29 01:49:05.675: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Nov 29 01:49:05.675: INFO: stdout: ""
Nov 29 01:49:05.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=services-39 execpod46hmt -- /bin/sh -x -c nc -zv -t -w 2 10.107.53.249 80'
Nov 29 01:49:05.902: INFO: stderr: "+ nc -zv -t -w 2 10.107.53.249 80\nConnection to 10.107.53.249 80 port [tcp/http] succeeded!\n"
Nov 29 01:49:05.902: INFO: stdout: ""
Nov 29 01:49:05.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=services-39 execpod46hmt -- /bin/sh -x -c nc -zv -t -w 2 10.10.103.165 32278'
Nov 29 01:49:06.127: INFO: stderr: "+ nc -zv -t -w 2 10.10.103.165 32278\nConnection to 10.10.103.165 32278 port [tcp/32278] succeeded!\n"
Nov 29 01:49:06.127: INFO: stdout: ""
Nov 29 01:49:06.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=services-39 execpod46hmt -- /bin/sh -x -c nc -zv -t -w 2 10.10.103.166 32278'
Nov 29 01:49:06.353: INFO: stderr: "+ nc -zv -t -w 2 10.10.103.166 32278\nConnection to 10.10.103.166 32278 port [tcp/32278] succeeded!\n"
Nov 29 01:49:06.353: INFO: stdout: ""
Nov 29 01:49:06.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=services-39 execpod46hmt -- /bin/sh -x -c nc -zv -t -w 2 10.10.103.165 32278'
Nov 29 01:49:06.567: INFO: stderr: "+ nc -zv -t -w 2 10.10.103.165 32278\nConnection to 10.10.103.165 32278 port [tcp/32278] succeeded!\n"
Nov 29 01:49:06.567: INFO: stdout: ""
Nov 29 01:49:06.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=services-39 execpod46hmt -- /bin/sh -x -c nc -zv -t -w 2 10.10.103.166 32278'
Nov 29 01:49:06.799: INFO: stderr: "+ nc -zv -t -w 2 10.10.103.166 32278\nConnection to 10.10.103.166 32278 port [tcp/32278] succeeded!\n"
Nov 29 01:49:06.799: INFO: stdout: ""
Nov 29 01:49:06.799: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:49:06.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-39" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:9.588 seconds]
[sig-network] Services
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":277,"completed":61,"skipped":905,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:49:06.847: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Nov 29 01:49:06.884: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9a329d52-cdcf-4ad6-be8b-b7557b480b2e" in namespace "projected-8221" to be "Succeeded or Failed"
Nov 29 01:49:06.887: INFO: Pod "downwardapi-volume-9a329d52-cdcf-4ad6-be8b-b7557b480b2e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.170956ms
Nov 29 01:49:08.892: INFO: Pod "downwardapi-volume-9a329d52-cdcf-4ad6-be8b-b7557b480b2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008080361s
Nov 29 01:49:10.895: INFO: Pod "downwardapi-volume-9a329d52-cdcf-4ad6-be8b-b7557b480b2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011092442s
STEP: Saw pod success
Nov 29 01:49:10.895: INFO: Pod "downwardapi-volume-9a329d52-cdcf-4ad6-be8b-b7557b480b2e" satisfied condition "Succeeded or Failed"
Nov 29 01:49:10.898: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod downwardapi-volume-9a329d52-cdcf-4ad6-be8b-b7557b480b2e container client-container: <nil>
STEP: delete the pod
Nov 29 01:49:10.922: INFO: Waiting for pod downwardapi-volume-9a329d52-cdcf-4ad6-be8b-b7557b480b2e to disappear
Nov 29 01:49:10.930: INFO: Pod downwardapi-volume-9a329d52-cdcf-4ad6-be8b-b7557b480b2e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:49:10.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8221" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":277,"completed":62,"skipped":917,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:49:10.939: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on tmpfs
Nov 29 01:49:10.980: INFO: Waiting up to 5m0s for pod "pod-cd5a317d-003c-41ef-b9f7-9b2319b2019e" in namespace "emptydir-7747" to be "Succeeded or Failed"
Nov 29 01:49:10.987: INFO: Pod "pod-cd5a317d-003c-41ef-b9f7-9b2319b2019e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.018957ms
Nov 29 01:49:12.991: INFO: Pod "pod-cd5a317d-003c-41ef-b9f7-9b2319b2019e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010883955s
Nov 29 01:49:14.997: INFO: Pod "pod-cd5a317d-003c-41ef-b9f7-9b2319b2019e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016931028s
STEP: Saw pod success
Nov 29 01:49:14.997: INFO: Pod "pod-cd5a317d-003c-41ef-b9f7-9b2319b2019e" satisfied condition "Succeeded or Failed"
Nov 29 01:49:14.999: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod pod-cd5a317d-003c-41ef-b9f7-9b2319b2019e container test-container: <nil>
STEP: delete the pod
Nov 29 01:49:15.022: INFO: Waiting for pod pod-cd5a317d-003c-41ef-b9f7-9b2319b2019e to disappear
Nov 29 01:49:15.031: INFO: Pod pod-cd5a317d-003c-41ef-b9f7-9b2319b2019e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:49:15.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7747" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":63,"skipped":930,"failed":0}
SSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:49:15.044: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 29 01:49:18.098: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:49:18.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6588" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":277,"completed":64,"skipped":933,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:49:18.134: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:157
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:49:18.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2190" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":277,"completed":65,"skipped":953,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:49:18.217: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Nov 29 01:49:22.790: INFO: Successfully updated pod "adopt-release-ksdq4"
STEP: Checking that the Job readopts the Pod
Nov 29 01:49:22.790: INFO: Waiting up to 15m0s for pod "adopt-release-ksdq4" in namespace "job-2183" to be "adopted"
Nov 29 01:49:22.802: INFO: Pod "adopt-release-ksdq4": Phase="Running", Reason="", readiness=true. Elapsed: 12.194462ms
Nov 29 01:49:24.807: INFO: Pod "adopt-release-ksdq4": Phase="Running", Reason="", readiness=true. Elapsed: 2.017843688s
Nov 29 01:49:24.808: INFO: Pod "adopt-release-ksdq4" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Nov 29 01:49:25.315: INFO: Successfully updated pod "adopt-release-ksdq4"
STEP: Checking that the Job releases the Pod
Nov 29 01:49:25.315: INFO: Waiting up to 15m0s for pod "adopt-release-ksdq4" in namespace "job-2183" to be "released"
Nov 29 01:49:25.321: INFO: Pod "adopt-release-ksdq4": Phase="Running", Reason="", readiness=true. Elapsed: 5.414903ms
Nov 29 01:49:27.324: INFO: Pod "adopt-release-ksdq4": Phase="Running", Reason="", readiness=true. Elapsed: 2.00866017s
Nov 29 01:49:27.324: INFO: Pod "adopt-release-ksdq4" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:49:27.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2183" for this suite.

• [SLOW TEST:9.116 seconds]
[sig-apps] Job
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":277,"completed":66,"skipped":972,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:49:27.334: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap configmap-2771/configmap-test-7cb83371-8d1e-429b-a285-c10da5aea04a
STEP: Creating a pod to test consume configMaps
Nov 29 01:49:27.368: INFO: Waiting up to 5m0s for pod "pod-configmaps-c6a2f965-bfa5-405a-a1df-908ce300fd1e" in namespace "configmap-2771" to be "Succeeded or Failed"
Nov 29 01:49:27.372: INFO: Pod "pod-configmaps-c6a2f965-bfa5-405a-a1df-908ce300fd1e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.609939ms
Nov 29 01:49:29.383: INFO: Pod "pod-configmaps-c6a2f965-bfa5-405a-a1df-908ce300fd1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014357478s
Nov 29 01:49:31.386: INFO: Pod "pod-configmaps-c6a2f965-bfa5-405a-a1df-908ce300fd1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018184779s
STEP: Saw pod success
Nov 29 01:49:31.388: INFO: Pod "pod-configmaps-c6a2f965-bfa5-405a-a1df-908ce300fd1e" satisfied condition "Succeeded or Failed"
Nov 29 01:49:31.390: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod pod-configmaps-c6a2f965-bfa5-405a-a1df-908ce300fd1e container env-test: <nil>
STEP: delete the pod
Nov 29 01:49:31.413: INFO: Waiting for pod pod-configmaps-c6a2f965-bfa5-405a-a1df-908ce300fd1e to disappear
Nov 29 01:49:31.428: INFO: Pod pod-configmaps-c6a2f965-bfa5-405a-a1df-908ce300fd1e no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:49:31.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2771" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":277,"completed":67,"skipped":1037,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:49:31.436: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Nov 29 01:49:39.593: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 29 01:49:39.597: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 29 01:49:41.597: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 29 01:49:41.600: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 29 01:49:43.597: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 29 01:49:43.600: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:49:43.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7639" for this suite.

• [SLOW TEST:12.173 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":277,"completed":68,"skipped":1054,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:49:43.610: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod busybox-2e641298-f70f-4222-aa7e-54954c92327b in namespace container-probe-6494
Nov 29 01:49:47.661: INFO: Started pod busybox-2e641298-f70f-4222-aa7e-54954c92327b in namespace container-probe-6494
STEP: checking the pod's current state and verifying that restartCount is present
Nov 29 01:49:47.664: INFO: Initial restart count of pod busybox-2e641298-f70f-4222-aa7e-54954c92327b is 0
Nov 29 01:50:35.795: INFO: Restart count of pod container-probe-6494/busybox-2e641298-f70f-4222-aa7e-54954c92327b is now 1 (48.131370374s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:50:35.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6494" for this suite.

• [SLOW TEST:52.216 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":277,"completed":69,"skipped":1113,"failed":0}
SS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:50:35.827: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating secret secrets-729/secret-test-9c9cb433-1a61-4f1e-9fda-445133075751
STEP: Creating a pod to test consume secrets
Nov 29 01:50:35.876: INFO: Waiting up to 5m0s for pod "pod-configmaps-f27ec3f7-a837-4310-9e74-2e311dffbd9d" in namespace "secrets-729" to be "Succeeded or Failed"
Nov 29 01:50:35.884: INFO: Pod "pod-configmaps-f27ec3f7-a837-4310-9e74-2e311dffbd9d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.683226ms
Nov 29 01:50:37.888: INFO: Pod "pod-configmaps-f27ec3f7-a837-4310-9e74-2e311dffbd9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011514858s
Nov 29 01:50:39.891: INFO: Pod "pod-configmaps-f27ec3f7-a837-4310-9e74-2e311dffbd9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014739709s
STEP: Saw pod success
Nov 29 01:50:39.891: INFO: Pod "pod-configmaps-f27ec3f7-a837-4310-9e74-2e311dffbd9d" satisfied condition "Succeeded or Failed"
Nov 29 01:50:39.893: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod pod-configmaps-f27ec3f7-a837-4310-9e74-2e311dffbd9d container env-test: <nil>
STEP: delete the pod
Nov 29 01:50:39.916: INFO: Waiting for pod pod-configmaps-f27ec3f7-a837-4310-9e74-2e311dffbd9d to disappear
Nov 29 01:50:39.922: INFO: Pod pod-configmaps-f27ec3f7-a837-4310-9e74-2e311dffbd9d no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:50:39.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-729" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":277,"completed":70,"skipped":1115,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:50:39.932: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:50:39.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3655" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":277,"completed":71,"skipped":1128,"failed":0}

------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:50:39.980: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-39e68fa0-38ba-45ca-ab15-869a88f9d92e
STEP: Creating a pod to test consume configMaps
Nov 29 01:50:40.064: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4b403051-6e1c-4e9d-9b19-153268675920" in namespace "projected-2400" to be "Succeeded or Failed"
Nov 29 01:50:40.068: INFO: Pod "pod-projected-configmaps-4b403051-6e1c-4e9d-9b19-153268675920": Phase="Pending", Reason="", readiness=false. Elapsed: 3.785028ms
Nov 29 01:50:42.071: INFO: Pod "pod-projected-configmaps-4b403051-6e1c-4e9d-9b19-153268675920": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006873727s
Nov 29 01:50:44.075: INFO: Pod "pod-projected-configmaps-4b403051-6e1c-4e9d-9b19-153268675920": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01042181s
STEP: Saw pod success
Nov 29 01:50:44.075: INFO: Pod "pod-projected-configmaps-4b403051-6e1c-4e9d-9b19-153268675920" satisfied condition "Succeeded or Failed"
Nov 29 01:50:44.078: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod pod-projected-configmaps-4b403051-6e1c-4e9d-9b19-153268675920 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 01:50:44.110: INFO: Waiting for pod pod-projected-configmaps-4b403051-6e1c-4e9d-9b19-153268675920 to disappear
Nov 29 01:50:44.117: INFO: Pod pod-projected-configmaps-4b403051-6e1c-4e9d-9b19-153268675920 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:50:44.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2400" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":277,"completed":72,"skipped":1128,"failed":0}
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:50:44.126: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:50:59.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1079" for this suite.
STEP: Destroying namespace "nsdeletetest-9012" for this suite.
Nov 29 01:50:59.334: INFO: Namespace nsdeletetest-9012 was already deleted
STEP: Destroying namespace "nsdeletetest-7052" for this suite.

• [SLOW TEST:15.219 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":277,"completed":73,"skipped":1131,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:50:59.349: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:50:59.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2915" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":277,"completed":74,"skipped":1165,"failed":0}

------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:50:59.464: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap that has name configmap-test-emptyKey-932dd652-c141-40f8-b39d-9c91aee54956
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:50:59.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6224" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":277,"completed":75,"skipped":1165,"failed":0}
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:50:59.526: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Nov 29 01:50:59.589: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 29 01:50:59.631: INFO: Waiting for terminating namespaces to be deleted...
Nov 29 01:50:59.644: INFO: 
Logging pods the kubelet thinks is on node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d before test
Nov 29 01:50:59.671: INFO: calico-node-gs8lr from kube-system started at 2020-11-29 01:13:01 +0000 UTC (1 container statuses recorded)
Nov 29 01:50:59.672: INFO: 	Container calico-node ready: true, restart count 0
Nov 29 01:50:59.672: INFO: nvidia-device-plugin-daemonset-f79b4 from kube-system started at 2020-11-29 01:13:11 +0000 UTC (1 container statuses recorded)
Nov 29 01:50:59.672: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Nov 29 01:50:59.672: INFO: ingress-nginx-controller-zd8lx from ccp started at 2020-11-29 01:13:11 +0000 UTC (1 container statuses recorded)
Nov 29 01:50:59.672: INFO: 	Container controller ready: true, restart count 0
Nov 29 01:50:59.672: INFO: sonobuoy-e2e-job-df1d2fd3866f4821 from sonobuoy started at 2020-11-29 01:15:14 +0000 UTC (2 container statuses recorded)
Nov 29 01:50:59.672: INFO: 	Container e2e ready: true, restart count 0
Nov 29 01:50:59.672: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 29 01:50:59.672: INFO: sonobuoy-systemd-logs-daemon-set-a18500f604534dc2-g2xkd from sonobuoy started at 2020-11-29 01:15:14 +0000 UTC (2 container statuses recorded)
Nov 29 01:50:59.672: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 29 01:50:59.672: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 29 01:50:59.672: INFO: kube-proxy-zjcgv from kube-system started at 2020-11-29 01:13:00 +0000 UTC (1 container statuses recorded)
Nov 29 01:50:59.672: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 29 01:50:59.672: INFO: metallb-speaker-5sjng from ccp started at 2020-11-29 01:13:11 +0000 UTC (1 container statuses recorded)
Nov 29 01:50:59.672: INFO: 	Container metallb-speaker ready: true, restart count 0
Nov 29 01:50:59.672: INFO: sonobuoy from sonobuoy started at 2020-11-29 01:14:36 +0000 UTC (1 container statuses recorded)
Nov 29 01:50:59.672: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 29 01:50:59.672: INFO: 
Logging pods the kubelet thinks is on node alex-cp1516-v3-vsp2-node-group-bc6233c429 before test
Nov 29 01:50:59.683: INFO: ingress-nginx-controller-vppmg from ccp started at 2020-11-29 01:13:08 +0000 UTC (1 container statuses recorded)
Nov 29 01:50:59.683: INFO: 	Container controller ready: true, restart count 0
Nov 29 01:50:59.684: INFO: calico-node-xhkv2 from kube-system started at 2020-11-29 01:12:58 +0000 UTC (1 container statuses recorded)
Nov 29 01:50:59.684: INFO: 	Container calico-node ready: true, restart count 0
Nov 29 01:50:59.684: INFO: nvidia-device-plugin-daemonset-chhlf from kube-system started at 2020-11-29 01:13:08 +0000 UTC (1 container statuses recorded)
Nov 29 01:50:59.684: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Nov 29 01:50:59.684: INFO: metallb-speaker-55nsq from ccp started at 2020-11-29 01:13:08 +0000 UTC (1 container statuses recorded)
Nov 29 01:50:59.684: INFO: 	Container metallb-speaker ready: true, restart count 0
Nov 29 01:50:59.684: INFO: kube-proxy-wcdhr from kube-system started at 2020-11-29 01:12:58 +0000 UTC (1 container statuses recorded)
Nov 29 01:50:59.684: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 29 01:50:59.684: INFO: metallb-controller-dd895cddd-v9jk5 from ccp started at 2020-11-29 01:13:08 +0000 UTC (1 container statuses recorded)
Nov 29 01:50:59.684: INFO: 	Container metallb-controller ready: true, restart count 0
Nov 29 01:50:59.684: INFO: sonobuoy-systemd-logs-daemon-set-a18500f604534dc2-vsjhg from sonobuoy started at 2020-11-29 01:15:14 +0000 UTC (2 container statuses recorded)
Nov 29 01:50:59.684: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 29 01:50:59.685: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 29 01:50:59.685: INFO: 
Logging pods the kubelet thinks is on node alex-cp1516-v3-vsp2-node-group-ea4104c57e before test
Nov 29 01:50:59.693: INFO: nvidia-device-plugin-daemonset-7r662 from kube-system started at 2020-11-29 01:13:10 +0000 UTC (1 container statuses recorded)
Nov 29 01:50:59.693: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Nov 29 01:50:59.693: INFO: ingress-nginx-defaultbackend-6cd8f668c8-6mw8w from ccp started at 2020-11-29 01:13:14 +0000 UTC (1 container statuses recorded)
Nov 29 01:50:59.693: INFO: 	Container ingress-nginx-default-backend ready: true, restart count 0
Nov 29 01:50:59.693: INFO: ingress-nginx-controller-rpdcv from ccp started at 2020-11-29 01:13:10 +0000 UTC (1 container statuses recorded)
Nov 29 01:50:59.693: INFO: 	Container controller ready: true, restart count 0
Nov 29 01:50:59.693: INFO: sonobuoy-systemd-logs-daemon-set-a18500f604534dc2-449j5 from sonobuoy started at 2020-11-29 01:15:13 +0000 UTC (2 container statuses recorded)
Nov 29 01:50:59.693: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 29 01:50:59.693: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 29 01:50:59.693: INFO: calico-node-c7rxt from kube-system started at 2020-11-29 01:13:00 +0000 UTC (1 container statuses recorded)
Nov 29 01:50:59.693: INFO: 	Container calico-node ready: true, restart count 0
Nov 29 01:50:59.693: INFO: kube-proxy-xsm2v from kube-system started at 2020-11-29 01:13:00 +0000 UTC (1 container statuses recorded)
Nov 29 01:50:59.693: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 29 01:50:59.693: INFO: metallb-speaker-vlg6h from ccp started at 2020-11-29 01:13:10 +0000 UTC (1 container statuses recorded)
Nov 29 01:50:59.693: INFO: 	Container metallb-speaker ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: verifying the node has the label node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d
STEP: verifying the node has the label node alex-cp1516-v3-vsp2-node-group-bc6233c429
STEP: verifying the node has the label node alex-cp1516-v3-vsp2-node-group-ea4104c57e
Nov 29 01:50:59.773: INFO: Pod ingress-nginx-controller-rpdcv requesting resource cpu=100m on Node alex-cp1516-v3-vsp2-node-group-ea4104c57e
Nov 29 01:50:59.774: INFO: Pod ingress-nginx-controller-vppmg requesting resource cpu=100m on Node alex-cp1516-v3-vsp2-node-group-bc6233c429
Nov 29 01:50:59.774: INFO: Pod ingress-nginx-controller-zd8lx requesting resource cpu=100m on Node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d
Nov 29 01:50:59.774: INFO: Pod ingress-nginx-defaultbackend-6cd8f668c8-6mw8w requesting resource cpu=0m on Node alex-cp1516-v3-vsp2-node-group-ea4104c57e
Nov 29 01:50:59.774: INFO: Pod metallb-controller-dd895cddd-v9jk5 requesting resource cpu=100m on Node alex-cp1516-v3-vsp2-node-group-bc6233c429
Nov 29 01:50:59.774: INFO: Pod metallb-speaker-55nsq requesting resource cpu=100m on Node alex-cp1516-v3-vsp2-node-group-bc6233c429
Nov 29 01:50:59.774: INFO: Pod metallb-speaker-5sjng requesting resource cpu=100m on Node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d
Nov 29 01:50:59.774: INFO: Pod metallb-speaker-vlg6h requesting resource cpu=100m on Node alex-cp1516-v3-vsp2-node-group-ea4104c57e
Nov 29 01:50:59.774: INFO: Pod calico-node-c7rxt requesting resource cpu=250m on Node alex-cp1516-v3-vsp2-node-group-ea4104c57e
Nov 29 01:50:59.774: INFO: Pod calico-node-gs8lr requesting resource cpu=250m on Node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d
Nov 29 01:50:59.774: INFO: Pod calico-node-xhkv2 requesting resource cpu=250m on Node alex-cp1516-v3-vsp2-node-group-bc6233c429
Nov 29 01:50:59.775: INFO: Pod kube-proxy-wcdhr requesting resource cpu=0m on Node alex-cp1516-v3-vsp2-node-group-bc6233c429
Nov 29 01:50:59.775: INFO: Pod kube-proxy-xsm2v requesting resource cpu=0m on Node alex-cp1516-v3-vsp2-node-group-ea4104c57e
Nov 29 01:50:59.775: INFO: Pod kube-proxy-zjcgv requesting resource cpu=0m on Node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d
Nov 29 01:50:59.775: INFO: Pod nvidia-device-plugin-daemonset-7r662 requesting resource cpu=0m on Node alex-cp1516-v3-vsp2-node-group-ea4104c57e
Nov 29 01:50:59.775: INFO: Pod nvidia-device-plugin-daemonset-chhlf requesting resource cpu=0m on Node alex-cp1516-v3-vsp2-node-group-bc6233c429
Nov 29 01:50:59.775: INFO: Pod nvidia-device-plugin-daemonset-f79b4 requesting resource cpu=0m on Node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d
Nov 29 01:50:59.775: INFO: Pod sonobuoy requesting resource cpu=0m on Node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d
Nov 29 01:50:59.775: INFO: Pod sonobuoy-e2e-job-df1d2fd3866f4821 requesting resource cpu=0m on Node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d
Nov 29 01:50:59.775: INFO: Pod sonobuoy-systemd-logs-daemon-set-a18500f604534dc2-449j5 requesting resource cpu=0m on Node alex-cp1516-v3-vsp2-node-group-ea4104c57e
Nov 29 01:50:59.776: INFO: Pod sonobuoy-systemd-logs-daemon-set-a18500f604534dc2-g2xkd requesting resource cpu=0m on Node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d
Nov 29 01:50:59.776: INFO: Pod sonobuoy-systemd-logs-daemon-set-a18500f604534dc2-vsjhg requesting resource cpu=0m on Node alex-cp1516-v3-vsp2-node-group-bc6233c429
STEP: Starting Pods to consume most of the cluster CPU.
Nov 29 01:50:59.776: INFO: Creating a pod which consumes cpu=2485m on Node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d
Nov 29 01:50:59.787: INFO: Creating a pod which consumes cpu=2415m on Node alex-cp1516-v3-vsp2-node-group-bc6233c429
Nov 29 01:50:59.803: INFO: Creating a pod which consumes cpu=2485m on Node alex-cp1516-v3-vsp2-node-group-ea4104c57e
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0153bc4a-815b-44be-93cf-b2d169c2d4a9.164bd7852c386f57], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6286/filler-pod-0153bc4a-815b-44be-93cf-b2d169c2d4a9 to alex-cp1516-v3-vsp2-node-group-bc6233c429]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0153bc4a-815b-44be-93cf-b2d169c2d4a9.164bd785c2409849], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0153bc4a-815b-44be-93cf-b2d169c2d4a9.164bd785ce69d896], Reason = [Created], Message = [Created container filler-pod-0153bc4a-815b-44be-93cf-b2d169c2d4a9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0153bc4a-815b-44be-93cf-b2d169c2d4a9.164bd785db7957a6], Reason = [Started], Message = [Started container filler-pod-0153bc4a-815b-44be-93cf-b2d169c2d4a9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a1814136-7406-481b-8a96-ed15add33f15.164bd7852be663d3], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6286/filler-pod-a1814136-7406-481b-8a96-ed15add33f15 to alex-cp1516-v3-vsp2-node-group-55fe8f9f1d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a1814136-7406-481b-8a96-ed15add33f15.164bd785a95f1c42], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.2"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a1814136-7406-481b-8a96-ed15add33f15.164bd785d3e294f1], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.2"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a1814136-7406-481b-8a96-ed15add33f15.164bd785de2a910d], Reason = [Created], Message = [Created container filler-pod-a1814136-7406-481b-8a96-ed15add33f15]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a1814136-7406-481b-8a96-ed15add33f15.164bd785eb647760], Reason = [Started], Message = [Started container filler-pod-a1814136-7406-481b-8a96-ed15add33f15]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c1aeee0e-0472-4c3a-9c42-f58196dc28e6.164bd7852d41ee67], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6286/filler-pod-c1aeee0e-0472-4c3a-9c42-f58196dc28e6 to alex-cp1516-v3-vsp2-node-group-ea4104c57e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c1aeee0e-0472-4c3a-9c42-f58196dc28e6.164bd785a338ddd6], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c1aeee0e-0472-4c3a-9c42-f58196dc28e6.164bd785aea0de4e], Reason = [Created], Message = [Created container filler-pod-c1aeee0e-0472-4c3a-9c42-f58196dc28e6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c1aeee0e-0472-4c3a-9c42-f58196dc28e6.164bd785bb45ea59], Reason = [Started], Message = [Started container filler-pod-c1aeee0e-0472-4c3a-9c42-f58196dc28e6]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.164bd7861d6ac6c1], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 3 Insufficient cpu.]
STEP: removing the label node off the node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node alex-cp1516-v3-vsp2-node-group-bc6233c429
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node alex-cp1516-v3-vsp2-node-group-ea4104c57e
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:51:04.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6286" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

• [SLOW TEST:5.412 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":277,"completed":76,"skipped":1173,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:51:04.938: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on tmpfs
Nov 29 01:51:04.995: INFO: Waiting up to 5m0s for pod "pod-0bab0058-9c1b-43fc-b71a-0cc44350f492" in namespace "emptydir-2314" to be "Succeeded or Failed"
Nov 29 01:51:05.007: INFO: Pod "pod-0bab0058-9c1b-43fc-b71a-0cc44350f492": Phase="Pending", Reason="", readiness=false. Elapsed: 12.245783ms
Nov 29 01:51:07.010: INFO: Pod "pod-0bab0058-9c1b-43fc-b71a-0cc44350f492": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015755818s
Nov 29 01:51:09.013: INFO: Pod "pod-0bab0058-9c1b-43fc-b71a-0cc44350f492": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01888915s
STEP: Saw pod success
Nov 29 01:51:09.013: INFO: Pod "pod-0bab0058-9c1b-43fc-b71a-0cc44350f492" satisfied condition "Succeeded or Failed"
Nov 29 01:51:09.016: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod pod-0bab0058-9c1b-43fc-b71a-0cc44350f492 container test-container: <nil>
STEP: delete the pod
Nov 29 01:51:09.038: INFO: Waiting for pod pod-0bab0058-9c1b-43fc-b71a-0cc44350f492 to disappear
Nov 29 01:51:09.065: INFO: Pod pod-0bab0058-9c1b-43fc-b71a-0cc44350f492 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:51:09.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2314" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":77,"skipped":1185,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:51:09.077: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: starting the proxy server
Nov 29 01:51:09.133: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-195255472 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:51:09.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3662" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":277,"completed":78,"skipped":1192,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:51:09.221: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 29 01:51:09.770: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 29 01:51:11.778: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211469, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211469, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211469, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211469, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 01:51:14.797: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:51:14.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3193" for this suite.
STEP: Destroying namespace "webhook-3193-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.719 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":277,"completed":79,"skipped":1248,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:51:14.941: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 01:51:14.976: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-84e8ec6a-7162-40a1-aaa2-a2da498dea73" in namespace "security-context-test-1435" to be "Succeeded or Failed"
Nov 29 01:51:14.978: INFO: Pod "busybox-privileged-false-84e8ec6a-7162-40a1-aaa2-a2da498dea73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021184ms
Nov 29 01:51:16.981: INFO: Pod "busybox-privileged-false-84e8ec6a-7162-40a1-aaa2-a2da498dea73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005299104s
Nov 29 01:51:18.991: INFO: Pod "busybox-privileged-false-84e8ec6a-7162-40a1-aaa2-a2da498dea73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015050424s
Nov 29 01:51:18.991: INFO: Pod "busybox-privileged-false-84e8ec6a-7162-40a1-aaa2-a2da498dea73" satisfied condition "Succeeded or Failed"
Nov 29 01:51:19.010: INFO: Got logs for pod "busybox-privileged-false-84e8ec6a-7162-40a1-aaa2-a2da498dea73": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:51:19.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1435" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":80,"skipped":1309,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:51:19.021: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Nov 29 01:51:19.066: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cfd0c570-77a8-4031-b090-42158d460d01" in namespace "downward-api-6761" to be "Succeeded or Failed"
Nov 29 01:51:19.101: INFO: Pod "downwardapi-volume-cfd0c570-77a8-4031-b090-42158d460d01": Phase="Pending", Reason="", readiness=false. Elapsed: 35.575359ms
Nov 29 01:51:21.117: INFO: Pod "downwardapi-volume-cfd0c570-77a8-4031-b090-42158d460d01": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050799135s
Nov 29 01:51:23.120: INFO: Pod "downwardapi-volume-cfd0c570-77a8-4031-b090-42158d460d01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054215154s
STEP: Saw pod success
Nov 29 01:51:23.120: INFO: Pod "downwardapi-volume-cfd0c570-77a8-4031-b090-42158d460d01" satisfied condition "Succeeded or Failed"
Nov 29 01:51:23.123: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod downwardapi-volume-cfd0c570-77a8-4031-b090-42158d460d01 container client-container: <nil>
STEP: delete the pod
Nov 29 01:51:23.148: INFO: Waiting for pod downwardapi-volume-cfd0c570-77a8-4031-b090-42158d460d01 to disappear
Nov 29 01:51:23.155: INFO: Pod downwardapi-volume-cfd0c570-77a8-4031-b090-42158d460d01 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:51:23.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6761" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":277,"completed":81,"skipped":1320,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:51:23.164: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Nov 29 01:51:23.209: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
Nov 29 01:51:28.864: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:51:50.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5905" for this suite.

• [SLOW TEST:27.043 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":277,"completed":82,"skipped":1340,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:51:50.207: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 29 01:51:50.942: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 29 01:51:52.952: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211510, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211510, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211510, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211510, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 01:51:55.971: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:51:56.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5611" for this suite.
STEP: Destroying namespace "webhook-5611-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.934 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":277,"completed":83,"skipped":1358,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:51:56.143: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-45e4332e-d41c-45ec-b487-59e54ae220b4
STEP: Creating a pod to test consume configMaps
Nov 29 01:51:56.193: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d488eeff-a4ac-41fa-ae42-b7f2cddaaae9" in namespace "projected-1983" to be "Succeeded or Failed"
Nov 29 01:51:56.214: INFO: Pod "pod-projected-configmaps-d488eeff-a4ac-41fa-ae42-b7f2cddaaae9": Phase="Pending", Reason="", readiness=false. Elapsed: 20.500915ms
Nov 29 01:51:58.217: INFO: Pod "pod-projected-configmaps-d488eeff-a4ac-41fa-ae42-b7f2cddaaae9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023944014s
Nov 29 01:52:00.220: INFO: Pod "pod-projected-configmaps-d488eeff-a4ac-41fa-ae42-b7f2cddaaae9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0272014s
STEP: Saw pod success
Nov 29 01:52:00.220: INFO: Pod "pod-projected-configmaps-d488eeff-a4ac-41fa-ae42-b7f2cddaaae9" satisfied condition "Succeeded or Failed"
Nov 29 01:52:00.222: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod pod-projected-configmaps-d488eeff-a4ac-41fa-ae42-b7f2cddaaae9 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 01:52:00.240: INFO: Waiting for pod pod-projected-configmaps-d488eeff-a4ac-41fa-ae42-b7f2cddaaae9 to disappear
Nov 29 01:52:00.249: INFO: Pod pod-projected-configmaps-d488eeff-a4ac-41fa-ae42-b7f2cddaaae9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:52:00.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1983" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":84,"skipped":1395,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:52:00.259: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-494.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-494.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-494.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-494.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-494.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-494.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 29 01:52:04.375: INFO: DNS probes using dns-494/dns-test-7c9b6256-389b-4bfd-8736-a4ddd6b02c9a succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:52:04.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-494" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":277,"completed":85,"skipped":1418,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:52:04.400: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with configMap that has name projected-configmap-test-upd-697a4f2b-e2c7-4b1e-a7c5-24e5bf316f2f
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-697a4f2b-e2c7-4b1e-a7c5-24e5bf316f2f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:53:12.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2582" for this suite.

• [SLOW TEST:68.390 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":86,"skipped":1422,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:53:12.791: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 29 01:53:13.238: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 29 01:53:15.247: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211592, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211592, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211592, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211592, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 01:53:18.271: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:53:18.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5709" for this suite.
STEP: Destroying namespace "webhook-5709-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.669 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":277,"completed":87,"skipped":1444,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:53:18.461: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: set up a multi version CRD
Nov 29 01:53:18.500: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:53:48.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1977" for this suite.

• [SLOW TEST:29.794 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":277,"completed":88,"skipped":1452,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:53:48.255: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-8434
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a new StatefulSet
Nov 29 01:53:48.321: INFO: Found 0 stateful pods, waiting for 3
Nov 29 01:53:58.326: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:53:58.326: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:53:58.326: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Nov 29 01:53:58.354: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Nov 29 01:54:08.388: INFO: Updating stateful set ss2
Nov 29 01:54:08.413: INFO: Waiting for Pod statefulset-8434/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 29 01:54:18.419: INFO: Waiting for Pod statefulset-8434/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Nov 29 01:54:28.533: INFO: Found 2 stateful pods, waiting for 3
Nov 29 01:54:38.538: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:54:38.539: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:54:38.539: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov 29 01:54:48.537: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:54:48.537: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 01:54:48.537: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Nov 29 01:54:48.563: INFO: Updating stateful set ss2
Nov 29 01:54:48.591: INFO: Waiting for Pod statefulset-8434/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 29 01:54:58.615: INFO: Updating stateful set ss2
Nov 29 01:54:58.630: INFO: Waiting for StatefulSet statefulset-8434/ss2 to complete update
Nov 29 01:54:58.630: INFO: Waiting for Pod statefulset-8434/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 29 01:55:08.637: INFO: Waiting for StatefulSet statefulset-8434/ss2 to complete update
Nov 29 01:55:08.637: INFO: Waiting for Pod statefulset-8434/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 29 01:55:18.643: INFO: Waiting for StatefulSet statefulset-8434/ss2 to complete update
Nov 29 01:55:18.643: INFO: Waiting for Pod statefulset-8434/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 29 01:55:28.636: INFO: Waiting for StatefulSet statefulset-8434/ss2 to complete update
Nov 29 01:55:28.636: INFO: Waiting for Pod statefulset-8434/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 29 01:55:38.636: INFO: Waiting for StatefulSet statefulset-8434/ss2 to complete update
Nov 29 01:55:38.636: INFO: Waiting for Pod statefulset-8434/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 29 01:55:48.640: INFO: Waiting for StatefulSet statefulset-8434/ss2 to complete update
Nov 29 01:55:48.640: INFO: Waiting for Pod statefulset-8434/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Nov 29 01:55:58.637: INFO: Deleting all statefulset in ns statefulset-8434
Nov 29 01:55:58.639: INFO: Scaling statefulset ss2 to 0
Nov 29 01:56:08.654: INFO: Waiting for statefulset status.replicas updated to 0
Nov 29 01:56:08.666: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:56:08.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8434" for this suite.

• [SLOW TEST:140.435 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":277,"completed":89,"skipped":1455,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:56:08.691: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name projected-secret-test-8ef4122f-2c1d-4ec4-b435-bd7c051cd6bb
STEP: Creating a pod to test consume secrets
Nov 29 01:56:08.756: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-be219755-041d-4a23-a062-88a2c1d83d4b" in namespace "projected-3699" to be "Succeeded or Failed"
Nov 29 01:56:08.765: INFO: Pod "pod-projected-secrets-be219755-041d-4a23-a062-88a2c1d83d4b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.092858ms
Nov 29 01:56:10.775: INFO: Pod "pod-projected-secrets-be219755-041d-4a23-a062-88a2c1d83d4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018567885s
Nov 29 01:56:12.778: INFO: Pod "pod-projected-secrets-be219755-041d-4a23-a062-88a2c1d83d4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021476721s
STEP: Saw pod success
Nov 29 01:56:12.778: INFO: Pod "pod-projected-secrets-be219755-041d-4a23-a062-88a2c1d83d4b" satisfied condition "Succeeded or Failed"
Nov 29 01:56:12.780: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod pod-projected-secrets-be219755-041d-4a23-a062-88a2c1d83d4b container secret-volume-test: <nil>
STEP: delete the pod
Nov 29 01:56:12.822: INFO: Waiting for pod pod-projected-secrets-be219755-041d-4a23-a062-88a2c1d83d4b to disappear
Nov 29 01:56:12.824: INFO: Pod pod-projected-secrets-be219755-041d-4a23-a062-88a2c1d83d4b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:56:12.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3699" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":277,"completed":90,"skipped":1476,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:56:12.834: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-map-9e1b0ac9-ea3f-49c5-aaa2-858108603223
STEP: Creating a pod to test consume configMaps
Nov 29 01:56:12.873: INFO: Waiting up to 5m0s for pod "pod-configmaps-de1dc534-2b07-4902-a2ba-edf1fee8b5c3" in namespace "configmap-7809" to be "Succeeded or Failed"
Nov 29 01:56:12.877: INFO: Pod "pod-configmaps-de1dc534-2b07-4902-a2ba-edf1fee8b5c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.704286ms
Nov 29 01:56:14.880: INFO: Pod "pod-configmaps-de1dc534-2b07-4902-a2ba-edf1fee8b5c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007733673s
Nov 29 01:56:16.884: INFO: Pod "pod-configmaps-de1dc534-2b07-4902-a2ba-edf1fee8b5c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011094334s
STEP: Saw pod success
Nov 29 01:56:16.884: INFO: Pod "pod-configmaps-de1dc534-2b07-4902-a2ba-edf1fee8b5c3" satisfied condition "Succeeded or Failed"
Nov 29 01:56:16.886: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod pod-configmaps-de1dc534-2b07-4902-a2ba-edf1fee8b5c3 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 01:56:16.913: INFO: Waiting for pod pod-configmaps-de1dc534-2b07-4902-a2ba-edf1fee8b5c3 to disappear
Nov 29 01:56:16.921: INFO: Pod pod-configmaps-de1dc534-2b07-4902-a2ba-edf1fee8b5c3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:56:16.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7809" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":91,"skipped":1503,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:56:16.931: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 29 01:56:19.991: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:56:20.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8065" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":277,"completed":92,"skipped":1511,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:56:20.020: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:56:26.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7269" for this suite.
STEP: Destroying namespace "nsdeletetest-4753" for this suite.
Nov 29 01:56:26.183: INFO: Namespace nsdeletetest-4753 was already deleted
STEP: Destroying namespace "nsdeletetest-8963" for this suite.

• [SLOW TEST:6.170 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":277,"completed":93,"skipped":1556,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:56:26.190: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1129 01:56:36.334477      22 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Nov 29 01:56:36.334: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:56:36.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9426" for this suite.

• [SLOW TEST:10.152 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":277,"completed":94,"skipped":1558,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:56:36.342: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 01:56:36.374: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:56:36.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3612" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":277,"completed":95,"skipped":1587,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:56:36.947: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Nov 29 01:56:37.003: INFO: Pod name pod-release: Found 0 pods out of 1
Nov 29 01:56:42.006: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:56:42.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-354" for this suite.

• [SLOW TEST:5.134 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":277,"completed":96,"skipped":1619,"failed":0}
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:56:42.081: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Nov 29 01:56:42.132: INFO: Waiting up to 5m0s for pod "downward-api-f7ca99f3-1762-425f-95f0-c3f12911918b" in namespace "downward-api-4989" to be "Succeeded or Failed"
Nov 29 01:56:42.135: INFO: Pod "downward-api-f7ca99f3-1762-425f-95f0-c3f12911918b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.724481ms
Nov 29 01:56:44.139: INFO: Pod "downward-api-f7ca99f3-1762-425f-95f0-c3f12911918b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007543774s
Nov 29 01:56:46.144: INFO: Pod "downward-api-f7ca99f3-1762-425f-95f0-c3f12911918b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011866234s
STEP: Saw pod success
Nov 29 01:56:46.144: INFO: Pod "downward-api-f7ca99f3-1762-425f-95f0-c3f12911918b" satisfied condition "Succeeded or Failed"
Nov 29 01:56:46.148: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod downward-api-f7ca99f3-1762-425f-95f0-c3f12911918b container dapi-container: <nil>
STEP: delete the pod
Nov 29 01:56:46.192: INFO: Waiting for pod downward-api-f7ca99f3-1762-425f-95f0-c3f12911918b to disappear
Nov 29 01:56:46.205: INFO: Pod downward-api-f7ca99f3-1762-425f-95f0-c3f12911918b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:56:46.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4989" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":277,"completed":97,"skipped":1619,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:56:46.222: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-secret-x2zb
STEP: Creating a pod to test atomic-volume-subpath
Nov 29 01:56:46.288: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-x2zb" in namespace "subpath-1190" to be "Succeeded or Failed"
Nov 29 01:56:46.313: INFO: Pod "pod-subpath-test-secret-x2zb": Phase="Pending", Reason="", readiness=false. Elapsed: 24.330977ms
Nov 29 01:56:48.316: INFO: Pod "pod-subpath-test-secret-x2zb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028182378s
Nov 29 01:56:50.320: INFO: Pod "pod-subpath-test-secret-x2zb": Phase="Running", Reason="", readiness=true. Elapsed: 4.032155119s
Nov 29 01:56:52.324: INFO: Pod "pod-subpath-test-secret-x2zb": Phase="Running", Reason="", readiness=true. Elapsed: 6.036254224s
Nov 29 01:56:54.328: INFO: Pod "pod-subpath-test-secret-x2zb": Phase="Running", Reason="", readiness=true. Elapsed: 8.039912969s
Nov 29 01:56:56.331: INFO: Pod "pod-subpath-test-secret-x2zb": Phase="Running", Reason="", readiness=true. Elapsed: 10.04311267s
Nov 29 01:56:58.335: INFO: Pod "pod-subpath-test-secret-x2zb": Phase="Running", Reason="", readiness=true. Elapsed: 12.046314444s
Nov 29 01:57:00.338: INFO: Pod "pod-subpath-test-secret-x2zb": Phase="Running", Reason="", readiness=true. Elapsed: 14.049843925s
Nov 29 01:57:02.341: INFO: Pod "pod-subpath-test-secret-x2zb": Phase="Running", Reason="", readiness=true. Elapsed: 16.053162542s
Nov 29 01:57:04.345: INFO: Pod "pod-subpath-test-secret-x2zb": Phase="Running", Reason="", readiness=true. Elapsed: 18.057188651s
Nov 29 01:57:06.348: INFO: Pod "pod-subpath-test-secret-x2zb": Phase="Running", Reason="", readiness=true. Elapsed: 20.060197459s
Nov 29 01:57:08.351: INFO: Pod "pod-subpath-test-secret-x2zb": Phase="Running", Reason="", readiness=true. Elapsed: 22.063240805s
Nov 29 01:57:10.355: INFO: Pod "pod-subpath-test-secret-x2zb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.066560339s
STEP: Saw pod success
Nov 29 01:57:10.355: INFO: Pod "pod-subpath-test-secret-x2zb" satisfied condition "Succeeded or Failed"
Nov 29 01:57:10.358: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod pod-subpath-test-secret-x2zb container test-container-subpath-secret-x2zb: <nil>
STEP: delete the pod
Nov 29 01:57:10.378: INFO: Waiting for pod pod-subpath-test-secret-x2zb to disappear
Nov 29 01:57:10.394: INFO: Pod pod-subpath-test-secret-x2zb no longer exists
STEP: Deleting pod pod-subpath-test-secret-x2zb
Nov 29 01:57:10.395: INFO: Deleting pod "pod-subpath-test-secret-x2zb" in namespace "subpath-1190"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:57:10.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1190" for this suite.

• [SLOW TEST:24.184 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":277,"completed":98,"skipped":1630,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:57:10.408: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9083.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9083.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 29 01:57:14.498: INFO: DNS probes using dns-9083/dns-test-58ef8857-cb67-49b2-bd0c-926e2f89e1a6 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:57:14.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9083" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":277,"completed":99,"skipped":1647,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:57:14.543: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on node default medium
Nov 29 01:57:14.601: INFO: Waiting up to 5m0s for pod "pod-cb2a038f-b206-401b-91d1-2cafbe0294e1" in namespace "emptydir-2501" to be "Succeeded or Failed"
Nov 29 01:57:14.610: INFO: Pod "pod-cb2a038f-b206-401b-91d1-2cafbe0294e1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.735389ms
Nov 29 01:57:16.612: INFO: Pod "pod-cb2a038f-b206-401b-91d1-2cafbe0294e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011156278s
Nov 29 01:57:18.617: INFO: Pod "pod-cb2a038f-b206-401b-91d1-2cafbe0294e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015252015s
STEP: Saw pod success
Nov 29 01:57:18.617: INFO: Pod "pod-cb2a038f-b206-401b-91d1-2cafbe0294e1" satisfied condition "Succeeded or Failed"
Nov 29 01:57:18.619: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod pod-cb2a038f-b206-401b-91d1-2cafbe0294e1 container test-container: <nil>
STEP: delete the pod
Nov 29 01:57:18.654: INFO: Waiting for pod pod-cb2a038f-b206-401b-91d1-2cafbe0294e1 to disappear
Nov 29 01:57:18.661: INFO: Pod pod-cb2a038f-b206-401b-91d1-2cafbe0294e1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:57:18.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2501" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":100,"skipped":1655,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:57:18.674: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: validating api versions
Nov 29 01:57:18.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 api-versions'
Nov 29 01:57:18.861: INFO: stderr: ""
Nov 29 01:57:18.861: INFO: stdout: "acme.cert-manager.io/v1\nacme.cert-manager.io/v1alpha2\nacme.cert-manager.io/v1alpha3\nacme.cert-manager.io/v1beta1\nadmissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncert-manager.io/v1\ncert-manager.io/v1alpha2\ncert-manager.io/v1alpha3\ncert-manager.io/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nhelm.ccp.cisco.com/v1alpha1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:57:18.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9261" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":277,"completed":101,"skipped":1664,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] version v1
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:57:18.871: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-blf25 in namespace proxy-1621
I1129 01:57:18.927740      22 runners.go:190] Created replication controller with name: proxy-service-blf25, namespace: proxy-1621, replica count: 1
I1129 01:57:19.978118      22 runners.go:190] proxy-service-blf25 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1129 01:57:20.978516      22 runners.go:190] proxy-service-blf25 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1129 01:57:21.978829      22 runners.go:190] proxy-service-blf25 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1129 01:57:22.979086      22 runners.go:190] proxy-service-blf25 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1129 01:57:23.979407      22 runners.go:190] proxy-service-blf25 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1129 01:57:24.979663      22 runners.go:190] proxy-service-blf25 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1129 01:57:25.979953      22 runners.go:190] proxy-service-blf25 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 29 01:57:25.982: INFO: setup took 7.080761613s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Nov 29 01:57:25.989: INFO: (0) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/rewriteme">test</a> (200; 6.942649ms)
Nov 29 01:57:25.989: INFO: (0) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:160/proxy/: foo (200; 7.303994ms)
Nov 29 01:57:25.989: INFO: (0) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:162/proxy/: bar (200; 7.019138ms)
Nov 29 01:57:25.989: INFO: (0) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/rewriteme">test<... (200; 6.734531ms)
Nov 29 01:57:25.990: INFO: (0) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:162/proxy/: bar (200; 6.984792ms)
Nov 29 01:57:25.990: INFO: (0) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/rewriteme">... (200; 7.199971ms)
Nov 29 01:57:25.993: INFO: (0) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname1/proxy/: foo (200; 10.338087ms)
Nov 29 01:57:25.993: INFO: (0) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname2/proxy/: bar (200; 10.873738ms)
Nov 29 01:57:25.993: INFO: (0) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname1/proxy/: foo (200; 10.685504ms)
Nov 29 01:57:25.993: INFO: (0) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:160/proxy/: foo (200; 11.0313ms)
Nov 29 01:57:25.993: INFO: (0) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname2/proxy/: bar (200; 10.902456ms)
Nov 29 01:57:25.994: INFO: (0) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname1/proxy/: tls baz (200; 11.336973ms)
Nov 29 01:57:25.994: INFO: (0) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:460/proxy/: tls baz (200; 11.934075ms)
Nov 29 01:57:25.995: INFO: (0) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/tlsrewritem... (200; 12.526149ms)
Nov 29 01:57:25.996: INFO: (0) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:462/proxy/: tls qux (200; 13.595316ms)
Nov 29 01:57:25.996: INFO: (0) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname2/proxy/: tls qux (200; 13.522612ms)
Nov 29 01:57:25.999: INFO: (1) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:462/proxy/: tls qux (200; 3.159321ms)
Nov 29 01:57:25.999: INFO: (1) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:160/proxy/: foo (200; 3.471161ms)
Nov 29 01:57:26.000: INFO: (1) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:162/proxy/: bar (200; 2.96428ms)
Nov 29 01:57:26.001: INFO: (1) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:460/proxy/: tls baz (200; 3.53458ms)
Nov 29 01:57:26.001: INFO: (1) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:160/proxy/: foo (200; 4.312193ms)
Nov 29 01:57:26.001: INFO: (1) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/rewriteme">test</a> (200; 3.425932ms)
Nov 29 01:57:26.002: INFO: (1) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/tlsrewritem... (200; 4.718296ms)
Nov 29 01:57:26.002: INFO: (1) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/rewriteme">test<... (200; 4.238077ms)
Nov 29 01:57:26.002: INFO: (1) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:162/proxy/: bar (200; 3.917824ms)
Nov 29 01:57:26.002: INFO: (1) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/rewriteme">... (200; 5.203041ms)
Nov 29 01:57:26.002: INFO: (1) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname1/proxy/: tls baz (200; 5.388896ms)
Nov 29 01:57:26.002: INFO: (1) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname2/proxy/: bar (200; 4.780923ms)
Nov 29 01:57:26.002: INFO: (1) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname2/proxy/: tls qux (200; 4.71351ms)
Nov 29 01:57:26.002: INFO: (1) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname1/proxy/: foo (200; 4.67963ms)
Nov 29 01:57:26.002: INFO: (1) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname2/proxy/: bar (200; 5.215826ms)
Nov 29 01:57:26.003: INFO: (1) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname1/proxy/: foo (200; 6.613762ms)
Nov 29 01:57:26.007: INFO: (2) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname2/proxy/: bar (200; 3.857208ms)
Nov 29 01:57:26.007: INFO: (2) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/rewriteme">test<... (200; 3.935849ms)
Nov 29 01:57:26.008: INFO: (2) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname2/proxy/: bar (200; 4.93608ms)
Nov 29 01:57:26.008: INFO: (2) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:160/proxy/: foo (200; 5.014938ms)
Nov 29 01:57:26.009: INFO: (2) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:462/proxy/: tls qux (200; 5.456482ms)
Nov 29 01:57:26.009: INFO: (2) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:162/proxy/: bar (200; 5.496423ms)
Nov 29 01:57:26.009: INFO: (2) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/tlsrewritem... (200; 5.928094ms)
Nov 29 01:57:26.009: INFO: (2) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:160/proxy/: foo (200; 5.705603ms)
Nov 29 01:57:26.010: INFO: (2) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:162/proxy/: bar (200; 6.709364ms)
Nov 29 01:57:26.010: INFO: (2) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/rewriteme">... (200; 6.911372ms)
Nov 29 01:57:26.011: INFO: (2) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/rewriteme">test</a> (200; 7.43821ms)
Nov 29 01:57:26.012: INFO: (2) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname2/proxy/: tls qux (200; 8.538703ms)
Nov 29 01:57:26.012: INFO: (2) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname1/proxy/: tls baz (200; 8.752915ms)
Nov 29 01:57:26.012: INFO: (2) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname1/proxy/: foo (200; 8.863416ms)
Nov 29 01:57:26.013: INFO: (2) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname1/proxy/: foo (200; 9.168518ms)
Nov 29 01:57:26.013: INFO: (2) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:460/proxy/: tls baz (200; 9.323531ms)
Nov 29 01:57:26.019: INFO: (3) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:162/proxy/: bar (200; 6.863647ms)
Nov 29 01:57:26.020: INFO: (3) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:162/proxy/: bar (200; 6.82515ms)
Nov 29 01:57:26.021: INFO: (3) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname1/proxy/: foo (200; 8.069716ms)
Nov 29 01:57:26.021: INFO: (3) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/rewriteme">test</a> (200; 8.136965ms)
Nov 29 01:57:26.021: INFO: (3) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/rewriteme">... (200; 8.144824ms)
Nov 29 01:57:26.021: INFO: (3) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:462/proxy/: tls qux (200; 8.28407ms)
Nov 29 01:57:26.021: INFO: (3) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/tlsrewritem... (200; 8.188855ms)
Nov 29 01:57:26.022: INFO: (3) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:460/proxy/: tls baz (200; 8.610322ms)
Nov 29 01:57:26.022: INFO: (3) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:160/proxy/: foo (200; 8.657376ms)
Nov 29 01:57:26.022: INFO: (3) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/rewriteme">test<... (200; 8.354466ms)
Nov 29 01:57:26.022: INFO: (3) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:160/proxy/: foo (200; 8.733726ms)
Nov 29 01:57:26.024: INFO: (3) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname2/proxy/: bar (200; 11.004888ms)
Nov 29 01:57:26.024: INFO: (3) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname2/proxy/: bar (200; 11.106597ms)
Nov 29 01:57:26.024: INFO: (3) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname1/proxy/: foo (200; 11.201918ms)
Nov 29 01:57:26.024: INFO: (3) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname1/proxy/: tls baz (200; 11.190753ms)
Nov 29 01:57:26.024: INFO: (3) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname2/proxy/: tls qux (200; 11.091739ms)
Nov 29 01:57:26.028: INFO: (4) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:462/proxy/: tls qux (200; 3.272859ms)
Nov 29 01:57:26.029: INFO: (4) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:162/proxy/: bar (200; 3.379323ms)
Nov 29 01:57:26.029: INFO: (4) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/rewriteme">test<... (200; 3.611985ms)
Nov 29 01:57:26.030: INFO: (4) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:162/proxy/: bar (200; 4.881381ms)
Nov 29 01:57:26.031: INFO: (4) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:160/proxy/: foo (200; 5.016567ms)
Nov 29 01:57:26.031: INFO: (4) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/rewriteme">test</a> (200; 4.96448ms)
Nov 29 01:57:26.031: INFO: (4) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname2/proxy/: tls qux (200; 5.729016ms)
Nov 29 01:57:26.031: INFO: (4) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname2/proxy/: bar (200; 6.435547ms)
Nov 29 01:57:26.031: INFO: (4) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname1/proxy/: foo (200; 5.820032ms)
Nov 29 01:57:26.032: INFO: (4) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/rewriteme">... (200; 5.794768ms)
Nov 29 01:57:26.032: INFO: (4) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname1/proxy/: foo (200; 6.169591ms)
Nov 29 01:57:26.033: INFO: (4) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname1/proxy/: tls baz (200; 6.579717ms)
Nov 29 01:57:26.033: INFO: (4) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:460/proxy/: tls baz (200; 6.243617ms)
Nov 29 01:57:26.033: INFO: (4) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:160/proxy/: foo (200; 6.717434ms)
Nov 29 01:57:26.033: INFO: (4) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname2/proxy/: bar (200; 8.361759ms)
Nov 29 01:57:26.034: INFO: (4) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/tlsrewritem... (200; 7.315737ms)
Nov 29 01:57:26.037: INFO: (5) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:162/proxy/: bar (200; 2.846065ms)
Nov 29 01:57:26.037: INFO: (5) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/rewriteme">test</a> (200; 3.01985ms)
Nov 29 01:57:26.038: INFO: (5) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:462/proxy/: tls qux (200; 3.496477ms)
Nov 29 01:57:26.038: INFO: (5) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:162/proxy/: bar (200; 4.111824ms)
Nov 29 01:57:26.039: INFO: (5) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname1/proxy/: foo (200; 5.330432ms)
Nov 29 01:57:26.039: INFO: (5) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname1/proxy/: foo (200; 4.943081ms)
Nov 29 01:57:26.039: INFO: (5) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/rewriteme">... (200; 4.869419ms)
Nov 29 01:57:26.040: INFO: (5) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:160/proxy/: foo (200; 5.062583ms)
Nov 29 01:57:26.040: INFO: (5) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:160/proxy/: foo (200; 5.557388ms)
Nov 29 01:57:26.040: INFO: (5) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/rewriteme">test<... (200; 5.243308ms)
Nov 29 01:57:26.040: INFO: (5) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:460/proxy/: tls baz (200; 5.617569ms)
Nov 29 01:57:26.040: INFO: (5) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/tlsrewritem... (200; 5.717991ms)
Nov 29 01:57:26.043: INFO: (5) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname1/proxy/: tls baz (200; 8.391119ms)
Nov 29 01:57:26.043: INFO: (5) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname2/proxy/: bar (200; 8.49358ms)
Nov 29 01:57:26.043: INFO: (5) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname2/proxy/: tls qux (200; 8.373ms)
Nov 29 01:57:26.043: INFO: (5) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname2/proxy/: bar (200; 8.475183ms)
Nov 29 01:57:26.051: INFO: (6) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname1/proxy/: tls baz (200; 6.917575ms)
Nov 29 01:57:26.051: INFO: (6) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname2/proxy/: bar (200; 7.050229ms)
Nov 29 01:57:26.051: INFO: (6) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname1/proxy/: foo (200; 7.624929ms)
Nov 29 01:57:26.052: INFO: (6) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/rewriteme">test</a> (200; 8.067453ms)
Nov 29 01:57:26.052: INFO: (6) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname2/proxy/: bar (200; 8.250452ms)
Nov 29 01:57:26.052: INFO: (6) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:162/proxy/: bar (200; 8.098631ms)
Nov 29 01:57:26.052: INFO: (6) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/rewriteme">test<... (200; 8.42707ms)
Nov 29 01:57:26.052: INFO: (6) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:160/proxy/: foo (200; 8.467483ms)
Nov 29 01:57:26.052: INFO: (6) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:462/proxy/: tls qux (200; 8.576515ms)
Nov 29 01:57:26.052: INFO: (6) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname2/proxy/: tls qux (200; 8.309538ms)
Nov 29 01:57:26.052: INFO: (6) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/tlsrewritem... (200; 8.457071ms)
Nov 29 01:57:26.052: INFO: (6) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname1/proxy/: foo (200; 8.326715ms)
Nov 29 01:57:26.052: INFO: (6) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:160/proxy/: foo (200; 8.727307ms)
Nov 29 01:57:26.052: INFO: (6) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/rewriteme">... (200; 8.67335ms)
Nov 29 01:57:26.052: INFO: (6) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:162/proxy/: bar (200; 8.38606ms)
Nov 29 01:57:26.052: INFO: (6) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:460/proxy/: tls baz (200; 8.781671ms)
Nov 29 01:57:26.056: INFO: (7) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/rewriteme">... (200; 3.422053ms)
Nov 29 01:57:26.056: INFO: (7) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/rewriteme">test</a> (200; 4.016931ms)
Nov 29 01:57:26.059: INFO: (7) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/tlsrewritem... (200; 6.419165ms)
Nov 29 01:57:26.059: INFO: (7) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/rewriteme">test<... (200; 6.024595ms)
Nov 29 01:57:26.059: INFO: (7) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:160/proxy/: foo (200; 6.681103ms)
Nov 29 01:57:26.059: INFO: (7) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:162/proxy/: bar (200; 5.984278ms)
Nov 29 01:57:26.060: INFO: (7) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname1/proxy/: foo (200; 6.70125ms)
Nov 29 01:57:26.060: INFO: (7) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:162/proxy/: bar (200; 6.800286ms)
Nov 29 01:57:26.061: INFO: (7) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:160/proxy/: foo (200; 6.827031ms)
Nov 29 01:57:26.061: INFO: (7) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:462/proxy/: tls qux (200; 7.673034ms)
Nov 29 01:57:26.061: INFO: (7) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname2/proxy/: tls qux (200; 7.477025ms)
Nov 29 01:57:26.061: INFO: (7) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname2/proxy/: bar (200; 7.733535ms)
Nov 29 01:57:26.061: INFO: (7) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:460/proxy/: tls baz (200; 7.973888ms)
Nov 29 01:57:26.061: INFO: (7) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname1/proxy/: tls baz (200; 8.564973ms)
Nov 29 01:57:26.061: INFO: (7) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname1/proxy/: foo (200; 8.080526ms)
Nov 29 01:57:26.061: INFO: (7) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname2/proxy/: bar (200; 7.653412ms)
Nov 29 01:57:26.066: INFO: (8) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:460/proxy/: tls baz (200; 4.504166ms)
Nov 29 01:57:26.066: INFO: (8) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/rewriteme">test<... (200; 4.728431ms)
Nov 29 01:57:26.066: INFO: (8) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/rewriteme">... (200; 4.697549ms)
Nov 29 01:57:26.066: INFO: (8) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/rewriteme">test</a> (200; 5.02579ms)
Nov 29 01:57:26.067: INFO: (8) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/tlsrewritem... (200; 5.006734ms)
Nov 29 01:57:26.067: INFO: (8) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:162/proxy/: bar (200; 5.140517ms)
Nov 29 01:57:26.069: INFO: (8) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname1/proxy/: foo (200; 7.565157ms)
Nov 29 01:57:26.069: INFO: (8) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname1/proxy/: tls baz (200; 7.599978ms)
Nov 29 01:57:26.069: INFO: (8) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:160/proxy/: foo (200; 7.585477ms)
Nov 29 01:57:26.070: INFO: (8) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:162/proxy/: bar (200; 8.456828ms)
Nov 29 01:57:26.070: INFO: (8) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:160/proxy/: foo (200; 8.20791ms)
Nov 29 01:57:26.070: INFO: (8) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname2/proxy/: tls qux (200; 8.832634ms)
Nov 29 01:57:26.070: INFO: (8) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:462/proxy/: tls qux (200; 9.006935ms)
Nov 29 01:57:26.071: INFO: (8) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname2/proxy/: bar (200; 9.340292ms)
Nov 29 01:57:26.071: INFO: (8) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname1/proxy/: foo (200; 9.439669ms)
Nov 29 01:57:26.071: INFO: (8) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname2/proxy/: bar (200; 9.548023ms)
Nov 29 01:57:26.077: INFO: (9) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:162/proxy/: bar (200; 6.063978ms)
Nov 29 01:57:26.078: INFO: (9) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:160/proxy/: foo (200; 6.536936ms)
Nov 29 01:57:26.078: INFO: (9) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname1/proxy/: foo (200; 6.771192ms)
Nov 29 01:57:26.078: INFO: (9) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:162/proxy/: bar (200; 6.796351ms)
Nov 29 01:57:26.079: INFO: (9) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/tlsrewritem... (200; 7.042783ms)
Nov 29 01:57:26.079: INFO: (9) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:462/proxy/: tls qux (200; 7.539656ms)
Nov 29 01:57:26.079: INFO: (9) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:160/proxy/: foo (200; 7.635006ms)
Nov 29 01:57:26.079: INFO: (9) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/rewriteme">... (200; 7.789785ms)
Nov 29 01:57:26.080: INFO: (9) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/rewriteme">test<... (200; 8.049648ms)
Nov 29 01:57:26.080: INFO: (9) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/rewriteme">test</a> (200; 8.482826ms)
Nov 29 01:57:26.080: INFO: (9) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:460/proxy/: tls baz (200; 8.180105ms)
Nov 29 01:57:26.081: INFO: (9) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname2/proxy/: bar (200; 9.110671ms)
Nov 29 01:57:26.081: INFO: (9) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname2/proxy/: bar (200; 9.556003ms)
Nov 29 01:57:26.081: INFO: (9) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname1/proxy/: foo (200; 9.746302ms)
Nov 29 01:57:26.082: INFO: (9) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname1/proxy/: tls baz (200; 10.193405ms)
Nov 29 01:57:26.083: INFO: (9) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname2/proxy/: tls qux (200; 11.414428ms)
Nov 29 01:57:26.089: INFO: (10) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/tlsrewritem... (200; 5.195031ms)
Nov 29 01:57:26.089: INFO: (10) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:460/proxy/: tls baz (200; 5.080072ms)
Nov 29 01:57:26.089: INFO: (10) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:162/proxy/: bar (200; 4.939266ms)
Nov 29 01:57:26.089: INFO: (10) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/rewriteme">test<... (200; 5.282102ms)
Nov 29 01:57:26.089: INFO: (10) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname2/proxy/: bar (200; 5.792799ms)
Nov 29 01:57:26.089: INFO: (10) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/rewriteme">test</a> (200; 5.754138ms)
Nov 29 01:57:26.092: INFO: (10) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:462/proxy/: tls qux (200; 8.589773ms)
Nov 29 01:57:26.092: INFO: (10) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/rewriteme">... (200; 8.374432ms)
Nov 29 01:57:26.093: INFO: (10) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:160/proxy/: foo (200; 9.105235ms)
Nov 29 01:57:26.093: INFO: (10) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:162/proxy/: bar (200; 9.171041ms)
Nov 29 01:57:26.093: INFO: (10) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname2/proxy/: bar (200; 9.32261ms)
Nov 29 01:57:26.093: INFO: (10) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:160/proxy/: foo (200; 9.099488ms)
Nov 29 01:57:26.093: INFO: (10) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname2/proxy/: tls qux (200; 9.608365ms)
Nov 29 01:57:26.093: INFO: (10) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname1/proxy/: foo (200; 9.699446ms)
Nov 29 01:57:26.093: INFO: (10) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname1/proxy/: tls baz (200; 9.831985ms)
Nov 29 01:57:26.094: INFO: (10) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname1/proxy/: foo (200; 10.515746ms)
Nov 29 01:57:26.098: INFO: (11) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:160/proxy/: foo (200; 3.624629ms)
Nov 29 01:57:26.099: INFO: (11) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/rewriteme">test</a> (200; 4.234068ms)
Nov 29 01:57:26.099: INFO: (11) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/tlsrewritem... (200; 4.312802ms)
Nov 29 01:57:26.099: INFO: (11) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:460/proxy/: tls baz (200; 4.681314ms)
Nov 29 01:57:26.099: INFO: (11) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/rewriteme">... (200; 4.683168ms)
Nov 29 01:57:26.100: INFO: (11) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname1/proxy/: foo (200; 5.260722ms)
Nov 29 01:57:26.100: INFO: (11) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:162/proxy/: bar (200; 5.281927ms)
Nov 29 01:57:26.100: INFO: (11) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:160/proxy/: foo (200; 6.002439ms)
Nov 29 01:57:26.100: INFO: (11) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:462/proxy/: tls qux (200; 6.071408ms)
Nov 29 01:57:26.100: INFO: (11) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/rewriteme">test<... (200; 5.998097ms)
Nov 29 01:57:26.101: INFO: (11) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:162/proxy/: bar (200; 6.114827ms)
Nov 29 01:57:26.102: INFO: (11) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname2/proxy/: tls qux (200; 7.769338ms)
Nov 29 01:57:26.102: INFO: (11) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname2/proxy/: bar (200; 7.68947ms)
Nov 29 01:57:26.102: INFO: (11) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname1/proxy/: foo (200; 7.872062ms)
Nov 29 01:57:26.102: INFO: (11) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname2/proxy/: bar (200; 7.818661ms)
Nov 29 01:57:26.103: INFO: (11) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname1/proxy/: tls baz (200; 8.348285ms)
Nov 29 01:57:26.108: INFO: (12) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:162/proxy/: bar (200; 4.097468ms)
Nov 29 01:57:26.108: INFO: (12) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:462/proxy/: tls qux (200; 4.731491ms)
Nov 29 01:57:26.109: INFO: (12) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/rewriteme">... (200; 5.204826ms)
Nov 29 01:57:26.109: INFO: (12) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:460/proxy/: tls baz (200; 5.054399ms)
Nov 29 01:57:26.109: INFO: (12) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/rewriteme">test<... (200; 4.86495ms)
Nov 29 01:57:26.109: INFO: (12) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/tlsrewritem... (200; 5.29354ms)
Nov 29 01:57:26.109: INFO: (12) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/rewriteme">test</a> (200; 5.636228ms)
Nov 29 01:57:26.109: INFO: (12) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:160/proxy/: foo (200; 5.526991ms)
Nov 29 01:57:26.109: INFO: (12) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:162/proxy/: bar (200; 5.910666ms)
Nov 29 01:57:26.109: INFO: (12) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:160/proxy/: foo (200; 6.013121ms)
Nov 29 01:57:26.111: INFO: (12) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname1/proxy/: foo (200; 8.45023ms)
Nov 29 01:57:26.112: INFO: (12) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname2/proxy/: bar (200; 7.466789ms)
Nov 29 01:57:26.112: INFO: (12) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname1/proxy/: foo (200; 7.824336ms)
Nov 29 01:57:26.112: INFO: (12) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname2/proxy/: tls qux (200; 7.74219ms)
Nov 29 01:57:26.113: INFO: (12) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname2/proxy/: bar (200; 9.879331ms)
Nov 29 01:57:26.113: INFO: (12) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname1/proxy/: tls baz (200; 8.988876ms)
Nov 29 01:57:26.116: INFO: (13) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:460/proxy/: tls baz (200; 2.707524ms)
Nov 29 01:57:26.119: INFO: (13) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/rewriteme">... (200; 5.451402ms)
Nov 29 01:57:26.119: INFO: (13) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:162/proxy/: bar (200; 5.949892ms)
Nov 29 01:57:26.119: INFO: (13) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/tlsrewritem... (200; 5.901406ms)
Nov 29 01:57:26.119: INFO: (13) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:160/proxy/: foo (200; 5.926626ms)
Nov 29 01:57:26.119: INFO: (13) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/rewriteme">test<... (200; 6.089294ms)
Nov 29 01:57:26.119: INFO: (13) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/rewriteme">test</a> (200; 5.991653ms)
Nov 29 01:57:26.119: INFO: (13) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:160/proxy/: foo (200; 6.421816ms)
Nov 29 01:57:26.120: INFO: (13) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:462/proxy/: tls qux (200; 7.34286ms)
Nov 29 01:57:26.121: INFO: (13) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:162/proxy/: bar (200; 7.288785ms)
Nov 29 01:57:26.121: INFO: (13) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname2/proxy/: bar (200; 7.978398ms)
Nov 29 01:57:26.121: INFO: (13) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname1/proxy/: tls baz (200; 8.219192ms)
Nov 29 01:57:26.121: INFO: (13) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname1/proxy/: foo (200; 8.418847ms)
Nov 29 01:57:26.122: INFO: (13) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname2/proxy/: bar (200; 8.345767ms)
Nov 29 01:57:26.122: INFO: (13) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname2/proxy/: tls qux (200; 8.42838ms)
Nov 29 01:57:26.123: INFO: (13) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname1/proxy/: foo (200; 9.358832ms)
Nov 29 01:57:26.127: INFO: (14) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:162/proxy/: bar (200; 4.190946ms)
Nov 29 01:57:26.128: INFO: (14) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/tlsrewritem... (200; 5.04292ms)
Nov 29 01:57:26.130: INFO: (14) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/rewriteme">test<... (200; 6.486238ms)
Nov 29 01:57:26.144: INFO: (14) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:160/proxy/: foo (200; 19.92674ms)
Nov 29 01:57:26.144: INFO: (14) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:162/proxy/: bar (200; 20.169669ms)
Nov 29 01:57:26.144: INFO: (14) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/rewriteme">test</a> (200; 20.08005ms)
Nov 29 01:57:26.144: INFO: (14) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname2/proxy/: bar (200; 20.740148ms)
Nov 29 01:57:26.144: INFO: (14) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname2/proxy/: tls qux (200; 20.63866ms)
Nov 29 01:57:26.144: INFO: (14) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname2/proxy/: bar (200; 21.031192ms)
Nov 29 01:57:26.145: INFO: (14) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:462/proxy/: tls qux (200; 20.716754ms)
Nov 29 01:57:26.145: INFO: (14) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/rewriteme">... (200; 20.568516ms)
Nov 29 01:57:26.145: INFO: (14) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname1/proxy/: tls baz (200; 20.646029ms)
Nov 29 01:57:26.145: INFO: (14) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:160/proxy/: foo (200; 20.629534ms)
Nov 29 01:57:26.145: INFO: (14) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:460/proxy/: tls baz (200; 20.990729ms)
Nov 29 01:57:26.145: INFO: (14) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname1/proxy/: foo (200; 21.430312ms)
Nov 29 01:57:26.145: INFO: (14) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname1/proxy/: foo (200; 21.599296ms)
Nov 29 01:57:26.152: INFO: (15) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:162/proxy/: bar (200; 6.209939ms)
Nov 29 01:57:26.152: INFO: (15) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:160/proxy/: foo (200; 6.733179ms)
Nov 29 01:57:26.152: INFO: (15) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/rewriteme">test<... (200; 6.937392ms)
Nov 29 01:57:26.153: INFO: (15) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:162/proxy/: bar (200; 6.820994ms)
Nov 29 01:57:26.153: INFO: (15) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:460/proxy/: tls baz (200; 6.84399ms)
Nov 29 01:57:26.153: INFO: (15) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/rewriteme">test</a> (200; 6.872283ms)
Nov 29 01:57:26.153: INFO: (15) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/rewriteme">... (200; 7.038018ms)
Nov 29 01:57:26.153: INFO: (15) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:462/proxy/: tls qux (200; 6.988281ms)
Nov 29 01:57:26.153: INFO: (15) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:160/proxy/: foo (200; 7.671077ms)
Nov 29 01:57:26.155: INFO: (15) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname2/proxy/: tls qux (200; 9.09081ms)
Nov 29 01:57:26.155: INFO: (15) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname2/proxy/: bar (200; 9.132576ms)
Nov 29 01:57:26.155: INFO: (15) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/tlsrewritem... (200; 9.244077ms)
Nov 29 01:57:26.155: INFO: (15) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname1/proxy/: foo (200; 9.343853ms)
Nov 29 01:57:26.155: INFO: (15) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname1/proxy/: tls baz (200; 9.45555ms)
Nov 29 01:57:26.155: INFO: (15) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname1/proxy/: foo (200; 9.234327ms)
Nov 29 01:57:26.155: INFO: (15) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname2/proxy/: bar (200; 9.392187ms)
Nov 29 01:57:26.159: INFO: (16) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/rewriteme">test</a> (200; 3.997204ms)
Nov 29 01:57:26.160: INFO: (16) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:462/proxy/: tls qux (200; 4.03477ms)
Nov 29 01:57:26.161: INFO: (16) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/rewriteme">... (200; 5.552358ms)
Nov 29 01:57:26.162: INFO: (16) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:162/proxy/: bar (200; 6.661734ms)
Nov 29 01:57:26.162: INFO: (16) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:160/proxy/: foo (200; 6.576573ms)
Nov 29 01:57:26.162: INFO: (16) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname1/proxy/: foo (200; 6.831612ms)
Nov 29 01:57:26.163: INFO: (16) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:160/proxy/: foo (200; 7.588885ms)
Nov 29 01:57:26.163: INFO: (16) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:460/proxy/: tls baz (200; 7.701178ms)
Nov 29 01:57:26.163: INFO: (16) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:162/proxy/: bar (200; 7.741668ms)
Nov 29 01:57:26.163: INFO: (16) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/rewriteme">test<... (200; 7.863543ms)
Nov 29 01:57:26.164: INFO: (16) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/tlsrewritem... (200; 8.074627ms)
Nov 29 01:57:26.164: INFO: (16) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname1/proxy/: tls baz (200; 8.423263ms)
Nov 29 01:57:26.164: INFO: (16) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname2/proxy/: bar (200; 8.620436ms)
Nov 29 01:57:26.164: INFO: (16) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname2/proxy/: bar (200; 8.784961ms)
Nov 29 01:57:26.164: INFO: (16) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname1/proxy/: foo (200; 8.670333ms)
Nov 29 01:57:26.164: INFO: (16) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname2/proxy/: tls qux (200; 8.879545ms)
Nov 29 01:57:26.167: INFO: (17) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:160/proxy/: foo (200; 2.707333ms)
Nov 29 01:57:26.169: INFO: (17) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:460/proxy/: tls baz (200; 4.430458ms)
Nov 29 01:57:26.172: INFO: (17) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/rewriteme">test</a> (200; 7.488212ms)
Nov 29 01:57:26.172: INFO: (17) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/rewriteme">... (200; 7.524147ms)
Nov 29 01:57:26.172: INFO: (17) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:162/proxy/: bar (200; 7.488404ms)
Nov 29 01:57:26.175: INFO: (17) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname1/proxy/: foo (200; 9.887917ms)
Nov 29 01:57:26.175: INFO: (17) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname1/proxy/: foo (200; 10.080093ms)
Nov 29 01:57:26.175: INFO: (17) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/tlsrewritem... (200; 10.40979ms)
Nov 29 01:57:26.175: INFO: (17) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:162/proxy/: bar (200; 10.422837ms)
Nov 29 01:57:26.175: INFO: (17) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname2/proxy/: tls qux (200; 10.634068ms)
Nov 29 01:57:26.175: INFO: (17) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/rewriteme">test<... (200; 10.636852ms)
Nov 29 01:57:26.175: INFO: (17) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname1/proxy/: tls baz (200; 10.914267ms)
Nov 29 01:57:26.175: INFO: (17) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname2/proxy/: bar (200; 10.952302ms)
Nov 29 01:57:26.175: INFO: (17) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:160/proxy/: foo (200; 10.842446ms)
Nov 29 01:57:26.176: INFO: (17) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:462/proxy/: tls qux (200; 10.953483ms)
Nov 29 01:57:26.176: INFO: (17) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname2/proxy/: bar (200; 11.075495ms)
Nov 29 01:57:26.182: INFO: (18) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:160/proxy/: foo (200; 5.549049ms)
Nov 29 01:57:26.182: INFO: (18) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/tlsrewritem... (200; 5.832384ms)
Nov 29 01:57:26.182: INFO: (18) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:160/proxy/: foo (200; 6.577915ms)
Nov 29 01:57:26.182: INFO: (18) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/rewriteme">test<... (200; 6.571379ms)
Nov 29 01:57:26.183: INFO: (18) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:162/proxy/: bar (200; 6.787406ms)
Nov 29 01:57:26.183: INFO: (18) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/rewriteme">... (200; 6.689371ms)
Nov 29 01:57:26.183: INFO: (18) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:162/proxy/: bar (200; 7.069603ms)
Nov 29 01:57:26.183: INFO: (18) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/rewriteme">test</a> (200; 7.346285ms)
Nov 29 01:57:26.183: INFO: (18) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:460/proxy/: tls baz (200; 7.512558ms)
Nov 29 01:57:26.183: INFO: (18) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:462/proxy/: tls qux (200; 7.494752ms)
Nov 29 01:57:26.185: INFO: (18) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname1/proxy/: foo (200; 8.874942ms)
Nov 29 01:57:26.185: INFO: (18) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname2/proxy/: bar (200; 8.946936ms)
Nov 29 01:57:26.185: INFO: (18) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname1/proxy/: foo (200; 9.163222ms)
Nov 29 01:57:26.185: INFO: (18) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname2/proxy/: bar (200; 9.291916ms)
Nov 29 01:57:26.185: INFO: (18) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname2/proxy/: tls qux (200; 9.215583ms)
Nov 29 01:57:26.185: INFO: (18) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname1/proxy/: tls baz (200; 9.390057ms)
Nov 29 01:57:26.189: INFO: (19) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:160/proxy/: foo (200; 3.737741ms)
Nov 29 01:57:26.190: INFO: (19) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:162/proxy/: bar (200; 4.52052ms)
Nov 29 01:57:26.191: INFO: (19) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:1080/proxy/rewriteme">... (200; 4.524977ms)
Nov 29 01:57:26.191: INFO: (19) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:443/proxy/tlsrewritem... (200; 4.899902ms)
Nov 29 01:57:26.191: INFO: (19) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:462/proxy/: tls qux (200; 5.03178ms)
Nov 29 01:57:26.191: INFO: (19) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:162/proxy/: bar (200; 5.422501ms)
Nov 29 01:57:26.192: INFO: (19) /api/v1/namespaces/proxy-1621/pods/http:proxy-service-blf25-9nhdt:160/proxy/: foo (200; 5.836119ms)
Nov 29 01:57:26.192: INFO: (19) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname2/proxy/: bar (200; 6.41825ms)
Nov 29 01:57:26.193: INFO: (19) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname2/proxy/: tls qux (200; 7.01375ms)
Nov 29 01:57:26.193: INFO: (19) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname1/proxy/: foo (200; 6.545771ms)
Nov 29 01:57:26.193: INFO: (19) /api/v1/namespaces/proxy-1621/services/proxy-service-blf25:portname1/proxy/: foo (200; 7.415507ms)
Nov 29 01:57:26.193: INFO: (19) /api/v1/namespaces/proxy-1621/services/http:proxy-service-blf25:portname2/proxy/: bar (200; 7.614746ms)
Nov 29 01:57:26.194: INFO: (19) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt:1080/proxy/rewriteme">test<... (200; 7.754952ms)
Nov 29 01:57:26.194: INFO: (19) /api/v1/namespaces/proxy-1621/services/https:proxy-service-blf25:tlsportname1/proxy/: tls baz (200; 7.195794ms)
Nov 29 01:57:26.194: INFO: (19) /api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/: <a href="/api/v1/namespaces/proxy-1621/pods/proxy-service-blf25-9nhdt/proxy/rewriteme">test</a> (200; 7.808073ms)
Nov 29 01:57:26.194: INFO: (19) /api/v1/namespaces/proxy-1621/pods/https:proxy-service-blf25-9nhdt:460/proxy/: tls baz (200; 7.693852ms)
STEP: deleting ReplicationController proxy-service-blf25 in namespace proxy-1621, will wait for the garbage collector to delete the pods
Nov 29 01:57:26.259: INFO: Deleting ReplicationController proxy-service-blf25 took: 12.263857ms
Nov 29 01:57:27.059: INFO: Terminating ReplicationController proxy-service-blf25 pods took: 800.25049ms
[AfterEach] version v1
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:57:39.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1621" for this suite.

• [SLOW TEST:21.098 seconds]
[sig-network] Proxy
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":277,"completed":102,"skipped":1694,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:57:39.970: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Nov 29 01:57:40.025: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:57:44.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1406" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":277,"completed":103,"skipped":1703,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:57:44.238: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test override arguments
Nov 29 01:57:44.282: INFO: Waiting up to 5m0s for pod "client-containers-26731043-e701-4357-9514-3bc8557e9b4f" in namespace "containers-7880" to be "Succeeded or Failed"
Nov 29 01:57:44.292: INFO: Pod "client-containers-26731043-e701-4357-9514-3bc8557e9b4f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.781424ms
Nov 29 01:57:46.295: INFO: Pod "client-containers-26731043-e701-4357-9514-3bc8557e9b4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012720584s
Nov 29 01:57:48.298: INFO: Pod "client-containers-26731043-e701-4357-9514-3bc8557e9b4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01582384s
STEP: Saw pod success
Nov 29 01:57:48.298: INFO: Pod "client-containers-26731043-e701-4357-9514-3bc8557e9b4f" satisfied condition "Succeeded or Failed"
Nov 29 01:57:48.301: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod client-containers-26731043-e701-4357-9514-3bc8557e9b4f container test-container: <nil>
STEP: delete the pod
Nov 29 01:57:48.324: INFO: Waiting for pod client-containers-26731043-e701-4357-9514-3bc8557e9b4f to disappear
Nov 29 01:57:48.332: INFO: Pod client-containers-26731043-e701-4357-9514-3bc8557e9b4f no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:57:48.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7880" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":277,"completed":104,"skipped":1727,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:57:48.347: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-dea81a14-930a-4c75-b79a-0710ba6cc8c1
STEP: Creating a pod to test consume secrets
Nov 29 01:57:48.383: INFO: Waiting up to 5m0s for pod "pod-secrets-8154bdc1-814a-4e7b-9726-5b9b8071b466" in namespace "secrets-3971" to be "Succeeded or Failed"
Nov 29 01:57:48.391: INFO: Pod "pod-secrets-8154bdc1-814a-4e7b-9726-5b9b8071b466": Phase="Pending", Reason="", readiness=false. Elapsed: 7.891149ms
Nov 29 01:57:50.394: INFO: Pod "pod-secrets-8154bdc1-814a-4e7b-9726-5b9b8071b466": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011028696s
Nov 29 01:57:52.397: INFO: Pod "pod-secrets-8154bdc1-814a-4e7b-9726-5b9b8071b466": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014232904s
STEP: Saw pod success
Nov 29 01:57:52.397: INFO: Pod "pod-secrets-8154bdc1-814a-4e7b-9726-5b9b8071b466" satisfied condition "Succeeded or Failed"
Nov 29 01:57:52.399: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod pod-secrets-8154bdc1-814a-4e7b-9726-5b9b8071b466 container secret-volume-test: <nil>
STEP: delete the pod
Nov 29 01:57:52.422: INFO: Waiting for pod pod-secrets-8154bdc1-814a-4e7b-9726-5b9b8071b466 to disappear
Nov 29 01:57:52.425: INFO: Pod pod-secrets-8154bdc1-814a-4e7b-9726-5b9b8071b466 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:57:52.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3971" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":105,"skipped":1730,"failed":0}
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:57:52.431: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Nov 29 01:58:00.509: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 29 01:58:00.513: INFO: Pod pod-with-prestop-http-hook still exists
Nov 29 01:58:02.513: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 29 01:58:02.517: INFO: Pod pod-with-prestop-http-hook still exists
Nov 29 01:58:04.513: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 29 01:58:04.516: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:58:04.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2754" for this suite.

• [SLOW TEST:12.106 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":277,"completed":106,"skipped":1733,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:58:04.538: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1418
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Nov 29 01:58:04.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-2163'
Nov 29 01:58:06.159: INFO: stderr: ""
Nov 29 01:58:06.159: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1423
Nov 29 01:58:06.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 delete pods e2e-test-httpd-pod --namespace=kubectl-2163'
Nov 29 01:58:09.583: INFO: stderr: ""
Nov 29 01:58:09.583: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:58:09.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2163" for this suite.

• [SLOW TEST:5.058 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":277,"completed":107,"skipped":1746,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:58:09.597: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Nov 29 01:58:09.657: INFO: Waiting up to 5m0s for pod "downwardapi-volume-19be8674-f1fb-4da8-b82e-c06d77c4ffa1" in namespace "downward-api-382" to be "Succeeded or Failed"
Nov 29 01:58:09.661: INFO: Pod "downwardapi-volume-19be8674-f1fb-4da8-b82e-c06d77c4ffa1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.413938ms
Nov 29 01:58:11.665: INFO: Pod "downwardapi-volume-19be8674-f1fb-4da8-b82e-c06d77c4ffa1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007885779s
Nov 29 01:58:13.668: INFO: Pod "downwardapi-volume-19be8674-f1fb-4da8-b82e-c06d77c4ffa1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011413313s
STEP: Saw pod success
Nov 29 01:58:13.668: INFO: Pod "downwardapi-volume-19be8674-f1fb-4da8-b82e-c06d77c4ffa1" satisfied condition "Succeeded or Failed"
Nov 29 01:58:13.670: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod downwardapi-volume-19be8674-f1fb-4da8-b82e-c06d77c4ffa1 container client-container: <nil>
STEP: delete the pod
Nov 29 01:58:13.698: INFO: Waiting for pod downwardapi-volume-19be8674-f1fb-4da8-b82e-c06d77c4ffa1 to disappear
Nov 29 01:58:13.701: INFO: Pod downwardapi-volume-19be8674-f1fb-4da8-b82e-c06d77c4ffa1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:58:13.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-382" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":277,"completed":108,"skipped":1769,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:58:13.709: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Nov 29 01:58:21.787: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1306 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 01:58:21.787: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
Nov 29 01:58:21.934: INFO: Exec stderr: ""
Nov 29 01:58:21.934: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1306 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 01:58:21.934: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
Nov 29 01:58:22.063: INFO: Exec stderr: ""
Nov 29 01:58:22.063: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1306 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 01:58:22.063: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
Nov 29 01:58:22.203: INFO: Exec stderr: ""
Nov 29 01:58:22.203: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1306 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 01:58:22.203: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
Nov 29 01:58:22.320: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Nov 29 01:58:22.320: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1306 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 01:58:22.320: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
Nov 29 01:58:22.441: INFO: Exec stderr: ""
Nov 29 01:58:22.441: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1306 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 01:58:22.441: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
Nov 29 01:58:22.569: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Nov 29 01:58:22.569: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1306 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 01:58:22.569: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
Nov 29 01:58:22.683: INFO: Exec stderr: ""
Nov 29 01:58:22.683: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1306 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 01:58:22.683: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
Nov 29 01:58:22.794: INFO: Exec stderr: ""
Nov 29 01:58:22.794: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1306 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 01:58:22.794: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
Nov 29 01:58:22.907: INFO: Exec stderr: ""
Nov 29 01:58:22.907: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1306 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 01:58:22.907: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
Nov 29 01:58:23.027: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:58:23.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1306" for this suite.

• [SLOW TEST:9.328 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":109,"skipped":1817,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:58:23.038: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Nov 29 01:58:27.600: INFO: Successfully updated pod "pod-update-activedeadlineseconds-9bd81bd0-ca4e-408a-8eea-a938d8e86b6f"
Nov 29 01:58:27.600: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-9bd81bd0-ca4e-408a-8eea-a938d8e86b6f" in namespace "pods-6210" to be "terminated due to deadline exceeded"
Nov 29 01:58:27.603: INFO: Pod "pod-update-activedeadlineseconds-9bd81bd0-ca4e-408a-8eea-a938d8e86b6f": Phase="Running", Reason="", readiness=true. Elapsed: 3.39044ms
Nov 29 01:58:29.608: INFO: Pod "pod-update-activedeadlineseconds-9bd81bd0-ca4e-408a-8eea-a938d8e86b6f": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.00840192s
Nov 29 01:58:29.608: INFO: Pod "pod-update-activedeadlineseconds-9bd81bd0-ca4e-408a-8eea-a938d8e86b6f" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:58:29.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6210" for this suite.

• [SLOW TEST:6.579 seconds]
[k8s.io] Pods
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":277,"completed":110,"skipped":1836,"failed":0}
S
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:58:29.617: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: getting the auto-created API token
Nov 29 01:58:30.187: INFO: created pod pod-service-account-defaultsa
Nov 29 01:58:30.187: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Nov 29 01:58:30.192: INFO: created pod pod-service-account-mountsa
Nov 29 01:58:30.192: INFO: pod pod-service-account-mountsa service account token volume mount: true
Nov 29 01:58:30.208: INFO: created pod pod-service-account-nomountsa
Nov 29 01:58:30.208: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Nov 29 01:58:30.217: INFO: created pod pod-service-account-defaultsa-mountspec
Nov 29 01:58:30.217: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Nov 29 01:58:30.243: INFO: created pod pod-service-account-mountsa-mountspec
Nov 29 01:58:30.243: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Nov 29 01:58:30.249: INFO: created pod pod-service-account-nomountsa-mountspec
Nov 29 01:58:30.249: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Nov 29 01:58:30.263: INFO: created pod pod-service-account-defaultsa-nomountspec
Nov 29 01:58:30.263: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Nov 29 01:58:30.268: INFO: created pod pod-service-account-mountsa-nomountspec
Nov 29 01:58:30.268: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Nov 29 01:58:30.284: INFO: created pod pod-service-account-nomountsa-nomountspec
Nov 29 01:58:30.285: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:58:30.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-425" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":277,"completed":111,"skipped":1837,"failed":0}
SS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:58:30.320: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 01:58:30.392: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-bb178078-d581-468d-877f-cb8b92833d97" in namespace "security-context-test-4274" to be "Succeeded or Failed"
Nov 29 01:58:30.400: INFO: Pod "busybox-readonly-false-bb178078-d581-468d-877f-cb8b92833d97": Phase="Pending", Reason="", readiness=false. Elapsed: 8.112579ms
Nov 29 01:58:32.403: INFO: Pod "busybox-readonly-false-bb178078-d581-468d-877f-cb8b92833d97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011185334s
Nov 29 01:58:34.407: INFO: Pod "busybox-readonly-false-bb178078-d581-468d-877f-cb8b92833d97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014601507s
Nov 29 01:58:34.407: INFO: Pod "busybox-readonly-false-bb178078-d581-468d-877f-cb8b92833d97" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:58:34.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4274" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":277,"completed":112,"skipped":1839,"failed":0}

------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:58:34.416: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name cm-test-opt-del-973ede47-09e1-493a-9d12-99cc04f4622d
STEP: Creating configMap with name cm-test-opt-upd-3a15cf81-922b-4b17-acf9-19de6d0eec81
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-973ede47-09e1-493a-9d12-99cc04f4622d
STEP: Updating configmap cm-test-opt-upd-3a15cf81-922b-4b17-acf9-19de6d0eec81
STEP: Creating configMap with name cm-test-opt-create-dedb5ce4-c306-4132-8272-401c50eb599a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:58:42.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1346" for this suite.

• [SLOW TEST:8.174 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":113,"skipped":1839,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:58:42.593: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Nov 29 01:58:42.629: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 29 01:58:42.648: INFO: Waiting for terminating namespaces to be deleted...
Nov 29 01:58:42.651: INFO: 
Logging pods the kubelet thinks is on node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d before test
Nov 29 01:58:42.669: INFO: kube-proxy-zjcgv from kube-system started at 2020-11-29 01:13:00 +0000 UTC (1 container statuses recorded)
Nov 29 01:58:42.669: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 29 01:58:42.669: INFO: metallb-speaker-5sjng from ccp started at 2020-11-29 01:13:11 +0000 UTC (1 container statuses recorded)
Nov 29 01:58:42.669: INFO: 	Container metallb-speaker ready: true, restart count 0
Nov 29 01:58:42.669: INFO: sonobuoy from sonobuoy started at 2020-11-29 01:14:36 +0000 UTC (1 container statuses recorded)
Nov 29 01:58:42.669: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 29 01:58:42.669: INFO: sonobuoy-systemd-logs-daemon-set-a18500f604534dc2-g2xkd from sonobuoy started at 2020-11-29 01:15:14 +0000 UTC (2 container statuses recorded)
Nov 29 01:58:42.669: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 29 01:58:42.669: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 29 01:58:42.669: INFO: calico-node-gs8lr from kube-system started at 2020-11-29 01:13:01 +0000 UTC (1 container statuses recorded)
Nov 29 01:58:42.669: INFO: 	Container calico-node ready: true, restart count 0
Nov 29 01:58:42.669: INFO: nvidia-device-plugin-daemonset-f79b4 from kube-system started at 2020-11-29 01:13:11 +0000 UTC (1 container statuses recorded)
Nov 29 01:58:42.669: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Nov 29 01:58:42.669: INFO: ingress-nginx-controller-zd8lx from ccp started at 2020-11-29 01:13:11 +0000 UTC (1 container statuses recorded)
Nov 29 01:58:42.669: INFO: 	Container controller ready: true, restart count 0
Nov 29 01:58:42.669: INFO: sonobuoy-e2e-job-df1d2fd3866f4821 from sonobuoy started at 2020-11-29 01:15:14 +0000 UTC (2 container statuses recorded)
Nov 29 01:58:42.669: INFO: 	Container e2e ready: true, restart count 0
Nov 29 01:58:42.669: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 29 01:58:42.669: INFO: 
Logging pods the kubelet thinks is on node alex-cp1516-v3-vsp2-node-group-bc6233c429 before test
Nov 29 01:58:42.681: INFO: ingress-nginx-controller-vppmg from ccp started at 2020-11-29 01:13:08 +0000 UTC (1 container statuses recorded)
Nov 29 01:58:42.681: INFO: 	Container controller ready: true, restart count 0
Nov 29 01:58:42.681: INFO: calico-node-xhkv2 from kube-system started at 2020-11-29 01:12:58 +0000 UTC (1 container statuses recorded)
Nov 29 01:58:42.681: INFO: 	Container calico-node ready: true, restart count 0
Nov 29 01:58:42.681: INFO: nvidia-device-plugin-daemonset-chhlf from kube-system started at 2020-11-29 01:13:08 +0000 UTC (1 container statuses recorded)
Nov 29 01:58:42.681: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Nov 29 01:58:42.681: INFO: metallb-speaker-55nsq from ccp started at 2020-11-29 01:13:08 +0000 UTC (1 container statuses recorded)
Nov 29 01:58:42.681: INFO: 	Container metallb-speaker ready: true, restart count 0
Nov 29 01:58:42.681: INFO: test-pod from e2e-kubelet-etc-hosts-1306 started at 2020-11-29 01:58:14 +0000 UTC (3 container statuses recorded)
Nov 29 01:58:42.681: INFO: 	Container busybox-1 ready: true, restart count 0
Nov 29 01:58:42.681: INFO: 	Container busybox-2 ready: true, restart count 0
Nov 29 01:58:42.681: INFO: 	Container busybox-3 ready: true, restart count 0
Nov 29 01:58:42.681: INFO: kube-proxy-wcdhr from kube-system started at 2020-11-29 01:12:58 +0000 UTC (1 container statuses recorded)
Nov 29 01:58:42.681: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 29 01:58:42.681: INFO: metallb-controller-dd895cddd-v9jk5 from ccp started at 2020-11-29 01:13:08 +0000 UTC (1 container statuses recorded)
Nov 29 01:58:42.681: INFO: 	Container metallb-controller ready: true, restart count 0
Nov 29 01:58:42.681: INFO: sonobuoy-systemd-logs-daemon-set-a18500f604534dc2-vsjhg from sonobuoy started at 2020-11-29 01:15:14 +0000 UTC (2 container statuses recorded)
Nov 29 01:58:42.681: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 29 01:58:42.681: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 29 01:58:42.681: INFO: 
Logging pods the kubelet thinks is on node alex-cp1516-v3-vsp2-node-group-ea4104c57e before test
Nov 29 01:58:42.692: INFO: kube-proxy-xsm2v from kube-system started at 2020-11-29 01:13:00 +0000 UTC (1 container statuses recorded)
Nov 29 01:58:42.692: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 29 01:58:42.692: INFO: metallb-speaker-vlg6h from ccp started at 2020-11-29 01:13:10 +0000 UTC (1 container statuses recorded)
Nov 29 01:58:42.692: INFO: 	Container metallb-speaker ready: true, restart count 0
Nov 29 01:58:42.692: INFO: ingress-nginx-controller-rpdcv from ccp started at 2020-11-29 01:13:10 +0000 UTC (1 container statuses recorded)
Nov 29 01:58:42.692: INFO: 	Container controller ready: true, restart count 0
Nov 29 01:58:42.692: INFO: sonobuoy-systemd-logs-daemon-set-a18500f604534dc2-449j5 from sonobuoy started at 2020-11-29 01:15:13 +0000 UTC (2 container statuses recorded)
Nov 29 01:58:42.692: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 29 01:58:42.692: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 29 01:58:42.692: INFO: calico-node-c7rxt from kube-system started at 2020-11-29 01:13:00 +0000 UTC (1 container statuses recorded)
Nov 29 01:58:42.692: INFO: 	Container calico-node ready: true, restart count 0
Nov 29 01:58:42.692: INFO: ingress-nginx-defaultbackend-6cd8f668c8-6mw8w from ccp started at 2020-11-29 01:13:14 +0000 UTC (1 container statuses recorded)
Nov 29 01:58:42.692: INFO: 	Container ingress-nginx-default-backend ready: true, restart count 0
Nov 29 01:58:42.692: INFO: test-host-network-pod from e2e-kubelet-etc-hosts-1306 started at 2020-11-29 01:58:17 +0000 UTC (2 container statuses recorded)
Nov 29 01:58:42.692: INFO: 	Container busybox-1 ready: true, restart count 0
Nov 29 01:58:42.692: INFO: 	Container busybox-2 ready: true, restart count 0
Nov 29 01:58:42.692: INFO: pod-projected-configmaps-88c2e1e8-b50f-4c52-8be9-19c0d793760d from projected-1346 started at 2020-11-29 01:58:34 +0000 UTC (3 container statuses recorded)
Nov 29 01:58:42.692: INFO: 	Container createcm-volume-test ready: true, restart count 0
Nov 29 01:58:42.692: INFO: 	Container delcm-volume-test ready: true, restart count 0
Nov 29 01:58:42.692: INFO: 	Container updcm-volume-test ready: true, restart count 0
Nov 29 01:58:42.692: INFO: nvidia-device-plugin-daemonset-7r662 from kube-system started at 2020-11-29 01:13:10 +0000 UTC (1 container statuses recorded)
Nov 29 01:58:42.692: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.164bd7f0f4a39e40], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:58:43.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4352" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":277,"completed":114,"skipped":1848,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:58:43.738: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod liveness-96d7b4e9-26a5-4747-8ff5-59b260381862 in namespace container-probe-8719
Nov 29 01:58:47.808: INFO: Started pod liveness-96d7b4e9-26a5-4747-8ff5-59b260381862 in namespace container-probe-8719
STEP: checking the pod's current state and verifying that restartCount is present
Nov 29 01:58:47.812: INFO: Initial restart count of pod liveness-96d7b4e9-26a5-4747-8ff5-59b260381862 is 0
Nov 29 01:59:11.856: INFO: Restart count of pod container-probe-8719/liveness-96d7b4e9-26a5-4747-8ff5-59b260381862 is now 1 (24.043940904s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:59:11.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8719" for this suite.

• [SLOW TEST:28.145 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":277,"completed":115,"skipped":1887,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:59:11.884: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 01:59:11.913: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Nov 29 01:59:16.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 --namespace=crd-publish-openapi-4992 create -f -'
Nov 29 01:59:18.806: INFO: stderr: ""
Nov 29 01:59:18.806: INFO: stdout: "e2e-test-crd-publish-openapi-9987-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Nov 29 01:59:18.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 --namespace=crd-publish-openapi-4992 delete e2e-test-crd-publish-openapi-9987-crds test-cr'
Nov 29 01:59:18.892: INFO: stderr: ""
Nov 29 01:59:18.892: INFO: stdout: "e2e-test-crd-publish-openapi-9987-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Nov 29 01:59:18.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 --namespace=crd-publish-openapi-4992 apply -f -'
Nov 29 01:59:19.152: INFO: stderr: ""
Nov 29 01:59:19.152: INFO: stdout: "e2e-test-crd-publish-openapi-9987-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Nov 29 01:59:19.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 --namespace=crd-publish-openapi-4992 delete e2e-test-crd-publish-openapi-9987-crds test-cr'
Nov 29 01:59:19.241: INFO: stderr: ""
Nov 29 01:59:19.241: INFO: stdout: "e2e-test-crd-publish-openapi-9987-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Nov 29 01:59:19.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 explain e2e-test-crd-publish-openapi-9987-crds'
Nov 29 01:59:19.514: INFO: stderr: ""
Nov 29 01:59:19.514: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9987-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:59:24.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4992" for this suite.

• [SLOW TEST:13.085 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":277,"completed":116,"skipped":1903,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:59:24.969: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:59:25.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-3281" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":277,"completed":117,"skipped":1924,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:59:25.087: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test override all
Nov 29 01:59:25.153: INFO: Waiting up to 5m0s for pod "client-containers-aebacfb0-52bb-4f31-b4eb-33684b37cfab" in namespace "containers-1292" to be "Succeeded or Failed"
Nov 29 01:59:25.159: INFO: Pod "client-containers-aebacfb0-52bb-4f31-b4eb-33684b37cfab": Phase="Pending", Reason="", readiness=false. Elapsed: 5.35203ms
Nov 29 01:59:27.162: INFO: Pod "client-containers-aebacfb0-52bb-4f31-b4eb-33684b37cfab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008479932s
Nov 29 01:59:29.164: INFO: Pod "client-containers-aebacfb0-52bb-4f31-b4eb-33684b37cfab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011130501s
STEP: Saw pod success
Nov 29 01:59:29.165: INFO: Pod "client-containers-aebacfb0-52bb-4f31-b4eb-33684b37cfab" satisfied condition "Succeeded or Failed"
Nov 29 01:59:29.173: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod client-containers-aebacfb0-52bb-4f31-b4eb-33684b37cfab container test-container: <nil>
STEP: delete the pod
Nov 29 01:59:29.217: INFO: Waiting for pod client-containers-aebacfb0-52bb-4f31-b4eb-33684b37cfab to disappear
Nov 29 01:59:29.226: INFO: Pod client-containers-aebacfb0-52bb-4f31-b4eb-33684b37cfab no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:59:29.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1292" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":277,"completed":118,"skipped":1934,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:59:29.234: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating all guestbook components
Nov 29 01:59:29.273: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-slave
  labels:
    app: agnhost
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: slave
    tier: backend

Nov 29 01:59:29.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 create -f - --namespace=kubectl-1881'
Nov 29 01:59:29.598: INFO: stderr: ""
Nov 29 01:59:29.598: INFO: stdout: "service/agnhost-slave created\n"
Nov 29 01:59:29.598: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-master
  labels:
    app: agnhost
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: master
    tier: backend

Nov 29 01:59:29.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 create -f - --namespace=kubectl-1881'
Nov 29 01:59:29.886: INFO: stderr: ""
Nov 29 01:59:29.886: INFO: stdout: "service/agnhost-master created\n"
Nov 29 01:59:29.887: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Nov 29 01:59:29.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 create -f - --namespace=kubectl-1881'
Nov 29 01:59:30.165: INFO: stderr: ""
Nov 29 01:59:30.165: INFO: stdout: "service/frontend created\n"
Nov 29 01:59:30.165: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Nov 29 01:59:30.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 create -f - --namespace=kubectl-1881'
Nov 29 01:59:30.408: INFO: stderr: ""
Nov 29 01:59:30.408: INFO: stdout: "deployment.apps/frontend created\n"
Nov 29 01:59:30.408: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Nov 29 01:59:30.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 create -f - --namespace=kubectl-1881'
Nov 29 01:59:30.658: INFO: stderr: ""
Nov 29 01:59:30.658: INFO: stdout: "deployment.apps/agnhost-master created\n"
Nov 29 01:59:30.658: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12
        args: [ "guestbook", "--slaveof", "agnhost-master", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Nov 29 01:59:30.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 create -f - --namespace=kubectl-1881'
Nov 29 01:59:30.981: INFO: stderr: ""
Nov 29 01:59:30.981: INFO: stdout: "deployment.apps/agnhost-slave created\n"
STEP: validating guestbook app
Nov 29 01:59:30.981: INFO: Waiting for all frontend pods to be Running.
Nov 29 01:59:41.032: INFO: Waiting for frontend to serve content.
Nov 29 01:59:41.042: INFO: Trying to add a new entry to the guestbook.
Nov 29 01:59:41.052: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Nov 29 01:59:41.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 delete --grace-period=0 --force -f - --namespace=kubectl-1881'
Nov 29 01:59:41.189: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 29 01:59:41.190: INFO: stdout: "service \"agnhost-slave\" force deleted\n"
STEP: using delete to clean up resources
Nov 29 01:59:41.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 delete --grace-period=0 --force -f - --namespace=kubectl-1881'
Nov 29 01:59:41.319: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 29 01:59:41.319: INFO: stdout: "service \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Nov 29 01:59:41.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 delete --grace-period=0 --force -f - --namespace=kubectl-1881'
Nov 29 01:59:41.439: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 29 01:59:41.439: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Nov 29 01:59:41.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 delete --grace-period=0 --force -f - --namespace=kubectl-1881'
Nov 29 01:59:41.545: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 29 01:59:41.545: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Nov 29 01:59:41.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 delete --grace-period=0 --force -f - --namespace=kubectl-1881'
Nov 29 01:59:41.648: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 29 01:59:41.648: INFO: stdout: "deployment.apps \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Nov 29 01:59:41.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 delete --grace-period=0 --force -f - --namespace=kubectl-1881'
Nov 29 01:59:41.739: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 29 01:59:41.739: INFO: stdout: "deployment.apps \"agnhost-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:59:41.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1881" for this suite.

• [SLOW TEST:12.512 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:310
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":277,"completed":119,"skipped":1963,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:59:41.746: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 29 01:59:42.557: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 29 01:59:44.569: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211981, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211981, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211981, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742211981, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 01:59:47.586: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 01:59:48.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8119" for this suite.
STEP: Destroying namespace "webhook-8119-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.388 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":277,"completed":120,"skipped":1972,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 01:59:48.135: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 01:59:48.171: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Nov 29 01:59:53.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 --namespace=crd-publish-openapi-3856 create -f -'
Nov 29 01:59:54.967: INFO: stderr: ""
Nov 29 01:59:54.967: INFO: stdout: "e2e-test-crd-publish-openapi-6825-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Nov 29 01:59:54.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 --namespace=crd-publish-openapi-3856 delete e2e-test-crd-publish-openapi-6825-crds test-foo'
Nov 29 01:59:55.073: INFO: stderr: ""
Nov 29 01:59:55.073: INFO: stdout: "e2e-test-crd-publish-openapi-6825-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Nov 29 01:59:55.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 --namespace=crd-publish-openapi-3856 apply -f -'
Nov 29 01:59:55.319: INFO: stderr: ""
Nov 29 01:59:55.319: INFO: stdout: "e2e-test-crd-publish-openapi-6825-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Nov 29 01:59:55.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 --namespace=crd-publish-openapi-3856 delete e2e-test-crd-publish-openapi-6825-crds test-foo'
Nov 29 01:59:55.412: INFO: stderr: ""
Nov 29 01:59:55.412: INFO: stdout: "e2e-test-crd-publish-openapi-6825-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Nov 29 01:59:55.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 --namespace=crd-publish-openapi-3856 create -f -'
Nov 29 01:59:55.663: INFO: rc: 1
Nov 29 01:59:55.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 --namespace=crd-publish-openapi-3856 apply -f -'
Nov 29 01:59:55.884: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Nov 29 01:59:55.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 --namespace=crd-publish-openapi-3856 create -f -'
Nov 29 01:59:56.132: INFO: rc: 1
Nov 29 01:59:56.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 --namespace=crd-publish-openapi-3856 apply -f -'
Nov 29 01:59:56.378: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Nov 29 01:59:56.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 explain e2e-test-crd-publish-openapi-6825-crds'
Nov 29 01:59:56.617: INFO: stderr: ""
Nov 29 01:59:56.617: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6825-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Nov 29 01:59:56.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 explain e2e-test-crd-publish-openapi-6825-crds.metadata'
Nov 29 01:59:56.880: INFO: stderr: ""
Nov 29 01:59:56.880: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6825-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Nov 29 01:59:56.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 explain e2e-test-crd-publish-openapi-6825-crds.spec'
Nov 29 01:59:57.130: INFO: stderr: ""
Nov 29 01:59:57.130: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6825-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Nov 29 01:59:57.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 explain e2e-test-crd-publish-openapi-6825-crds.spec.bars'
Nov 29 01:59:57.366: INFO: stderr: ""
Nov 29 01:59:57.366: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6825-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Nov 29 01:59:57.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 explain e2e-test-crd-publish-openapi-6825-crds.spec.bars2'
Nov 29 01:59:57.614: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:00:03.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3856" for this suite.

• [SLOW TEST:15.046 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":277,"completed":121,"skipped":2036,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:00:03.182: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov 29 02:00:03.256: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:00:03.261: INFO: Number of nodes with available pods: 0
Nov 29 02:00:03.261: INFO: Node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d is running more than one daemon pod
Nov 29 02:00:04.266: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:00:04.269: INFO: Number of nodes with available pods: 0
Nov 29 02:00:04.269: INFO: Node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d is running more than one daemon pod
Nov 29 02:00:05.267: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:00:05.270: INFO: Number of nodes with available pods: 0
Nov 29 02:00:05.270: INFO: Node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d is running more than one daemon pod
Nov 29 02:00:06.266: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:00:06.269: INFO: Number of nodes with available pods: 3
Nov 29 02:00:06.269: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Nov 29 02:00:06.287: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:00:06.290: INFO: Number of nodes with available pods: 2
Nov 29 02:00:06.290: INFO: Node alex-cp1516-v3-vsp2-node-group-bc6233c429 is running more than one daemon pod
Nov 29 02:00:07.296: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:00:07.299: INFO: Number of nodes with available pods: 2
Nov 29 02:00:07.299: INFO: Node alex-cp1516-v3-vsp2-node-group-bc6233c429 is running more than one daemon pod
Nov 29 02:00:08.296: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:00:08.301: INFO: Number of nodes with available pods: 2
Nov 29 02:00:08.301: INFO: Node alex-cp1516-v3-vsp2-node-group-bc6233c429 is running more than one daemon pod
Nov 29 02:00:09.300: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:00:09.303: INFO: Number of nodes with available pods: 2
Nov 29 02:00:09.303: INFO: Node alex-cp1516-v3-vsp2-node-group-bc6233c429 is running more than one daemon pod
Nov 29 02:00:10.295: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:00:10.298: INFO: Number of nodes with available pods: 2
Nov 29 02:00:10.298: INFO: Node alex-cp1516-v3-vsp2-node-group-bc6233c429 is running more than one daemon pod
Nov 29 02:00:11.296: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:00:11.298: INFO: Number of nodes with available pods: 2
Nov 29 02:00:11.298: INFO: Node alex-cp1516-v3-vsp2-node-group-bc6233c429 is running more than one daemon pod
Nov 29 02:00:12.295: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:00:12.299: INFO: Number of nodes with available pods: 3
Nov 29 02:00:12.299: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9333, will wait for the garbage collector to delete the pods
Nov 29 02:00:12.362: INFO: Deleting DaemonSet.extensions daemon-set took: 7.401246ms
Nov 29 02:00:13.163: INFO: Terminating DaemonSet.extensions daemon-set pods took: 801.510456ms
Nov 29 02:00:16.166: INFO: Number of nodes with available pods: 0
Nov 29 02:00:16.166: INFO: Number of running nodes: 0, number of available pods: 0
Nov 29 02:00:16.169: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9333/daemonsets","resourceVersion":"25164"},"items":null}

Nov 29 02:00:16.171: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9333/pods","resourceVersion":"25164"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:00:16.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9333" for this suite.

• [SLOW TEST:13.008 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":277,"completed":122,"skipped":2070,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:00:16.190: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on node default medium
Nov 29 02:00:16.226: INFO: Waiting up to 5m0s for pod "pod-ba1ecef2-afca-499f-a412-ed0041eb5902" in namespace "emptydir-8954" to be "Succeeded or Failed"
Nov 29 02:00:16.245: INFO: Pod "pod-ba1ecef2-afca-499f-a412-ed0041eb5902": Phase="Pending", Reason="", readiness=false. Elapsed: 18.961629ms
Nov 29 02:00:18.248: INFO: Pod "pod-ba1ecef2-afca-499f-a412-ed0041eb5902": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022445445s
Nov 29 02:00:20.253: INFO: Pod "pod-ba1ecef2-afca-499f-a412-ed0041eb5902": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027164322s
STEP: Saw pod success
Nov 29 02:00:20.253: INFO: Pod "pod-ba1ecef2-afca-499f-a412-ed0041eb5902" satisfied condition "Succeeded or Failed"
Nov 29 02:00:20.256: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod pod-ba1ecef2-afca-499f-a412-ed0041eb5902 container test-container: <nil>
STEP: delete the pod
Nov 29 02:00:20.289: INFO: Waiting for pod pod-ba1ecef2-afca-499f-a412-ed0041eb5902 to disappear
Nov 29 02:00:20.291: INFO: Pod pod-ba1ecef2-afca-499f-a412-ed0041eb5902 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:00:20.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8954" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":123,"skipped":2071,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:00:20.300: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 29 02:00:20.752: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 29 02:00:22.760: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742212020, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742212020, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742212020, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742212020, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 02:00:25.805: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 02:00:25.808: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8867-crds.webhook.example.com via the AdmissionRegistration API
Nov 29 02:00:26.343: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:00:27.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7469" for this suite.
STEP: Destroying namespace "webhook-7469-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.916 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":277,"completed":124,"skipped":2071,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:00:27.222: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-6783
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 29 02:00:27.283: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 29 02:00:27.344: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 29 02:00:29.348: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 29 02:00:31.348: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 02:00:33.347: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 02:00:35.347: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 02:00:37.347: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 02:00:39.348: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 02:00:41.348: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 02:00:43.350: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 02:00:45.347: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 02:00:47.347: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 02:00:49.347: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov 29 02:00:49.359: INFO: The status of Pod netserver-1 is Running (Ready = true)
Nov 29 02:00:49.365: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Nov 29 02:00:53.394: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.191.151:8080/dial?request=hostname&protocol=http&host=192.168.93.218&port=8080&tries=1'] Namespace:pod-network-test-6783 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 02:00:53.394: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
Nov 29 02:00:53.530: INFO: Waiting for responses: map[]
Nov 29 02:00:53.533: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.191.151:8080/dial?request=hostname&protocol=http&host=192.168.191.149&port=8080&tries=1'] Namespace:pod-network-test-6783 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 02:00:53.533: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
Nov 29 02:00:53.662: INFO: Waiting for responses: map[]
Nov 29 02:00:53.669: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.191.151:8080/dial?request=hostname&protocol=http&host=192.168.36.204&port=8080&tries=1'] Namespace:pod-network-test-6783 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 02:00:53.669: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
Nov 29 02:00:53.805: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:00:53.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6783" for this suite.

• [SLOW TEST:26.595 seconds]
[sig-network] Networking
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":277,"completed":125,"skipped":2094,"failed":0}
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:00:53.818: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Nov 29 02:00:53.845: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 29 02:00:53.865: INFO: Waiting for terminating namespaces to be deleted...
Nov 29 02:00:53.867: INFO: 
Logging pods the kubelet thinks is on node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d before test
Nov 29 02:00:53.881: INFO: metallb-speaker-5sjng from ccp started at 2020-11-29 01:13:11 +0000 UTC (1 container statuses recorded)
Nov 29 02:00:53.881: INFO: 	Container metallb-speaker ready: true, restart count 0
Nov 29 02:00:53.881: INFO: sonobuoy from sonobuoy started at 2020-11-29 01:14:36 +0000 UTC (1 container statuses recorded)
Nov 29 02:00:53.881: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 29 02:00:53.881: INFO: kube-proxy-zjcgv from kube-system started at 2020-11-29 01:13:00 +0000 UTC (1 container statuses recorded)
Nov 29 02:00:53.881: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 29 02:00:53.881: INFO: nvidia-device-plugin-daemonset-f79b4 from kube-system started at 2020-11-29 01:13:11 +0000 UTC (1 container statuses recorded)
Nov 29 02:00:53.881: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Nov 29 02:00:53.881: INFO: ingress-nginx-controller-zd8lx from ccp started at 2020-11-29 01:13:11 +0000 UTC (1 container statuses recorded)
Nov 29 02:00:53.881: INFO: 	Container controller ready: true, restart count 0
Nov 29 02:00:53.881: INFO: sonobuoy-e2e-job-df1d2fd3866f4821 from sonobuoy started at 2020-11-29 01:15:14 +0000 UTC (2 container statuses recorded)
Nov 29 02:00:53.881: INFO: 	Container e2e ready: true, restart count 0
Nov 29 02:00:53.881: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 29 02:00:53.881: INFO: sonobuoy-systemd-logs-daemon-set-a18500f604534dc2-g2xkd from sonobuoy started at 2020-11-29 01:15:14 +0000 UTC (2 container statuses recorded)
Nov 29 02:00:53.881: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 29 02:00:53.881: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 29 02:00:53.881: INFO: netserver-0 from pod-network-test-6783 started at 2020-11-29 02:00:27 +0000 UTC (1 container statuses recorded)
Nov 29 02:00:53.881: INFO: 	Container webserver ready: true, restart count 0
Nov 29 02:00:53.881: INFO: calico-node-gs8lr from kube-system started at 2020-11-29 01:13:01 +0000 UTC (1 container statuses recorded)
Nov 29 02:00:53.881: INFO: 	Container calico-node ready: true, restart count 0
Nov 29 02:00:53.881: INFO: 
Logging pods the kubelet thinks is on node alex-cp1516-v3-vsp2-node-group-bc6233c429 before test
Nov 29 02:00:53.897: INFO: test-container-pod from pod-network-test-6783 started at 2020-11-29 02:00:49 +0000 UTC (1 container statuses recorded)
Nov 29 02:00:53.897: INFO: 	Container webserver ready: true, restart count 0
Nov 29 02:00:53.897: INFO: kube-proxy-wcdhr from kube-system started at 2020-11-29 01:12:58 +0000 UTC (1 container statuses recorded)
Nov 29 02:00:53.897: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 29 02:00:53.897: INFO: metallb-controller-dd895cddd-v9jk5 from ccp started at 2020-11-29 01:13:08 +0000 UTC (1 container statuses recorded)
Nov 29 02:00:53.897: INFO: 	Container metallb-controller ready: true, restart count 0
Nov 29 02:00:53.897: INFO: sonobuoy-systemd-logs-daemon-set-a18500f604534dc2-vsjhg from sonobuoy started at 2020-11-29 01:15:14 +0000 UTC (2 container statuses recorded)
Nov 29 02:00:53.897: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 29 02:00:53.897: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 29 02:00:53.897: INFO: ingress-nginx-controller-vppmg from ccp started at 2020-11-29 01:13:08 +0000 UTC (1 container statuses recorded)
Nov 29 02:00:53.897: INFO: 	Container controller ready: true, restart count 0
Nov 29 02:00:53.897: INFO: netserver-1 from pod-network-test-6783 started at 2020-11-29 02:00:27 +0000 UTC (1 container statuses recorded)
Nov 29 02:00:53.897: INFO: 	Container webserver ready: true, restart count 0
Nov 29 02:00:53.897: INFO: calico-node-xhkv2 from kube-system started at 2020-11-29 01:12:58 +0000 UTC (1 container statuses recorded)
Nov 29 02:00:53.897: INFO: 	Container calico-node ready: true, restart count 0
Nov 29 02:00:53.897: INFO: nvidia-device-plugin-daemonset-chhlf from kube-system started at 2020-11-29 01:13:08 +0000 UTC (1 container statuses recorded)
Nov 29 02:00:53.897: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Nov 29 02:00:53.897: INFO: metallb-speaker-55nsq from ccp started at 2020-11-29 01:13:08 +0000 UTC (1 container statuses recorded)
Nov 29 02:00:53.897: INFO: 	Container metallb-speaker ready: true, restart count 0
Nov 29 02:00:53.897: INFO: 
Logging pods the kubelet thinks is on node alex-cp1516-v3-vsp2-node-group-ea4104c57e before test
Nov 29 02:00:53.907: INFO: nvidia-device-plugin-daemonset-7r662 from kube-system started at 2020-11-29 01:13:10 +0000 UTC (1 container statuses recorded)
Nov 29 02:00:53.907: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Nov 29 02:00:53.907: INFO: ingress-nginx-defaultbackend-6cd8f668c8-6mw8w from ccp started at 2020-11-29 01:13:14 +0000 UTC (1 container statuses recorded)
Nov 29 02:00:53.907: INFO: 	Container ingress-nginx-default-backend ready: true, restart count 0
Nov 29 02:00:53.907: INFO: netserver-2 from pod-network-test-6783 started at 2020-11-29 02:00:27 +0000 UTC (1 container statuses recorded)
Nov 29 02:00:53.907: INFO: 	Container webserver ready: true, restart count 0
Nov 29 02:00:53.907: INFO: calico-node-c7rxt from kube-system started at 2020-11-29 01:13:00 +0000 UTC (1 container statuses recorded)
Nov 29 02:00:53.907: INFO: 	Container calico-node ready: true, restart count 0
Nov 29 02:00:53.907: INFO: kube-proxy-xsm2v from kube-system started at 2020-11-29 01:13:00 +0000 UTC (1 container statuses recorded)
Nov 29 02:00:53.907: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 29 02:00:53.907: INFO: metallb-speaker-vlg6h from ccp started at 2020-11-29 01:13:10 +0000 UTC (1 container statuses recorded)
Nov 29 02:00:53.907: INFO: 	Container metallb-speaker ready: true, restart count 0
Nov 29 02:00:53.907: INFO: ingress-nginx-controller-rpdcv from ccp started at 2020-11-29 01:13:10 +0000 UTC (1 container statuses recorded)
Nov 29 02:00:53.907: INFO: 	Container controller ready: true, restart count 0
Nov 29 02:00:53.907: INFO: sonobuoy-systemd-logs-daemon-set-a18500f604534dc2-449j5 from sonobuoy started at 2020-11-29 01:15:13 +0000 UTC (2 container statuses recorded)
Nov 29 02:00:53.907: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 29 02:00:53.907: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-a65c5bfe-e33a-4f81-b9b4-bb933ac64db7 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-a65c5bfe-e33a-4f81-b9b4-bb933ac64db7 off the node alex-cp1516-v3-vsp2-node-group-ea4104c57e
STEP: verifying the node doesn't have the label kubernetes.io/e2e-a65c5bfe-e33a-4f81-b9b4-bb933ac64db7
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:06:02.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9272" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

• [SLOW TEST:308.194 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":277,"completed":126,"skipped":2099,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:06:02.012: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Nov 29 02:06:02.053: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fdf5bae3-ded9-418b-84b7-29ff5087ef59" in namespace "projected-4038" to be "Succeeded or Failed"
Nov 29 02:06:02.057: INFO: Pod "downwardapi-volume-fdf5bae3-ded9-418b-84b7-29ff5087ef59": Phase="Pending", Reason="", readiness=false. Elapsed: 3.643323ms
Nov 29 02:06:04.060: INFO: Pod "downwardapi-volume-fdf5bae3-ded9-418b-84b7-29ff5087ef59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006865541s
Nov 29 02:06:06.064: INFO: Pod "downwardapi-volume-fdf5bae3-ded9-418b-84b7-29ff5087ef59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010586325s
STEP: Saw pod success
Nov 29 02:06:06.064: INFO: Pod "downwardapi-volume-fdf5bae3-ded9-418b-84b7-29ff5087ef59" satisfied condition "Succeeded or Failed"
Nov 29 02:06:06.066: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod downwardapi-volume-fdf5bae3-ded9-418b-84b7-29ff5087ef59 container client-container: <nil>
STEP: delete the pod
Nov 29 02:06:06.099: INFO: Waiting for pod downwardapi-volume-fdf5bae3-ded9-418b-84b7-29ff5087ef59 to disappear
Nov 29 02:06:06.103: INFO: Pod downwardapi-volume-fdf5bae3-ded9-418b-84b7-29ff5087ef59 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:06:06.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4038" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":277,"completed":127,"skipped":2100,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:06:06.111: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:06:17.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6231" for this suite.

• [SLOW TEST:11.135 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":277,"completed":128,"skipped":2101,"failed":0}
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:06:17.246: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 02:06:17.302: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Nov 29 02:06:17.316: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:06:17.325: INFO: Number of nodes with available pods: 0
Nov 29 02:06:17.325: INFO: Node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d is running more than one daemon pod
Nov 29 02:06:18.331: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:06:18.334: INFO: Number of nodes with available pods: 0
Nov 29 02:06:18.335: INFO: Node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d is running more than one daemon pod
Nov 29 02:06:19.331: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:06:19.334: INFO: Number of nodes with available pods: 0
Nov 29 02:06:19.334: INFO: Node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d is running more than one daemon pod
Nov 29 02:06:20.334: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:06:20.338: INFO: Number of nodes with available pods: 3
Nov 29 02:06:20.338: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Nov 29 02:06:20.370: INFO: Wrong image for pod: daemon-set-bqvnw. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 02:06:20.370: INFO: Wrong image for pod: daemon-set-qjmgh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 02:06:20.370: INFO: Wrong image for pod: daemon-set-v66vv. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 02:06:20.379: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:06:21.384: INFO: Wrong image for pod: daemon-set-bqvnw. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 02:06:21.384: INFO: Wrong image for pod: daemon-set-qjmgh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 02:06:21.384: INFO: Wrong image for pod: daemon-set-v66vv. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 02:06:21.393: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:06:22.384: INFO: Wrong image for pod: daemon-set-bqvnw. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 02:06:22.384: INFO: Wrong image for pod: daemon-set-qjmgh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 02:06:22.384: INFO: Wrong image for pod: daemon-set-v66vv. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 02:06:22.390: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:06:23.385: INFO: Wrong image for pod: daemon-set-bqvnw. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 02:06:23.385: INFO: Wrong image for pod: daemon-set-qjmgh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 02:06:23.385: INFO: Wrong image for pod: daemon-set-v66vv. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 02:06:23.385: INFO: Pod daemon-set-v66vv is not available
Nov 29 02:06:23.391: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:06:24.386: INFO: Pod daemon-set-64x46 is not available
Nov 29 02:06:24.386: INFO: Wrong image for pod: daemon-set-bqvnw. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 02:06:24.386: INFO: Wrong image for pod: daemon-set-qjmgh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 02:06:24.391: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:06:25.386: INFO: Pod daemon-set-64x46 is not available
Nov 29 02:06:25.386: INFO: Wrong image for pod: daemon-set-bqvnw. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 02:06:25.386: INFO: Wrong image for pod: daemon-set-qjmgh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 02:06:25.393: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:06:26.384: INFO: Wrong image for pod: daemon-set-bqvnw. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 02:06:26.384: INFO: Wrong image for pod: daemon-set-qjmgh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 02:06:26.388: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:06:27.383: INFO: Wrong image for pod: daemon-set-bqvnw. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 02:06:27.383: INFO: Pod daemon-set-bqvnw is not available
Nov 29 02:06:27.383: INFO: Wrong image for pod: daemon-set-qjmgh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 02:06:27.387: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:06:28.387: INFO: Wrong image for pod: daemon-set-bqvnw. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 02:06:28.387: INFO: Pod daemon-set-bqvnw is not available
Nov 29 02:06:28.387: INFO: Wrong image for pod: daemon-set-qjmgh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 02:06:28.392: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:06:29.383: INFO: Wrong image for pod: daemon-set-bqvnw. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 02:06:29.383: INFO: Pod daemon-set-bqvnw is not available
Nov 29 02:06:29.384: INFO: Wrong image for pod: daemon-set-qjmgh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 02:06:29.388: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:06:30.387: INFO: Pod daemon-set-lrnjs is not available
Nov 29 02:06:30.387: INFO: Wrong image for pod: daemon-set-qjmgh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 02:06:30.391: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:06:31.384: INFO: Pod daemon-set-lrnjs is not available
Nov 29 02:06:31.384: INFO: Wrong image for pod: daemon-set-qjmgh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 02:06:31.389: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:06:32.384: INFO: Wrong image for pod: daemon-set-qjmgh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 02:06:32.392: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:06:33.385: INFO: Wrong image for pod: daemon-set-qjmgh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 02:06:33.392: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:06:34.383: INFO: Wrong image for pod: daemon-set-qjmgh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Nov 29 02:06:34.384: INFO: Pod daemon-set-qjmgh is not available
Nov 29 02:06:34.389: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:06:35.383: INFO: Pod daemon-set-w4x58 is not available
Nov 29 02:06:35.387: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Nov 29 02:06:35.406: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:06:35.411: INFO: Number of nodes with available pods: 2
Nov 29 02:06:35.411: INFO: Node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d is running more than one daemon pod
Nov 29 02:06:36.421: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:06:36.425: INFO: Number of nodes with available pods: 2
Nov 29 02:06:36.425: INFO: Node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d is running more than one daemon pod
Nov 29 02:06:37.418: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:06:37.424: INFO: Number of nodes with available pods: 3
Nov 29 02:06:37.424: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3166, will wait for the garbage collector to delete the pods
Nov 29 02:06:37.505: INFO: Deleting DaemonSet.extensions daemon-set took: 6.038685ms
Nov 29 02:06:38.306: INFO: Terminating DaemonSet.extensions daemon-set pods took: 800.22059ms
Nov 29 02:06:47.509: INFO: Number of nodes with available pods: 0
Nov 29 02:06:47.509: INFO: Number of running nodes: 0, number of available pods: 0
Nov 29 02:06:47.511: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3166/daemonsets","resourceVersion":"27874"},"items":null}

Nov 29 02:06:47.513: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3166/pods","resourceVersion":"27874"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:06:47.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3166" for this suite.

• [SLOW TEST:30.288 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":277,"completed":129,"skipped":2103,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:06:47.535: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod busybox-f7d37e9d-f141-4fb1-a87a-bf64ebd6e59e in namespace container-probe-478
Nov 29 02:06:51.600: INFO: Started pod busybox-f7d37e9d-f141-4fb1-a87a-bf64ebd6e59e in namespace container-probe-478
STEP: checking the pod's current state and verifying that restartCount is present
Nov 29 02:06:51.606: INFO: Initial restart count of pod busybox-f7d37e9d-f141-4fb1-a87a-bf64ebd6e59e is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:10:52.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-478" for this suite.

• [SLOW TEST:244.520 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":277,"completed":130,"skipped":2147,"failed":0}
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:10:52.056: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: getting the auto-created API token
STEP: reading a file in the container
Nov 29 02:10:56.614: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8118 pod-service-account-cf0d94b3-e147-4cb7-91b5-f1f52d063596 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Nov 29 02:10:58.371: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8118 pod-service-account-cf0d94b3-e147-4cb7-91b5-f1f52d063596 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Nov 29 02:10:58.594: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8118 pod-service-account-cf0d94b3-e147-4cb7-91b5-f1f52d063596 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:10:58.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8118" for this suite.

• [SLOW TEST:6.786 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":277,"completed":131,"skipped":2156,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:10:58.841: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:171
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating server pod server in namespace prestop-9826
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-9826
STEP: Deleting pre-stop pod
Nov 29 02:11:11.974: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:11:11.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-9826" for this suite.

• [SLOW TEST:13.152 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":277,"completed":132,"skipped":2165,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:11:11.993: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 02:11:12.044: INFO: Waiting up to 5m0s for pod "busybox-user-65534-a48d81ed-7919-4681-af39-d6b83fcb7d20" in namespace "security-context-test-152" to be "Succeeded or Failed"
Nov 29 02:11:12.052: INFO: Pod "busybox-user-65534-a48d81ed-7919-4681-af39-d6b83fcb7d20": Phase="Pending", Reason="", readiness=false. Elapsed: 8.348497ms
Nov 29 02:11:14.055: INFO: Pod "busybox-user-65534-a48d81ed-7919-4681-af39-d6b83fcb7d20": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010817572s
Nov 29 02:11:16.058: INFO: Pod "busybox-user-65534-a48d81ed-7919-4681-af39-d6b83fcb7d20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014004646s
Nov 29 02:11:16.058: INFO: Pod "busybox-user-65534-a48d81ed-7919-4681-af39-d6b83fcb7d20" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:11:16.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-152" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":133,"skipped":2177,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:11:16.066: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir volume type on node default medium
Nov 29 02:11:16.096: INFO: Waiting up to 5m0s for pod "pod-b344d831-82f9-40ec-a838-0c2f23d1e1fa" in namespace "emptydir-2674" to be "Succeeded or Failed"
Nov 29 02:11:16.104: INFO: Pod "pod-b344d831-82f9-40ec-a838-0c2f23d1e1fa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.037701ms
Nov 29 02:11:18.108: INFO: Pod "pod-b344d831-82f9-40ec-a838-0c2f23d1e1fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011581986s
Nov 29 02:11:20.111: INFO: Pod "pod-b344d831-82f9-40ec-a838-0c2f23d1e1fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014974302s
STEP: Saw pod success
Nov 29 02:11:20.111: INFO: Pod "pod-b344d831-82f9-40ec-a838-0c2f23d1e1fa" satisfied condition "Succeeded or Failed"
Nov 29 02:11:20.113: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod pod-b344d831-82f9-40ec-a838-0c2f23d1e1fa container test-container: <nil>
STEP: delete the pod
Nov 29 02:11:20.155: INFO: Waiting for pod pod-b344d831-82f9-40ec-a838-0c2f23d1e1fa to disappear
Nov 29 02:11:20.165: INFO: Pod pod-b344d831-82f9-40ec-a838-0c2f23d1e1fa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:11:20.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2674" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":134,"skipped":2198,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:11:20.175: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Starting the proxy
Nov 29 02:11:20.199: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-195255472 proxy --unix-socket=/tmp/kubectl-proxy-unix889561215/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:11:20.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3487" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":277,"completed":135,"skipped":2201,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:11:20.277: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:11:33.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3786" for this suite.

• [SLOW TEST:13.112 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":277,"completed":136,"skipped":2202,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:11:33.393: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Nov 29 02:11:33.436: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
Nov 29 02:11:39.075: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:12:00.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5121" for this suite.

• [SLOW TEST:27.434 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":277,"completed":137,"skipped":2214,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:12:00.827: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Update Demo
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:271
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a replication controller
Nov 29 02:12:00.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 create -f - --namespace=kubectl-6057'
Nov 29 02:12:01.195: INFO: stderr: ""
Nov 29 02:12:01.195: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 29 02:12:01.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6057'
Nov 29 02:12:01.321: INFO: stderr: ""
Nov 29 02:12:01.321: INFO: stdout: "update-demo-nautilus-cfjwg update-demo-nautilus-lqrcc "
Nov 29 02:12:01.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get pods update-demo-nautilus-cfjwg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6057'
Nov 29 02:12:01.431: INFO: stderr: ""
Nov 29 02:12:01.431: INFO: stdout: ""
Nov 29 02:12:01.431: INFO: update-demo-nautilus-cfjwg is created but not running
Nov 29 02:12:06.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6057'
Nov 29 02:12:06.519: INFO: stderr: ""
Nov 29 02:12:06.519: INFO: stdout: "update-demo-nautilus-cfjwg update-demo-nautilus-lqrcc "
Nov 29 02:12:06.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get pods update-demo-nautilus-cfjwg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6057'
Nov 29 02:12:06.615: INFO: stderr: ""
Nov 29 02:12:06.615: INFO: stdout: "true"
Nov 29 02:12:06.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get pods update-demo-nautilus-cfjwg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6057'
Nov 29 02:12:06.703: INFO: stderr: ""
Nov 29 02:12:06.703: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 29 02:12:06.703: INFO: validating pod update-demo-nautilus-cfjwg
Nov 29 02:12:06.706: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 29 02:12:06.707: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 29 02:12:06.707: INFO: update-demo-nautilus-cfjwg is verified up and running
Nov 29 02:12:06.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get pods update-demo-nautilus-lqrcc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6057'
Nov 29 02:12:06.807: INFO: stderr: ""
Nov 29 02:12:06.807: INFO: stdout: "true"
Nov 29 02:12:06.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get pods update-demo-nautilus-lqrcc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6057'
Nov 29 02:12:06.892: INFO: stderr: ""
Nov 29 02:12:06.892: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 29 02:12:06.892: INFO: validating pod update-demo-nautilus-lqrcc
Nov 29 02:12:06.896: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 29 02:12:06.896: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 29 02:12:06.896: INFO: update-demo-nautilus-lqrcc is verified up and running
STEP: scaling down the replication controller
Nov 29 02:12:06.898: INFO: scanned /root for discovery docs: <nil>
Nov 29 02:12:06.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-6057'
Nov 29 02:12:08.022: INFO: stderr: ""
Nov 29 02:12:08.022: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 29 02:12:08.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6057'
Nov 29 02:12:08.130: INFO: stderr: ""
Nov 29 02:12:08.130: INFO: stdout: "update-demo-nautilus-cfjwg update-demo-nautilus-lqrcc "
STEP: Replicas for name=update-demo: expected=1 actual=2
Nov 29 02:12:13.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6057'
Nov 29 02:12:13.218: INFO: stderr: ""
Nov 29 02:12:13.218: INFO: stdout: "update-demo-nautilus-lqrcc "
Nov 29 02:12:13.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get pods update-demo-nautilus-lqrcc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6057'
Nov 29 02:12:13.299: INFO: stderr: ""
Nov 29 02:12:13.299: INFO: stdout: "true"
Nov 29 02:12:13.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get pods update-demo-nautilus-lqrcc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6057'
Nov 29 02:12:13.389: INFO: stderr: ""
Nov 29 02:12:13.389: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 29 02:12:13.389: INFO: validating pod update-demo-nautilus-lqrcc
Nov 29 02:12:13.392: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 29 02:12:13.392: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 29 02:12:13.392: INFO: update-demo-nautilus-lqrcc is verified up and running
STEP: scaling up the replication controller
Nov 29 02:12:13.394: INFO: scanned /root for discovery docs: <nil>
Nov 29 02:12:13.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-6057'
Nov 29 02:12:14.512: INFO: stderr: ""
Nov 29 02:12:14.512: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 29 02:12:14.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6057'
Nov 29 02:12:14.600: INFO: stderr: ""
Nov 29 02:12:14.600: INFO: stdout: "update-demo-nautilus-4npjx update-demo-nautilus-lqrcc "
Nov 29 02:12:14.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get pods update-demo-nautilus-4npjx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6057'
Nov 29 02:12:14.692: INFO: stderr: ""
Nov 29 02:12:14.692: INFO: stdout: ""
Nov 29 02:12:14.692: INFO: update-demo-nautilus-4npjx is created but not running
Nov 29 02:12:19.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6057'
Nov 29 02:12:19.780: INFO: stderr: ""
Nov 29 02:12:19.780: INFO: stdout: "update-demo-nautilus-4npjx update-demo-nautilus-lqrcc "
Nov 29 02:12:19.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get pods update-demo-nautilus-4npjx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6057'
Nov 29 02:12:19.868: INFO: stderr: ""
Nov 29 02:12:19.868: INFO: stdout: "true"
Nov 29 02:12:19.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get pods update-demo-nautilus-4npjx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6057'
Nov 29 02:12:19.958: INFO: stderr: ""
Nov 29 02:12:19.958: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 29 02:12:19.958: INFO: validating pod update-demo-nautilus-4npjx
Nov 29 02:12:19.963: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 29 02:12:19.963: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 29 02:12:19.963: INFO: update-demo-nautilus-4npjx is verified up and running
Nov 29 02:12:19.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get pods update-demo-nautilus-lqrcc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6057'
Nov 29 02:12:20.055: INFO: stderr: ""
Nov 29 02:12:20.055: INFO: stdout: "true"
Nov 29 02:12:20.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get pods update-demo-nautilus-lqrcc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6057'
Nov 29 02:12:20.144: INFO: stderr: ""
Nov 29 02:12:20.144: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 29 02:12:20.144: INFO: validating pod update-demo-nautilus-lqrcc
Nov 29 02:12:20.147: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 29 02:12:20.147: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 29 02:12:20.147: INFO: update-demo-nautilus-lqrcc is verified up and running
STEP: using delete to clean up resources
Nov 29 02:12:20.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 delete --grace-period=0 --force -f - --namespace=kubectl-6057'
Nov 29 02:12:20.242: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 29 02:12:20.242: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov 29 02:12:20.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6057'
Nov 29 02:12:20.360: INFO: stderr: "No resources found in kubectl-6057 namespace.\n"
Nov 29 02:12:20.360: INFO: stdout: ""
Nov 29 02:12:20.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get pods -l name=update-demo --namespace=kubectl-6057 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 29 02:12:20.457: INFO: stderr: ""
Nov 29 02:12:20.457: INFO: stdout: "update-demo-nautilus-4npjx\nupdate-demo-nautilus-lqrcc\n"
Nov 29 02:12:20.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6057'
Nov 29 02:12:21.068: INFO: stderr: "No resources found in kubectl-6057 namespace.\n"
Nov 29 02:12:21.069: INFO: stdout: ""
Nov 29 02:12:21.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get pods -l name=update-demo --namespace=kubectl-6057 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 29 02:12:21.177: INFO: stderr: ""
Nov 29 02:12:21.177: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:12:21.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6057" for this suite.

• [SLOW TEST:20.359 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:269
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":277,"completed":138,"skipped":2228,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:12:21.187: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Nov 29 02:12:21.238: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:12:24.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1027" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":277,"completed":139,"skipped":2246,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:12:24.952: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:12:32.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7868" for this suite.

• [SLOW TEST:8.054 seconds]
[sig-apps] Job
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":277,"completed":140,"skipped":2258,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:12:33.006: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 29 02:12:33.981: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 29 02:12:35.990: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742212753, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742212753, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742212753, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742212753, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 02:12:39.020: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:12:51.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6547" for this suite.
STEP: Destroying namespace "webhook-6547-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:18.214 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":277,"completed":141,"skipped":2298,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:12:51.220: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 02:12:51.273: INFO: The status of Pod test-webserver-3c3a9e99-70f9-4540-8264-4b5f8846f29e is Pending, waiting for it to be Running (with Ready = true)
Nov 29 02:12:53.276: INFO: The status of Pod test-webserver-3c3a9e99-70f9-4540-8264-4b5f8846f29e is Pending, waiting for it to be Running (with Ready = true)
Nov 29 02:12:55.275: INFO: The status of Pod test-webserver-3c3a9e99-70f9-4540-8264-4b5f8846f29e is Running (Ready = false)
Nov 29 02:12:57.277: INFO: The status of Pod test-webserver-3c3a9e99-70f9-4540-8264-4b5f8846f29e is Running (Ready = false)
Nov 29 02:12:59.276: INFO: The status of Pod test-webserver-3c3a9e99-70f9-4540-8264-4b5f8846f29e is Running (Ready = false)
Nov 29 02:13:01.276: INFO: The status of Pod test-webserver-3c3a9e99-70f9-4540-8264-4b5f8846f29e is Running (Ready = false)
Nov 29 02:13:03.276: INFO: The status of Pod test-webserver-3c3a9e99-70f9-4540-8264-4b5f8846f29e is Running (Ready = false)
Nov 29 02:13:05.280: INFO: The status of Pod test-webserver-3c3a9e99-70f9-4540-8264-4b5f8846f29e is Running (Ready = false)
Nov 29 02:13:07.276: INFO: The status of Pod test-webserver-3c3a9e99-70f9-4540-8264-4b5f8846f29e is Running (Ready = false)
Nov 29 02:13:09.275: INFO: The status of Pod test-webserver-3c3a9e99-70f9-4540-8264-4b5f8846f29e is Running (Ready = false)
Nov 29 02:13:11.276: INFO: The status of Pod test-webserver-3c3a9e99-70f9-4540-8264-4b5f8846f29e is Running (Ready = false)
Nov 29 02:13:13.276: INFO: The status of Pod test-webserver-3c3a9e99-70f9-4540-8264-4b5f8846f29e is Running (Ready = true)
Nov 29 02:13:13.278: INFO: Container started at 2020-11-29 02:12:53 +0000 UTC, pod became ready at 2020-11-29 02:13:12 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:13:13.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6001" for this suite.

• [SLOW TEST:22.065 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":277,"completed":142,"skipped":2307,"failed":0}
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:13:13.285: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Nov 29 02:13:13.311: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 29 02:14:13.344: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 02:14:13.347: INFO: Starting informer...
STEP: Starting pod...
Nov 29 02:14:13.558: INFO: Pod is running on alex-cp1516-v3-vsp2-node-group-bc6233c429. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Nov 29 02:14:13.593: INFO: Pod wasn't evicted. Proceeding
Nov 29 02:14:13.593: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Nov 29 02:15:28.646: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:15:28.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-6196" for this suite.

• [SLOW TEST:135.369 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":277,"completed":143,"skipped":2307,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:15:28.655: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Nov 29 02:15:28.743: INFO: Waiting up to 5m0s for pod "downwardapi-volume-26c8bb50-ee24-4406-8021-e981473b8a2d" in namespace "downward-api-423" to be "Succeeded or Failed"
Nov 29 02:15:28.749: INFO: Pod "downwardapi-volume-26c8bb50-ee24-4406-8021-e981473b8a2d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.102497ms
Nov 29 02:15:30.752: INFO: Pod "downwardapi-volume-26c8bb50-ee24-4406-8021-e981473b8a2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008052832s
Nov 29 02:15:32.755: INFO: Pod "downwardapi-volume-26c8bb50-ee24-4406-8021-e981473b8a2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011453243s
STEP: Saw pod success
Nov 29 02:15:32.755: INFO: Pod "downwardapi-volume-26c8bb50-ee24-4406-8021-e981473b8a2d" satisfied condition "Succeeded or Failed"
Nov 29 02:15:32.758: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod downwardapi-volume-26c8bb50-ee24-4406-8021-e981473b8a2d container client-container: <nil>
STEP: delete the pod
Nov 29 02:15:32.787: INFO: Waiting for pod downwardapi-volume-26c8bb50-ee24-4406-8021-e981473b8a2d to disappear
Nov 29 02:15:32.795: INFO: Pod downwardapi-volume-26c8bb50-ee24-4406-8021-e981473b8a2d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:15:32.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-423" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":277,"completed":144,"skipped":2311,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:15:32.806: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:15:58.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5553" for this suite.

• [SLOW TEST:25.323 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  blackbox test
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
    when starting a container that exits
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":277,"completed":145,"skipped":2319,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:15:58.130: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-af35c56b-5dde-40d7-96e2-5a3d4017b2d5
STEP: Creating a pod to test consume configMaps
Nov 29 02:15:58.190: INFO: Waiting up to 5m0s for pod "pod-configmaps-242f2b47-fa07-438b-bedb-dcae7c1a0150" in namespace "configmap-4081" to be "Succeeded or Failed"
Nov 29 02:15:58.214: INFO: Pod "pod-configmaps-242f2b47-fa07-438b-bedb-dcae7c1a0150": Phase="Pending", Reason="", readiness=false. Elapsed: 23.781618ms
Nov 29 02:16:00.218: INFO: Pod "pod-configmaps-242f2b47-fa07-438b-bedb-dcae7c1a0150": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028181175s
Nov 29 02:16:02.221: INFO: Pod "pod-configmaps-242f2b47-fa07-438b-bedb-dcae7c1a0150": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03135808s
STEP: Saw pod success
Nov 29 02:16:02.222: INFO: Pod "pod-configmaps-242f2b47-fa07-438b-bedb-dcae7c1a0150" satisfied condition "Succeeded or Failed"
Nov 29 02:16:02.224: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod pod-configmaps-242f2b47-fa07-438b-bedb-dcae7c1a0150 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 02:16:02.256: INFO: Waiting for pod pod-configmaps-242f2b47-fa07-438b-bedb-dcae7c1a0150 to disappear
Nov 29 02:16:02.263: INFO: Pod pod-configmaps-242f2b47-fa07-438b-bedb-dcae7c1a0150 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:16:02.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4081" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":277,"completed":146,"skipped":2321,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:16:02.273: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-map-a5a2c5eb-bc88-4b62-9c4b-0c0ec814d600
STEP: Creating a pod to test consume configMaps
Nov 29 02:16:02.328: INFO: Waiting up to 5m0s for pod "pod-configmaps-a0b81ba2-d261-4fed-987b-b4c9b5e8ad50" in namespace "configmap-326" to be "Succeeded or Failed"
Nov 29 02:16:02.335: INFO: Pod "pod-configmaps-a0b81ba2-d261-4fed-987b-b4c9b5e8ad50": Phase="Pending", Reason="", readiness=false. Elapsed: 7.501484ms
Nov 29 02:16:04.338: INFO: Pod "pod-configmaps-a0b81ba2-d261-4fed-987b-b4c9b5e8ad50": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010430816s
Nov 29 02:16:06.341: INFO: Pod "pod-configmaps-a0b81ba2-d261-4fed-987b-b4c9b5e8ad50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013468708s
STEP: Saw pod success
Nov 29 02:16:06.341: INFO: Pod "pod-configmaps-a0b81ba2-d261-4fed-987b-b4c9b5e8ad50" satisfied condition "Succeeded or Failed"
Nov 29 02:16:06.343: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod pod-configmaps-a0b81ba2-d261-4fed-987b-b4c9b5e8ad50 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 02:16:06.448: INFO: Waiting for pod pod-configmaps-a0b81ba2-d261-4fed-987b-b4c9b5e8ad50 to disappear
Nov 29 02:16:06.458: INFO: Pod pod-configmaps-a0b81ba2-d261-4fed-987b-b4c9b5e8ad50 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:16:06.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-326" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":277,"completed":147,"skipped":2338,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:16:06.467: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 29 02:16:06.885: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 29 02:16:08.898: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742212966, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742212966, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742212966, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742212966, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 02:16:11.935: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Nov 29 02:16:15.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 attach --namespace=webhook-8235 to-be-attached-pod -i -c=container1'
Nov 29 02:16:16.074: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:16:16.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8235" for this suite.
STEP: Destroying namespace "webhook-8235-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.709 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":277,"completed":148,"skipped":2399,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:16:16.177: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-786
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 29 02:16:16.211: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 29 02:16:16.270: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 29 02:16:18.273: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 29 02:16:20.274: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 02:16:22.274: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 02:16:24.273: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 02:16:26.273: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 02:16:28.273: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 02:16:30.275: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov 29 02:16:30.279: INFO: The status of Pod netserver-1 is Running (Ready = false)
Nov 29 02:16:32.282: INFO: The status of Pod netserver-1 is Running (Ready = false)
Nov 29 02:16:34.282: INFO: The status of Pod netserver-1 is Running (Ready = true)
Nov 29 02:16:34.287: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Nov 29 02:16:38.309: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.191.173:8080/dial?request=hostname&protocol=udp&host=192.168.93.221&port=8081&tries=1'] Namespace:pod-network-test-786 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 02:16:38.309: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
Nov 29 02:16:38.422: INFO: Waiting for responses: map[]
Nov 29 02:16:38.424: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.191.173:8080/dial?request=hostname&protocol=udp&host=192.168.191.174&port=8081&tries=1'] Namespace:pod-network-test-786 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 02:16:38.424: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
Nov 29 02:16:38.540: INFO: Waiting for responses: map[]
Nov 29 02:16:38.543: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.191.173:8080/dial?request=hostname&protocol=udp&host=192.168.36.218&port=8081&tries=1'] Namespace:pod-network-test-786 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 02:16:38.543: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
Nov 29 02:16:38.654: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:16:38.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-786" for this suite.

• [SLOW TEST:22.484 seconds]
[sig-network] Networking
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":277,"completed":149,"skipped":2421,"failed":0}
S
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:16:38.662: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 02:16:38.684: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3572
I1129 02:16:38.696221      22 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3572, replica count: 1
I1129 02:16:39.746671      22 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1129 02:16:40.746926      22 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1129 02:16:41.747218      22 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 29 02:16:41.865: INFO: Created: latency-svc-zrjvh
Nov 29 02:16:41.874: INFO: Got endpoints: latency-svc-zrjvh [27.382009ms]
Nov 29 02:16:41.903: INFO: Created: latency-svc-kz7jt
Nov 29 02:16:41.913: INFO: Got endpoints: latency-svc-kz7jt [38.449775ms]
Nov 29 02:16:41.927: INFO: Created: latency-svc-mmc66
Nov 29 02:16:41.949: INFO: Got endpoints: latency-svc-mmc66 [74.710742ms]
Nov 29 02:16:41.950: INFO: Created: latency-svc-6k4jw
Nov 29 02:16:41.970: INFO: Got endpoints: latency-svc-6k4jw [94.637452ms]
Nov 29 02:16:41.979: INFO: Created: latency-svc-tjm82
Nov 29 02:16:41.985: INFO: Got endpoints: latency-svc-tjm82 [109.69722ms]
Nov 29 02:16:41.990: INFO: Created: latency-svc-tcgdg
Nov 29 02:16:42.004: INFO: Got endpoints: latency-svc-tcgdg [129.084108ms]
Nov 29 02:16:42.018: INFO: Created: latency-svc-ntlvc
Nov 29 02:16:42.029: INFO: Got endpoints: latency-svc-ntlvc [154.180007ms]
Nov 29 02:16:42.039: INFO: Created: latency-svc-km2zn
Nov 29 02:16:42.045: INFO: Got endpoints: latency-svc-km2zn [169.727492ms]
Nov 29 02:16:42.063: INFO: Created: latency-svc-r6j86
Nov 29 02:16:42.077: INFO: Got endpoints: latency-svc-r6j86 [202.276173ms]
Nov 29 02:16:42.082: INFO: Created: latency-svc-pw6sf
Nov 29 02:16:42.092: INFO: Got endpoints: latency-svc-pw6sf [216.854758ms]
Nov 29 02:16:42.099: INFO: Created: latency-svc-c6xm2
Nov 29 02:16:42.111: INFO: Got endpoints: latency-svc-c6xm2 [236.292778ms]
Nov 29 02:16:42.115: INFO: Created: latency-svc-bxkjq
Nov 29 02:16:42.127: INFO: Got endpoints: latency-svc-bxkjq [251.326074ms]
Nov 29 02:16:42.136: INFO: Created: latency-svc-jcgtt
Nov 29 02:16:42.167: INFO: Got endpoints: latency-svc-jcgtt [291.468355ms]
Nov 29 02:16:42.227: INFO: Created: latency-svc-qlzkj
Nov 29 02:16:42.242: INFO: Got endpoints: latency-svc-qlzkj [367.547267ms]
Nov 29 02:16:42.318: INFO: Created: latency-svc-4x5qs
Nov 29 02:16:42.343: INFO: Got endpoints: latency-svc-4x5qs [468.030181ms]
Nov 29 02:16:42.348: INFO: Created: latency-svc-zgnlh
Nov 29 02:16:42.354: INFO: Got endpoints: latency-svc-zgnlh [479.604133ms]
Nov 29 02:16:42.368: INFO: Created: latency-svc-rgb8z
Nov 29 02:16:42.378: INFO: Got endpoints: latency-svc-rgb8z [465.17507ms]
Nov 29 02:16:42.389: INFO: Created: latency-svc-v7q2q
Nov 29 02:16:42.397: INFO: Got endpoints: latency-svc-v7q2q [447.653537ms]
Nov 29 02:16:42.403: INFO: Created: latency-svc-d9ltl
Nov 29 02:16:42.413: INFO: Got endpoints: latency-svc-d9ltl [443.490597ms]
Nov 29 02:16:42.438: INFO: Created: latency-svc-q5jgv
Nov 29 02:16:42.469: INFO: Got endpoints: latency-svc-q5jgv [483.760658ms]
Nov 29 02:16:42.488: INFO: Created: latency-svc-5mmkd
Nov 29 02:16:42.500: INFO: Got endpoints: latency-svc-5mmkd [495.455932ms]
Nov 29 02:16:42.515: INFO: Created: latency-svc-hv48n
Nov 29 02:16:42.544: INFO: Got endpoints: latency-svc-hv48n [514.998595ms]
Nov 29 02:16:42.563: INFO: Created: latency-svc-ppzfh
Nov 29 02:16:42.588: INFO: Got endpoints: latency-svc-ppzfh [543.560895ms]
Nov 29 02:16:42.629: INFO: Created: latency-svc-fgr9l
Nov 29 02:16:42.643: INFO: Got endpoints: latency-svc-fgr9l [565.264321ms]
Nov 29 02:16:42.643: INFO: Created: latency-svc-qm8tc
Nov 29 02:16:42.656: INFO: Got endpoints: latency-svc-qm8tc [564.205466ms]
Nov 29 02:16:42.677: INFO: Created: latency-svc-gmhkt
Nov 29 02:16:42.682: INFO: Got endpoints: latency-svc-gmhkt [570.813219ms]
Nov 29 02:16:42.719: INFO: Created: latency-svc-2f8r9
Nov 29 02:16:42.727: INFO: Got endpoints: latency-svc-2f8r9 [599.827094ms]
Nov 29 02:16:42.750: INFO: Created: latency-svc-fz9qw
Nov 29 02:16:42.759: INFO: Got endpoints: latency-svc-fz9qw [591.88023ms]
Nov 29 02:16:42.784: INFO: Created: latency-svc-7rfkx
Nov 29 02:16:42.793: INFO: Got endpoints: latency-svc-7rfkx [551.132173ms]
Nov 29 02:16:42.808: INFO: Created: latency-svc-vxhj2
Nov 29 02:16:42.833: INFO: Got endpoints: latency-svc-vxhj2 [489.760048ms]
Nov 29 02:16:42.846: INFO: Created: latency-svc-kfplk
Nov 29 02:16:42.861: INFO: Got endpoints: latency-svc-kfplk [506.295212ms]
Nov 29 02:16:42.875: INFO: Created: latency-svc-tfrxf
Nov 29 02:16:42.885: INFO: Got endpoints: latency-svc-tfrxf [506.531762ms]
Nov 29 02:16:42.906: INFO: Created: latency-svc-45vvw
Nov 29 02:16:42.926: INFO: Got endpoints: latency-svc-45vvw [529.171477ms]
Nov 29 02:16:42.957: INFO: Created: latency-svc-c45z6
Nov 29 02:16:42.966: INFO: Created: latency-svc-fjr55
Nov 29 02:16:42.967: INFO: Got endpoints: latency-svc-c45z6 [553.940703ms]
Nov 29 02:16:42.985: INFO: Got endpoints: latency-svc-fjr55 [515.823066ms]
Nov 29 02:16:43.001: INFO: Created: latency-svc-8pg6x
Nov 29 02:16:43.012: INFO: Got endpoints: latency-svc-8pg6x [512.155376ms]
Nov 29 02:16:43.025: INFO: Created: latency-svc-vnpzj
Nov 29 02:16:43.035: INFO: Got endpoints: latency-svc-vnpzj [490.626979ms]
Nov 29 02:16:43.074: INFO: Created: latency-svc-2tth8
Nov 29 02:16:43.077: INFO: Got endpoints: latency-svc-2tth8 [487.073527ms]
Nov 29 02:16:43.085: INFO: Created: latency-svc-t6kjq
Nov 29 02:16:43.095: INFO: Got endpoints: latency-svc-t6kjq [452.263664ms]
Nov 29 02:16:43.123: INFO: Created: latency-svc-t5z8d
Nov 29 02:16:43.136: INFO: Got endpoints: latency-svc-t5z8d [479.534652ms]
Nov 29 02:16:43.152: INFO: Created: latency-svc-6hqkf
Nov 29 02:16:43.170: INFO: Got endpoints: latency-svc-6hqkf [487.95095ms]
Nov 29 02:16:43.198: INFO: Created: latency-svc-pxh6h
Nov 29 02:16:43.209: INFO: Got endpoints: latency-svc-pxh6h [482.056771ms]
Nov 29 02:16:43.219: INFO: Created: latency-svc-zg8j9
Nov 29 02:16:43.229: INFO: Got endpoints: latency-svc-zg8j9 [470.417306ms]
Nov 29 02:16:43.258: INFO: Created: latency-svc-gwkw9
Nov 29 02:16:43.265: INFO: Got endpoints: latency-svc-gwkw9 [55.896356ms]
Nov 29 02:16:43.278: INFO: Created: latency-svc-hlnxz
Nov 29 02:16:43.327: INFO: Got endpoints: latency-svc-hlnxz [533.298534ms]
Nov 29 02:16:43.336: INFO: Created: latency-svc-rphmh
Nov 29 02:16:43.344: INFO: Got endpoints: latency-svc-rphmh [511.220959ms]
Nov 29 02:16:43.364: INFO: Created: latency-svc-bqtf5
Nov 29 02:16:43.373: INFO: Got endpoints: latency-svc-bqtf5 [511.277081ms]
Nov 29 02:16:43.395: INFO: Created: latency-svc-gc8hv
Nov 29 02:16:43.408: INFO: Got endpoints: latency-svc-gc8hv [522.249059ms]
Nov 29 02:16:43.416: INFO: Created: latency-svc-jw7p5
Nov 29 02:16:43.461: INFO: Got endpoints: latency-svc-jw7p5 [534.726772ms]
Nov 29 02:16:43.465: INFO: Created: latency-svc-7f7sn
Nov 29 02:16:43.465: INFO: Got endpoints: latency-svc-7f7sn [497.863598ms]
Nov 29 02:16:43.477: INFO: Created: latency-svc-t8f8f
Nov 29 02:16:43.491: INFO: Got endpoints: latency-svc-t8f8f [505.702334ms]
Nov 29 02:16:43.494: INFO: Created: latency-svc-nfd9j
Nov 29 02:16:43.508: INFO: Got endpoints: latency-svc-nfd9j [495.827115ms]
Nov 29 02:16:43.523: INFO: Created: latency-svc-ft4vx
Nov 29 02:16:43.533: INFO: Got endpoints: latency-svc-ft4vx [497.645821ms]
Nov 29 02:16:43.559: INFO: Created: latency-svc-rpp7j
Nov 29 02:16:43.609: INFO: Got endpoints: latency-svc-rpp7j [531.649892ms]
Nov 29 02:16:43.621: INFO: Created: latency-svc-m7tnh
Nov 29 02:16:43.632: INFO: Got endpoints: latency-svc-m7tnh [536.481038ms]
Nov 29 02:16:43.661: INFO: Created: latency-svc-zvtdx
Nov 29 02:16:43.673: INFO: Got endpoints: latency-svc-zvtdx [537.16846ms]
Nov 29 02:16:43.695: INFO: Created: latency-svc-hv67r
Nov 29 02:16:43.728: INFO: Got endpoints: latency-svc-hv67r [557.114588ms]
Nov 29 02:16:43.755: INFO: Created: latency-svc-8tqnh
Nov 29 02:16:43.765: INFO: Got endpoints: latency-svc-8tqnh [535.918133ms]
Nov 29 02:16:43.787: INFO: Created: latency-svc-88tx8
Nov 29 02:16:43.798: INFO: Got endpoints: latency-svc-88tx8 [533.271905ms]
Nov 29 02:16:43.818: INFO: Created: latency-svc-mjfx4
Nov 29 02:16:43.849: INFO: Got endpoints: latency-svc-mjfx4 [521.915164ms]
Nov 29 02:16:43.888: INFO: Created: latency-svc-wds79
Nov 29 02:16:43.901: INFO: Got endpoints: latency-svc-wds79 [556.529548ms]
Nov 29 02:16:43.925: INFO: Created: latency-svc-nljws
Nov 29 02:16:43.934: INFO: Got endpoints: latency-svc-nljws [561.65549ms]
Nov 29 02:16:43.976: INFO: Created: latency-svc-fwcf7
Nov 29 02:16:43.979: INFO: Got endpoints: latency-svc-fwcf7 [570.960859ms]
Nov 29 02:16:43.997: INFO: Created: latency-svc-d5mzp
Nov 29 02:16:44.006: INFO: Got endpoints: latency-svc-d5mzp [544.958464ms]
Nov 29 02:16:44.029: INFO: Created: latency-svc-cg8zn
Nov 29 02:16:44.050: INFO: Got endpoints: latency-svc-cg8zn [584.254251ms]
Nov 29 02:16:44.097: INFO: Created: latency-svc-8tw7s
Nov 29 02:16:44.101: INFO: Got endpoints: latency-svc-8tw7s [610.115327ms]
Nov 29 02:16:44.130: INFO: Created: latency-svc-f25rx
Nov 29 02:16:44.133: INFO: Got endpoints: latency-svc-f25rx [625.356944ms]
Nov 29 02:16:44.154: INFO: Created: latency-svc-45n2f
Nov 29 02:16:44.164: INFO: Got endpoints: latency-svc-45n2f [631.336869ms]
Nov 29 02:16:44.176: INFO: Created: latency-svc-jb9mh
Nov 29 02:16:44.186: INFO: Got endpoints: latency-svc-jb9mh [576.549675ms]
Nov 29 02:16:44.220: INFO: Created: latency-svc-8s9dn
Nov 29 02:16:44.223: INFO: Got endpoints: latency-svc-8s9dn [591.333085ms]
Nov 29 02:16:44.236: INFO: Created: latency-svc-s58m2
Nov 29 02:16:44.245: INFO: Got endpoints: latency-svc-s58m2 [571.74759ms]
Nov 29 02:16:44.270: INFO: Created: latency-svc-8rz8n
Nov 29 02:16:44.284: INFO: Got endpoints: latency-svc-8rz8n [556.486336ms]
Nov 29 02:16:44.300: INFO: Created: latency-svc-xg5t4
Nov 29 02:16:44.310: INFO: Got endpoints: latency-svc-xg5t4 [544.141726ms]
Nov 29 02:16:44.347: INFO: Created: latency-svc-bhnf7
Nov 29 02:16:44.361: INFO: Got endpoints: latency-svc-bhnf7 [562.983133ms]
Nov 29 02:16:44.373: INFO: Created: latency-svc-9zq4z
Nov 29 02:16:44.385: INFO: Got endpoints: latency-svc-9zq4z [536.097894ms]
Nov 29 02:16:44.390: INFO: Created: latency-svc-8lrxh
Nov 29 02:16:44.400: INFO: Got endpoints: latency-svc-8lrxh [499.265794ms]
Nov 29 02:16:44.428: INFO: Created: latency-svc-bsmtj
Nov 29 02:16:44.436: INFO: Got endpoints: latency-svc-bsmtj [501.403464ms]
Nov 29 02:16:44.441: INFO: Created: latency-svc-sfn5l
Nov 29 02:16:44.486: INFO: Got endpoints: latency-svc-sfn5l [507.452269ms]
Nov 29 02:16:44.495: INFO: Created: latency-svc-mzhbg
Nov 29 02:16:44.500: INFO: Got endpoints: latency-svc-mzhbg [493.725818ms]
Nov 29 02:16:44.514: INFO: Created: latency-svc-n47fr
Nov 29 02:16:44.527: INFO: Got endpoints: latency-svc-n47fr [477.055391ms]
Nov 29 02:16:44.534: INFO: Created: latency-svc-9zkhr
Nov 29 02:16:44.540: INFO: Got endpoints: latency-svc-9zkhr [439.360654ms]
Nov 29 02:16:44.555: INFO: Created: latency-svc-ckp8k
Nov 29 02:16:44.563: INFO: Got endpoints: latency-svc-ckp8k [430.002285ms]
Nov 29 02:16:44.610: INFO: Created: latency-svc-8bqzj
Nov 29 02:16:44.614: INFO: Created: latency-svc-9mkl2
Nov 29 02:16:44.616: INFO: Got endpoints: latency-svc-8bqzj [451.837536ms]
Nov 29 02:16:44.618: INFO: Got endpoints: latency-svc-9mkl2 [432.233684ms]
Nov 29 02:16:44.645: INFO: Created: latency-svc-gjtfb
Nov 29 02:16:44.657: INFO: Got endpoints: latency-svc-gjtfb [433.504893ms]
Nov 29 02:16:44.668: INFO: Created: latency-svc-bmdmn
Nov 29 02:16:44.684: INFO: Got endpoints: latency-svc-bmdmn [438.271271ms]
Nov 29 02:16:44.695: INFO: Created: latency-svc-rpp68
Nov 29 02:16:44.739: INFO: Got endpoints: latency-svc-rpp68 [455.303297ms]
Nov 29 02:16:44.742: INFO: Created: latency-svc-k2mln
Nov 29 02:16:44.763: INFO: Created: latency-svc-c8dmh
Nov 29 02:16:44.774: INFO: Got endpoints: latency-svc-k2mln [463.765319ms]
Nov 29 02:16:44.785: INFO: Created: latency-svc-4bh8g
Nov 29 02:16:44.805: INFO: Created: latency-svc-6h8dc
Nov 29 02:16:44.836: INFO: Got endpoints: latency-svc-c8dmh [474.970205ms]
Nov 29 02:16:44.865: INFO: Created: latency-svc-6w7dh
Nov 29 02:16:44.881: INFO: Got endpoints: latency-svc-4bh8g [495.365944ms]
Nov 29 02:16:44.881: INFO: Created: latency-svc-pmbtk
Nov 29 02:16:44.912: INFO: Created: latency-svc-5cs4n
Nov 29 02:16:44.920: INFO: Got endpoints: latency-svc-6h8dc [519.763784ms]
Nov 29 02:16:44.950: INFO: Created: latency-svc-5n4hp
Nov 29 02:16:44.992: INFO: Got endpoints: latency-svc-6w7dh [555.549278ms]
Nov 29 02:16:45.013: INFO: Created: latency-svc-svwxx
Nov 29 02:16:45.019: INFO: Got endpoints: latency-svc-pmbtk [533.310816ms]
Nov 29 02:16:45.034: INFO: Created: latency-svc-wkhrp
Nov 29 02:16:45.061: INFO: Created: latency-svc-5sd5l
Nov 29 02:16:45.079: INFO: Got endpoints: latency-svc-5cs4n [579.283694ms]
Nov 29 02:16:45.132: INFO: Got endpoints: latency-svc-5n4hp [605.683701ms]
Nov 29 02:16:45.142: INFO: Created: latency-svc-bvrbx
Nov 29 02:16:45.151: INFO: Created: latency-svc-txlzc
Nov 29 02:16:45.171: INFO: Created: latency-svc-xssl7
Nov 29 02:16:45.174: INFO: Got endpoints: latency-svc-svwxx [633.558474ms]
Nov 29 02:16:45.198: INFO: Created: latency-svc-ddmhh
Nov 29 02:16:45.220: INFO: Created: latency-svc-t7jrg
Nov 29 02:16:45.225: INFO: Got endpoints: latency-svc-wkhrp [661.923745ms]
Nov 29 02:16:45.262: INFO: Created: latency-svc-q4hvj
Nov 29 02:16:45.272: INFO: Got endpoints: latency-svc-5sd5l [656.469028ms]
Nov 29 02:16:45.286: INFO: Created: latency-svc-l6qcj
Nov 29 02:16:45.321: INFO: Got endpoints: latency-svc-bvrbx [702.606021ms]
Nov 29 02:16:45.322: INFO: Created: latency-svc-ddsn5
Nov 29 02:16:45.340: INFO: Created: latency-svc-ch28s
Nov 29 02:16:45.393: INFO: Got endpoints: latency-svc-txlzc [735.820004ms]
Nov 29 02:16:45.410: INFO: Created: latency-svc-q9tlm
Nov 29 02:16:45.429: INFO: Got endpoints: latency-svc-xssl7 [745.645662ms]
Nov 29 02:16:45.442: INFO: Created: latency-svc-wd7bw
Nov 29 02:16:45.475: INFO: Got endpoints: latency-svc-ddmhh [735.206037ms]
Nov 29 02:16:45.501: INFO: Created: latency-svc-f8j66
Nov 29 02:16:45.525: INFO: Got endpoints: latency-svc-t7jrg [750.915751ms]
Nov 29 02:16:45.526: INFO: Created: latency-svc-vj4z2
Nov 29 02:16:45.562: INFO: Created: latency-svc-7cdm4
Nov 29 02:16:45.574: INFO: Got endpoints: latency-svc-q4hvj [737.227717ms]
Nov 29 02:16:45.602: INFO: Created: latency-svc-zntgk
Nov 29 02:16:45.633: INFO: Got endpoints: latency-svc-l6qcj [752.325579ms]
Nov 29 02:16:45.654: INFO: Created: latency-svc-vfwgw
Nov 29 02:16:45.673: INFO: Created: latency-svc-5wjzm
Nov 29 02:16:45.674: INFO: Got endpoints: latency-svc-ddsn5 [753.489284ms]
Nov 29 02:16:45.703: INFO: Created: latency-svc-xbcwm
Nov 29 02:16:45.726: INFO: Got endpoints: latency-svc-ch28s [733.878737ms]
Nov 29 02:16:45.726: INFO: Created: latency-svc-8z6bd
Nov 29 02:16:45.775: INFO: Got endpoints: latency-svc-q9tlm [755.85531ms]
Nov 29 02:16:45.793: INFO: Created: latency-svc-zdmrh
Nov 29 02:16:45.814: INFO: Created: latency-svc-k5gfl
Nov 29 02:16:45.832: INFO: Got endpoints: latency-svc-wd7bw [752.777944ms]
Nov 29 02:16:45.846: INFO: Created: latency-svc-c4xn6
Nov 29 02:16:45.864: INFO: Created: latency-svc-m4k56
Nov 29 02:16:45.899: INFO: Got endpoints: latency-svc-f8j66 [766.284596ms]
Nov 29 02:16:45.903: INFO: Created: latency-svc-7cwc6
Nov 29 02:16:45.914: INFO: Created: latency-svc-fm8hq
Nov 29 02:16:45.923: INFO: Got endpoints: latency-svc-vj4z2 [749.536449ms]
Nov 29 02:16:45.930: INFO: Created: latency-svc-njzlz
Nov 29 02:16:45.947: INFO: Created: latency-svc-gggrs
Nov 29 02:16:45.963: INFO: Created: latency-svc-vdnt2
Nov 29 02:16:45.972: INFO: Got endpoints: latency-svc-7cdm4 [746.524527ms]
Nov 29 02:16:45.992: INFO: Created: latency-svc-tl7bc
Nov 29 02:16:46.023: INFO: Got endpoints: latency-svc-zntgk [750.433114ms]
Nov 29 02:16:46.040: INFO: Created: latency-svc-tdlw7
Nov 29 02:16:46.072: INFO: Got endpoints: latency-svc-vfwgw [751.375055ms]
Nov 29 02:16:46.093: INFO: Created: latency-svc-l4q9q
Nov 29 02:16:46.121: INFO: Got endpoints: latency-svc-5wjzm [727.891811ms]
Nov 29 02:16:46.168: INFO: Created: latency-svc-fl52s
Nov 29 02:16:46.176: INFO: Got endpoints: latency-svc-xbcwm [746.543524ms]
Nov 29 02:16:46.200: INFO: Created: latency-svc-bcbb6
Nov 29 02:16:46.223: INFO: Got endpoints: latency-svc-8z6bd [748.559658ms]
Nov 29 02:16:46.345: INFO: Got endpoints: latency-svc-k5gfl [771.873412ms]
Nov 29 02:16:46.346: INFO: Got endpoints: latency-svc-zdmrh [821.376017ms]
Nov 29 02:16:46.350: INFO: Created: latency-svc-mnhn5
Nov 29 02:16:46.382: INFO: Created: latency-svc-6l2ck
Nov 29 02:16:46.382: INFO: Got endpoints: latency-svc-c4xn6 [748.64462ms]
Nov 29 02:16:46.398: INFO: Created: latency-svc-9q9hv
Nov 29 02:16:46.420: INFO: Created: latency-svc-srpx9
Nov 29 02:16:46.421: INFO: Got endpoints: latency-svc-m4k56 [747.677236ms]
Nov 29 02:16:46.465: INFO: Created: latency-svc-7722p
Nov 29 02:16:46.475: INFO: Got endpoints: latency-svc-7cwc6 [749.754858ms]
Nov 29 02:16:46.497: INFO: Created: latency-svc-s554d
Nov 29 02:16:46.525: INFO: Got endpoints: latency-svc-fm8hq [749.411101ms]
Nov 29 02:16:46.549: INFO: Created: latency-svc-qgbr4
Nov 29 02:16:46.585: INFO: Got endpoints: latency-svc-njzlz [752.444924ms]
Nov 29 02:16:46.607: INFO: Created: latency-svc-rc6sp
Nov 29 02:16:46.630: INFO: Got endpoints: latency-svc-gggrs [730.868564ms]
Nov 29 02:16:46.643: INFO: Created: latency-svc-gpxfv
Nov 29 02:16:46.676: INFO: Got endpoints: latency-svc-vdnt2 [752.771257ms]
Nov 29 02:16:46.709: INFO: Created: latency-svc-r8975
Nov 29 02:16:46.722: INFO: Got endpoints: latency-svc-tl7bc [749.774586ms]
Nov 29 02:16:46.742: INFO: Created: latency-svc-rqfqh
Nov 29 02:16:46.782: INFO: Got endpoints: latency-svc-tdlw7 [758.612501ms]
Nov 29 02:16:46.828: INFO: Got endpoints: latency-svc-l4q9q [755.571642ms]
Nov 29 02:16:46.830: INFO: Created: latency-svc-tdbz4
Nov 29 02:16:46.853: INFO: Created: latency-svc-bmnxr
Nov 29 02:16:46.868: INFO: Got endpoints: latency-svc-fl52s [747.171658ms]
Nov 29 02:16:46.898: INFO: Created: latency-svc-tq5bb
Nov 29 02:16:46.923: INFO: Got endpoints: latency-svc-bcbb6 [747.166755ms]
Nov 29 02:16:46.974: INFO: Got endpoints: latency-svc-mnhn5 [750.758583ms]
Nov 29 02:16:46.989: INFO: Created: latency-svc-6zwrh
Nov 29 02:16:47.026: INFO: Got endpoints: latency-svc-6l2ck [680.541628ms]
Nov 29 02:16:47.028: INFO: Created: latency-svc-dj98h
Nov 29 02:16:47.051: INFO: Created: latency-svc-j8blv
Nov 29 02:16:47.081: INFO: Got endpoints: latency-svc-9q9hv [735.122906ms]
Nov 29 02:16:47.104: INFO: Created: latency-svc-25bjt
Nov 29 02:16:47.120: INFO: Got endpoints: latency-svc-srpx9 [738.053382ms]
Nov 29 02:16:47.149: INFO: Created: latency-svc-mr489
Nov 29 02:16:47.220: INFO: Got endpoints: latency-svc-7722p [798.188241ms]
Nov 29 02:16:47.224: INFO: Got endpoints: latency-svc-s554d [748.247463ms]
Nov 29 02:16:47.377: INFO: Got endpoints: latency-svc-rc6sp [792.220017ms]
Nov 29 02:16:47.378: INFO: Got endpoints: latency-svc-qgbr4 [852.667778ms]
Nov 29 02:16:47.378: INFO: Created: latency-svc-rmtr8
Nov 29 02:16:47.403: INFO: Got endpoints: latency-svc-gpxfv [773.475378ms]
Nov 29 02:16:47.441: INFO: Got endpoints: latency-svc-r8975 [764.46515ms]
Nov 29 02:16:47.441: INFO: Created: latency-svc-flf8j
Nov 29 02:16:47.532: INFO: Got endpoints: latency-svc-tdbz4 [750.65569ms]
Nov 29 02:16:47.532: INFO: Got endpoints: latency-svc-rqfqh [810.433358ms]
Nov 29 02:16:47.548: INFO: Created: latency-svc-g72bx
Nov 29 02:16:47.573: INFO: Got endpoints: latency-svc-bmnxr [745.433459ms]
Nov 29 02:16:47.602: INFO: Created: latency-svc-v4kpb
Nov 29 02:16:47.618: INFO: Created: latency-svc-lfqwp
Nov 29 02:16:47.673: INFO: Got endpoints: latency-svc-tq5bb [805.229823ms]
Nov 29 02:16:47.674: INFO: Created: latency-svc-tfv8c
Nov 29 02:16:47.678: INFO: Got endpoints: latency-svc-6zwrh [754.961332ms]
Nov 29 02:16:47.695: INFO: Created: latency-svc-88fd2
Nov 29 02:16:47.714: INFO: Created: latency-svc-dxs9s
Nov 29 02:16:47.738: INFO: Got endpoints: latency-svc-dj98h [763.704496ms]
Nov 29 02:16:47.739: INFO: Created: latency-svc-zxchw
Nov 29 02:16:47.762: INFO: Created: latency-svc-zttdp
Nov 29 02:16:47.796: INFO: Got endpoints: latency-svc-j8blv [769.964352ms]
Nov 29 02:16:47.804: INFO: Created: latency-svc-6z8wf
Nov 29 02:16:47.821: INFO: Got endpoints: latency-svc-25bjt [739.861989ms]
Nov 29 02:16:47.825: INFO: Created: latency-svc-jckvh
Nov 29 02:16:47.857: INFO: Created: latency-svc-9q5xl
Nov 29 02:16:47.876: INFO: Got endpoints: latency-svc-mr489 [756.226791ms]
Nov 29 02:16:47.877: INFO: Created: latency-svc-98jth
Nov 29 02:16:47.920: INFO: Created: latency-svc-8th6q
Nov 29 02:16:47.921: INFO: Got endpoints: latency-svc-rmtr8 [701.594104ms]
Nov 29 02:16:47.951: INFO: Created: latency-svc-2mlx2
Nov 29 02:16:47.971: INFO: Got endpoints: latency-svc-flf8j [747.780996ms]
Nov 29 02:16:47.989: INFO: Created: latency-svc-5prvx
Nov 29 02:16:48.034: INFO: Got endpoints: latency-svc-g72bx [657.313117ms]
Nov 29 02:16:48.053: INFO: Created: latency-svc-zrt97
Nov 29 02:16:48.081: INFO: Got endpoints: latency-svc-v4kpb [703.254662ms]
Nov 29 02:16:48.103: INFO: Created: latency-svc-n97vp
Nov 29 02:16:48.122: INFO: Got endpoints: latency-svc-lfqwp [718.505525ms]
Nov 29 02:16:48.159: INFO: Created: latency-svc-bb4r6
Nov 29 02:16:48.168: INFO: Got endpoints: latency-svc-tfv8c [727.15931ms]
Nov 29 02:16:48.195: INFO: Created: latency-svc-gpxgp
Nov 29 02:16:48.230: INFO: Got endpoints: latency-svc-88fd2 [697.859266ms]
Nov 29 02:16:48.252: INFO: Created: latency-svc-zwdx7
Nov 29 02:16:48.279: INFO: Got endpoints: latency-svc-dxs9s [745.578109ms]
Nov 29 02:16:48.294: INFO: Created: latency-svc-c9j8h
Nov 29 02:16:48.329: INFO: Got endpoints: latency-svc-zxchw [755.033167ms]
Nov 29 02:16:48.351: INFO: Created: latency-svc-mxcnv
Nov 29 02:16:48.368: INFO: Got endpoints: latency-svc-zttdp [695.112824ms]
Nov 29 02:16:48.430: INFO: Got endpoints: latency-svc-6z8wf [752.004016ms]
Nov 29 02:16:48.439: INFO: Created: latency-svc-tmrgd
Nov 29 02:16:48.452: INFO: Created: latency-svc-gjjd8
Nov 29 02:16:48.478: INFO: Got endpoints: latency-svc-jckvh [739.760263ms]
Nov 29 02:16:48.543: INFO: Created: latency-svc-bs5ks
Nov 29 02:16:48.554: INFO: Got endpoints: latency-svc-9q5xl [757.480416ms]
Nov 29 02:16:48.568: INFO: Got endpoints: latency-svc-98jth [745.605269ms]
Nov 29 02:16:48.570: INFO: Created: latency-svc-8cc5v
Nov 29 02:16:48.598: INFO: Created: latency-svc-rzznb
Nov 29 02:16:48.619: INFO: Got endpoints: latency-svc-8th6q [742.404112ms]
Nov 29 02:16:48.635: INFO: Created: latency-svc-mg4km
Nov 29 02:16:48.672: INFO: Got endpoints: latency-svc-2mlx2 [750.847965ms]
Nov 29 02:16:48.689: INFO: Created: latency-svc-bwmm5
Nov 29 02:16:48.717: INFO: Got endpoints: latency-svc-5prvx [745.965745ms]
Nov 29 02:16:48.745: INFO: Created: latency-svc-m2x88
Nov 29 02:16:48.786: INFO: Got endpoints: latency-svc-zrt97 [751.241452ms]
Nov 29 02:16:48.814: INFO: Created: latency-svc-dppd4
Nov 29 02:16:48.823: INFO: Got endpoints: latency-svc-n97vp [741.986167ms]
Nov 29 02:16:48.848: INFO: Created: latency-svc-vsmck
Nov 29 02:16:48.874: INFO: Got endpoints: latency-svc-bb4r6 [752.361828ms]
Nov 29 02:16:48.920: INFO: Created: latency-svc-fdsld
Nov 29 02:16:48.924: INFO: Got endpoints: latency-svc-gpxgp [755.834713ms]
Nov 29 02:16:48.962: INFO: Created: latency-svc-p9tfs
Nov 29 02:16:48.973: INFO: Got endpoints: latency-svc-zwdx7 [739.316928ms]
Nov 29 02:16:49.003: INFO: Created: latency-svc-wrbn5
Nov 29 02:16:49.018: INFO: Got endpoints: latency-svc-c9j8h [739.424763ms]
Nov 29 02:16:49.055: INFO: Created: latency-svc-mwlzv
Nov 29 02:16:49.077: INFO: Got endpoints: latency-svc-mxcnv [747.965494ms]
Nov 29 02:16:49.105: INFO: Created: latency-svc-xks68
Nov 29 02:16:49.124: INFO: Got endpoints: latency-svc-tmrgd [755.142868ms]
Nov 29 02:16:49.171: INFO: Got endpoints: latency-svc-gjjd8 [740.267919ms]
Nov 29 02:16:49.171: INFO: Created: latency-svc-9mff7
Nov 29 02:16:49.198: INFO: Created: latency-svc-p6hjf
Nov 29 02:16:49.219: INFO: Got endpoints: latency-svc-bs5ks [740.669727ms]
Nov 29 02:16:49.243: INFO: Created: latency-svc-jhmsw
Nov 29 02:16:49.283: INFO: Got endpoints: latency-svc-8cc5v [728.625075ms]
Nov 29 02:16:49.299: INFO: Created: latency-svc-lm4gn
Nov 29 02:16:49.319: INFO: Got endpoints: latency-svc-rzznb [750.384482ms]
Nov 29 02:16:49.352: INFO: Created: latency-svc-tt452
Nov 29 02:16:49.369: INFO: Got endpoints: latency-svc-mg4km [750.178527ms]
Nov 29 02:16:49.417: INFO: Created: latency-svc-6gr57
Nov 29 02:16:49.430: INFO: Got endpoints: latency-svc-bwmm5 [757.298222ms]
Nov 29 02:16:49.453: INFO: Created: latency-svc-kzf8z
Nov 29 02:16:49.476: INFO: Got endpoints: latency-svc-m2x88 [758.226955ms]
Nov 29 02:16:49.512: INFO: Created: latency-svc-vbzml
Nov 29 02:16:49.543: INFO: Got endpoints: latency-svc-dppd4 [756.306931ms]
Nov 29 02:16:49.578: INFO: Got endpoints: latency-svc-vsmck [754.641579ms]
Nov 29 02:16:49.579: INFO: Created: latency-svc-2w7w9
Nov 29 02:16:49.605: INFO: Created: latency-svc-lxmd5
Nov 29 02:16:49.618: INFO: Got endpoints: latency-svc-fdsld [743.659251ms]
Nov 29 02:16:49.692: INFO: Got endpoints: latency-svc-p9tfs [767.351098ms]
Nov 29 02:16:49.706: INFO: Created: latency-svc-l52v7
Nov 29 02:16:49.717: INFO: Created: latency-svc-b858t
Nov 29 02:16:49.724: INFO: Got endpoints: latency-svc-wrbn5 [750.833751ms]
Nov 29 02:16:49.772: INFO: Got endpoints: latency-svc-mwlzv [753.62613ms]
Nov 29 02:16:49.829: INFO: Got endpoints: latency-svc-xks68 [751.999386ms]
Nov 29 02:16:49.874: INFO: Got endpoints: latency-svc-9mff7 [749.949315ms]
Nov 29 02:16:49.928: INFO: Got endpoints: latency-svc-p6hjf [756.391654ms]
Nov 29 02:16:49.969: INFO: Got endpoints: latency-svc-jhmsw [749.632192ms]
Nov 29 02:16:50.020: INFO: Got endpoints: latency-svc-lm4gn [737.491351ms]
Nov 29 02:16:50.075: INFO: Got endpoints: latency-svc-tt452 [755.613626ms]
Nov 29 02:16:50.119: INFO: Got endpoints: latency-svc-6gr57 [749.375384ms]
Nov 29 02:16:50.172: INFO: Got endpoints: latency-svc-kzf8z [741.868322ms]
Nov 29 02:16:50.222: INFO: Got endpoints: latency-svc-vbzml [746.044626ms]
Nov 29 02:16:50.275: INFO: Got endpoints: latency-svc-2w7w9 [732.53909ms]
Nov 29 02:16:50.323: INFO: Got endpoints: latency-svc-lxmd5 [745.022993ms]
Nov 29 02:16:50.372: INFO: Got endpoints: latency-svc-l52v7 [753.35817ms]
Nov 29 02:16:50.418: INFO: Got endpoints: latency-svc-b858t [725.92974ms]
Nov 29 02:16:50.419: INFO: Latencies: [38.449775ms 55.896356ms 74.710742ms 94.637452ms 109.69722ms 129.084108ms 154.180007ms 169.727492ms 202.276173ms 216.854758ms 236.292778ms 251.326074ms 291.468355ms 367.547267ms 430.002285ms 432.233684ms 433.504893ms 438.271271ms 439.360654ms 443.490597ms 447.653537ms 451.837536ms 452.263664ms 455.303297ms 463.765319ms 465.17507ms 468.030181ms 470.417306ms 474.970205ms 477.055391ms 479.534652ms 479.604133ms 482.056771ms 483.760658ms 487.073527ms 487.95095ms 489.760048ms 490.626979ms 493.725818ms 495.365944ms 495.455932ms 495.827115ms 497.645821ms 497.863598ms 499.265794ms 501.403464ms 505.702334ms 506.295212ms 506.531762ms 507.452269ms 511.220959ms 511.277081ms 512.155376ms 514.998595ms 515.823066ms 519.763784ms 521.915164ms 522.249059ms 529.171477ms 531.649892ms 533.271905ms 533.298534ms 533.310816ms 534.726772ms 535.918133ms 536.097894ms 536.481038ms 537.16846ms 543.560895ms 544.141726ms 544.958464ms 551.132173ms 553.940703ms 555.549278ms 556.486336ms 556.529548ms 557.114588ms 561.65549ms 562.983133ms 564.205466ms 565.264321ms 570.813219ms 570.960859ms 571.74759ms 576.549675ms 579.283694ms 584.254251ms 591.333085ms 591.88023ms 599.827094ms 605.683701ms 610.115327ms 625.356944ms 631.336869ms 633.558474ms 656.469028ms 657.313117ms 661.923745ms 680.541628ms 695.112824ms 697.859266ms 701.594104ms 702.606021ms 703.254662ms 718.505525ms 725.92974ms 727.15931ms 727.891811ms 728.625075ms 730.868564ms 732.53909ms 733.878737ms 735.122906ms 735.206037ms 735.820004ms 737.227717ms 737.491351ms 738.053382ms 739.316928ms 739.424763ms 739.760263ms 739.861989ms 740.267919ms 740.669727ms 741.868322ms 741.986167ms 742.404112ms 743.659251ms 745.022993ms 745.433459ms 745.578109ms 745.605269ms 745.645662ms 745.965745ms 746.044626ms 746.524527ms 746.543524ms 747.166755ms 747.171658ms 747.677236ms 747.780996ms 747.965494ms 748.247463ms 748.559658ms 748.64462ms 749.375384ms 749.411101ms 749.536449ms 749.632192ms 749.754858ms 749.774586ms 749.949315ms 750.178527ms 750.384482ms 750.433114ms 750.65569ms 750.758583ms 750.833751ms 750.847965ms 750.915751ms 751.241452ms 751.375055ms 751.999386ms 752.004016ms 752.325579ms 752.361828ms 752.444924ms 752.771257ms 752.777944ms 753.35817ms 753.489284ms 753.62613ms 754.641579ms 754.961332ms 755.033167ms 755.142868ms 755.571642ms 755.613626ms 755.834713ms 755.85531ms 756.226791ms 756.306931ms 756.391654ms 757.298222ms 757.480416ms 758.226955ms 758.612501ms 763.704496ms 764.46515ms 766.284596ms 767.351098ms 769.964352ms 771.873412ms 773.475378ms 792.220017ms 798.188241ms 805.229823ms 810.433358ms 821.376017ms 852.667778ms]
Nov 29 02:16:50.419: INFO: 50 %ile: 697.859266ms
Nov 29 02:16:50.419: INFO: 90 %ile: 756.226791ms
Nov 29 02:16:50.419: INFO: 99 %ile: 821.376017ms
Nov 29 02:16:50.419: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:16:50.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-3572" for this suite.

• [SLOW TEST:11.782 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":277,"completed":150,"skipped":2422,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:16:50.445: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-upd-5e170cbc-4b79-4ada-8dd7-d4b20624d9f0
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:16:54.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9211" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":151,"skipped":2435,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:16:54.571: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Nov 29 02:16:54.609: INFO: Waiting up to 5m0s for pod "downward-api-88164a3a-07da-4cd5-b522-00aea616ef1a" in namespace "downward-api-9039" to be "Succeeded or Failed"
Nov 29 02:16:54.616: INFO: Pod "downward-api-88164a3a-07da-4cd5-b522-00aea616ef1a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.583069ms
Nov 29 02:16:56.620: INFO: Pod "downward-api-88164a3a-07da-4cd5-b522-00aea616ef1a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011755453s
Nov 29 02:16:58.633: INFO: Pod "downward-api-88164a3a-07da-4cd5-b522-00aea616ef1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024212952s
STEP: Saw pod success
Nov 29 02:16:58.633: INFO: Pod "downward-api-88164a3a-07da-4cd5-b522-00aea616ef1a" satisfied condition "Succeeded or Failed"
Nov 29 02:16:58.642: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod downward-api-88164a3a-07da-4cd5-b522-00aea616ef1a container dapi-container: <nil>
STEP: delete the pod
Nov 29 02:16:58.714: INFO: Waiting for pod downward-api-88164a3a-07da-4cd5-b522-00aea616ef1a to disappear
Nov 29 02:16:58.726: INFO: Pod downward-api-88164a3a-07da-4cd5-b522-00aea616ef1a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:16:58.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9039" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":277,"completed":152,"skipped":2447,"failed":0}
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:16:58.742: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Nov 29 02:16:58.876: INFO: Waiting up to 5m0s for pod "downward-api-ca55142e-19b8-4176-a075-2a9e4f754104" in namespace "downward-api-232" to be "Succeeded or Failed"
Nov 29 02:16:58.907: INFO: Pod "downward-api-ca55142e-19b8-4176-a075-2a9e4f754104": Phase="Pending", Reason="", readiness=false. Elapsed: 30.109957ms
Nov 29 02:17:00.920: INFO: Pod "downward-api-ca55142e-19b8-4176-a075-2a9e4f754104": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043440745s
Nov 29 02:17:02.928: INFO: Pod "downward-api-ca55142e-19b8-4176-a075-2a9e4f754104": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051347395s
STEP: Saw pod success
Nov 29 02:17:02.928: INFO: Pod "downward-api-ca55142e-19b8-4176-a075-2a9e4f754104" satisfied condition "Succeeded or Failed"
Nov 29 02:17:02.930: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod downward-api-ca55142e-19b8-4176-a075-2a9e4f754104 container dapi-container: <nil>
STEP: delete the pod
Nov 29 02:17:03.017: INFO: Waiting for pod downward-api-ca55142e-19b8-4176-a075-2a9e4f754104 to disappear
Nov 29 02:17:03.023: INFO: Pod downward-api-ca55142e-19b8-4176-a075-2a9e4f754104 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:17:03.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-232" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":277,"completed":153,"skipped":2454,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:17:03.062: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test env composition
Nov 29 02:17:03.156: INFO: Waiting up to 5m0s for pod "var-expansion-8bbdcc06-c1c4-47d6-97fd-16a7d48689cc" in namespace "var-expansion-4903" to be "Succeeded or Failed"
Nov 29 02:17:03.172: INFO: Pod "var-expansion-8bbdcc06-c1c4-47d6-97fd-16a7d48689cc": Phase="Pending", Reason="", readiness=false. Elapsed: 15.610848ms
Nov 29 02:17:05.182: INFO: Pod "var-expansion-8bbdcc06-c1c4-47d6-97fd-16a7d48689cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02520475s
Nov 29 02:17:07.186: INFO: Pod "var-expansion-8bbdcc06-c1c4-47d6-97fd-16a7d48689cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028952488s
STEP: Saw pod success
Nov 29 02:17:07.186: INFO: Pod "var-expansion-8bbdcc06-c1c4-47d6-97fd-16a7d48689cc" satisfied condition "Succeeded or Failed"
Nov 29 02:17:07.190: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod var-expansion-8bbdcc06-c1c4-47d6-97fd-16a7d48689cc container dapi-container: <nil>
STEP: delete the pod
Nov 29 02:17:07.247: INFO: Waiting for pod var-expansion-8bbdcc06-c1c4-47d6-97fd-16a7d48689cc to disappear
Nov 29 02:17:07.255: INFO: Pod var-expansion-8bbdcc06-c1c4-47d6-97fd-16a7d48689cc no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:17:07.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4903" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":277,"completed":154,"skipped":2491,"failed":0}
SS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:17:07.264: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 29 02:17:10.329: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:17:10.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3418" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":277,"completed":155,"skipped":2493,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:17:10.349: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Nov 29 02:17:10.389: INFO: Waiting up to 5m0s for pod "downwardapi-volume-45b236bd-5325-4b57-ba8a-cc51e3f48223" in namespace "downward-api-6539" to be "Succeeded or Failed"
Nov 29 02:17:10.407: INFO: Pod "downwardapi-volume-45b236bd-5325-4b57-ba8a-cc51e3f48223": Phase="Pending", Reason="", readiness=false. Elapsed: 18.152615ms
Nov 29 02:17:12.411: INFO: Pod "downwardapi-volume-45b236bd-5325-4b57-ba8a-cc51e3f48223": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021688918s
Nov 29 02:17:14.414: INFO: Pod "downwardapi-volume-45b236bd-5325-4b57-ba8a-cc51e3f48223": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024758139s
STEP: Saw pod success
Nov 29 02:17:14.414: INFO: Pod "downwardapi-volume-45b236bd-5325-4b57-ba8a-cc51e3f48223" satisfied condition "Succeeded or Failed"
Nov 29 02:17:14.416: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod downwardapi-volume-45b236bd-5325-4b57-ba8a-cc51e3f48223 container client-container: <nil>
STEP: delete the pod
Nov 29 02:17:14.434: INFO: Waiting for pod downwardapi-volume-45b236bd-5325-4b57-ba8a-cc51e3f48223 to disappear
Nov 29 02:17:14.443: INFO: Pod downwardapi-volume-45b236bd-5325-4b57-ba8a-cc51e3f48223 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:17:14.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6539" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":277,"completed":156,"skipped":2496,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:17:14.462: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov 29 02:17:14.528: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:17:14.537: INFO: Number of nodes with available pods: 0
Nov 29 02:17:14.538: INFO: Node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d is running more than one daemon pod
Nov 29 02:17:15.543: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:17:15.553: INFO: Number of nodes with available pods: 0
Nov 29 02:17:15.553: INFO: Node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d is running more than one daemon pod
Nov 29 02:17:16.543: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:17:16.546: INFO: Number of nodes with available pods: 0
Nov 29 02:17:16.546: INFO: Node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d is running more than one daemon pod
Nov 29 02:17:17.543: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:17:17.545: INFO: Number of nodes with available pods: 3
Nov 29 02:17:17.545: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Nov 29 02:17:17.592: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:17:17.597: INFO: Number of nodes with available pods: 2
Nov 29 02:17:17.597: INFO: Node alex-cp1516-v3-vsp2-node-group-ea4104c57e is running more than one daemon pod
Nov 29 02:17:18.602: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:17:18.604: INFO: Number of nodes with available pods: 2
Nov 29 02:17:18.604: INFO: Node alex-cp1516-v3-vsp2-node-group-ea4104c57e is running more than one daemon pod
Nov 29 02:17:19.602: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:17:19.611: INFO: Number of nodes with available pods: 2
Nov 29 02:17:19.611: INFO: Node alex-cp1516-v3-vsp2-node-group-ea4104c57e is running more than one daemon pod
Nov 29 02:17:20.603: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:17:20.606: INFO: Number of nodes with available pods: 3
Nov 29 02:17:20.606: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-77, will wait for the garbage collector to delete the pods
Nov 29 02:17:20.667: INFO: Deleting DaemonSet.extensions daemon-set took: 4.325709ms
Nov 29 02:17:21.467: INFO: Terminating DaemonSet.extensions daemon-set pods took: 800.585151ms
Nov 29 02:17:30.770: INFO: Number of nodes with available pods: 0
Nov 29 02:17:30.770: INFO: Number of running nodes: 0, number of available pods: 0
Nov 29 02:17:30.773: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-77/daemonsets","resourceVersion":"34825"},"items":null}

Nov 29 02:17:30.775: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-77/pods","resourceVersion":"34825"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:17:30.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-77" for this suite.

• [SLOW TEST:16.337 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":277,"completed":157,"skipped":2553,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:17:30.800: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Nov 29 02:17:30.845: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1f25788c-1498-46ee-bb64-959d28d33d0d" in namespace "downward-api-3825" to be "Succeeded or Failed"
Nov 29 02:17:30.876: INFO: Pod "downwardapi-volume-1f25788c-1498-46ee-bb64-959d28d33d0d": Phase="Pending", Reason="", readiness=false. Elapsed: 30.521418ms
Nov 29 02:17:32.879: INFO: Pod "downwardapi-volume-1f25788c-1498-46ee-bb64-959d28d33d0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034067911s
Nov 29 02:17:34.882: INFO: Pod "downwardapi-volume-1f25788c-1498-46ee-bb64-959d28d33d0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037081251s
STEP: Saw pod success
Nov 29 02:17:34.882: INFO: Pod "downwardapi-volume-1f25788c-1498-46ee-bb64-959d28d33d0d" satisfied condition "Succeeded or Failed"
Nov 29 02:17:34.885: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod downwardapi-volume-1f25788c-1498-46ee-bb64-959d28d33d0d container client-container: <nil>
STEP: delete the pod
Nov 29 02:17:34.901: INFO: Waiting for pod downwardapi-volume-1f25788c-1498-46ee-bb64-959d28d33d0d to disappear
Nov 29 02:17:34.911: INFO: Pod downwardapi-volume-1f25788c-1498-46ee-bb64-959d28d33d0d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:17:34.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3825" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":277,"completed":158,"skipped":2575,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:17:34.928: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Nov 29 02:17:34.965: INFO: Waiting up to 5m0s for pod "downwardapi-volume-81145f9f-8645-4f1e-87c5-6d46ae2dfdef" in namespace "projected-9722" to be "Succeeded or Failed"
Nov 29 02:17:34.968: INFO: Pod "downwardapi-volume-81145f9f-8645-4f1e-87c5-6d46ae2dfdef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.16859ms
Nov 29 02:17:36.971: INFO: Pod "downwardapi-volume-81145f9f-8645-4f1e-87c5-6d46ae2dfdef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005116407s
Nov 29 02:17:38.974: INFO: Pod "downwardapi-volume-81145f9f-8645-4f1e-87c5-6d46ae2dfdef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008413381s
STEP: Saw pod success
Nov 29 02:17:38.974: INFO: Pod "downwardapi-volume-81145f9f-8645-4f1e-87c5-6d46ae2dfdef" satisfied condition "Succeeded or Failed"
Nov 29 02:17:38.976: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod downwardapi-volume-81145f9f-8645-4f1e-87c5-6d46ae2dfdef container client-container: <nil>
STEP: delete the pod
Nov 29 02:17:39.004: INFO: Waiting for pod downwardapi-volume-81145f9f-8645-4f1e-87c5-6d46ae2dfdef to disappear
Nov 29 02:17:39.012: INFO: Pod downwardapi-volume-81145f9f-8645-4f1e-87c5-6d46ae2dfdef no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:17:39.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9722" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":277,"completed":159,"skipped":2580,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:17:39.085: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 29 02:17:39.874: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 29 02:17:41.881: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742213059, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742213059, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742213059, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742213059, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 02:17:44.897: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:17:44.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7845" for this suite.
STEP: Destroying namespace "webhook-7845-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.955 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":277,"completed":160,"skipped":2583,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:17:45.040: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on tmpfs
Nov 29 02:17:45.108: INFO: Waiting up to 5m0s for pod "pod-e22952e4-d3ed-44ea-8221-9ddb395619ee" in namespace "emptydir-7272" to be "Succeeded or Failed"
Nov 29 02:17:45.122: INFO: Pod "pod-e22952e4-d3ed-44ea-8221-9ddb395619ee": Phase="Pending", Reason="", readiness=false. Elapsed: 14.078695ms
Nov 29 02:17:47.126: INFO: Pod "pod-e22952e4-d3ed-44ea-8221-9ddb395619ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018061745s
Nov 29 02:17:49.129: INFO: Pod "pod-e22952e4-d3ed-44ea-8221-9ddb395619ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02097887s
STEP: Saw pod success
Nov 29 02:17:49.129: INFO: Pod "pod-e22952e4-d3ed-44ea-8221-9ddb395619ee" satisfied condition "Succeeded or Failed"
Nov 29 02:17:49.131: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod pod-e22952e4-d3ed-44ea-8221-9ddb395619ee container test-container: <nil>
STEP: delete the pod
Nov 29 02:17:49.150: INFO: Waiting for pod pod-e22952e4-d3ed-44ea-8221-9ddb395619ee to disappear
Nov 29 02:17:49.158: INFO: Pod pod-e22952e4-d3ed-44ea-8221-9ddb395619ee no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:17:49.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7272" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":161,"skipped":2589,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:17:49.172: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-projected-all-test-volume-30c0933a-d0a3-47ec-bf5d-7982090d8bd6
STEP: Creating secret with name secret-projected-all-test-volume-61451a9f-8628-4f74-a420-f5b62d87439c
STEP: Creating a pod to test Check all projections for projected volume plugin
Nov 29 02:17:49.220: INFO: Waiting up to 5m0s for pod "projected-volume-cb09aa10-1845-41ac-8df9-71713ce5e7de" in namespace "projected-7187" to be "Succeeded or Failed"
Nov 29 02:17:49.228: INFO: Pod "projected-volume-cb09aa10-1845-41ac-8df9-71713ce5e7de": Phase="Pending", Reason="", readiness=false. Elapsed: 8.679015ms
Nov 29 02:17:51.232: INFO: Pod "projected-volume-cb09aa10-1845-41ac-8df9-71713ce5e7de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011956206s
Nov 29 02:17:53.236: INFO: Pod "projected-volume-cb09aa10-1845-41ac-8df9-71713ce5e7de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01610871s
STEP: Saw pod success
Nov 29 02:17:53.236: INFO: Pod "projected-volume-cb09aa10-1845-41ac-8df9-71713ce5e7de" satisfied condition "Succeeded or Failed"
Nov 29 02:17:53.239: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod projected-volume-cb09aa10-1845-41ac-8df9-71713ce5e7de container projected-all-volume-test: <nil>
STEP: delete the pod
Nov 29 02:17:53.261: INFO: Waiting for pod projected-volume-cb09aa10-1845-41ac-8df9-71713ce5e7de to disappear
Nov 29 02:17:53.286: INFO: Pod projected-volume-cb09aa10-1845-41ac-8df9-71713ce5e7de no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:17:53.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7187" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":277,"completed":162,"skipped":2650,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:17:53.296: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod liveness-65cadf5b-f840-4b11-b7bf-4cbdf8056f07 in namespace container-probe-935
Nov 29 02:17:57.341: INFO: Started pod liveness-65cadf5b-f840-4b11-b7bf-4cbdf8056f07 in namespace container-probe-935
STEP: checking the pod's current state and verifying that restartCount is present
Nov 29 02:17:57.343: INFO: Initial restart count of pod liveness-65cadf5b-f840-4b11-b7bf-4cbdf8056f07 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:21:57.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-935" for this suite.

• [SLOW TEST:244.537 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":277,"completed":163,"skipped":2652,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:21:57.834: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 02:22:01.908: INFO: Waiting up to 5m0s for pod "client-envvars-6416095a-3bb3-4480-b2ab-d5ed7bcaedfc" in namespace "pods-6048" to be "Succeeded or Failed"
Nov 29 02:22:01.916: INFO: Pod "client-envvars-6416095a-3bb3-4480-b2ab-d5ed7bcaedfc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.956645ms
Nov 29 02:22:03.920: INFO: Pod "client-envvars-6416095a-3bb3-4480-b2ab-d5ed7bcaedfc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011564908s
Nov 29 02:22:05.923: INFO: Pod "client-envvars-6416095a-3bb3-4480-b2ab-d5ed7bcaedfc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014850023s
STEP: Saw pod success
Nov 29 02:22:05.923: INFO: Pod "client-envvars-6416095a-3bb3-4480-b2ab-d5ed7bcaedfc" satisfied condition "Succeeded or Failed"
Nov 29 02:22:05.925: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod client-envvars-6416095a-3bb3-4480-b2ab-d5ed7bcaedfc container env3cont: <nil>
STEP: delete the pod
Nov 29 02:22:05.950: INFO: Waiting for pod client-envvars-6416095a-3bb3-4480-b2ab-d5ed7bcaedfc to disappear
Nov 29 02:22:05.959: INFO: Pod client-envvars-6416095a-3bb3-4480-b2ab-d5ed7bcaedfc no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:22:05.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6048" for this suite.

• [SLOW TEST:8.132 seconds]
[k8s.io] Pods
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":277,"completed":164,"skipped":2701,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:22:05.967: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-7cdfd39a-c4ee-4a3a-b590-341336c02a01
STEP: Creating a pod to test consume secrets
Nov 29 02:22:06.047: INFO: Waiting up to 5m0s for pod "pod-secrets-4add7c85-55f9-473a-92e5-bdb8b827179c" in namespace "secrets-6756" to be "Succeeded or Failed"
Nov 29 02:22:06.051: INFO: Pod "pod-secrets-4add7c85-55f9-473a-92e5-bdb8b827179c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.639419ms
Nov 29 02:22:08.054: INFO: Pod "pod-secrets-4add7c85-55f9-473a-92e5-bdb8b827179c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007552381s
Nov 29 02:22:10.058: INFO: Pod "pod-secrets-4add7c85-55f9-473a-92e5-bdb8b827179c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01117508s
STEP: Saw pod success
Nov 29 02:22:10.058: INFO: Pod "pod-secrets-4add7c85-55f9-473a-92e5-bdb8b827179c" satisfied condition "Succeeded or Failed"
Nov 29 02:22:10.060: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod pod-secrets-4add7c85-55f9-473a-92e5-bdb8b827179c container secret-volume-test: <nil>
STEP: delete the pod
Nov 29 02:22:10.074: INFO: Waiting for pod pod-secrets-4add7c85-55f9-473a-92e5-bdb8b827179c to disappear
Nov 29 02:22:10.081: INFO: Pod pod-secrets-4add7c85-55f9-473a-92e5-bdb8b827179c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:22:10.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6756" for this suite.
STEP: Destroying namespace "secret-namespace-59" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":277,"completed":165,"skipped":2731,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:22:10.100: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-f9c542b8-0919-4d41-a0d4-6ccf8c7daf4b
STEP: Creating a pod to test consume configMaps
Nov 29 02:22:10.152: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-377459eb-a4b7-4974-81d8-0a1e65bb249c" in namespace "projected-388" to be "Succeeded or Failed"
Nov 29 02:22:10.158: INFO: Pod "pod-projected-configmaps-377459eb-a4b7-4974-81d8-0a1e65bb249c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.367141ms
Nov 29 02:22:12.161: INFO: Pod "pod-projected-configmaps-377459eb-a4b7-4974-81d8-0a1e65bb249c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008308636s
Nov 29 02:22:14.163: INFO: Pod "pod-projected-configmaps-377459eb-a4b7-4974-81d8-0a1e65bb249c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01108301s
STEP: Saw pod success
Nov 29 02:22:14.163: INFO: Pod "pod-projected-configmaps-377459eb-a4b7-4974-81d8-0a1e65bb249c" satisfied condition "Succeeded or Failed"
Nov 29 02:22:14.165: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod pod-projected-configmaps-377459eb-a4b7-4974-81d8-0a1e65bb249c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 02:22:14.184: INFO: Waiting for pod pod-projected-configmaps-377459eb-a4b7-4974-81d8-0a1e65bb249c to disappear
Nov 29 02:22:14.189: INFO: Pod pod-projected-configmaps-377459eb-a4b7-4974-81d8-0a1e65bb249c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:22:14.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-388" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":277,"completed":166,"skipped":2755,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:22:14.196: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Nov 29 02:22:14.230: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c81012b7-ca45-406a-86ce-e982b7056318" in namespace "downward-api-3238" to be "Succeeded or Failed"
Nov 29 02:22:14.236: INFO: Pod "downwardapi-volume-c81012b7-ca45-406a-86ce-e982b7056318": Phase="Pending", Reason="", readiness=false. Elapsed: 5.528145ms
Nov 29 02:22:16.239: INFO: Pod "downwardapi-volume-c81012b7-ca45-406a-86ce-e982b7056318": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008486897s
Nov 29 02:22:18.242: INFO: Pod "downwardapi-volume-c81012b7-ca45-406a-86ce-e982b7056318": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011639726s
STEP: Saw pod success
Nov 29 02:22:18.242: INFO: Pod "downwardapi-volume-c81012b7-ca45-406a-86ce-e982b7056318" satisfied condition "Succeeded or Failed"
Nov 29 02:22:18.244: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod downwardapi-volume-c81012b7-ca45-406a-86ce-e982b7056318 container client-container: <nil>
STEP: delete the pod
Nov 29 02:22:18.260: INFO: Waiting for pod downwardapi-volume-c81012b7-ca45-406a-86ce-e982b7056318 to disappear
Nov 29 02:22:18.270: INFO: Pod downwardapi-volume-c81012b7-ca45-406a-86ce-e982b7056318 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:22:18.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3238" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":167,"skipped":2773,"failed":0}

------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:22:18.278: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-c6f74158-0983-4be7-ad82-e373666b459e
STEP: Creating a pod to test consume secrets
Nov 29 02:22:18.320: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f799e7ed-0796-447f-9b42-875bfd3f2375" in namespace "projected-8164" to be "Succeeded or Failed"
Nov 29 02:22:18.327: INFO: Pod "pod-projected-secrets-f799e7ed-0796-447f-9b42-875bfd3f2375": Phase="Pending", Reason="", readiness=false. Elapsed: 7.228405ms
Nov 29 02:22:20.330: INFO: Pod "pod-projected-secrets-f799e7ed-0796-447f-9b42-875bfd3f2375": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010098104s
Nov 29 02:22:22.333: INFO: Pod "pod-projected-secrets-f799e7ed-0796-447f-9b42-875bfd3f2375": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013162579s
STEP: Saw pod success
Nov 29 02:22:22.333: INFO: Pod "pod-projected-secrets-f799e7ed-0796-447f-9b42-875bfd3f2375" satisfied condition "Succeeded or Failed"
Nov 29 02:22:22.335: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod pod-projected-secrets-f799e7ed-0796-447f-9b42-875bfd3f2375 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 29 02:22:22.363: INFO: Waiting for pod pod-projected-secrets-f799e7ed-0796-447f-9b42-875bfd3f2375 to disappear
Nov 29 02:22:22.370: INFO: Pod pod-projected-secrets-f799e7ed-0796-447f-9b42-875bfd3f2375 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:22:22.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8164" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":277,"completed":168,"skipped":2773,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:22:22.379: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir volume type on tmpfs
Nov 29 02:22:22.427: INFO: Waiting up to 5m0s for pod "pod-104a2a17-53b8-414a-a6e0-d6e76b58ca27" in namespace "emptydir-1146" to be "Succeeded or Failed"
Nov 29 02:22:22.440: INFO: Pod "pod-104a2a17-53b8-414a-a6e0-d6e76b58ca27": Phase="Pending", Reason="", readiness=false. Elapsed: 12.8815ms
Nov 29 02:22:24.443: INFO: Pod "pod-104a2a17-53b8-414a-a6e0-d6e76b58ca27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015932965s
Nov 29 02:22:26.445: INFO: Pod "pod-104a2a17-53b8-414a-a6e0-d6e76b58ca27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018702539s
STEP: Saw pod success
Nov 29 02:22:26.445: INFO: Pod "pod-104a2a17-53b8-414a-a6e0-d6e76b58ca27" satisfied condition "Succeeded or Failed"
Nov 29 02:22:26.448: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod pod-104a2a17-53b8-414a-a6e0-d6e76b58ca27 container test-container: <nil>
STEP: delete the pod
Nov 29 02:22:26.466: INFO: Waiting for pod pod-104a2a17-53b8-414a-a6e0-d6e76b58ca27 to disappear
Nov 29 02:22:26.475: INFO: Pod pod-104a2a17-53b8-414a-a6e0-d6e76b58ca27 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:22:26.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1146" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":169,"skipped":2777,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:22:26.485: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-2727
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 29 02:22:26.529: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 29 02:22:26.563: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 29 02:22:28.567: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 29 02:22:30.567: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 02:22:32.566: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 02:22:34.567: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 02:22:36.567: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 02:22:38.566: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 02:22:40.567: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov 29 02:22:40.571: INFO: The status of Pod netserver-1 is Running (Ready = false)
Nov 29 02:22:42.574: INFO: The status of Pod netserver-1 is Running (Ready = false)
Nov 29 02:22:44.574: INFO: The status of Pod netserver-1 is Running (Ready = true)
Nov 29 02:22:44.578: INFO: The status of Pod netserver-2 is Running (Ready = false)
Nov 29 02:22:46.581: INFO: The status of Pod netserver-2 is Running (Ready = false)
Nov 29 02:22:48.581: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Nov 29 02:22:52.621: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.93.223 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2727 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 02:22:52.621: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
Nov 29 02:22:53.729: INFO: Found all expected endpoints: [netserver-0]
Nov 29 02:22:53.732: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.191.186 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2727 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 02:22:53.732: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
Nov 29 02:22:54.846: INFO: Found all expected endpoints: [netserver-1]
Nov 29 02:22:54.848: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.36.231 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2727 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 02:22:54.848: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
Nov 29 02:22:55.973: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:22:55.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2727" for this suite.

• [SLOW TEST:29.497 seconds]
[sig-network] Networking
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":170,"skipped":2793,"failed":0}
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:22:55.983: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test override command
Nov 29 02:22:56.016: INFO: Waiting up to 5m0s for pod "client-containers-8d2ddd06-c22e-4200-b64d-c19b548282ac" in namespace "containers-8212" to be "Succeeded or Failed"
Nov 29 02:22:56.022: INFO: Pod "client-containers-8d2ddd06-c22e-4200-b64d-c19b548282ac": Phase="Pending", Reason="", readiness=false. Elapsed: 6.520854ms
Nov 29 02:22:58.027: INFO: Pod "client-containers-8d2ddd06-c22e-4200-b64d-c19b548282ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010704064s
Nov 29 02:23:00.030: INFO: Pod "client-containers-8d2ddd06-c22e-4200-b64d-c19b548282ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013878288s
STEP: Saw pod success
Nov 29 02:23:00.030: INFO: Pod "client-containers-8d2ddd06-c22e-4200-b64d-c19b548282ac" satisfied condition "Succeeded or Failed"
Nov 29 02:23:00.035: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod client-containers-8d2ddd06-c22e-4200-b64d-c19b548282ac container test-container: <nil>
STEP: delete the pod
Nov 29 02:23:00.057: INFO: Waiting for pod client-containers-8d2ddd06-c22e-4200-b64d-c19b548282ac to disappear
Nov 29 02:23:00.065: INFO: Pod client-containers-8d2ddd06-c22e-4200-b64d-c19b548282ac no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:23:00.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8212" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":277,"completed":171,"skipped":2796,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:23:00.076: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-map-1b0ab66d-969b-4b85-a866-8a74e658ceea
STEP: Creating a pod to test consume configMaps
Nov 29 02:23:00.143: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0608a7b6-de3e-4516-b249-5dc3a8d85279" in namespace "projected-2107" to be "Succeeded or Failed"
Nov 29 02:23:00.174: INFO: Pod "pod-projected-configmaps-0608a7b6-de3e-4516-b249-5dc3a8d85279": Phase="Pending", Reason="", readiness=false. Elapsed: 31.247304ms
Nov 29 02:23:02.178: INFO: Pod "pod-projected-configmaps-0608a7b6-de3e-4516-b249-5dc3a8d85279": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034890757s
Nov 29 02:23:04.181: INFO: Pod "pod-projected-configmaps-0608a7b6-de3e-4516-b249-5dc3a8d85279": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038191048s
STEP: Saw pod success
Nov 29 02:23:04.181: INFO: Pod "pod-projected-configmaps-0608a7b6-de3e-4516-b249-5dc3a8d85279" satisfied condition "Succeeded or Failed"
Nov 29 02:23:04.184: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod pod-projected-configmaps-0608a7b6-de3e-4516-b249-5dc3a8d85279 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 02:23:04.209: INFO: Waiting for pod pod-projected-configmaps-0608a7b6-de3e-4516-b249-5dc3a8d85279 to disappear
Nov 29 02:23:04.210: INFO: Pod pod-projected-configmaps-0608a7b6-de3e-4516-b249-5dc3a8d85279 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:23:04.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2107" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":277,"completed":172,"skipped":2818,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:23:04.219: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Nov 29 02:23:04.502: INFO: Pod name wrapped-volume-race-776dcac9-9635-45fb-8641-a54678194552: Found 0 pods out of 5
Nov 29 02:23:09.511: INFO: Pod name wrapped-volume-race-776dcac9-9635-45fb-8641-a54678194552: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-776dcac9-9635-45fb-8641-a54678194552 in namespace emptydir-wrapper-8575, will wait for the garbage collector to delete the pods
Nov 29 02:23:21.620: INFO: Deleting ReplicationController wrapped-volume-race-776dcac9-9635-45fb-8641-a54678194552 took: 7.04267ms
Nov 29 02:23:22.420: INFO: Terminating ReplicationController wrapped-volume-race-776dcac9-9635-45fb-8641-a54678194552 pods took: 800.237551ms
STEP: Creating RC which spawns configmap-volume pods
Nov 29 02:23:30.038: INFO: Pod name wrapped-volume-race-b85cb02a-6620-488d-a12c-f52cde717299: Found 0 pods out of 5
Nov 29 02:23:35.043: INFO: Pod name wrapped-volume-race-b85cb02a-6620-488d-a12c-f52cde717299: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b85cb02a-6620-488d-a12c-f52cde717299 in namespace emptydir-wrapper-8575, will wait for the garbage collector to delete the pods
Nov 29 02:23:47.131: INFO: Deleting ReplicationController wrapped-volume-race-b85cb02a-6620-488d-a12c-f52cde717299 took: 8.423787ms
Nov 29 02:23:48.031: INFO: Terminating ReplicationController wrapped-volume-race-b85cb02a-6620-488d-a12c-f52cde717299 pods took: 900.450632ms
STEP: Creating RC which spawns configmap-volume pods
Nov 29 02:23:57.866: INFO: Pod name wrapped-volume-race-0f1b86f3-bcf3-40fa-bba5-8dbe0a40e543: Found 0 pods out of 5
Nov 29 02:24:02.873: INFO: Pod name wrapped-volume-race-0f1b86f3-bcf3-40fa-bba5-8dbe0a40e543: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-0f1b86f3-bcf3-40fa-bba5-8dbe0a40e543 in namespace emptydir-wrapper-8575, will wait for the garbage collector to delete the pods
Nov 29 02:24:14.973: INFO: Deleting ReplicationController wrapped-volume-race-0f1b86f3-bcf3-40fa-bba5-8dbe0a40e543 took: 7.212259ms
Nov 29 02:24:15.773: INFO: Terminating ReplicationController wrapped-volume-race-0f1b86f3-bcf3-40fa-bba5-8dbe0a40e543 pods took: 800.364301ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:24:31.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8575" for this suite.

• [SLOW TEST:86.930 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":277,"completed":173,"skipped":2847,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:24:31.150: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-da2e430f-6458-45ad-a224-ec2e40aa6f20
STEP: Creating a pod to test consume configMaps
Nov 29 02:24:31.198: INFO: Waiting up to 5m0s for pod "pod-configmaps-041fd847-0b02-4bb6-8cbb-55f3168d933b" in namespace "configmap-2848" to be "Succeeded or Failed"
Nov 29 02:24:31.200: INFO: Pod "pod-configmaps-041fd847-0b02-4bb6-8cbb-55f3168d933b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.163466ms
Nov 29 02:24:33.203: INFO: Pod "pod-configmaps-041fd847-0b02-4bb6-8cbb-55f3168d933b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00508081s
Nov 29 02:24:35.206: INFO: Pod "pod-configmaps-041fd847-0b02-4bb6-8cbb-55f3168d933b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008061879s
STEP: Saw pod success
Nov 29 02:24:35.206: INFO: Pod "pod-configmaps-041fd847-0b02-4bb6-8cbb-55f3168d933b" satisfied condition "Succeeded or Failed"
Nov 29 02:24:35.208: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod pod-configmaps-041fd847-0b02-4bb6-8cbb-55f3168d933b container configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 02:24:35.236: INFO: Waiting for pod pod-configmaps-041fd847-0b02-4bb6-8cbb-55f3168d933b to disappear
Nov 29 02:24:35.247: INFO: Pod pod-configmaps-041fd847-0b02-4bb6-8cbb-55f3168d933b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:24:35.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2848" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":277,"completed":174,"skipped":2862,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:24:35.256: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 02:24:35.295: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Nov 29 02:24:40.305: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 29 02:24:40.306: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Nov 29 02:24:44.344: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2912 /apis/apps/v1/namespaces/deployment-2912/deployments/test-cleanup-deployment ae4461c1-b55b-46ff-b931-4d156f0677f5 38925 1 2020-11-29 02:24:39 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2020-11-29 02:24:39 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-11-29 02:24:42 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00405e258 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-11-29 02:24:39 +0000 UTC,LastTransitionTime:2020-11-29 02:24:39 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-b4867b47f" has successfully progressed.,LastUpdateTime:2020-11-29 02:24:42 +0000 UTC,LastTransitionTime:2020-11-29 02:24:39 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 29 02:24:44.346: INFO: New ReplicaSet "test-cleanup-deployment-b4867b47f" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-b4867b47f  deployment-2912 /apis/apps/v1/namespaces/deployment-2912/replicasets/test-cleanup-deployment-b4867b47f 593fa853-d061-4c55-ba60-cd48873c021b 38914 1 2020-11-29 02:24:39 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:b4867b47f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment ae4461c1-b55b-46ff-b931-4d156f0677f5 0xc00405e6b0 0xc00405e6b1}] []  [{kube-controller-manager Update apps/v1 2020-11-29 02:24:41 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 101 52 52 54 49 99 49 45 98 53 53 98 45 52 54 102 102 45 98 57 51 49 45 52 100 49 53 54 102 48 54 55 55 102 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: b4867b47f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:b4867b47f] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00405e738 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 29 02:24:44.350: INFO: Pod "test-cleanup-deployment-b4867b47f-2p22r" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-b4867b47f-2p22r test-cleanup-deployment-b4867b47f- deployment-2912 /api/v1/namespaces/deployment-2912/pods/test-cleanup-deployment-b4867b47f-2p22r d8552ad2-6b6d-4518-a685-cbb80eab7471 38913 0 2020-11-29 02:24:39 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:b4867b47f] map[cni.projectcalico.org/podIP:192.168.191.133/32 cni.projectcalico.org/podIPs:192.168.191.133/32] [{apps/v1 ReplicaSet test-cleanup-deployment-b4867b47f 593fa853-d061-4c55-ba60-cd48873c021b 0xc00405eb90 0xc00405eb91}] []  [{kube-controller-manager Update v1 2020-11-29 02:24:39 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 53 57 51 102 97 56 53 51 45 100 48 54 49 45 52 99 53 53 45 98 97 54 48 45 99 100 52 56 56 55 51 99 48 50 49 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-11-29 02:24:40 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-11-29 02:24:41 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 57 50 46 49 54 56 46 49 57 49 46 49 51 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nx7l2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nx7l2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nx7l2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-bc6233c429,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:24:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:24:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:24:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:24:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.165,PodIP:192.168.191.133,StartTime:2020-11-29 02:24:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-29 02:24:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,ImageID:docker-pullable://us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c,ContainerID:docker://0c9acb795d87527379f2a40e4aab5e290c18554721c29f4fd255ffe722646ed1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.191.133,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:24:44.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2912" for this suite.

• [SLOW TEST:9.102 seconds]
[sig-apps] Deployment
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":277,"completed":175,"skipped":2870,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:24:44.359: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl label
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1206
STEP: creating the pod
Nov 29 02:24:44.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 create -f - --namespace=kubectl-8612'
Nov 29 02:24:46.121: INFO: stderr: ""
Nov 29 02:24:46.121: INFO: stdout: "pod/pause created\n"
Nov 29 02:24:46.121: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Nov 29 02:24:46.121: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8612" to be "running and ready"
Nov 29 02:24:46.130: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 9.491512ms
Nov 29 02:24:48.134: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012924104s
Nov 29 02:24:50.137: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.015956539s
Nov 29 02:24:50.137: INFO: Pod "pause" satisfied condition "running and ready"
Nov 29 02:24:50.137: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: adding the label testing-label with value testing-label-value to a pod
Nov 29 02:24:50.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 label pods pause testing-label=testing-label-value --namespace=kubectl-8612'
Nov 29 02:24:50.243: INFO: stderr: ""
Nov 29 02:24:50.243: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Nov 29 02:24:50.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get pod pause -L testing-label --namespace=kubectl-8612'
Nov 29 02:24:50.329: INFO: stderr: ""
Nov 29 02:24:50.329: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Nov 29 02:24:50.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 label pods pause testing-label- --namespace=kubectl-8612'
Nov 29 02:24:50.448: INFO: stderr: ""
Nov 29 02:24:50.449: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Nov 29 02:24:50.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get pod pause -L testing-label --namespace=kubectl-8612'
Nov 29 02:24:50.543: INFO: stderr: ""
Nov 29 02:24:50.543: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1213
STEP: using delete to clean up resources
Nov 29 02:24:50.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 delete --grace-period=0 --force -f - --namespace=kubectl-8612'
Nov 29 02:24:50.656: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 29 02:24:50.656: INFO: stdout: "pod \"pause\" force deleted\n"
Nov 29 02:24:50.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get rc,svc -l name=pause --no-headers --namespace=kubectl-8612'
Nov 29 02:24:50.756: INFO: stderr: "No resources found in kubectl-8612 namespace.\n"
Nov 29 02:24:50.756: INFO: stdout: ""
Nov 29 02:24:50.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get pods -l name=pause --namespace=kubectl-8612 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 29 02:24:50.844: INFO: stderr: ""
Nov 29 02:24:50.844: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:24:50.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8612" for this suite.

• [SLOW TEST:6.494 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1203
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":277,"completed":176,"skipped":2874,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:24:50.855: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1226.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1226.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1226.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1226.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 29 02:24:54.918: INFO: DNS probes using dns-test-5dbd9c4b-ba28-4ebc-83be-c1189b1c361d succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1226.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1226.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1226.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1226.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 29 02:24:58.984: INFO: File wheezy_udp@dns-test-service-3.dns-1226.svc.cluster.local from pod  dns-1226/dns-test-91a00a94-a1f4-4134-b0ed-929955ecc0cc contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 29 02:24:58.988: INFO: File jessie_udp@dns-test-service-3.dns-1226.svc.cluster.local from pod  dns-1226/dns-test-91a00a94-a1f4-4134-b0ed-929955ecc0cc contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 29 02:24:58.988: INFO: Lookups using dns-1226/dns-test-91a00a94-a1f4-4134-b0ed-929955ecc0cc failed for: [wheezy_udp@dns-test-service-3.dns-1226.svc.cluster.local jessie_udp@dns-test-service-3.dns-1226.svc.cluster.local]

Nov 29 02:25:03.992: INFO: File wheezy_udp@dns-test-service-3.dns-1226.svc.cluster.local from pod  dns-1226/dns-test-91a00a94-a1f4-4134-b0ed-929955ecc0cc contains '' instead of 'bar.example.com.'
Nov 29 02:25:03.995: INFO: File jessie_udp@dns-test-service-3.dns-1226.svc.cluster.local from pod  dns-1226/dns-test-91a00a94-a1f4-4134-b0ed-929955ecc0cc contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 29 02:25:03.995: INFO: Lookups using dns-1226/dns-test-91a00a94-a1f4-4134-b0ed-929955ecc0cc failed for: [wheezy_udp@dns-test-service-3.dns-1226.svc.cluster.local jessie_udp@dns-test-service-3.dns-1226.svc.cluster.local]

Nov 29 02:25:08.991: INFO: File wheezy_udp@dns-test-service-3.dns-1226.svc.cluster.local from pod  dns-1226/dns-test-91a00a94-a1f4-4134-b0ed-929955ecc0cc contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 29 02:25:08.995: INFO: File jessie_udp@dns-test-service-3.dns-1226.svc.cluster.local from pod  dns-1226/dns-test-91a00a94-a1f4-4134-b0ed-929955ecc0cc contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 29 02:25:08.995: INFO: Lookups using dns-1226/dns-test-91a00a94-a1f4-4134-b0ed-929955ecc0cc failed for: [wheezy_udp@dns-test-service-3.dns-1226.svc.cluster.local jessie_udp@dns-test-service-3.dns-1226.svc.cluster.local]

Nov 29 02:25:13.992: INFO: File wheezy_udp@dns-test-service-3.dns-1226.svc.cluster.local from pod  dns-1226/dns-test-91a00a94-a1f4-4134-b0ed-929955ecc0cc contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 29 02:25:13.995: INFO: File jessie_udp@dns-test-service-3.dns-1226.svc.cluster.local from pod  dns-1226/dns-test-91a00a94-a1f4-4134-b0ed-929955ecc0cc contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 29 02:25:13.995: INFO: Lookups using dns-1226/dns-test-91a00a94-a1f4-4134-b0ed-929955ecc0cc failed for: [wheezy_udp@dns-test-service-3.dns-1226.svc.cluster.local jessie_udp@dns-test-service-3.dns-1226.svc.cluster.local]

Nov 29 02:25:18.991: INFO: File wheezy_udp@dns-test-service-3.dns-1226.svc.cluster.local from pod  dns-1226/dns-test-91a00a94-a1f4-4134-b0ed-929955ecc0cc contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 29 02:25:18.994: INFO: File jessie_udp@dns-test-service-3.dns-1226.svc.cluster.local from pod  dns-1226/dns-test-91a00a94-a1f4-4134-b0ed-929955ecc0cc contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 29 02:25:18.994: INFO: Lookups using dns-1226/dns-test-91a00a94-a1f4-4134-b0ed-929955ecc0cc failed for: [wheezy_udp@dns-test-service-3.dns-1226.svc.cluster.local jessie_udp@dns-test-service-3.dns-1226.svc.cluster.local]

Nov 29 02:25:23.995: INFO: DNS probes using dns-test-91a00a94-a1f4-4134-b0ed-929955ecc0cc succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1226.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1226.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1226.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1226.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 29 02:25:28.109: INFO: DNS probes using dns-test-20ec2ca5-5b98-4273-912b-e10b066b156f succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:25:28.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1226" for this suite.

• [SLOW TEST:37.322 seconds]
[sig-network] DNS
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":277,"completed":177,"skipped":2896,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:25:28.178: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Nov 29 02:25:28.231: INFO: Waiting up to 5m0s for pod "downward-api-c1a8f247-d59f-43cb-bd78-1cc0e1672e32" in namespace "downward-api-4730" to be "Succeeded or Failed"
Nov 29 02:25:28.233: INFO: Pod "downward-api-c1a8f247-d59f-43cb-bd78-1cc0e1672e32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.624152ms
Nov 29 02:25:30.237: INFO: Pod "downward-api-c1a8f247-d59f-43cb-bd78-1cc0e1672e32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006385855s
Nov 29 02:25:32.240: INFO: Pod "downward-api-c1a8f247-d59f-43cb-bd78-1cc0e1672e32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009511732s
STEP: Saw pod success
Nov 29 02:25:32.240: INFO: Pod "downward-api-c1a8f247-d59f-43cb-bd78-1cc0e1672e32" satisfied condition "Succeeded or Failed"
Nov 29 02:25:32.243: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod downward-api-c1a8f247-d59f-43cb-bd78-1cc0e1672e32 container dapi-container: <nil>
STEP: delete the pod
Nov 29 02:25:32.272: INFO: Waiting for pod downward-api-c1a8f247-d59f-43cb-bd78-1cc0e1672e32 to disappear
Nov 29 02:25:32.277: INFO: Pod downward-api-c1a8f247-d59f-43cb-bd78-1cc0e1672e32 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:25:32.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4730" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":277,"completed":178,"skipped":2939,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:25:32.286: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-0341438b-55b7-44fa-a98c-f8eccc563b64
STEP: Creating a pod to test consume secrets
Nov 29 02:25:32.382: INFO: Waiting up to 5m0s for pod "pod-secrets-a6902071-4ec0-4a84-bf31-f0bcaea72985" in namespace "secrets-915" to be "Succeeded or Failed"
Nov 29 02:25:32.399: INFO: Pod "pod-secrets-a6902071-4ec0-4a84-bf31-f0bcaea72985": Phase="Pending", Reason="", readiness=false. Elapsed: 17.28737ms
Nov 29 02:25:34.402: INFO: Pod "pod-secrets-a6902071-4ec0-4a84-bf31-f0bcaea72985": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020395091s
Nov 29 02:25:36.405: INFO: Pod "pod-secrets-a6902071-4ec0-4a84-bf31-f0bcaea72985": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023222465s
STEP: Saw pod success
Nov 29 02:25:36.405: INFO: Pod "pod-secrets-a6902071-4ec0-4a84-bf31-f0bcaea72985" satisfied condition "Succeeded or Failed"
Nov 29 02:25:36.407: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod pod-secrets-a6902071-4ec0-4a84-bf31-f0bcaea72985 container secret-volume-test: <nil>
STEP: delete the pod
Nov 29 02:25:36.424: INFO: Waiting for pod pod-secrets-a6902071-4ec0-4a84-bf31-f0bcaea72985 to disappear
Nov 29 02:25:36.431: INFO: Pod pod-secrets-a6902071-4ec0-4a84-bf31-f0bcaea72985 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:25:36.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-915" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":179,"skipped":2942,"failed":0}

------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:25:36.439: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-9020
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating statefulset ss in namespace statefulset-9020
Nov 29 02:25:36.522: INFO: Found 0 stateful pods, waiting for 1
Nov 29 02:25:46.529: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Nov 29 02:25:46.541: INFO: Deleting all statefulset in ns statefulset-9020
Nov 29 02:25:46.551: INFO: Scaling statefulset ss to 0
Nov 29 02:26:06.581: INFO: Waiting for statefulset status.replicas updated to 0
Nov 29 02:26:06.583: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:26:06.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9020" for this suite.

• [SLOW TEST:30.164 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":277,"completed":180,"skipped":2942,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:26:06.603: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Nov 29 02:26:06.642: INFO: Created pod &Pod{ObjectMeta:{dns-4527  dns-4527 /api/v1/namespaces/dns-4527/pods/dns-4527 bc61e837-740e-499f-a7f5-0b321ddcd334 39736 0 2020-11-29 02:26:05 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2020-11-29 02:26:05 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 114 103 115 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 67 111 110 102 105 103 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 115 101 114 118 101 114 115 34 58 123 125 44 34 102 58 115 101 97 114 99 104 101 115 34 58 123 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v6lc6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v6lc6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v6lc6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 02:26:06.650: INFO: The status of Pod dns-4527 is Pending, waiting for it to be Running (with Ready = true)
Nov 29 02:26:08.653: INFO: The status of Pod dns-4527 is Pending, waiting for it to be Running (with Ready = true)
Nov 29 02:26:10.653: INFO: The status of Pod dns-4527 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Nov 29 02:26:10.653: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-4527 PodName:dns-4527 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 02:26:10.653: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Verifying customized DNS server is configured on pod...
Nov 29 02:26:10.776: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-4527 PodName:dns-4527 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 02:26:10.776: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
Nov 29 02:26:10.888: INFO: Deleting pod dns-4527...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:26:10.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4527" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":277,"completed":181,"skipped":2947,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:26:10.916: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:27:10.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6324" for this suite.

• [SLOW TEST:60.058 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":277,"completed":182,"skipped":2958,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:27:10.975: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-projected-6dqr
STEP: Creating a pod to test atomic-volume-subpath
Nov 29 02:27:11.024: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-6dqr" in namespace "subpath-5918" to be "Succeeded or Failed"
Nov 29 02:27:11.032: INFO: Pod "pod-subpath-test-projected-6dqr": Phase="Pending", Reason="", readiness=false. Elapsed: 7.423212ms
Nov 29 02:27:13.035: INFO: Pod "pod-subpath-test-projected-6dqr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010633145s
Nov 29 02:27:15.038: INFO: Pod "pod-subpath-test-projected-6dqr": Phase="Running", Reason="", readiness=true. Elapsed: 4.013586406s
Nov 29 02:27:17.041: INFO: Pod "pod-subpath-test-projected-6dqr": Phase="Running", Reason="", readiness=true. Elapsed: 6.016578027s
Nov 29 02:27:19.044: INFO: Pod "pod-subpath-test-projected-6dqr": Phase="Running", Reason="", readiness=true. Elapsed: 8.019489943s
Nov 29 02:27:21.046: INFO: Pod "pod-subpath-test-projected-6dqr": Phase="Running", Reason="", readiness=true. Elapsed: 10.022254179s
Nov 29 02:27:23.050: INFO: Pod "pod-subpath-test-projected-6dqr": Phase="Running", Reason="", readiness=true. Elapsed: 12.025557768s
Nov 29 02:27:25.053: INFO: Pod "pod-subpath-test-projected-6dqr": Phase="Running", Reason="", readiness=true. Elapsed: 14.028461923s
Nov 29 02:27:27.056: INFO: Pod "pod-subpath-test-projected-6dqr": Phase="Running", Reason="", readiness=true. Elapsed: 16.031420148s
Nov 29 02:27:29.059: INFO: Pod "pod-subpath-test-projected-6dqr": Phase="Running", Reason="", readiness=true. Elapsed: 18.034604514s
Nov 29 02:27:31.062: INFO: Pod "pod-subpath-test-projected-6dqr": Phase="Running", Reason="", readiness=true. Elapsed: 20.037719025s
Nov 29 02:27:33.066: INFO: Pod "pod-subpath-test-projected-6dqr": Phase="Running", Reason="", readiness=true. Elapsed: 22.041351775s
Nov 29 02:27:35.069: INFO: Pod "pod-subpath-test-projected-6dqr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.044586164s
STEP: Saw pod success
Nov 29 02:27:35.069: INFO: Pod "pod-subpath-test-projected-6dqr" satisfied condition "Succeeded or Failed"
Nov 29 02:27:35.072: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod pod-subpath-test-projected-6dqr container test-container-subpath-projected-6dqr: <nil>
STEP: delete the pod
Nov 29 02:27:35.101: INFO: Waiting for pod pod-subpath-test-projected-6dqr to disappear
Nov 29 02:27:35.106: INFO: Pod pod-subpath-test-projected-6dqr no longer exists
STEP: Deleting pod pod-subpath-test-projected-6dqr
Nov 29 02:27:35.106: INFO: Deleting pod "pod-subpath-test-projected-6dqr" in namespace "subpath-5918"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:27:35.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5918" for this suite.

• [SLOW TEST:24.144 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":277,"completed":183,"skipped":2962,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:27:35.120: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 02:27:35.152: INFO: Creating deployment "webserver-deployment"
Nov 29 02:27:35.156: INFO: Waiting for observed generation 1
Nov 29 02:27:37.173: INFO: Waiting for all required pods to come up
Nov 29 02:27:37.184: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Nov 29 02:27:39.197: INFO: Waiting for deployment "webserver-deployment" to complete
Nov 29 02:27:39.203: INFO: Updating deployment "webserver-deployment" with a non-existent image
Nov 29 02:27:39.211: INFO: Updating deployment webserver-deployment
Nov 29 02:27:39.211: INFO: Waiting for observed generation 2
Nov 29 02:27:41.227: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Nov 29 02:27:41.230: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Nov 29 02:27:41.235: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Nov 29 02:27:41.244: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Nov 29 02:27:41.244: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Nov 29 02:27:41.247: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Nov 29 02:27:41.252: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Nov 29 02:27:41.252: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Nov 29 02:27:41.269: INFO: Updating deployment webserver-deployment
Nov 29 02:27:41.269: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Nov 29 02:27:41.287: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Nov 29 02:27:41.297: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Nov 29 02:27:41.353: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-5687 /apis/apps/v1/namespaces/deployment-5687/deployments/webserver-deployment 3f546e8d-028a-4e1d-adfb-606125920454 40637 3 2020-11-29 02:27:34 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-11-29 02:27:40 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-11-29 02:27:40 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 110 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003f1ea78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-6676bcd6d4" is progressing.,LastUpdateTime:2020-11-29 02:27:38 +0000 UTC,LastTransitionTime:2020-11-29 02:27:34 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-11-29 02:27:40 +0000 UTC,LastTransitionTime:2020-11-29 02:27:40 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Nov 29 02:27:41.417: INFO: New ReplicaSet "webserver-deployment-6676bcd6d4" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-6676bcd6d4  deployment-5687 /apis/apps/v1/namespaces/deployment-5687/replicasets/webserver-deployment-6676bcd6d4 dbe7af1c-ba98-4075-9e0b-0d059c07b71b 40632 3 2020-11-29 02:27:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 3f546e8d-028a-4e1d-adfb-606125920454 0xc003f1eee7 0xc003f1eee8}] []  [{kube-controller-manager Update apps/v1 2020-11-29 02:27:40 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 51 102 53 52 54 101 56 100 45 48 50 56 97 45 52 101 49 100 45 97 100 102 98 45 54 48 54 49 50 53 57 50 48 52 53 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 6676bcd6d4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003f1ef78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 29 02:27:41.417: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Nov 29 02:27:41.417: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-84855cf797  deployment-5687 /apis/apps/v1/namespaces/deployment-5687/replicasets/webserver-deployment-84855cf797 baf6c03c-5baa-4e06-94ed-4e4f3faa80d4 40629 3 2020-11-29 02:27:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 3f546e8d-028a-4e1d-adfb-606125920454 0xc003f1efd7 0xc003f1efd8}] []  [{kube-controller-manager Update apps/v1 2020-11-29 02:27:40 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 51 102 53 52 54 101 56 100 45 48 50 56 97 45 52 101 49 100 45 97 100 102 98 45 54 48 54 49 50 53 57 50 48 52 53 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 84855cf797,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003f1f048 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Nov 29 02:27:41.449: INFO: Pod "webserver-deployment-6676bcd6d4-2cmlm" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-2cmlm webserver-deployment-6676bcd6d4- deployment-5687 /api/v1/namespaces/deployment-5687/pods/webserver-deployment-6676bcd6d4-2cmlm ddd02111-8ce3-42c3-af07-a12f30f0a6d7 40665 0 2020-11-29 02:27:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 dbe7af1c-ba98-4075-9e0b-0d059c07b71b 0xc0052adb57 0xc0052adb58}] []  [{kube-controller-manager Update v1 2020-11-29 02:27:40 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 100 98 101 55 97 102 49 99 45 98 97 57 56 45 52 48 55 53 45 57 101 48 98 45 48 100 48 53 57 99 48 55 98 55 49 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fqmbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fqmbh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fqmbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-ea4104c57e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 02:27:41.449: INFO: Pod "webserver-deployment-6676bcd6d4-59gml" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-59gml webserver-deployment-6676bcd6d4- deployment-5687 /api/v1/namespaces/deployment-5687/pods/webserver-deployment-6676bcd6d4-59gml da2af86f-af8d-4a4d-a7fd-e425590d05a6 40669 0 2020-11-29 02:27:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 dbe7af1c-ba98-4075-9e0b-0d059c07b71b 0xc0052adc80 0xc0052adc81}] []  [{kube-controller-manager Update v1 2020-11-29 02:27:40 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 100 98 101 55 97 102 49 99 45 98 97 57 56 45 52 48 55 53 45 57 101 48 98 45 48 100 48 53 57 99 48 55 98 55 49 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fqmbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fqmbh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fqmbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-ea4104c57e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 02:27:41.450: INFO: Pod "webserver-deployment-6676bcd6d4-97w88" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-97w88 webserver-deployment-6676bcd6d4- deployment-5687 /api/v1/namespaces/deployment-5687/pods/webserver-deployment-6676bcd6d4-97w88 a42be981-a9d5-48b2-adce-11b4a9bc94ae 40667 0 2020-11-29 02:27:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 dbe7af1c-ba98-4075-9e0b-0d059c07b71b 0xc0052addb0 0xc0052addb1}] []  [{kube-controller-manager Update v1 2020-11-29 02:27:40 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 100 98 101 55 97 102 49 99 45 98 97 57 56 45 52 48 55 53 45 57 101 48 98 45 48 100 48 53 57 99 48 55 98 55 49 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fqmbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fqmbh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fqmbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-bc6233c429,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 02:27:41.450: INFO: Pod "webserver-deployment-6676bcd6d4-98dgj" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-98dgj webserver-deployment-6676bcd6d4- deployment-5687 /api/v1/namespaces/deployment-5687/pods/webserver-deployment-6676bcd6d4-98dgj feaac3b2-d070-4a0c-b976-9a63d5d45d83 40668 0 2020-11-29 02:27:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 dbe7af1c-ba98-4075-9e0b-0d059c07b71b 0xc0052adee0 0xc0052adee1}] []  [{kube-controller-manager Update v1 2020-11-29 02:27:40 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 100 98 101 55 97 102 49 99 45 98 97 57 56 45 52 48 55 53 45 57 101 48 98 45 48 100 48 53 57 99 48 55 98 55 49 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fqmbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fqmbh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fqmbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-55fe8f9f1d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 02:27:41.450: INFO: Pod "webserver-deployment-6676bcd6d4-blr24" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-blr24 webserver-deployment-6676bcd6d4- deployment-5687 /api/v1/namespaces/deployment-5687/pods/webserver-deployment-6676bcd6d4-blr24 c07fe820-480f-4c5e-87c1-96e7c4d6a4c1 40619 0 2020-11-29 02:27:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[cni.projectcalico.org/podIP:192.168.191.145/32 cni.projectcalico.org/podIPs:192.168.191.145/32] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 dbe7af1c-ba98-4075-9e0b-0d059c07b71b 0xc004fbc010 0xc004fbc011}] []  [{kube-controller-manager Update v1 2020-11-29 02:27:38 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 100 98 101 55 97 102 49 99 45 98 97 57 56 45 52 48 55 53 45 57 101 48 98 45 48 100 48 53 57 99 48 55 98 55 49 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-11-29 02:27:38 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}} {calico Update v1 2020-11-29 02:27:40 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fqmbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fqmbh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fqmbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-bc6233c429,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.165,PodIP:,StartTime:2020-11-29 02:27:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 02:27:41.451: INFO: Pod "webserver-deployment-6676bcd6d4-c4rqg" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-c4rqg webserver-deployment-6676bcd6d4- deployment-5687 /api/v1/namespaces/deployment-5687/pods/webserver-deployment-6676bcd6d4-c4rqg fefba972-3af5-491a-9a84-8cf28dc3205c 40687 0 2020-11-29 02:27:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 dbe7af1c-ba98-4075-9e0b-0d059c07b71b 0xc004fbc1c7 0xc004fbc1c8}] []  [{kube-controller-manager Update v1 2020-11-29 02:27:40 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 100 98 101 55 97 102 49 99 45 98 97 57 56 45 52 48 55 53 45 57 101 48 98 45 48 100 48 53 57 99 48 55 98 55 49 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fqmbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fqmbh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fqmbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-ea4104c57e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 02:27:41.451: INFO: Pod "webserver-deployment-6676bcd6d4-cc5nh" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-cc5nh webserver-deployment-6676bcd6d4- deployment-5687 /api/v1/namespaces/deployment-5687/pods/webserver-deployment-6676bcd6d4-cc5nh 7ba0e3f7-0a37-486a-95b7-8c25485e2ab5 40670 0 2020-11-29 02:27:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 dbe7af1c-ba98-4075-9e0b-0d059c07b71b 0xc004fbc2f0 0xc004fbc2f1}] []  [{kube-controller-manager Update v1 2020-11-29 02:27:40 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 100 98 101 55 97 102 49 99 45 98 97 57 56 45 52 48 55 53 45 57 101 48 98 45 48 100 48 53 57 99 48 55 98 55 49 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fqmbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fqmbh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fqmbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-bc6233c429,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 02:27:41.451: INFO: Pod "webserver-deployment-6676bcd6d4-cwj6j" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-cwj6j webserver-deployment-6676bcd6d4- deployment-5687 /api/v1/namespaces/deployment-5687/pods/webserver-deployment-6676bcd6d4-cwj6j 388c206b-7730-456e-bb94-a0ac1a36d578 40628 0 2020-11-29 02:27:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[cni.projectcalico.org/podIP:192.168.191.146/32 cni.projectcalico.org/podIPs:192.168.191.146/32] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 dbe7af1c-ba98-4075-9e0b-0d059c07b71b 0xc004fbc420 0xc004fbc421}] []  [{kube-controller-manager Update v1 2020-11-29 02:27:38 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 100 98 101 55 97 102 49 99 45 98 97 57 56 45 52 48 55 53 45 57 101 48 98 45 48 100 48 53 57 99 48 55 98 55 49 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-11-29 02:27:38 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}} {calico Update v1 2020-11-29 02:27:40 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fqmbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fqmbh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fqmbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-bc6233c429,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.165,PodIP:,StartTime:2020-11-29 02:27:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 02:27:41.452: INFO: Pod "webserver-deployment-6676bcd6d4-lp4mt" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-lp4mt webserver-deployment-6676bcd6d4- deployment-5687 /api/v1/namespaces/deployment-5687/pods/webserver-deployment-6676bcd6d4-lp4mt 88916466-4fbf-4626-a146-e28eadf163e7 40615 0 2020-11-29 02:27:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[cni.projectcalico.org/podIP:192.168.36.249/32 cni.projectcalico.org/podIPs:192.168.36.249/32] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 dbe7af1c-ba98-4075-9e0b-0d059c07b71b 0xc004fbc5d7 0xc004fbc5d8}] []  [{kube-controller-manager Update v1 2020-11-29 02:27:38 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 100 98 101 55 97 102 49 99 45 98 97 57 56 45 52 48 55 53 45 57 101 48 98 45 48 100 48 53 57 99 48 55 98 55 49 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-11-29 02:27:38 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}} {calico Update v1 2020-11-29 02:27:40 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fqmbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fqmbh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fqmbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-ea4104c57e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.164,PodIP:,StartTime:2020-11-29 02:27:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 02:27:41.452: INFO: Pod "webserver-deployment-6676bcd6d4-m7cnk" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-m7cnk webserver-deployment-6676bcd6d4- deployment-5687 /api/v1/namespaces/deployment-5687/pods/webserver-deployment-6676bcd6d4-m7cnk 70a66a37-ffe9-465a-bc30-b59741a7e413 40680 0 2020-11-29 02:27:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 dbe7af1c-ba98-4075-9e0b-0d059c07b71b 0xc004fbc787 0xc004fbc788}] []  [{kube-controller-manager Update v1 2020-11-29 02:27:40 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 100 98 101 55 97 102 49 99 45 98 97 57 56 45 52 48 55 53 45 57 101 48 98 45 48 100 48 53 57 99 48 55 98 55 49 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fqmbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fqmbh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fqmbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-55fe8f9f1d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 02:27:41.453: INFO: Pod "webserver-deployment-6676bcd6d4-s2xd2" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-s2xd2 webserver-deployment-6676bcd6d4- deployment-5687 /api/v1/namespaces/deployment-5687/pods/webserver-deployment-6676bcd6d4-s2xd2 4d0f1e1c-488e-4f7c-8200-effd57b71092 40681 0 2020-11-29 02:27:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 dbe7af1c-ba98-4075-9e0b-0d059c07b71b 0xc004fbc8b0 0xc004fbc8b1}] []  [{kube-controller-manager Update v1 2020-11-29 02:27:40 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 100 98 101 55 97 102 49 99 45 98 97 57 56 45 52 48 55 53 45 57 101 48 98 45 48 100 48 53 57 99 48 55 98 55 49 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-11-29 02:27:40 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fqmbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fqmbh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fqmbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-55fe8f9f1d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.166,PodIP:,StartTime:2020-11-29 02:27:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 02:27:41.453: INFO: Pod "webserver-deployment-6676bcd6d4-w6krw" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-w6krw webserver-deployment-6676bcd6d4- deployment-5687 /api/v1/namespaces/deployment-5687/pods/webserver-deployment-6676bcd6d4-w6krw 2396b782-665a-43fe-a869-e9e5788ea09c 40611 0 2020-11-29 02:27:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[cni.projectcalico.org/podIP:192.168.93.232/32 cni.projectcalico.org/podIPs:192.168.93.232/32] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 dbe7af1c-ba98-4075-9e0b-0d059c07b71b 0xc004fbca47 0xc004fbca48}] []  [{kube-controller-manager Update v1 2020-11-29 02:27:38 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 100 98 101 55 97 102 49 99 45 98 97 57 56 45 52 48 55 53 45 57 101 48 98 45 48 100 48 53 57 99 48 55 98 55 49 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-11-29 02:27:38 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}} {calico Update v1 2020-11-29 02:27:39 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fqmbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fqmbh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fqmbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-55fe8f9f1d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.166,PodIP:,StartTime:2020-11-29 02:27:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 02:27:41.453: INFO: Pod "webserver-deployment-6676bcd6d4-xjfwj" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-xjfwj webserver-deployment-6676bcd6d4- deployment-5687 /api/v1/namespaces/deployment-5687/pods/webserver-deployment-6676bcd6d4-xjfwj 6ed97d19-376c-4795-b5cc-22e18394b274 40623 0 2020-11-29 02:27:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[cni.projectcalico.org/podIP:192.168.36.250/32 cni.projectcalico.org/podIPs:192.168.36.250/32] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 dbe7af1c-ba98-4075-9e0b-0d059c07b71b 0xc004fbcbf7 0xc004fbcbf8}] []  [{kube-controller-manager Update v1 2020-11-29 02:27:38 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 100 98 101 55 97 102 49 99 45 98 97 57 56 45 52 48 55 53 45 57 101 48 98 45 48 100 48 53 57 99 48 55 98 55 49 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-11-29 02:27:38 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}} {calico Update v1 2020-11-29 02:27:40 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fqmbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fqmbh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fqmbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-ea4104c57e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.164,PodIP:,StartTime:2020-11-29 02:27:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 02:27:41.454: INFO: Pod "webserver-deployment-84855cf797-72tg6" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-72tg6 webserver-deployment-84855cf797- deployment-5687 /api/v1/namespaces/deployment-5687/pods/webserver-deployment-84855cf797-72tg6 a8054ce5-818a-479d-8813-36ee86add8ae 40643 0 2020-11-29 02:27:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 baf6c03c-5baa-4e06-94ed-4e4f3faa80d4 0xc004fbcda7 0xc004fbcda8}] []  [{kube-controller-manager Update v1 2020-11-29 02:27:40 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 97 102 54 99 48 51 99 45 53 98 97 97 45 52 101 48 54 45 57 52 101 100 45 52 101 52 102 51 102 97 97 56 48 100 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fqmbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fqmbh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fqmbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-bc6233c429,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 02:27:41.454: INFO: Pod "webserver-deployment-84855cf797-7vp4v" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-7vp4v webserver-deployment-84855cf797- deployment-5687 /api/v1/namespaces/deployment-5687/pods/webserver-deployment-84855cf797-7vp4v db9805fe-4ef3-41dd-8f93-3e3ca227e206 40676 0 2020-11-29 02:27:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 baf6c03c-5baa-4e06-94ed-4e4f3faa80d4 0xc004fbcec0 0xc004fbcec1}] []  [{kube-controller-manager Update v1 2020-11-29 02:27:40 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 97 102 54 99 48 51 99 45 53 98 97 97 45 52 101 48 54 45 57 52 101 100 45 52 101 52 102 51 102 97 97 56 48 100 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fqmbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fqmbh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fqmbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-ea4104c57e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 02:27:41.454: INFO: Pod "webserver-deployment-84855cf797-9759d" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-9759d webserver-deployment-84855cf797- deployment-5687 /api/v1/namespaces/deployment-5687/pods/webserver-deployment-84855cf797-9759d 5275b585-0274-45a0-9c8f-88f5e0a09ba4 40508 0 2020-11-29 02:27:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:192.168.93.230/32 cni.projectcalico.org/podIPs:192.168.93.230/32] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 baf6c03c-5baa-4e06-94ed-4e4f3faa80d4 0xc004fbcfe0 0xc004fbcfe1}] []  [{kube-controller-manager Update v1 2020-11-29 02:27:34 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 97 102 54 99 48 51 99 45 53 98 97 97 45 52 101 48 54 45 57 52 101 100 45 52 101 52 102 51 102 97 97 56 48 100 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-11-29 02:27:36 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-11-29 02:27:37 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 57 50 46 49 54 56 46 57 51 46 50 51 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fqmbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fqmbh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fqmbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-55fe8f9f1d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.166,PodIP:192.168.93.230,StartTime:2020-11-29 02:27:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-29 02:27:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://664d647a0326a41d0f07899489b5740f729a9ff57cc2308b9abf45310b985296,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.93.230,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 02:27:41.455: INFO: Pod "webserver-deployment-84855cf797-9szfb" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-9szfb webserver-deployment-84855cf797- deployment-5687 /api/v1/namespaces/deployment-5687/pods/webserver-deployment-84855cf797-9szfb 38860d09-288c-49b4-920a-0e6ef3ba2f76 40674 0 2020-11-29 02:27:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 baf6c03c-5baa-4e06-94ed-4e4f3faa80d4 0xc004fbd197 0xc004fbd198}] []  [{kube-controller-manager Update v1 2020-11-29 02:27:40 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 97 102 54 99 48 51 99 45 53 98 97 97 45 52 101 48 54 45 57 52 101 100 45 52 101 52 102 51 102 97 97 56 48 100 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-11-29 02:27:40 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fqmbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fqmbh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fqmbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-bc6233c429,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.165,PodIP:,StartTime:2020-11-29 02:27:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 02:27:41.455: INFO: Pod "webserver-deployment-84855cf797-dvg85" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-dvg85 webserver-deployment-84855cf797- deployment-5687 /api/v1/namespaces/deployment-5687/pods/webserver-deployment-84855cf797-dvg85 3882d319-4c7e-43ed-9ed8-faee34e85876 40519 0 2020-11-29 02:27:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:192.168.36.246/32 cni.projectcalico.org/podIPs:192.168.36.246/32] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 baf6c03c-5baa-4e06-94ed-4e4f3faa80d4 0xc004fbd317 0xc004fbd318}] []  [{kube-controller-manager Update v1 2020-11-29 02:27:34 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 97 102 54 99 48 51 99 45 53 98 97 97 45 52 101 48 54 45 57 52 101 100 45 52 101 52 102 51 102 97 97 56 48 100 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-11-29 02:27:36 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-11-29 02:27:37 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 57 50 46 49 54 56 46 51 54 46 50 52 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fqmbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fqmbh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fqmbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-ea4104c57e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.164,PodIP:192.168.36.246,StartTime:2020-11-29 02:27:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-29 02:27:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://b7223d663db59cbbde3615d9f15d88705176b6977107e895ca1835b214a00f71,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.36.246,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 02:27:41.455: INFO: Pod "webserver-deployment-84855cf797-fc77b" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-fc77b webserver-deployment-84855cf797- deployment-5687 /api/v1/namespaces/deployment-5687/pods/webserver-deployment-84855cf797-fc77b 5a087748-cadb-4488-b915-94eb6962789a 40516 0 2020-11-29 02:27:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:192.168.36.247/32 cni.projectcalico.org/podIPs:192.168.36.247/32] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 baf6c03c-5baa-4e06-94ed-4e4f3faa80d4 0xc004fbd4e7 0xc004fbd4e8}] []  [{kube-controller-manager Update v1 2020-11-29 02:27:34 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 97 102 54 99 48 51 99 45 53 98 97 97 45 52 101 48 54 45 57 52 101 100 45 52 101 52 102 51 102 97 97 56 48 100 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-11-29 02:27:36 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-11-29 02:27:37 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 57 50 46 49 54 56 46 51 54 46 50 52 55 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fqmbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fqmbh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fqmbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-ea4104c57e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.164,PodIP:192.168.36.247,StartTime:2020-11-29 02:27:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-29 02:27:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://b6e60f6fe5ab441c20ac058e04d9965c1e55f7b5c0cacb09bf9d3a103c9d138f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.36.247,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 02:27:41.456: INFO: Pod "webserver-deployment-84855cf797-fn2wr" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-fn2wr webserver-deployment-84855cf797- deployment-5687 /api/v1/namespaces/deployment-5687/pods/webserver-deployment-84855cf797-fn2wr c8a9dbe8-0bb5-488b-b529-f50286b8b95d 40513 0 2020-11-29 02:27:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:192.168.93.231/32 cni.projectcalico.org/podIPs:192.168.93.231/32] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 baf6c03c-5baa-4e06-94ed-4e4f3faa80d4 0xc004fbd697 0xc004fbd698}] []  [{kube-controller-manager Update v1 2020-11-29 02:27:34 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 97 102 54 99 48 51 99 45 53 98 97 97 45 52 101 48 54 45 57 52 101 100 45 52 101 52 102 51 102 97 97 56 48 100 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-11-29 02:27:36 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-11-29 02:27:37 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 57 50 46 49 54 56 46 57 51 46 50 51 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fqmbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fqmbh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fqmbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-55fe8f9f1d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.166,PodIP:192.168.93.231,StartTime:2020-11-29 02:27:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-29 02:27:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://a9c4e9fbc6197c888296f8dd04e6fbfd229bed8ac4e11b2f55868eb760f7543b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.93.231,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 02:27:41.456: INFO: Pod "webserver-deployment-84855cf797-lpbvz" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-lpbvz webserver-deployment-84855cf797- deployment-5687 /api/v1/namespaces/deployment-5687/pods/webserver-deployment-84855cf797-lpbvz 7ed4ace7-5983-4df7-9d3b-717227de3258 40683 0 2020-11-29 02:27:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 baf6c03c-5baa-4e06-94ed-4e4f3faa80d4 0xc004fbd847 0xc004fbd848}] []  [{kube-controller-manager Update v1 2020-11-29 02:27:40 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 97 102 54 99 48 51 99 45 53 98 97 97 45 52 101 48 54 45 57 52 101 100 45 52 101 52 102 51 102 97 97 56 48 100 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fqmbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fqmbh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fqmbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-bc6233c429,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 02:27:41.456: INFO: Pod "webserver-deployment-84855cf797-m6s27" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-m6s27 webserver-deployment-84855cf797- deployment-5687 /api/v1/namespaces/deployment-5687/pods/webserver-deployment-84855cf797-m6s27 2098f14d-18b6-4cee-88e6-cd8d4e9d4f38 40658 0 2020-11-29 02:27:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 baf6c03c-5baa-4e06-94ed-4e4f3faa80d4 0xc004fbd960 0xc004fbd961}] []  [{kube-controller-manager Update v1 2020-11-29 02:27:40 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 97 102 54 99 48 51 99 45 53 98 97 97 45 52 101 48 54 45 57 52 101 100 45 52 101 52 102 51 102 97 97 56 48 100 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fqmbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fqmbh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fqmbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-bc6233c429,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 02:27:41.456: INFO: Pod "webserver-deployment-84855cf797-qdm2v" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-qdm2v webserver-deployment-84855cf797- deployment-5687 /api/v1/namespaces/deployment-5687/pods/webserver-deployment-84855cf797-qdm2v af902bcb-d79f-49b7-9c00-950fd59928ac 40673 0 2020-11-29 02:27:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 baf6c03c-5baa-4e06-94ed-4e4f3faa80d4 0xc004fbda80 0xc004fbda81}] []  [{kube-controller-manager Update v1 2020-11-29 02:27:40 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 97 102 54 99 48 51 99 45 53 98 97 97 45 52 101 48 54 45 57 52 101 100 45 52 101 52 102 51 102 97 97 56 48 100 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fqmbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fqmbh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fqmbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-bc6233c429,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 02:27:41.456: INFO: Pod "webserver-deployment-84855cf797-qr7qt" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-qr7qt webserver-deployment-84855cf797- deployment-5687 /api/v1/namespaces/deployment-5687/pods/webserver-deployment-84855cf797-qr7qt 93b2ed5b-3336-4fc2-9670-9b45ff5b1f9b 40650 0 2020-11-29 02:27:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 baf6c03c-5baa-4e06-94ed-4e4f3faa80d4 0xc004fbdba0 0xc004fbdba1}] []  [{kube-controller-manager Update v1 2020-11-29 02:27:40 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 97 102 54 99 48 51 99 45 53 98 97 97 45 52 101 48 54 45 57 52 101 100 45 52 101 52 102 51 102 97 97 56 48 100 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fqmbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fqmbh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fqmbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-55fe8f9f1d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 02:27:41.457: INFO: Pod "webserver-deployment-84855cf797-r2ndc" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-r2ndc webserver-deployment-84855cf797- deployment-5687 /api/v1/namespaces/deployment-5687/pods/webserver-deployment-84855cf797-r2ndc ecadabc6-488e-46a9-87bd-12eb11086a28 40671 0 2020-11-29 02:27:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 baf6c03c-5baa-4e06-94ed-4e4f3faa80d4 0xc004fbdcc0 0xc004fbdcc1}] []  [{kube-controller-manager Update v1 2020-11-29 02:27:40 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 97 102 54 99 48 51 99 45 53 98 97 97 45 52 101 48 54 45 57 52 101 100 45 52 101 52 102 51 102 97 97 56 48 100 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fqmbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fqmbh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fqmbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-ea4104c57e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 02:27:41.457: INFO: Pod "webserver-deployment-84855cf797-rtvrx" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-rtvrx webserver-deployment-84855cf797- deployment-5687 /api/v1/namespaces/deployment-5687/pods/webserver-deployment-84855cf797-rtvrx 829bf676-11ed-41e7-bdbe-29dee1a4d7a1 40672 0 2020-11-29 02:27:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 baf6c03c-5baa-4e06-94ed-4e4f3faa80d4 0xc004fbdde0 0xc004fbdde1}] []  [{kube-controller-manager Update v1 2020-11-29 02:27:40 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 97 102 54 99 48 51 99 45 53 98 97 97 45 52 101 48 54 45 57 52 101 100 45 52 101 52 102 51 102 97 97 56 48 100 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fqmbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fqmbh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fqmbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-55fe8f9f1d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 02:27:41.457: INFO: Pod "webserver-deployment-84855cf797-scfvh" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-scfvh webserver-deployment-84855cf797- deployment-5687 /api/v1/namespaces/deployment-5687/pods/webserver-deployment-84855cf797-scfvh 634f267e-ca05-42d4-a364-8b1c642f05e1 40498 0 2020-11-29 02:27:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:192.168.191.144/32 cni.projectcalico.org/podIPs:192.168.191.144/32] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 baf6c03c-5baa-4e06-94ed-4e4f3faa80d4 0xc004fbdf00 0xc004fbdf01}] []  [{kube-controller-manager Update v1 2020-11-29 02:27:34 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 97 102 54 99 48 51 99 45 53 98 97 97 45 52 101 48 54 45 57 52 101 100 45 52 101 52 102 51 102 97 97 56 48 100 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-11-29 02:27:36 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-11-29 02:27:37 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 57 50 46 49 54 56 46 49 57 49 46 49 52 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fqmbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fqmbh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fqmbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-bc6233c429,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.165,PodIP:192.168.191.144,StartTime:2020-11-29 02:27:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-29 02:27:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://223ee592ae9c3a30d2b9174cd11621233cb2974e4bdd965482c56362be3399ab,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.191.144,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 02:27:41.458: INFO: Pod "webserver-deployment-84855cf797-v24fh" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-v24fh webserver-deployment-84855cf797- deployment-5687 /api/v1/namespaces/deployment-5687/pods/webserver-deployment-84855cf797-v24fh f43eb721-5c5f-44d3-962f-b03b2ff7e604 40682 0 2020-11-29 02:27:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 baf6c03c-5baa-4e06-94ed-4e4f3faa80d4 0xc001c180b7 0xc001c180b8}] []  [{kube-controller-manager Update v1 2020-11-29 02:27:40 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 97 102 54 99 48 51 99 45 53 98 97 97 45 52 101 48 54 45 57 52 101 100 45 52 101 52 102 51 102 97 97 56 48 100 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-11-29 02:27:40 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fqmbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fqmbh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fqmbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-ea4104c57e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.164,PodIP:,StartTime:2020-11-29 02:27:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 02:27:41.458: INFO: Pod "webserver-deployment-84855cf797-v567b" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-v567b webserver-deployment-84855cf797- deployment-5687 /api/v1/namespaces/deployment-5687/pods/webserver-deployment-84855cf797-v567b 87e7536c-b765-4c8e-9f57-41f0b0fc0b9a 40662 0 2020-11-29 02:27:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 baf6c03c-5baa-4e06-94ed-4e4f3faa80d4 0xc001c18237 0xc001c18238}] []  [{kube-controller-manager Update v1 2020-11-29 02:27:40 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 97 102 54 99 48 51 99 45 53 98 97 97 45 52 101 48 54 45 57 52 101 100 45 52 101 52 102 51 102 97 97 56 48 100 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fqmbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fqmbh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fqmbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-55fe8f9f1d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 02:27:41.458: INFO: Pod "webserver-deployment-84855cf797-xtc8x" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-xtc8x webserver-deployment-84855cf797- deployment-5687 /api/v1/namespaces/deployment-5687/pods/webserver-deployment-84855cf797-xtc8x 0d001c7a-6b86-4424-b715-94b955b88ca8 40504 0 2020-11-29 02:27:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:192.168.93.229/32 cni.projectcalico.org/podIPs:192.168.93.229/32] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 baf6c03c-5baa-4e06-94ed-4e4f3faa80d4 0xc001c18350 0xc001c18351}] []  [{kube-controller-manager Update v1 2020-11-29 02:27:34 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 97 102 54 99 48 51 99 45 53 98 97 97 45 52 101 48 54 45 57 52 101 100 45 52 101 52 102 51 102 97 97 56 48 100 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-11-29 02:27:36 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-11-29 02:27:37 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 57 50 46 49 54 56 46 57 51 46 50 50 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fqmbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fqmbh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fqmbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-55fe8f9f1d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.166,PodIP:192.168.93.229,StartTime:2020-11-29 02:27:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-29 02:27:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://76244b9ad44e526c6ea1a35bb9f2993a7fb698d344f27ede040ff5c44586a0ea,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.93.229,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 02:27:41.459: INFO: Pod "webserver-deployment-84855cf797-zbr87" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-zbr87 webserver-deployment-84855cf797- deployment-5687 /api/v1/namespaces/deployment-5687/pods/webserver-deployment-84855cf797-zbr87 3c8b9660-742d-4029-923a-bc8381a45d78 40495 0 2020-11-29 02:27:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:192.168.191.143/32 cni.projectcalico.org/podIPs:192.168.191.143/32] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 baf6c03c-5baa-4e06-94ed-4e4f3faa80d4 0xc001c18517 0xc001c18518}] []  [{kube-controller-manager Update v1 2020-11-29 02:27:34 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 97 102 54 99 48 51 99 45 53 98 97 97 45 52 101 48 54 45 57 52 101 100 45 52 101 52 102 51 102 97 97 56 48 100 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-11-29 02:27:36 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-11-29 02:27:37 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 57 50 46 49 54 56 46 49 57 49 46 49 52 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fqmbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fqmbh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fqmbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-bc6233c429,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.165,PodIP:192.168.191.143,StartTime:2020-11-29 02:27:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-29 02:27:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://65a1bbef8eeeb90dea81c6ce88e0b2f941b7708fa910a0508fd3da66d3368226,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.191.143,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 02:27:41.459: INFO: Pod "webserver-deployment-84855cf797-zjlwl" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-zjlwl webserver-deployment-84855cf797- deployment-5687 /api/v1/namespaces/deployment-5687/pods/webserver-deployment-84855cf797-zjlwl dc149ff8-a0ef-4612-8132-18e472bfaa4f 40472 0 2020-11-29 02:27:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:192.168.36.245/32 cni.projectcalico.org/podIPs:192.168.36.245/32] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 baf6c03c-5baa-4e06-94ed-4e4f3faa80d4 0xc001c186c7 0xc001c186c8}] []  [{kube-controller-manager Update v1 2020-11-29 02:27:34 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 97 102 54 99 48 51 99 45 53 98 97 97 45 52 101 48 54 45 57 52 101 100 45 52 101 52 102 51 102 97 97 56 48 100 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-11-29 02:27:35 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-11-29 02:27:36 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 57 50 46 49 54 56 46 51 54 46 50 52 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fqmbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fqmbh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fqmbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-ea4104c57e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.164,PodIP:192.168.36.245,StartTime:2020-11-29 02:27:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-29 02:27:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://635bfa6bcc2b24fe61769318de3b6d8fc9ce64233055a13c4d42d159b82db355,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.36.245,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 29 02:27:41.459: INFO: Pod "webserver-deployment-84855cf797-zt2c7" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-zt2c7 webserver-deployment-84855cf797- deployment-5687 /api/v1/namespaces/deployment-5687/pods/webserver-deployment-84855cf797-zt2c7 d259828a-c847-4b64-9755-316d31ac3a3c 40653 0 2020-11-29 02:27:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 baf6c03c-5baa-4e06-94ed-4e4f3faa80d4 0xc001c18877 0xc001c18878}] []  [{kube-controller-manager Update v1 2020-11-29 02:27:40 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 97 102 54 99 48 51 99 45 53 98 97 97 45 52 101 48 54 45 57 52 101 100 45 52 101 52 102 51 102 97 97 56 48 100 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fqmbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fqmbh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fqmbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-ea4104c57e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:27:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:27:41.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5687" for this suite.

• [SLOW TEST:6.369 seconds]
[sig-apps] Deployment
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":277,"completed":184,"skipped":2988,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:27:41.491: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:27:52.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2340" for this suite.

• [SLOW TEST:11.154 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":277,"completed":185,"skipped":3039,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:27:52.645: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 29 02:27:53.067: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 29 02:27:55.080: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742213672, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742213672, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742213672, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742213672, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 02:27:58.099: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:27:58.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6371" for this suite.
STEP: Destroying namespace "webhook-6371-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.589 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":277,"completed":186,"skipped":3039,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:27:58.236: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Nov 29 02:27:58.861: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:27:58.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W1129 02:27:58.861169      22 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-9456" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":277,"completed":187,"skipped":3061,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:27:58.871: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 29 02:28:00.296: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 29 02:28:02.303: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742213679, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742213679, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742213679, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742213679, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 02:28:05.327: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:28:05.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9398" for this suite.
STEP: Destroying namespace "webhook-9398-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.889 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":277,"completed":188,"skipped":3062,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:28:05.770: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on node default medium
Nov 29 02:28:05.843: INFO: Waiting up to 5m0s for pod "pod-e43df41c-3b49-4382-9b56-8c7982f84045" in namespace "emptydir-692" to be "Succeeded or Failed"
Nov 29 02:28:05.854: INFO: Pod "pod-e43df41c-3b49-4382-9b56-8c7982f84045": Phase="Pending", Reason="", readiness=false. Elapsed: 9.968033ms
Nov 29 02:28:07.857: INFO: Pod "pod-e43df41c-3b49-4382-9b56-8c7982f84045": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012985181s
Nov 29 02:28:09.860: INFO: Pod "pod-e43df41c-3b49-4382-9b56-8c7982f84045": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016418449s
STEP: Saw pod success
Nov 29 02:28:09.860: INFO: Pod "pod-e43df41c-3b49-4382-9b56-8c7982f84045" satisfied condition "Succeeded or Failed"
Nov 29 02:28:09.863: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod pod-e43df41c-3b49-4382-9b56-8c7982f84045 container test-container: <nil>
STEP: delete the pod
Nov 29 02:28:09.894: INFO: Waiting for pod pod-e43df41c-3b49-4382-9b56-8c7982f84045 to disappear
Nov 29 02:28:09.902: INFO: Pod pod-e43df41c-3b49-4382-9b56-8c7982f84045 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:28:09.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-692" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":189,"skipped":3088,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:28:09.917: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Nov 29 02:28:14.496: INFO: Successfully updated pod "annotationupdate0faff67e-6a63-4988-984e-3f9ab3152c02"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:28:16.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6851" for this suite.

• [SLOW TEST:6.618 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":277,"completed":190,"skipped":3102,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:28:16.535: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Nov 29 02:28:16.573: INFO: Waiting up to 5m0s for pod "downward-api-f83bd45e-cfff-45f9-86ed-f21e0bea6570" in namespace "downward-api-5437" to be "Succeeded or Failed"
Nov 29 02:28:16.577: INFO: Pod "downward-api-f83bd45e-cfff-45f9-86ed-f21e0bea6570": Phase="Pending", Reason="", readiness=false. Elapsed: 3.840682ms
Nov 29 02:28:18.581: INFO: Pod "downward-api-f83bd45e-cfff-45f9-86ed-f21e0bea6570": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007183154s
Nov 29 02:28:20.585: INFO: Pod "downward-api-f83bd45e-cfff-45f9-86ed-f21e0bea6570": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011410025s
STEP: Saw pod success
Nov 29 02:28:20.585: INFO: Pod "downward-api-f83bd45e-cfff-45f9-86ed-f21e0bea6570" satisfied condition "Succeeded or Failed"
Nov 29 02:28:20.588: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod downward-api-f83bd45e-cfff-45f9-86ed-f21e0bea6570 container dapi-container: <nil>
STEP: delete the pod
Nov 29 02:28:20.612: INFO: Waiting for pod downward-api-f83bd45e-cfff-45f9-86ed-f21e0bea6570 to disappear
Nov 29 02:28:20.623: INFO: Pod downward-api-f83bd45e-cfff-45f9-86ed-f21e0bea6570 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:28:20.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5437" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":277,"completed":191,"skipped":3124,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:28:20.642: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-configmap-9rrv
STEP: Creating a pod to test atomic-volume-subpath
Nov 29 02:28:20.707: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-9rrv" in namespace "subpath-3037" to be "Succeeded or Failed"
Nov 29 02:28:20.712: INFO: Pod "pod-subpath-test-configmap-9rrv": Phase="Pending", Reason="", readiness=false. Elapsed: 5.159098ms
Nov 29 02:28:22.716: INFO: Pod "pod-subpath-test-configmap-9rrv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00937925s
Nov 29 02:28:24.721: INFO: Pod "pod-subpath-test-configmap-9rrv": Phase="Running", Reason="", readiness=true. Elapsed: 4.014245483s
Nov 29 02:28:26.725: INFO: Pod "pod-subpath-test-configmap-9rrv": Phase="Running", Reason="", readiness=true. Elapsed: 6.018184899s
Nov 29 02:28:28.730: INFO: Pod "pod-subpath-test-configmap-9rrv": Phase="Running", Reason="", readiness=true. Elapsed: 8.023099826s
Nov 29 02:28:30.738: INFO: Pod "pod-subpath-test-configmap-9rrv": Phase="Running", Reason="", readiness=true. Elapsed: 10.031552265s
Nov 29 02:28:32.742: INFO: Pod "pod-subpath-test-configmap-9rrv": Phase="Running", Reason="", readiness=true. Elapsed: 12.035000988s
Nov 29 02:28:34.745: INFO: Pod "pod-subpath-test-configmap-9rrv": Phase="Running", Reason="", readiness=true. Elapsed: 14.038161948s
Nov 29 02:28:36.749: INFO: Pod "pod-subpath-test-configmap-9rrv": Phase="Running", Reason="", readiness=true. Elapsed: 16.042815458s
Nov 29 02:28:38.756: INFO: Pod "pod-subpath-test-configmap-9rrv": Phase="Running", Reason="", readiness=true. Elapsed: 18.049209643s
Nov 29 02:28:40.760: INFO: Pod "pod-subpath-test-configmap-9rrv": Phase="Running", Reason="", readiness=true. Elapsed: 20.053348021s
Nov 29 02:28:42.764: INFO: Pod "pod-subpath-test-configmap-9rrv": Phase="Running", Reason="", readiness=true. Elapsed: 22.057020145s
Nov 29 02:28:44.768: INFO: Pod "pod-subpath-test-configmap-9rrv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.060982887s
STEP: Saw pod success
Nov 29 02:28:44.768: INFO: Pod "pod-subpath-test-configmap-9rrv" satisfied condition "Succeeded or Failed"
Nov 29 02:28:44.771: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod pod-subpath-test-configmap-9rrv container test-container-subpath-configmap-9rrv: <nil>
STEP: delete the pod
Nov 29 02:28:44.807: INFO: Waiting for pod pod-subpath-test-configmap-9rrv to disappear
Nov 29 02:28:44.810: INFO: Pod pod-subpath-test-configmap-9rrv no longer exists
STEP: Deleting pod pod-subpath-test-configmap-9rrv
Nov 29 02:28:44.810: INFO: Deleting pod "pod-subpath-test-configmap-9rrv" in namespace "subpath-3037"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:28:44.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3037" for this suite.

• [SLOW TEST:24.180 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":277,"completed":192,"skipped":3155,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:28:44.822: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 02:28:44.850: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Nov 29 02:28:44.868: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 29 02:28:49.871: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 29 02:28:49.871: INFO: Creating deployment "test-rolling-update-deployment"
Nov 29 02:28:49.876: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Nov 29 02:28:49.887: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Nov 29 02:28:51.892: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Nov 29 02:28:51.894: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742213729, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742213729, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742213729, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742213729, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-59d5cb45c7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 29 02:28:53.899: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Nov 29 02:28:53.908: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4455 /apis/apps/v1/namespaces/deployment-4455/deployments/test-rolling-update-deployment bd637cb3-0a24-4685-952e-7c77d392b3db 41904 1 2020-11-29 02:28:49 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2020-11-29 02:28:49 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-11-29 02:28:51 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002caff08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-11-29 02:28:49 +0000 UTC,LastTransitionTime:2020-11-29 02:28:49 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-59d5cb45c7" has successfully progressed.,LastUpdateTime:2020-11-29 02:28:51 +0000 UTC,LastTransitionTime:2020-11-29 02:28:49 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 29 02:28:53.912: INFO: New ReplicaSet "test-rolling-update-deployment-59d5cb45c7" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-59d5cb45c7  deployment-4455 /apis/apps/v1/namespaces/deployment-4455/replicasets/test-rolling-update-deployment-59d5cb45c7 3422cc41-8dfd-4c61-b9c1-549eaa6ca6be 41893 1 2020-11-29 02:28:49 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:59d5cb45c7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment bd637cb3-0a24-4685-952e-7c77d392b3db 0xc0038ce497 0xc0038ce498}] []  [{kube-controller-manager Update apps/v1 2020-11-29 02:28:51 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 100 54 51 55 99 98 51 45 48 97 50 52 45 52 54 56 53 45 57 53 50 101 45 55 99 55 55 100 51 57 50 98 51 100 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 59d5cb45c7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:59d5cb45c7] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0038ce528 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 29 02:28:53.912: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Nov 29 02:28:53.912: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4455 /apis/apps/v1/namespaces/deployment-4455/replicasets/test-rolling-update-controller 0c1721c6-62fc-46c4-9c9f-97780d218b19 41902 2 2020-11-29 02:28:44 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment bd637cb3-0a24-4685-952e-7c77d392b3db 0xc0038ce387 0xc0038ce388}] []  [{e2e.test Update apps/v1 2020-11-29 02:28:44 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-11-29 02:28:51 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 100 54 51 55 99 98 51 45 48 97 50 52 45 52 54 56 53 45 57 53 50 101 45 55 99 55 55 100 51 57 50 98 51 100 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0038ce428 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 29 02:28:53.916: INFO: Pod "test-rolling-update-deployment-59d5cb45c7-brjwd" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-59d5cb45c7-brjwd test-rolling-update-deployment-59d5cb45c7- deployment-4455 /api/v1/namespaces/deployment-4455/pods/test-rolling-update-deployment-59d5cb45c7-brjwd 835dc502-139e-4210-82e2-39a728ca5967 41892 0 2020-11-29 02:28:49 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:59d5cb45c7] map[cni.projectcalico.org/podIP:192.168.191.153/32 cni.projectcalico.org/podIPs:192.168.191.153/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-59d5cb45c7 3422cc41-8dfd-4c61-b9c1-549eaa6ca6be 0xc003435507 0xc003435508}] []  [{kube-controller-manager Update v1 2020-11-29 02:28:49 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 51 52 50 50 99 99 52 49 45 56 100 102 100 45 52 99 54 49 45 98 57 99 49 45 53 52 57 101 97 97 54 99 97 54 98 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-11-29 02:28:50 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-11-29 02:28:51 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 57 50 46 49 54 56 46 49 57 49 46 49 53 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mj4c4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mj4c4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mj4c4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-bc6233c429,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:28:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:28:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:28:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:28:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.165,PodIP:192.168.191.153,StartTime:2020-11-29 02:28:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-29 02:28:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,ImageID:docker-pullable://us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c,ContainerID:docker://baa41a84bb26a7bac5183cf680f6830a2b3ad356c50ed70916aefd999fa80508,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.191.153,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:28:53.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4455" for this suite.

• [SLOW TEST:9.108 seconds]
[sig-apps] Deployment
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":277,"completed":193,"skipped":3195,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:28:53.931: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name cm-test-opt-del-397589a4-1532-4db6-92e6-ed4bd0752948
STEP: Creating configMap with name cm-test-opt-upd-f2620037-c318-4a55-b29c-7210bd8bdb67
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-397589a4-1532-4db6-92e6-ed4bd0752948
STEP: Updating configmap cm-test-opt-upd-f2620037-c318-4a55-b29c-7210bd8bdb67
STEP: Creating configMap with name cm-test-opt-create-c1208bb5-660e-4147-8bd9-2c23a5140c2e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:29:00.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2444" for this suite.

• [SLOW TEST:6.184 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":194,"skipped":3234,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:29:00.115: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Nov 29 02:29:00.162: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8f9e465a-961f-4cce-9d4d-74745b28da45" in namespace "projected-9666" to be "Succeeded or Failed"
Nov 29 02:29:00.168: INFO: Pod "downwardapi-volume-8f9e465a-961f-4cce-9d4d-74745b28da45": Phase="Pending", Reason="", readiness=false. Elapsed: 5.939855ms
Nov 29 02:29:02.172: INFO: Pod "downwardapi-volume-8f9e465a-961f-4cce-9d4d-74745b28da45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009746069s
Nov 29 02:29:04.176: INFO: Pod "downwardapi-volume-8f9e465a-961f-4cce-9d4d-74745b28da45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013887398s
STEP: Saw pod success
Nov 29 02:29:04.176: INFO: Pod "downwardapi-volume-8f9e465a-961f-4cce-9d4d-74745b28da45" satisfied condition "Succeeded or Failed"
Nov 29 02:29:04.179: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod downwardapi-volume-8f9e465a-961f-4cce-9d4d-74745b28da45 container client-container: <nil>
STEP: delete the pod
Nov 29 02:29:04.202: INFO: Waiting for pod downwardapi-volume-8f9e465a-961f-4cce-9d4d-74745b28da45 to disappear
Nov 29 02:29:04.212: INFO: Pod downwardapi-volume-8f9e465a-961f-4cce-9d4d-74745b28da45 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:29:04.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9666" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":195,"skipped":3244,"failed":0}
SSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:29:04.222: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Nov 29 02:29:04.262: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 29 02:30:04.300: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 02:30:04.303: INFO: Starting informer...
STEP: Starting pods...
Nov 29 02:30:04.529: INFO: Pod1 is running on alex-cp1516-v3-vsp2-node-group-ea4104c57e. Tainting Node
Nov 29 02:30:08.746: INFO: Pod2 is running on alex-cp1516-v3-vsp2-node-group-ea4104c57e. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Nov 29 02:30:16.412: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Nov 29 02:30:39.856: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:30:39.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-8743" for this suite.

• [SLOW TEST:95.673 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":277,"completed":196,"skipped":3248,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:30:39.899: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-map-30dbc5b5-6b8d-4288-91bd-0fa0870761ca
STEP: Creating a pod to test consume configMaps
Nov 29 02:30:40.022: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-77fc88cd-86b9-4ef5-8a4f-6933f7c7b407" in namespace "projected-8473" to be "Succeeded or Failed"
Nov 29 02:30:40.032: INFO: Pod "pod-projected-configmaps-77fc88cd-86b9-4ef5-8a4f-6933f7c7b407": Phase="Pending", Reason="", readiness=false. Elapsed: 9.22291ms
Nov 29 02:30:42.036: INFO: Pod "pod-projected-configmaps-77fc88cd-86b9-4ef5-8a4f-6933f7c7b407": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013119548s
Nov 29 02:30:44.039: INFO: Pod "pod-projected-configmaps-77fc88cd-86b9-4ef5-8a4f-6933f7c7b407": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01625651s
STEP: Saw pod success
Nov 29 02:30:44.039: INFO: Pod "pod-projected-configmaps-77fc88cd-86b9-4ef5-8a4f-6933f7c7b407" satisfied condition "Succeeded or Failed"
Nov 29 02:30:44.041: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod pod-projected-configmaps-77fc88cd-86b9-4ef5-8a4f-6933f7c7b407 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 02:30:44.068: INFO: Waiting for pod pod-projected-configmaps-77fc88cd-86b9-4ef5-8a4f-6933f7c7b407 to disappear
Nov 29 02:30:44.075: INFO: Pod pod-projected-configmaps-77fc88cd-86b9-4ef5-8a4f-6933f7c7b407 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:30:44.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8473" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":197,"skipped":3262,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:30:44.084: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Nov 29 02:30:44.130: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7853 /api/v1/namespaces/watch-7853/configmaps/e2e-watch-test-watch-closed 40163bbf-433c-4fe1-bcec-f825e7a7cd4c 42843 0 2020-11-29 02:30:43 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-11-29 02:30:43 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 29 02:30:44.130: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7853 /api/v1/namespaces/watch-7853/configmaps/e2e-watch-test-watch-closed 40163bbf-433c-4fe1-bcec-f825e7a7cd4c 42844 0 2020-11-29 02:30:43 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-11-29 02:30:43 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Nov 29 02:30:44.148: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7853 /api/v1/namespaces/watch-7853/configmaps/e2e-watch-test-watch-closed 40163bbf-433c-4fe1-bcec-f825e7a7cd4c 42845 0 2020-11-29 02:30:43 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-11-29 02:30:43 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 29 02:30:44.148: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7853 /api/v1/namespaces/watch-7853/configmaps/e2e-watch-test-watch-closed 40163bbf-433c-4fe1-bcec-f825e7a7cd4c 42846 0 2020-11-29 02:30:43 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-11-29 02:30:43 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:30:44.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7853" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":277,"completed":198,"skipped":3273,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:30:44.156: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-map-f2e881d9-b7ec-454e-8d80-51ba51c609b4
STEP: Creating a pod to test consume secrets
Nov 29 02:30:44.196: INFO: Waiting up to 5m0s for pod "pod-secrets-a6e6a003-dbc4-4e60-95c5-7088eb04f089" in namespace "secrets-7989" to be "Succeeded or Failed"
Nov 29 02:30:44.202: INFO: Pod "pod-secrets-a6e6a003-dbc4-4e60-95c5-7088eb04f089": Phase="Pending", Reason="", readiness=false. Elapsed: 6.203809ms
Nov 29 02:30:46.205: INFO: Pod "pod-secrets-a6e6a003-dbc4-4e60-95c5-7088eb04f089": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009470865s
Nov 29 02:30:48.209: INFO: Pod "pod-secrets-a6e6a003-dbc4-4e60-95c5-7088eb04f089": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0129646s
STEP: Saw pod success
Nov 29 02:30:48.209: INFO: Pod "pod-secrets-a6e6a003-dbc4-4e60-95c5-7088eb04f089" satisfied condition "Succeeded or Failed"
Nov 29 02:30:48.211: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod pod-secrets-a6e6a003-dbc4-4e60-95c5-7088eb04f089 container secret-volume-test: <nil>
STEP: delete the pod
Nov 29 02:30:48.234: INFO: Waiting for pod pod-secrets-a6e6a003-dbc4-4e60-95c5-7088eb04f089 to disappear
Nov 29 02:30:48.242: INFO: Pod pod-secrets-a6e6a003-dbc4-4e60-95c5-7088eb04f089 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:30:48.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7989" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":277,"completed":199,"skipped":3304,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:30:48.265: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on tmpfs
Nov 29 02:30:48.307: INFO: Waiting up to 5m0s for pod "pod-833e4c57-931d-41bf-834f-c0729988dfef" in namespace "emptydir-8577" to be "Succeeded or Failed"
Nov 29 02:30:48.310: INFO: Pod "pod-833e4c57-931d-41bf-834f-c0729988dfef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.44349ms
Nov 29 02:30:50.313: INFO: Pod "pod-833e4c57-931d-41bf-834f-c0729988dfef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00554524s
Nov 29 02:30:52.316: INFO: Pod "pod-833e4c57-931d-41bf-834f-c0729988dfef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008694473s
STEP: Saw pod success
Nov 29 02:30:52.316: INFO: Pod "pod-833e4c57-931d-41bf-834f-c0729988dfef" satisfied condition "Succeeded or Failed"
Nov 29 02:30:52.318: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod pod-833e4c57-931d-41bf-834f-c0729988dfef container test-container: <nil>
STEP: delete the pod
Nov 29 02:30:52.337: INFO: Waiting for pod pod-833e4c57-931d-41bf-834f-c0729988dfef to disappear
Nov 29 02:30:52.339: INFO: Pod pod-833e4c57-931d-41bf-834f-c0729988dfef no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:30:52.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8577" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":200,"skipped":3333,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:30:52.349: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-c4f6f6df-77ad-4fdd-9b69-ffecb284efb1
STEP: Creating a pod to test consume secrets
Nov 29 02:30:52.389: INFO: Waiting up to 5m0s for pod "pod-secrets-486684cc-900b-4d29-befa-5aa830d3dfc2" in namespace "secrets-8276" to be "Succeeded or Failed"
Nov 29 02:30:52.399: INFO: Pod "pod-secrets-486684cc-900b-4d29-befa-5aa830d3dfc2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.280771ms
Nov 29 02:30:54.402: INFO: Pod "pod-secrets-486684cc-900b-4d29-befa-5aa830d3dfc2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013492633s
Nov 29 02:30:56.405: INFO: Pod "pod-secrets-486684cc-900b-4d29-befa-5aa830d3dfc2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016524213s
STEP: Saw pod success
Nov 29 02:30:56.405: INFO: Pod "pod-secrets-486684cc-900b-4d29-befa-5aa830d3dfc2" satisfied condition "Succeeded or Failed"
Nov 29 02:30:56.408: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod pod-secrets-486684cc-900b-4d29-befa-5aa830d3dfc2 container secret-volume-test: <nil>
STEP: delete the pod
Nov 29 02:30:56.426: INFO: Waiting for pod pod-secrets-486684cc-900b-4d29-befa-5aa830d3dfc2 to disappear
Nov 29 02:30:56.437: INFO: Pod pod-secrets-486684cc-900b-4d29-befa-5aa830d3dfc2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:30:56.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8276" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":277,"completed":201,"skipped":3357,"failed":0}
S
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:30:56.454: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:31:01.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-263" for this suite.

• [SLOW TEST:5.077 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":277,"completed":202,"skipped":3358,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:31:01.534: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Nov 29 02:31:01.570: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 29 02:31:01.582: INFO: Waiting for terminating namespaces to be deleted...
Nov 29 02:31:01.584: INFO: 
Logging pods the kubelet thinks is on node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d before test
Nov 29 02:31:01.604: INFO: metallb-speaker-5sjng from ccp started at 2020-11-29 01:13:11 +0000 UTC (1 container statuses recorded)
Nov 29 02:31:01.604: INFO: 	Container metallb-speaker ready: true, restart count 0
Nov 29 02:31:01.604: INFO: sonobuoy from sonobuoy started at 2020-11-29 01:14:36 +0000 UTC (1 container statuses recorded)
Nov 29 02:31:01.604: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 29 02:31:01.604: INFO: calico-node-gs8lr from kube-system started at 2020-11-29 01:13:01 +0000 UTC (1 container statuses recorded)
Nov 29 02:31:01.604: INFO: 	Container calico-node ready: true, restart count 0
Nov 29 02:31:01.604: INFO: sonobuoy-systemd-logs-daemon-set-a18500f604534dc2-g2xkd from sonobuoy started at 2020-11-29 01:15:14 +0000 UTC (2 container statuses recorded)
Nov 29 02:31:01.604: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Nov 29 02:31:01.604: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 29 02:31:01.604: INFO: kube-proxy-zjcgv from kube-system started at 2020-11-29 01:13:00 +0000 UTC (1 container statuses recorded)
Nov 29 02:31:01.604: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 29 02:31:01.604: INFO: nvidia-device-plugin-daemonset-f79b4 from kube-system started at 2020-11-29 01:13:11 +0000 UTC (1 container statuses recorded)
Nov 29 02:31:01.604: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Nov 29 02:31:01.604: INFO: ingress-nginx-controller-zd8lx from ccp started at 2020-11-29 01:13:11 +0000 UTC (1 container statuses recorded)
Nov 29 02:31:01.604: INFO: 	Container controller ready: true, restart count 0
Nov 29 02:31:01.604: INFO: sonobuoy-e2e-job-df1d2fd3866f4821 from sonobuoy started at 2020-11-29 01:15:14 +0000 UTC (2 container statuses recorded)
Nov 29 02:31:01.604: INFO: 	Container e2e ready: true, restart count 0
Nov 29 02:31:01.604: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 29 02:31:01.604: INFO: 
Logging pods the kubelet thinks is on node alex-cp1516-v3-vsp2-node-group-bc6233c429 before test
Nov 29 02:31:01.627: INFO: calico-node-xhkv2 from kube-system started at 2020-11-29 01:12:58 +0000 UTC (1 container statuses recorded)
Nov 29 02:31:01.627: INFO: 	Container calico-node ready: true, restart count 0
Nov 29 02:31:01.627: INFO: metallb-controller-dd895cddd-knr69 from ccp started at 2020-11-29 02:14:14 +0000 UTC (1 container statuses recorded)
Nov 29 02:31:01.627: INFO: 	Container metallb-controller ready: true, restart count 0
Nov 29 02:31:01.627: INFO: metallb-speaker-8qx7b from ccp started at 2020-11-29 02:14:14 +0000 UTC (1 container statuses recorded)
Nov 29 02:31:01.627: INFO: 	Container metallb-speaker ready: true, restart count 0
Nov 29 02:31:01.627: INFO: sonobuoy-systemd-logs-daemon-set-a18500f604534dc2-vsjhg from sonobuoy started at 2020-11-29 01:15:14 +0000 UTC (2 container statuses recorded)
Nov 29 02:31:01.627: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Nov 29 02:31:01.627: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 29 02:31:01.627: INFO: ingress-nginx-controller-spj9n from ccp started at 2020-11-29 02:14:37 +0000 UTC (1 container statuses recorded)
Nov 29 02:31:01.627: INFO: 	Container controller ready: true, restart count 0
Nov 29 02:31:01.627: INFO: ingress-nginx-defaultbackend-6cd8f668c8-79p2d from ccp started at 2020-11-29 02:30:09 +0000 UTC (1 container statuses recorded)
Nov 29 02:31:01.627: INFO: 	Container ingress-nginx-default-backend ready: true, restart count 0
Nov 29 02:31:01.627: INFO: kube-proxy-wcdhr from kube-system started at 2020-11-29 01:12:58 +0000 UTC (1 container statuses recorded)
Nov 29 02:31:01.627: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 29 02:31:01.627: INFO: nvidia-device-plugin-daemonset-f8wbp from kube-system started at 2020-11-29 02:14:27 +0000 UTC (1 container statuses recorded)
Nov 29 02:31:01.627: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Nov 29 02:31:01.627: INFO: 
Logging pods the kubelet thinks is on node alex-cp1516-v3-vsp2-node-group-ea4104c57e before test
Nov 29 02:31:01.634: INFO: sonobuoy-systemd-logs-daemon-set-a18500f604534dc2-449j5 from sonobuoy started at 2020-11-29 01:15:13 +0000 UTC (2 container statuses recorded)
Nov 29 02:31:01.634: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Nov 29 02:31:01.634: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 29 02:31:01.634: INFO: ingress-nginx-controller-wbm4g from ccp started at 2020-11-29 02:30:39 +0000 UTC (1 container statuses recorded)
Nov 29 02:31:01.634: INFO: 	Container controller ready: true, restart count 0
Nov 29 02:31:01.634: INFO: pod-adoption from replication-controller-263 started at 2020-11-29 02:30:56 +0000 UTC (1 container statuses recorded)
Nov 29 02:31:01.634: INFO: 	Container pod-adoption ready: true, restart count 0
Nov 29 02:31:01.634: INFO: nvidia-device-plugin-daemonset-6ks48 from kube-system started at 2020-11-29 02:30:39 +0000 UTC (1 container statuses recorded)
Nov 29 02:31:01.634: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Nov 29 02:31:01.634: INFO: calico-node-c7rxt from kube-system started at 2020-11-29 01:13:00 +0000 UTC (1 container statuses recorded)
Nov 29 02:31:01.634: INFO: 	Container calico-node ready: true, restart count 0
Nov 29 02:31:01.634: INFO: kube-proxy-xsm2v from kube-system started at 2020-11-29 01:13:00 +0000 UTC (1 container statuses recorded)
Nov 29 02:31:01.634: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 29 02:31:01.634: INFO: metallb-speaker-dclbw from ccp started at 2020-11-29 02:30:39 +0000 UTC (1 container statuses recorded)
Nov 29 02:31:01.634: INFO: 	Container metallb-speaker ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-bd7dce0c-253b-4bff-8ca9-9ae1a4fd8182 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-bd7dce0c-253b-4bff-8ca9-9ae1a4fd8182 off the node alex-cp1516-v3-vsp2-node-group-ea4104c57e
STEP: verifying the node doesn't have the label kubernetes.io/e2e-bd7dce0c-253b-4bff-8ca9-9ae1a4fd8182
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:31:17.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1205" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

• [SLOW TEST:16.217 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":277,"completed":203,"skipped":3390,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:31:17.751: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4879 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4879;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4879 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4879;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4879.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4879.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4879.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4879.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4879.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4879.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4879.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4879.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4879.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4879.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4879.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4879.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4879.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 2.138.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.138.2_udp@PTR;check="$$(dig +tcp +noall +answer +search 2.138.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.138.2_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4879 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4879;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4879 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4879;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4879.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4879.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4879.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4879.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4879.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4879.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4879.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4879.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4879.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4879.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4879.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4879.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4879.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 2.138.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.138.2_udp@PTR;check="$$(dig +tcp +noall +answer +search 2.138.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.138.2_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 29 02:31:21.838: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:21.840: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:21.843: INFO: Unable to read wheezy_udp@dns-test-service.dns-4879 from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:21.845: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4879 from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:21.848: INFO: Unable to read wheezy_udp@dns-test-service.dns-4879.svc from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:21.850: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4879.svc from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:21.852: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4879.svc from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:21.870: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:21.873: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:21.875: INFO: Unable to read jessie_udp@dns-test-service.dns-4879 from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:21.877: INFO: Unable to read jessie_tcp@dns-test-service.dns-4879 from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:21.881: INFO: Unable to read jessie_udp@dns-test-service.dns-4879.svc from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:21.883: INFO: Unable to read jessie_tcp@dns-test-service.dns-4879.svc from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:21.888: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4879.svc from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:21.903: INFO: Lookups using dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4879 wheezy_tcp@dns-test-service.dns-4879 wheezy_udp@dns-test-service.dns-4879.svc wheezy_tcp@dns-test-service.dns-4879.svc wheezy_udp@_http._tcp.dns-test-service.dns-4879.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4879 jessie_tcp@dns-test-service.dns-4879 jessie_udp@dns-test-service.dns-4879.svc jessie_tcp@dns-test-service.dns-4879.svc jessie_tcp@_http._tcp.dns-test-service.dns-4879.svc]

Nov 29 02:31:26.908: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:26.911: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:26.915: INFO: Unable to read wheezy_udp@dns-test-service.dns-4879 from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:26.918: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4879 from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:26.921: INFO: Unable to read wheezy_udp@dns-test-service.dns-4879.svc from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:26.929: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4879.svc from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:26.968: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:26.972: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:26.976: INFO: Unable to read jessie_udp@dns-test-service.dns-4879 from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:26.979: INFO: Unable to read jessie_tcp@dns-test-service.dns-4879 from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:26.986: INFO: Unable to read jessie_udp@dns-test-service.dns-4879.svc from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:26.990: INFO: Unable to read jessie_tcp@dns-test-service.dns-4879.svc from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:27.029: INFO: Lookups using dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4879 wheezy_tcp@dns-test-service.dns-4879 wheezy_udp@dns-test-service.dns-4879.svc wheezy_tcp@dns-test-service.dns-4879.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4879 jessie_tcp@dns-test-service.dns-4879 jessie_udp@dns-test-service.dns-4879.svc jessie_tcp@dns-test-service.dns-4879.svc]

Nov 29 02:31:31.909: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:31.913: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:31.916: INFO: Unable to read wheezy_udp@dns-test-service.dns-4879 from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:31.920: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4879 from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:31.924: INFO: Unable to read wheezy_udp@dns-test-service.dns-4879.svc from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:31.927: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4879.svc from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:31.957: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:31.960: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:31.964: INFO: Unable to read jessie_udp@dns-test-service.dns-4879 from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:31.967: INFO: Unable to read jessie_tcp@dns-test-service.dns-4879 from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:31.969: INFO: Unable to read jessie_udp@dns-test-service.dns-4879.svc from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:31.971: INFO: Unable to read jessie_tcp@dns-test-service.dns-4879.svc from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:31.993: INFO: Lookups using dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4879 wheezy_tcp@dns-test-service.dns-4879 wheezy_udp@dns-test-service.dns-4879.svc wheezy_tcp@dns-test-service.dns-4879.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4879 jessie_tcp@dns-test-service.dns-4879 jessie_udp@dns-test-service.dns-4879.svc jessie_tcp@dns-test-service.dns-4879.svc]

Nov 29 02:31:36.908: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:36.913: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:36.918: INFO: Unable to read wheezy_udp@dns-test-service.dns-4879 from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:36.922: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4879 from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:36.936: INFO: Unable to read wheezy_udp@dns-test-service.dns-4879.svc from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:37.024: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4879.svc from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:37.086: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:37.090: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:37.099: INFO: Unable to read jessie_udp@dns-test-service.dns-4879 from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:37.116: INFO: Unable to read jessie_tcp@dns-test-service.dns-4879 from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:37.129: INFO: Unable to read jessie_udp@dns-test-service.dns-4879.svc from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:37.132: INFO: Unable to read jessie_tcp@dns-test-service.dns-4879.svc from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:37.158: INFO: Lookups using dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4879 wheezy_tcp@dns-test-service.dns-4879 wheezy_udp@dns-test-service.dns-4879.svc wheezy_tcp@dns-test-service.dns-4879.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4879 jessie_tcp@dns-test-service.dns-4879 jessie_udp@dns-test-service.dns-4879.svc jessie_tcp@dns-test-service.dns-4879.svc]

Nov 29 02:31:41.907: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:41.911: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:41.914: INFO: Unable to read wheezy_udp@dns-test-service.dns-4879 from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:41.917: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4879 from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:41.921: INFO: Unable to read wheezy_udp@dns-test-service.dns-4879.svc from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:41.924: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4879.svc from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:41.953: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:41.956: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:41.959: INFO: Unable to read jessie_udp@dns-test-service.dns-4879 from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:41.965: INFO: Unable to read jessie_tcp@dns-test-service.dns-4879 from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:41.968: INFO: Unable to read jessie_udp@dns-test-service.dns-4879.svc from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:41.971: INFO: Unable to read jessie_tcp@dns-test-service.dns-4879.svc from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:42.003: INFO: Lookups using dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4879 wheezy_tcp@dns-test-service.dns-4879 wheezy_udp@dns-test-service.dns-4879.svc wheezy_tcp@dns-test-service.dns-4879.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4879 jessie_tcp@dns-test-service.dns-4879 jessie_udp@dns-test-service.dns-4879.svc jessie_tcp@dns-test-service.dns-4879.svc]

Nov 29 02:31:46.910: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:46.916: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:46.924: INFO: Unable to read wheezy_udp@dns-test-service.dns-4879 from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:46.929: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4879 from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:46.934: INFO: Unable to read wheezy_udp@dns-test-service.dns-4879.svc from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:46.940: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4879.svc from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:46.993: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:46.998: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:47.002: INFO: Unable to read jessie_udp@dns-test-service.dns-4879 from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:47.006: INFO: Unable to read jessie_tcp@dns-test-service.dns-4879 from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:47.010: INFO: Unable to read jessie_udp@dns-test-service.dns-4879.svc from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:47.018: INFO: Unable to read jessie_tcp@dns-test-service.dns-4879.svc from pod dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474: the server could not find the requested resource (get pods dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474)
Nov 29 02:31:47.049: INFO: Lookups using dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4879 wheezy_tcp@dns-test-service.dns-4879 wheezy_udp@dns-test-service.dns-4879.svc wheezy_tcp@dns-test-service.dns-4879.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4879 jessie_tcp@dns-test-service.dns-4879 jessie_udp@dns-test-service.dns-4879.svc jessie_tcp@dns-test-service.dns-4879.svc]

Nov 29 02:31:52.067: INFO: DNS probes using dns-4879/dns-test-cf2991e4-f3da-41c1-89f8-6a9a56166474 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:31:52.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4879" for this suite.

• [SLOW TEST:34.437 seconds]
[sig-network] DNS
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":277,"completed":204,"skipped":3400,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:31:52.189: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 02:31:52.240: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Nov 29 02:31:57.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 --namespace=crd-publish-openapi-9033 create -f -'
Nov 29 02:31:59.736: INFO: stderr: ""
Nov 29 02:31:59.736: INFO: stdout: "e2e-test-crd-publish-openapi-5470-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Nov 29 02:31:59.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 --namespace=crd-publish-openapi-9033 delete e2e-test-crd-publish-openapi-5470-crds test-cr'
Nov 29 02:31:59.828: INFO: stderr: ""
Nov 29 02:31:59.829: INFO: stdout: "e2e-test-crd-publish-openapi-5470-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Nov 29 02:31:59.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 --namespace=crd-publish-openapi-9033 apply -f -'
Nov 29 02:32:00.115: INFO: stderr: ""
Nov 29 02:32:00.115: INFO: stdout: "e2e-test-crd-publish-openapi-5470-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Nov 29 02:32:00.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 --namespace=crd-publish-openapi-9033 delete e2e-test-crd-publish-openapi-5470-crds test-cr'
Nov 29 02:32:00.225: INFO: stderr: ""
Nov 29 02:32:00.225: INFO: stdout: "e2e-test-crd-publish-openapi-5470-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Nov 29 02:32:00.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 explain e2e-test-crd-publish-openapi-5470-crds'
Nov 29 02:32:00.483: INFO: stderr: ""
Nov 29 02:32:00.483: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5470-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:32:06.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9033" for this suite.

• [SLOW TEST:13.963 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":277,"completed":205,"skipped":3425,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:32:06.153: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-map-db3fd796-fb0d-4995-b51e-79ca8c05a33c
STEP: Creating a pod to test consume configMaps
Nov 29 02:32:06.217: INFO: Waiting up to 5m0s for pod "pod-configmaps-7863f450-bc14-401b-8015-93060debbaa6" in namespace "configmap-3128" to be "Succeeded or Failed"
Nov 29 02:32:06.220: INFO: Pod "pod-configmaps-7863f450-bc14-401b-8015-93060debbaa6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.154269ms
Nov 29 02:32:08.224: INFO: Pod "pod-configmaps-7863f450-bc14-401b-8015-93060debbaa6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007626966s
Nov 29 02:32:10.229: INFO: Pod "pod-configmaps-7863f450-bc14-401b-8015-93060debbaa6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01205632s
STEP: Saw pod success
Nov 29 02:32:10.229: INFO: Pod "pod-configmaps-7863f450-bc14-401b-8015-93060debbaa6" satisfied condition "Succeeded or Failed"
Nov 29 02:32:10.232: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod pod-configmaps-7863f450-bc14-401b-8015-93060debbaa6 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 02:32:10.263: INFO: Waiting for pod pod-configmaps-7863f450-bc14-401b-8015-93060debbaa6 to disappear
Nov 29 02:32:10.273: INFO: Pod pod-configmaps-7863f450-bc14-401b-8015-93060debbaa6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:32:10.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3128" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":277,"completed":206,"skipped":3438,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:32:10.287: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Update Demo
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:271
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a replication controller
Nov 29 02:32:10.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 create -f - --namespace=kubectl-826'
Nov 29 02:32:10.707: INFO: stderr: ""
Nov 29 02:32:10.707: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 29 02:32:10.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-826'
Nov 29 02:32:10.810: INFO: stderr: ""
Nov 29 02:32:10.810: INFO: stdout: "update-demo-nautilus-g4kdn update-demo-nautilus-jg4qv "
Nov 29 02:32:10.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get pods update-demo-nautilus-g4kdn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-826'
Nov 29 02:32:10.897: INFO: stderr: ""
Nov 29 02:32:10.897: INFO: stdout: ""
Nov 29 02:32:10.897: INFO: update-demo-nautilus-g4kdn is created but not running
Nov 29 02:32:15.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-826'
Nov 29 02:32:16.002: INFO: stderr: ""
Nov 29 02:32:16.002: INFO: stdout: "update-demo-nautilus-g4kdn update-demo-nautilus-jg4qv "
Nov 29 02:32:16.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get pods update-demo-nautilus-g4kdn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-826'
Nov 29 02:32:16.099: INFO: stderr: ""
Nov 29 02:32:16.099: INFO: stdout: "true"
Nov 29 02:32:16.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get pods update-demo-nautilus-g4kdn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-826'
Nov 29 02:32:16.189: INFO: stderr: ""
Nov 29 02:32:16.189: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 29 02:32:16.189: INFO: validating pod update-demo-nautilus-g4kdn
Nov 29 02:32:16.197: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 29 02:32:16.197: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 29 02:32:16.197: INFO: update-demo-nautilus-g4kdn is verified up and running
Nov 29 02:32:16.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get pods update-demo-nautilus-jg4qv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-826'
Nov 29 02:32:16.286: INFO: stderr: ""
Nov 29 02:32:16.286: INFO: stdout: "true"
Nov 29 02:32:16.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get pods update-demo-nautilus-jg4qv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-826'
Nov 29 02:32:16.381: INFO: stderr: ""
Nov 29 02:32:16.381: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 29 02:32:16.381: INFO: validating pod update-demo-nautilus-jg4qv
Nov 29 02:32:16.388: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 29 02:32:16.388: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 29 02:32:16.388: INFO: update-demo-nautilus-jg4qv is verified up and running
STEP: using delete to clean up resources
Nov 29 02:32:16.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 delete --grace-period=0 --force -f - --namespace=kubectl-826'
Nov 29 02:32:16.496: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 29 02:32:16.496: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov 29 02:32:16.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-826'
Nov 29 02:32:16.587: INFO: stderr: "No resources found in kubectl-826 namespace.\n"
Nov 29 02:32:16.587: INFO: stdout: ""
Nov 29 02:32:16.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get pods -l name=update-demo --namespace=kubectl-826 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 29 02:32:16.680: INFO: stderr: ""
Nov 29 02:32:16.680: INFO: stdout: "update-demo-nautilus-g4kdn\nupdate-demo-nautilus-jg4qv\n"
Nov 29 02:32:17.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-826'
Nov 29 02:32:17.297: INFO: stderr: "No resources found in kubectl-826 namespace.\n"
Nov 29 02:32:17.297: INFO: stdout: ""
Nov 29 02:32:17.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get pods -l name=update-demo --namespace=kubectl-826 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 29 02:32:17.398: INFO: stderr: ""
Nov 29 02:32:17.399: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:32:17.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-826" for this suite.

• [SLOW TEST:7.126 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:269
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":277,"completed":207,"skipped":3452,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:32:17.413: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 02:32:17.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 create -f - --namespace=kubectl-1868'
Nov 29 02:32:17.705: INFO: stderr: ""
Nov 29 02:32:17.705: INFO: stdout: "replicationcontroller/agnhost-master created\n"
Nov 29 02:32:17.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 create -f - --namespace=kubectl-1868'
Nov 29 02:32:17.973: INFO: stderr: ""
Nov 29 02:32:17.973: INFO: stdout: "service/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Nov 29 02:32:18.976: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 29 02:32:18.976: INFO: Found 0 / 1
Nov 29 02:32:19.978: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 29 02:32:19.978: INFO: Found 0 / 1
Nov 29 02:32:20.978: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 29 02:32:20.978: INFO: Found 1 / 1
Nov 29 02:32:20.978: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 29 02:32:20.981: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 29 02:32:20.981: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 29 02:32:20.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 describe pod agnhost-master-gxfxj --namespace=kubectl-1868'
Nov 29 02:32:21.110: INFO: stderr: ""
Nov 29 02:32:21.110: INFO: stdout: "Name:         agnhost-master-gxfxj\nNamespace:    kubectl-1868\nPriority:     0\nNode:         alex-cp1516-v3-vsp2-node-group-ea4104c57e/10.10.103.164\nStart Time:   Sun, 29 Nov 2020 02:32:17 +0000\nLabels:       app=agnhost\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 192.168.36.222/32\n              cni.projectcalico.org/podIPs: 192.168.36.222/32\nStatus:       Running\nIP:           192.168.36.222\nIPs:\n  IP:           192.168.36.222\nControlled By:  ReplicationController/agnhost-master\nContainers:\n  agnhost-master:\n    Container ID:   docker://de32862d0132253aba780f42e8291fd3f197b37890b4ef7cbe4ced8acd123dc7\n    Image:          us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12\n    Image ID:       docker-pullable://us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sun, 29 Nov 2020 02:32:19 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-54sgn (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-54sgn:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-54sgn\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  4s    default-scheduler  Successfully assigned kubectl-1868/agnhost-master-gxfxj to alex-cp1516-v3-vsp2-node-group-ea4104c57e\n  Normal  Pulled     2s    kubelet            Container image \"us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-master\n  Normal  Started    2s    kubelet            Started container agnhost-master\n"
Nov 29 02:32:21.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 describe rc agnhost-master --namespace=kubectl-1868'
Nov 29 02:32:21.245: INFO: stderr: ""
Nov 29 02:32:21.245: INFO: stdout: "Name:         agnhost-master\nNamespace:    kubectl-1868\nSelector:     app=agnhost,role=master\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=master\n  Containers:\n   agnhost-master:\n    Image:        us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-master-gxfxj\n"
Nov 29 02:32:21.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 describe service agnhost-master --namespace=kubectl-1868'
Nov 29 02:32:21.367: INFO: stderr: ""
Nov 29 02:32:21.367: INFO: stdout: "Name:              agnhost-master\nNamespace:         kubectl-1868\nLabels:            app=agnhost\n                   role=master\nAnnotations:       <none>\nSelector:          app=agnhost,role=master\nType:              ClusterIP\nIP:                10.98.199.17\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.36.222:6379\nSession Affinity:  None\nEvents:            <none>\n"
Nov 29 02:32:21.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 describe node alex-cp1516-v3-vsp2-master-gro-e1a0775a24'
Nov 29 02:32:21.516: INFO: stderr: ""
Nov 29 02:32:21.516: INFO: stdout: "Name:               alex-cp1516-v3-vsp2-master-gro-e1a0775a24\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    cert-manager=v1\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=alex-cp1516-v3-vsp2-master-gro-e1a0775a24\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.10.103.167/22\n                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.42.192\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sun, 29 Nov 2020 01:09:06 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  alex-cp1516-v3-vsp2-master-gro-e1a0775a24\n  AcquireTime:     <unset>\n  RenewTime:       Sun, 29 Nov 2020 02:32:19 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Sun, 29 Nov 2020 01:09:35 +0000   Sun, 29 Nov 2020 01:09:35 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Sun, 29 Nov 2020 02:31:35 +0000   Sun, 29 Nov 2020 01:09:01 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Sun, 29 Nov 2020 02:31:35 +0000   Sun, 29 Nov 2020 01:09:01 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Sun, 29 Nov 2020 02:31:35 +0000   Sun, 29 Nov 2020 01:09:01 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Sun, 29 Nov 2020 02:31:35 +0000   Sun, 29 Nov 2020 01:09:37 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  ExternalIP:  10.10.103.167\n  InternalIP:  10.10.103.167\n  Hostname:    alex-cp1516-v3-vsp2-master-gro-e1a0775a24\nCapacity:\n  cpu:                4\n  ephemeral-storage:  40700140Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             32940684Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  37509248962\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             32838284Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 524299015f704e8e8a5743e591add0fb\n  System UUID:                86151542-40D7-E75D-2A09-67C1B248A9EC\n  Boot ID:                    8133d77d-9830-4a0a-a678-cb0faac36e9b\n  Kernel Version:             4.15.0-122-generic\n  OS Image:                   Ubuntu 18.04.5 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://19.3.13\n  Kubelet Version:            v1.18.12\n  Kube-Proxy Version:         v1.18.12\nPodCIDR:                      192.168.0.0/24\nPodCIDRs:                     192.168.0.0/24\nProviderID:                   vsphere://42151586-d740-5de7-2a09-67c1b248a9ec\nNon-terminated Pods:          (15 in total)\n  Namespace                   Name                                                                 CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                                 ------------  ----------  ---------------  -------------  ---\n  ccp                         ccp-helm-operator-86589fbc96-lm4xr                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         82m\n  ccp                         ccp-vip-manager-alex-cp1516-v3-vsp2-master-gro-e1a0775a24            0 (0%)        0 (0%)      0 (0%)           0 (0%)         83m\n  ccp                         cert-manager-56c4fddc45-kf6kh                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         82m\n  ccp                         cert-manager-cainjector-7b857c7f5d-h5cqm                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         82m\n  ccp                         cert-manager-webhook-6f84bc6c5f-dxprj                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         82m\n  kube-system                 calico-kube-controllers-8666d5f96f-4krwp                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         82m\n  kube-system                 calico-node-fsq8g                                                    250m (6%)     0 (0%)      0 (0%)           0 (0%)         82m\n  kube-system                 coredns-56986b46f-zmt7l                                              100m (2%)     0 (0%)      70Mi (0%)        170Mi (0%)     82m\n  kube-system                 coredns-56986b46f-zptsr                                              100m (2%)     0 (0%)      70Mi (0%)        170Mi (0%)     82m\n  kube-system                 etcd-alex-cp1516-v3-vsp2-master-gro-e1a0775a24                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         83m\n  kube-system                 kube-apiserver-alex-cp1516-v3-vsp2-master-gro-e1a0775a24             250m (6%)     0 (0%)      0 (0%)           0 (0%)         83m\n  kube-system                 kube-controller-manager-alex-cp1516-v3-vsp2-master-gro-e1a0775a24    200m (5%)     0 (0%)      0 (0%)           0 (0%)         83m\n  kube-system                 kube-proxy-fxzcb                                                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         82m\n  kube-system                 kube-scheduler-alex-cp1516-v3-vsp2-master-gro-e1a0775a24             100m (2%)     0 (0%)      0 (0%)           0 (0%)         83m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-a18500f604534dc2-nn4gn              0 (0%)        0 (0%)      0 (0%)           0 (0%)         77m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                1 (25%)     0 (0%)\n  memory             140Mi (0%)  340Mi (1%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
Nov 29 02:32:21.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 describe namespace kubectl-1868'
Nov 29 02:32:21.622: INFO: stderr: ""
Nov 29 02:32:21.622: INFO: stdout: "Name:         kubectl-1868\nLabels:       e2e-framework=kubectl\n              e2e-run=182ce59e-d243-406e-8b8f-ff5a8834b4a3\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:32:21.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1868" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":277,"completed":208,"skipped":3474,"failed":0}

------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:32:21.629: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Nov 29 02:32:22.415: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Nov 29 02:32:24.424: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742213941, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742213941, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742213941, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742213941, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-65c6cd5fdf\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 02:32:27.442: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 02:32:27.445: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:32:28.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9920" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.110 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":277,"completed":209,"skipped":3474,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:32:28.740: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name s-test-opt-del-849b33b2-9730-4d5e-a08a-972797b27571
STEP: Creating secret with name s-test-opt-upd-a5114a9d-b0b6-4c1c-99b4-58bc3c44eed3
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-849b33b2-9730-4d5e-a08a-972797b27571
STEP: Updating secret s-test-opt-upd-a5114a9d-b0b6-4c1c-99b4-58bc3c44eed3
STEP: Creating secret with name s-test-opt-create-48749ba7-a448-4fc5-b9a4-5363a16d185d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:32:34.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6029" for this suite.

• [SLOW TEST:6.194 seconds]
[sig-storage] Secrets
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":210,"skipped":3483,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:32:34.934: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 02:32:35.003: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"2dd93e47-92d7-45c5-9ad0-3a6f2cd36adf", Controller:(*bool)(0xc0061e3c36), BlockOwnerDeletion:(*bool)(0xc0061e3c37)}}
Nov 29 02:32:35.009: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"72e825ec-185c-4581-b84b-b9f51ed0dfee", Controller:(*bool)(0xc0052251c6), BlockOwnerDeletion:(*bool)(0xc0052251c7)}}
Nov 29 02:32:35.016: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"e638cc4b-0ab8-4773-a406-0198dd817769", Controller:(*bool)(0xc008ca8666), BlockOwnerDeletion:(*bool)(0xc008ca8667)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:32:40.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6749" for this suite.

• [SLOW TEST:5.150 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":277,"completed":211,"skipped":3485,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:32:40.084: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Nov 29 02:32:40.143: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f8771d93-078f-4e49-8927-699e5e4680f3" in namespace "projected-5316" to be "Succeeded or Failed"
Nov 29 02:32:40.159: INFO: Pod "downwardapi-volume-f8771d93-078f-4e49-8927-699e5e4680f3": Phase="Pending", Reason="", readiness=false. Elapsed: 16.0452ms
Nov 29 02:32:42.162: INFO: Pod "downwardapi-volume-f8771d93-078f-4e49-8927-699e5e4680f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018783864s
Nov 29 02:32:44.166: INFO: Pod "downwardapi-volume-f8771d93-078f-4e49-8927-699e5e4680f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023158138s
STEP: Saw pod success
Nov 29 02:32:44.166: INFO: Pod "downwardapi-volume-f8771d93-078f-4e49-8927-699e5e4680f3" satisfied condition "Succeeded or Failed"
Nov 29 02:32:44.169: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-bc6233c429 pod downwardapi-volume-f8771d93-078f-4e49-8927-699e5e4680f3 container client-container: <nil>
STEP: delete the pod
Nov 29 02:32:44.196: INFO: Waiting for pod downwardapi-volume-f8771d93-078f-4e49-8927-699e5e4680f3 to disappear
Nov 29 02:32:44.202: INFO: Pod downwardapi-volume-f8771d93-078f-4e49-8927-699e5e4680f3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:32:44.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5316" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":212,"skipped":3551,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:32:44.214: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Nov 29 02:32:44.258: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:32:48.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-841" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":277,"completed":213,"skipped":3569,"failed":0}
SSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:32:48.415: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Nov 29 02:32:56.510: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 29 02:32:56.516: INFO: Pod pod-with-poststart-http-hook still exists
Nov 29 02:32:58.516: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 29 02:32:58.521: INFO: Pod pod-with-poststart-http-hook still exists
Nov 29 02:33:00.516: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 29 02:33:00.520: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:33:00.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3690" for this suite.

• [SLOW TEST:12.117 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":277,"completed":214,"skipped":3573,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:33:00.532: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:33:00.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6716" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":277,"completed":215,"skipped":3576,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:33:00.586: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:33:04.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4738" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":277,"completed":216,"skipped":3590,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:33:04.641: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on tmpfs
Nov 29 02:33:04.674: INFO: Waiting up to 5m0s for pod "pod-1c314830-be5f-49df-b27f-3e149d431c1c" in namespace "emptydir-58" to be "Succeeded or Failed"
Nov 29 02:33:04.677: INFO: Pod "pod-1c314830-be5f-49df-b27f-3e149d431c1c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.481333ms
Nov 29 02:33:06.681: INFO: Pod "pod-1c314830-be5f-49df-b27f-3e149d431c1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006775664s
Nov 29 02:33:08.684: INFO: Pod "pod-1c314830-be5f-49df-b27f-3e149d431c1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010217772s
STEP: Saw pod success
Nov 29 02:33:08.684: INFO: Pod "pod-1c314830-be5f-49df-b27f-3e149d431c1c" satisfied condition "Succeeded or Failed"
Nov 29 02:33:08.687: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod pod-1c314830-be5f-49df-b27f-3e149d431c1c container test-container: <nil>
STEP: delete the pod
Nov 29 02:33:08.707: INFO: Waiting for pod pod-1c314830-be5f-49df-b27f-3e149d431c1c to disappear
Nov 29 02:33:08.712: INFO: Pod pod-1c314830-be5f-49df-b27f-3e149d431c1c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:33:08.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-58" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":217,"skipped":3599,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:33:08.720: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-94d97903-e751-43f9-b6c6-7abf6ca7a05b
STEP: Creating a pod to test consume configMaps
Nov 29 02:33:08.789: INFO: Waiting up to 5m0s for pod "pod-configmaps-8da30dd5-ec1e-48f9-a57f-a516d58fdfe5" in namespace "configmap-3841" to be "Succeeded or Failed"
Nov 29 02:33:08.797: INFO: Pod "pod-configmaps-8da30dd5-ec1e-48f9-a57f-a516d58fdfe5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.52523ms
Nov 29 02:33:10.801: INFO: Pod "pod-configmaps-8da30dd5-ec1e-48f9-a57f-a516d58fdfe5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012170298s
Nov 29 02:33:12.804: INFO: Pod "pod-configmaps-8da30dd5-ec1e-48f9-a57f-a516d58fdfe5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015074933s
STEP: Saw pod success
Nov 29 02:33:12.804: INFO: Pod "pod-configmaps-8da30dd5-ec1e-48f9-a57f-a516d58fdfe5" satisfied condition "Succeeded or Failed"
Nov 29 02:33:12.806: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod pod-configmaps-8da30dd5-ec1e-48f9-a57f-a516d58fdfe5 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 02:33:12.825: INFO: Waiting for pod pod-configmaps-8da30dd5-ec1e-48f9-a57f-a516d58fdfe5 to disappear
Nov 29 02:33:12.829: INFO: Pod pod-configmaps-8da30dd5-ec1e-48f9-a57f-a516d58fdfe5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:33:12.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3841" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":277,"completed":218,"skipped":3601,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:33:12.837: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-347.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-347.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-347.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-347.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-347.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-347.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-347.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-347.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-347.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-347.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 29 02:33:22.908: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:22.911: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:22.914: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:22.919: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:22.932: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:22.936: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:22.940: INFO: Unable to read jessie_udp@dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:22.942: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:22.948: INFO: Lookups using dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local wheezy_udp@dns-test-service-2.dns-347.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-347.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local jessie_udp@dns-test-service-2.dns-347.svc.cluster.local jessie_tcp@dns-test-service-2.dns-347.svc.cluster.local]

Nov 29 02:33:27.951: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:27.954: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:27.957: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:27.960: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:27.978: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:27.980: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:27.982: INFO: Unable to read jessie_udp@dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:27.985: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:28.005: INFO: Lookups using dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local wheezy_udp@dns-test-service-2.dns-347.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-347.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local jessie_udp@dns-test-service-2.dns-347.svc.cluster.local jessie_tcp@dns-test-service-2.dns-347.svc.cluster.local]

Nov 29 02:33:32.952: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:32.956: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:32.960: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:32.964: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:32.975: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:32.978: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:32.981: INFO: Unable to read jessie_udp@dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:32.983: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:32.988: INFO: Lookups using dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local wheezy_udp@dns-test-service-2.dns-347.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-347.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local jessie_udp@dns-test-service-2.dns-347.svc.cluster.local jessie_tcp@dns-test-service-2.dns-347.svc.cluster.local]

Nov 29 02:33:37.953: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:37.957: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:37.960: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:37.963: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:37.973: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:37.976: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:37.978: INFO: Unable to read jessie_udp@dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:37.981: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:37.986: INFO: Lookups using dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local wheezy_udp@dns-test-service-2.dns-347.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-347.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local jessie_udp@dns-test-service-2.dns-347.svc.cluster.local jessie_tcp@dns-test-service-2.dns-347.svc.cluster.local]

Nov 29 02:33:42.953: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:42.958: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:42.963: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:42.967: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:42.977: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:42.984: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:42.988: INFO: Unable to read jessie_udp@dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:42.991: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:42.997: INFO: Lookups using dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local wheezy_udp@dns-test-service-2.dns-347.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-347.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-347.svc.cluster.local jessie_udp@dns-test-service-2.dns-347.svc.cluster.local jessie_tcp@dns-test-service-2.dns-347.svc.cluster.local]

Nov 29 02:33:47.962: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:47.975: INFO: Unable to read jessie_udp@dns-test-service-2.dns-347.svc.cluster.local from pod dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c: the server could not find the requested resource (get pods dns-test-485c42cd-6499-45fa-856b-54e8a99d794c)
Nov 29 02:33:47.989: INFO: Lookups using dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c failed for: [wheezy_tcp@dns-test-service-2.dns-347.svc.cluster.local jessie_udp@dns-test-service-2.dns-347.svc.cluster.local]

Nov 29 02:33:52.984: INFO: DNS probes using dns-347/dns-test-485c42cd-6499-45fa-856b-54e8a99d794c succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:33:53.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-347" for this suite.

• [SLOW TEST:40.222 seconds]
[sig-network] DNS
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":277,"completed":219,"skipped":3617,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:33:53.060: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating service nodeport-test with type=NodePort in namespace services-6361
STEP: creating replication controller nodeport-test in namespace services-6361
I1129 02:33:53.156817      22 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-6361, replica count: 2
Nov 29 02:33:56.208: INFO: Creating new exec pod
I1129 02:33:56.208422      22 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 29 02:34:01.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=services-6361 execpodtndlg -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Nov 29 02:34:01.462: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Nov 29 02:34:01.462: INFO: stdout: ""
Nov 29 02:34:01.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=services-6361 execpodtndlg -- /bin/sh -x -c nc -zv -t -w 2 10.101.52.183 80'
Nov 29 02:34:01.679: INFO: stderr: "+ nc -zv -t -w 2 10.101.52.183 80\nConnection to 10.101.52.183 80 port [tcp/http] succeeded!\n"
Nov 29 02:34:01.679: INFO: stdout: ""
Nov 29 02:34:01.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=services-6361 execpodtndlg -- /bin/sh -x -c nc -zv -t -w 2 10.10.103.164 32617'
Nov 29 02:34:01.889: INFO: stderr: "+ nc -zv -t -w 2 10.10.103.164 32617\nConnection to 10.10.103.164 32617 port [tcp/32617] succeeded!\n"
Nov 29 02:34:01.889: INFO: stdout: ""
Nov 29 02:34:01.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=services-6361 execpodtndlg -- /bin/sh -x -c nc -zv -t -w 2 10.10.103.165 32617'
Nov 29 02:34:02.099: INFO: stderr: "+ nc -zv -t -w 2 10.10.103.165 32617\nConnection to 10.10.103.165 32617 port [tcp/32617] succeeded!\n"
Nov 29 02:34:02.099: INFO: stdout: ""
Nov 29 02:34:02.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=services-6361 execpodtndlg -- /bin/sh -x -c nc -zv -t -w 2 10.10.103.164 32617'
Nov 29 02:34:02.309: INFO: stderr: "+ nc -zv -t -w 2 10.10.103.164 32617\nConnection to 10.10.103.164 32617 port [tcp/32617] succeeded!\n"
Nov 29 02:34:02.309: INFO: stdout: ""
Nov 29 02:34:02.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=services-6361 execpodtndlg -- /bin/sh -x -c nc -zv -t -w 2 10.10.103.165 32617'
Nov 29 02:34:02.515: INFO: stderr: "+ nc -zv -t -w 2 10.10.103.165 32617\nConnection to 10.10.103.165 32617 port [tcp/32617] succeeded!\n"
Nov 29 02:34:02.515: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:34:02.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6361" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:9.465 seconds]
[sig-network] Services
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":277,"completed":220,"skipped":3621,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:34:02.526: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating replication controller my-hostname-basic-bc6b0be9-20b7-4727-a2f2-678c650fe822
Nov 29 02:34:02.561: INFO: Pod name my-hostname-basic-bc6b0be9-20b7-4727-a2f2-678c650fe822: Found 0 pods out of 1
Nov 29 02:34:07.569: INFO: Pod name my-hostname-basic-bc6b0be9-20b7-4727-a2f2-678c650fe822: Found 1 pods out of 1
Nov 29 02:34:07.569: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-bc6b0be9-20b7-4727-a2f2-678c650fe822" are running
Nov 29 02:34:07.572: INFO: Pod "my-hostname-basic-bc6b0be9-20b7-4727-a2f2-678c650fe822-85qkx" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-29 02:34:02 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-29 02:34:04 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-29 02:34:04 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-29 02:34:01 +0000 UTC Reason: Message:}])
Nov 29 02:34:07.572: INFO: Trying to dial the pod
Nov 29 02:34:12.581: INFO: Controller my-hostname-basic-bc6b0be9-20b7-4727-a2f2-678c650fe822: Got expected result from replica 1 [my-hostname-basic-bc6b0be9-20b7-4727-a2f2-678c650fe822-85qkx]: "my-hostname-basic-bc6b0be9-20b7-4727-a2f2-678c650fe822-85qkx", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:34:12.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9457" for this suite.

• [SLOW TEST:10.069 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":277,"completed":221,"skipped":3660,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:34:12.595: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Nov 29 02:34:12.648: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8412 /api/v1/namespaces/watch-8412/configmaps/e2e-watch-test-resource-version f7607265-731e-4af3-a9b0-5690c4579561 45130 0 2020-11-29 02:34:11 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-11-29 02:34:11 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 29 02:34:12.648: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8412 /api/v1/namespaces/watch-8412/configmaps/e2e-watch-test-resource-version f7607265-731e-4af3-a9b0-5690c4579561 45131 0 2020-11-29 02:34:11 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-11-29 02:34:11 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:34:12.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8412" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":277,"completed":222,"skipped":3667,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:34:12.656: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 02:34:12.680: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Creating first CR 
Nov 29 02:34:13.244: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-29T02:34:12Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-29T02:34:12Z]] name:name1 resourceVersion:45143 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:d4775301-65d0-4af4-8b5f-3346d08487de] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Nov 29 02:34:23.251: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-29T02:34:22Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-29T02:34:22Z]] name:name2 resourceVersion:45220 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:6316c350-662b-43ab-bfb3-077fe34ffd8c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Nov 29 02:34:33.256: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-29T02:34:12Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-29T02:34:32Z]] name:name1 resourceVersion:45276 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:d4775301-65d0-4af4-8b5f-3346d08487de] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Nov 29 02:34:43.261: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-29T02:34:22Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-29T02:34:42Z]] name:name2 resourceVersion:45330 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:6316c350-662b-43ab-bfb3-077fe34ffd8c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Nov 29 02:34:53.275: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-29T02:34:12Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-29T02:34:32Z]] name:name1 resourceVersion:45385 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:d4775301-65d0-4af4-8b5f-3346d08487de] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Nov 29 02:35:03.285: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-29T02:34:22Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-29T02:34:42Z]] name:name2 resourceVersion:45446 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:6316c350-662b-43ab-bfb3-077fe34ffd8c] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:35:13.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-889" for this suite.

• [SLOW TEST:61.145 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":277,"completed":223,"skipped":3685,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:35:13.805: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap configmap-9880/configmap-test-85cb80c9-35ee-40bd-8054-4580d76a4707
STEP: Creating a pod to test consume configMaps
Nov 29 02:35:13.844: INFO: Waiting up to 5m0s for pod "pod-configmaps-5e8b9aab-a11f-4895-99fa-b7672c5dc6f8" in namespace "configmap-9880" to be "Succeeded or Failed"
Nov 29 02:35:13.851: INFO: Pod "pod-configmaps-5e8b9aab-a11f-4895-99fa-b7672c5dc6f8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.817655ms
Nov 29 02:35:15.854: INFO: Pod "pod-configmaps-5e8b9aab-a11f-4895-99fa-b7672c5dc6f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009724919s
Nov 29 02:35:17.857: INFO: Pod "pod-configmaps-5e8b9aab-a11f-4895-99fa-b7672c5dc6f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013618341s
STEP: Saw pod success
Nov 29 02:35:17.857: INFO: Pod "pod-configmaps-5e8b9aab-a11f-4895-99fa-b7672c5dc6f8" satisfied condition "Succeeded or Failed"
Nov 29 02:35:17.860: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod pod-configmaps-5e8b9aab-a11f-4895-99fa-b7672c5dc6f8 container env-test: <nil>
STEP: delete the pod
Nov 29 02:35:17.885: INFO: Waiting for pod pod-configmaps-5e8b9aab-a11f-4895-99fa-b7672c5dc6f8 to disappear
Nov 29 02:35:17.897: INFO: Pod pod-configmaps-5e8b9aab-a11f-4895-99fa-b7672c5dc6f8 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:35:17.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9880" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":277,"completed":224,"skipped":3714,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] version v1
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:35:17.910: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 02:35:17.955: INFO: (0) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 11.509024ms)
Nov 29 02:35:17.959: INFO: (1) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 3.315801ms)
Nov 29 02:35:17.963: INFO: (2) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 4.564182ms)
Nov 29 02:35:17.967: INFO: (3) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 3.284959ms)
Nov 29 02:35:17.970: INFO: (4) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 3.031082ms)
Nov 29 02:35:17.973: INFO: (5) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 3.281374ms)
Nov 29 02:35:17.976: INFO: (6) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.687302ms)
Nov 29 02:35:17.981: INFO: (7) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 4.49648ms)
Nov 29 02:35:17.984: INFO: (8) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 3.062349ms)
Nov 29 02:35:17.986: INFO: (9) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.174179ms)
Nov 29 02:35:17.989: INFO: (10) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.744843ms)
Nov 29 02:35:17.991: INFO: (11) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.239358ms)
Nov 29 02:35:17.994: INFO: (12) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.584886ms)
Nov 29 02:35:17.996: INFO: (13) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.589632ms)
Nov 29 02:35:17.999: INFO: (14) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.545025ms)
Nov 29 02:35:18.004: INFO: (15) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 5.097326ms)
Nov 29 02:35:18.007: INFO: (16) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 3.07562ms)
Nov 29 02:35:18.011: INFO: (17) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 3.487257ms)
Nov 29 02:35:18.014: INFO: (18) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.689652ms)
Nov 29 02:35:18.017: INFO: (19) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.791775ms)
[AfterEach] version v1
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:35:18.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5123" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]","total":277,"completed":225,"skipped":3722,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:35:18.025: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Nov 29 02:35:22.591: INFO: Successfully updated pod "labelsupdatebe162984-d383-4920-8bff-099fd90ddc42"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:35:24.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5900" for this suite.

• [SLOW TEST:6.587 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":277,"completed":226,"skipped":3729,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:35:24.614: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating service endpoint-test2 in namespace services-4476
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4476 to expose endpoints map[]
Nov 29 02:35:24.690: INFO: successfully validated that service endpoint-test2 in namespace services-4476 exposes endpoints map[] (19.735657ms elapsed)
STEP: Creating pod pod1 in namespace services-4476
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4476 to expose endpoints map[pod1:[80]]
Nov 29 02:35:27.747: INFO: successfully validated that service endpoint-test2 in namespace services-4476 exposes endpoints map[pod1:[80]] (3.039017363s elapsed)
STEP: Creating pod pod2 in namespace services-4476
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4476 to expose endpoints map[pod1:[80] pod2:[80]]
Nov 29 02:35:30.830: INFO: successfully validated that service endpoint-test2 in namespace services-4476 exposes endpoints map[pod1:[80] pod2:[80]] (3.079095035s elapsed)
STEP: Deleting pod pod1 in namespace services-4476
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4476 to expose endpoints map[pod2:[80]]
Nov 29 02:35:30.862: INFO: successfully validated that service endpoint-test2 in namespace services-4476 exposes endpoints map[pod2:[80]] (25.284492ms elapsed)
STEP: Deleting pod pod2 in namespace services-4476
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4476 to expose endpoints map[]
Nov 29 02:35:30.887: INFO: successfully validated that service endpoint-test2 in namespace services-4476 exposes endpoints map[] (6.729285ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:35:30.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4476" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:6.351 seconds]
[sig-network] Services
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":277,"completed":227,"skipped":3744,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:35:30.965: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 02:35:31.032: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-6c83068e-1e7d-4889-b201-fbc4a56fcd18" in namespace "security-context-test-3126" to be "Succeeded or Failed"
Nov 29 02:35:31.039: INFO: Pod "alpine-nnp-false-6c83068e-1e7d-4889-b201-fbc4a56fcd18": Phase="Pending", Reason="", readiness=false. Elapsed: 7.634197ms
Nov 29 02:35:33.043: INFO: Pod "alpine-nnp-false-6c83068e-1e7d-4889-b201-fbc4a56fcd18": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010973507s
Nov 29 02:35:35.046: INFO: Pod "alpine-nnp-false-6c83068e-1e7d-4889-b201-fbc4a56fcd18": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014067625s
Nov 29 02:35:37.050: INFO: Pod "alpine-nnp-false-6c83068e-1e7d-4889-b201-fbc4a56fcd18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017970116s
Nov 29 02:35:37.050: INFO: Pod "alpine-nnp-false-6c83068e-1e7d-4889-b201-fbc4a56fcd18" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:35:37.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3126" for this suite.

• [SLOW TEST:6.098 seconds]
[k8s.io] Security Context
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:291
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":228,"skipped":3753,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:35:37.064: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 02:35:37.095: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Nov 29 02:35:39.132: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:35:40.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6416" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":277,"completed":229,"skipped":3764,"failed":0}
S
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:35:40.150: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:35:40.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3318" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":277,"completed":230,"skipped":3765,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:35:40.266: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Nov 29 02:35:40.305: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Nov 29 02:36:01.328: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
Nov 29 02:36:06.965: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:36:28.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4525" for this suite.

• [SLOW TEST:47.975 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":277,"completed":231,"skipped":3811,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:36:28.241: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 02:36:28.275: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Nov 29 02:36:33.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 --namespace=crd-publish-openapi-7831 create -f -'
Nov 29 02:36:35.673: INFO: stderr: ""
Nov 29 02:36:35.673: INFO: stdout: "e2e-test-crd-publish-openapi-2068-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Nov 29 02:36:35.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 --namespace=crd-publish-openapi-7831 delete e2e-test-crd-publish-openapi-2068-crds test-cr'
Nov 29 02:36:35.780: INFO: stderr: ""
Nov 29 02:36:35.780: INFO: stdout: "e2e-test-crd-publish-openapi-2068-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Nov 29 02:36:35.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 --namespace=crd-publish-openapi-7831 apply -f -'
Nov 29 02:36:36.101: INFO: stderr: ""
Nov 29 02:36:36.101: INFO: stdout: "e2e-test-crd-publish-openapi-2068-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Nov 29 02:36:36.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 --namespace=crd-publish-openapi-7831 delete e2e-test-crd-publish-openapi-2068-crds test-cr'
Nov 29 02:36:36.199: INFO: stderr: ""
Nov 29 02:36:36.199: INFO: stdout: "e2e-test-crd-publish-openapi-2068-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Nov 29 02:36:36.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 explain e2e-test-crd-publish-openapi-2068-crds'
Nov 29 02:36:36.464: INFO: stderr: ""
Nov 29 02:36:36.464: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2068-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:36:42.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7831" for this suite.

• [SLOW TEST:13.870 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":277,"completed":232,"skipped":3822,"failed":0}
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:36:42.111: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test substitution in container's args
Nov 29 02:36:42.154: INFO: Waiting up to 5m0s for pod "var-expansion-05eb896a-e2b1-4e11-bad3-6ca37775b0ec" in namespace "var-expansion-3032" to be "Succeeded or Failed"
Nov 29 02:36:42.161: INFO: Pod "var-expansion-05eb896a-e2b1-4e11-bad3-6ca37775b0ec": Phase="Pending", Reason="", readiness=false. Elapsed: 6.84423ms
Nov 29 02:36:44.164: INFO: Pod "var-expansion-05eb896a-e2b1-4e11-bad3-6ca37775b0ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010084964s
Nov 29 02:36:46.168: INFO: Pod "var-expansion-05eb896a-e2b1-4e11-bad3-6ca37775b0ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0135458s
STEP: Saw pod success
Nov 29 02:36:46.168: INFO: Pod "var-expansion-05eb896a-e2b1-4e11-bad3-6ca37775b0ec" satisfied condition "Succeeded or Failed"
Nov 29 02:36:46.169: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod var-expansion-05eb896a-e2b1-4e11-bad3-6ca37775b0ec container dapi-container: <nil>
STEP: delete the pod
Nov 29 02:36:46.182: INFO: Waiting for pod var-expansion-05eb896a-e2b1-4e11-bad3-6ca37775b0ec to disappear
Nov 29 02:36:46.184: INFO: Pod var-expansion-05eb896a-e2b1-4e11-bad3-6ca37775b0ec no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:36:46.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3032" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":277,"completed":233,"skipped":3826,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:36:46.192: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Nov 29 02:36:46.233: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Nov 29 02:36:55.272: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:36:55.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9293" for this suite.

• [SLOW TEST:9.094 seconds]
[k8s.io] Pods
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":277,"completed":234,"skipped":3837,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:36:55.287: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 02:36:55.359: INFO: Create a RollingUpdate DaemonSet
Nov 29 02:36:55.364: INFO: Check that daemon pods launch on every node of the cluster
Nov 29 02:36:55.372: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:36:55.377: INFO: Number of nodes with available pods: 0
Nov 29 02:36:55.377: INFO: Node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d is running more than one daemon pod
Nov 29 02:36:56.383: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:36:56.386: INFO: Number of nodes with available pods: 0
Nov 29 02:36:56.386: INFO: Node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d is running more than one daemon pod
Nov 29 02:36:57.382: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:36:57.386: INFO: Number of nodes with available pods: 0
Nov 29 02:36:57.386: INFO: Node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d is running more than one daemon pod
Nov 29 02:36:58.384: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:36:58.388: INFO: Number of nodes with available pods: 1
Nov 29 02:36:58.388: INFO: Node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d is running more than one daemon pod
Nov 29 02:36:59.383: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:36:59.386: INFO: Number of nodes with available pods: 2
Nov 29 02:36:59.386: INFO: Node alex-cp1516-v3-vsp2-node-group-ea4104c57e is running more than one daemon pod
Nov 29 02:37:00.382: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:37:00.385: INFO: Number of nodes with available pods: 3
Nov 29 02:37:00.385: INFO: Number of running nodes: 3, number of available pods: 3
Nov 29 02:37:00.385: INFO: Update the DaemonSet to trigger a rollout
Nov 29 02:37:00.392: INFO: Updating DaemonSet daemon-set
Nov 29 02:37:10.427: INFO: Roll back the DaemonSet before rollout is complete
Nov 29 02:37:10.437: INFO: Updating DaemonSet daemon-set
Nov 29 02:37:10.437: INFO: Make sure DaemonSet rollback is complete
Nov 29 02:37:10.440: INFO: Wrong image for pod: daemon-set-g2dsc. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Nov 29 02:37:10.440: INFO: Pod daemon-set-g2dsc is not available
Nov 29 02:37:10.444: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:37:11.448: INFO: Wrong image for pod: daemon-set-g2dsc. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Nov 29 02:37:11.448: INFO: Pod daemon-set-g2dsc is not available
Nov 29 02:37:11.451: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:37:12.448: INFO: Wrong image for pod: daemon-set-g2dsc. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Nov 29 02:37:12.448: INFO: Pod daemon-set-g2dsc is not available
Nov 29 02:37:12.452: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:37:13.447: INFO: Wrong image for pod: daemon-set-g2dsc. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Nov 29 02:37:13.447: INFO: Pod daemon-set-g2dsc is not available
Nov 29 02:37:13.451: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:37:14.448: INFO: Wrong image for pod: daemon-set-g2dsc. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Nov 29 02:37:14.448: INFO: Pod daemon-set-g2dsc is not available
Nov 29 02:37:14.452: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:37:15.448: INFO: Wrong image for pod: daemon-set-g2dsc. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Nov 29 02:37:15.448: INFO: Pod daemon-set-g2dsc is not available
Nov 29 02:37:15.452: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:37:16.448: INFO: Wrong image for pod: daemon-set-g2dsc. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Nov 29 02:37:16.448: INFO: Pod daemon-set-g2dsc is not available
Nov 29 02:37:16.451: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:37:17.448: INFO: Wrong image for pod: daemon-set-g2dsc. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Nov 29 02:37:17.448: INFO: Pod daemon-set-g2dsc is not available
Nov 29 02:37:17.452: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:37:18.448: INFO: Wrong image for pod: daemon-set-g2dsc. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Nov 29 02:37:18.448: INFO: Pod daemon-set-g2dsc is not available
Nov 29 02:37:18.452: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:37:19.448: INFO: Wrong image for pod: daemon-set-g2dsc. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Nov 29 02:37:19.448: INFO: Pod daemon-set-g2dsc is not available
Nov 29 02:37:19.451: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 29 02:37:20.448: INFO: Pod daemon-set-rsmkh is not available
Nov 29 02:37:20.453: INFO: DaemonSet pods can't tolerate node alex-cp1516-v3-vsp2-master-gro-e1a0775a24 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1179, will wait for the garbage collector to delete the pods
Nov 29 02:37:20.514: INFO: Deleting DaemonSet.extensions daemon-set took: 4.019457ms
Nov 29 02:37:21.314: INFO: Terminating DaemonSet.extensions daemon-set pods took: 800.471085ms
Nov 29 02:37:30.717: INFO: Number of nodes with available pods: 0
Nov 29 02:37:30.717: INFO: Number of running nodes: 0, number of available pods: 0
Nov 29 02:37:30.719: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1179/daemonsets","resourceVersion":"46805"},"items":null}

Nov 29 02:37:30.722: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1179/pods","resourceVersion":"46805"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:37:30.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1179" for this suite.

• [SLOW TEST:35.455 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":277,"completed":235,"skipped":3866,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:37:30.742: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:37:30.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-4950" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":277,"completed":236,"skipped":3881,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:37:30.795: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: validating cluster-info
Nov 29 02:37:30.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 cluster-info'
Nov 29 02:37:30.927: INFO: stderr: ""
Nov 29 02:37:30.927: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:37:30.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6603" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":277,"completed":237,"skipped":3886,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:37:30.934: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 02:37:30.962: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:37:31.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-967" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":277,"completed":238,"skipped":3894,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:37:31.995: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Nov 29 02:37:32.022: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 29 02:37:32.040: INFO: Waiting for terminating namespaces to be deleted...
Nov 29 02:37:32.043: INFO: 
Logging pods the kubelet thinks is on node alex-cp1516-v3-vsp2-node-group-55fe8f9f1d before test
Nov 29 02:37:32.060: INFO: metallb-speaker-5sjng from ccp started at 2020-11-29 01:13:11 +0000 UTC (1 container statuses recorded)
Nov 29 02:37:32.060: INFO: 	Container metallb-speaker ready: true, restart count 0
Nov 29 02:37:32.060: INFO: sonobuoy from sonobuoy started at 2020-11-29 01:14:36 +0000 UTC (1 container statuses recorded)
Nov 29 02:37:32.060: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 29 02:37:32.060: INFO: calico-node-gs8lr from kube-system started at 2020-11-29 01:13:01 +0000 UTC (1 container statuses recorded)
Nov 29 02:37:32.060: INFO: 	Container calico-node ready: true, restart count 0
Nov 29 02:37:32.060: INFO: sonobuoy-systemd-logs-daemon-set-a18500f604534dc2-g2xkd from sonobuoy started at 2020-11-29 01:15:14 +0000 UTC (2 container statuses recorded)
Nov 29 02:37:32.060: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Nov 29 02:37:32.060: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 29 02:37:32.060: INFO: kube-proxy-zjcgv from kube-system started at 2020-11-29 01:13:00 +0000 UTC (1 container statuses recorded)
Nov 29 02:37:32.060: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 29 02:37:32.060: INFO: nvidia-device-plugin-daemonset-f79b4 from kube-system started at 2020-11-29 01:13:11 +0000 UTC (1 container statuses recorded)
Nov 29 02:37:32.060: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Nov 29 02:37:32.060: INFO: ingress-nginx-controller-zd8lx from ccp started at 2020-11-29 01:13:11 +0000 UTC (1 container statuses recorded)
Nov 29 02:37:32.060: INFO: 	Container controller ready: true, restart count 0
Nov 29 02:37:32.060: INFO: sonobuoy-e2e-job-df1d2fd3866f4821 from sonobuoy started at 2020-11-29 01:15:14 +0000 UTC (2 container statuses recorded)
Nov 29 02:37:32.060: INFO: 	Container e2e ready: true, restart count 0
Nov 29 02:37:32.060: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 29 02:37:32.060: INFO: 
Logging pods the kubelet thinks is on node alex-cp1516-v3-vsp2-node-group-bc6233c429 before test
Nov 29 02:37:32.072: INFO: calico-node-xhkv2 from kube-system started at 2020-11-29 01:12:58 +0000 UTC (1 container statuses recorded)
Nov 29 02:37:32.072: INFO: 	Container calico-node ready: true, restart count 0
Nov 29 02:37:32.072: INFO: metallb-controller-dd895cddd-knr69 from ccp started at 2020-11-29 02:14:14 +0000 UTC (1 container statuses recorded)
Nov 29 02:37:32.072: INFO: 	Container metallb-controller ready: true, restart count 0
Nov 29 02:37:32.072: INFO: metallb-speaker-8qx7b from ccp started at 2020-11-29 02:14:14 +0000 UTC (1 container statuses recorded)
Nov 29 02:37:32.072: INFO: 	Container metallb-speaker ready: true, restart count 0
Nov 29 02:37:32.072: INFO: sonobuoy-systemd-logs-daemon-set-a18500f604534dc2-vsjhg from sonobuoy started at 2020-11-29 01:15:14 +0000 UTC (2 container statuses recorded)
Nov 29 02:37:32.072: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Nov 29 02:37:32.072: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 29 02:37:32.072: INFO: ingress-nginx-controller-spj9n from ccp started at 2020-11-29 02:14:37 +0000 UTC (1 container statuses recorded)
Nov 29 02:37:32.072: INFO: 	Container controller ready: true, restart count 0
Nov 29 02:37:32.072: INFO: ingress-nginx-defaultbackend-6cd8f668c8-79p2d from ccp started at 2020-11-29 02:30:09 +0000 UTC (1 container statuses recorded)
Nov 29 02:37:32.072: INFO: 	Container ingress-nginx-default-backend ready: true, restart count 0
Nov 29 02:37:32.072: INFO: kube-proxy-wcdhr from kube-system started at 2020-11-29 01:12:58 +0000 UTC (1 container statuses recorded)
Nov 29 02:37:32.072: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 29 02:37:32.072: INFO: nvidia-device-plugin-daemonset-f8wbp from kube-system started at 2020-11-29 02:14:27 +0000 UTC (1 container statuses recorded)
Nov 29 02:37:32.072: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Nov 29 02:37:32.072: INFO: 
Logging pods the kubelet thinks is on node alex-cp1516-v3-vsp2-node-group-ea4104c57e before test
Nov 29 02:37:32.084: INFO: sonobuoy-systemd-logs-daemon-set-a18500f604534dc2-449j5 from sonobuoy started at 2020-11-29 01:15:13 +0000 UTC (2 container statuses recorded)
Nov 29 02:37:32.084: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Nov 29 02:37:32.084: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 29 02:37:32.084: INFO: ingress-nginx-controller-wbm4g from ccp started at 2020-11-29 02:30:39 +0000 UTC (1 container statuses recorded)
Nov 29 02:37:32.084: INFO: 	Container controller ready: true, restart count 0
Nov 29 02:37:32.084: INFO: kube-proxy-xsm2v from kube-system started at 2020-11-29 01:13:00 +0000 UTC (1 container statuses recorded)
Nov 29 02:37:32.084: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 29 02:37:32.084: INFO: nvidia-device-plugin-daemonset-6ks48 from kube-system started at 2020-11-29 02:30:39 +0000 UTC (1 container statuses recorded)
Nov 29 02:37:32.084: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Nov 29 02:37:32.084: INFO: calico-node-c7rxt from kube-system started at 2020-11-29 01:13:00 +0000 UTC (1 container statuses recorded)
Nov 29 02:37:32.084: INFO: 	Container calico-node ready: true, restart count 0
Nov 29 02:37:32.084: INFO: metallb-speaker-dclbw from ccp started at 2020-11-29 02:30:39 +0000 UTC (1 container statuses recorded)
Nov 29 02:37:32.084: INFO: 	Container metallb-speaker ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-c11ee74e-e855-4fb8-9f9b-5cf580f72b6b 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-c11ee74e-e855-4fb8-9f9b-5cf580f72b6b off the node alex-cp1516-v3-vsp2-node-group-ea4104c57e
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c11ee74e-e855-4fb8-9f9b-5cf580f72b6b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:37:40.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-714" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

• [SLOW TEST:8.214 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":277,"completed":239,"skipped":3909,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:37:40.213: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 02:37:40.242: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:37:46.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4523" for this suite.

• [SLOW TEST:6.418 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":277,"completed":240,"skipped":3928,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:37:46.632: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-4355
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a new StatefulSet
Nov 29 02:37:46.673: INFO: Found 0 stateful pods, waiting for 3
Nov 29 02:37:56.676: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 02:37:56.676: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 02:37:56.676: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 02:37:56.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-4355 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 29 02:37:56.919: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 29 02:37:56.919: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 29 02:37:56.919: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Nov 29 02:38:06.948: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Nov 29 02:38:16.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-4355 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 02:38:17.170: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 29 02:38:17.170: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 29 02:38:17.170: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 29 02:38:27.188: INFO: Waiting for StatefulSet statefulset-4355/ss2 to complete update
Nov 29 02:38:27.188: INFO: Waiting for Pod statefulset-4355/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 29 02:38:27.188: INFO: Waiting for Pod statefulset-4355/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 29 02:38:27.188: INFO: Waiting for Pod statefulset-4355/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 29 02:38:37.193: INFO: Waiting for StatefulSet statefulset-4355/ss2 to complete update
Nov 29 02:38:37.193: INFO: Waiting for Pod statefulset-4355/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 29 02:38:47.205: INFO: Waiting for StatefulSet statefulset-4355/ss2 to complete update
Nov 29 02:38:47.205: INFO: Waiting for Pod statefulset-4355/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Nov 29 02:38:57.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-4355 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 29 02:38:57.422: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 29 02:38:57.422: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 29 02:38:57.422: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 29 02:39:07.450: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Nov 29 02:39:17.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=statefulset-4355 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 29 02:39:17.673: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 29 02:39:17.673: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 29 02:39:17.673: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 29 02:39:27.688: INFO: Waiting for StatefulSet statefulset-4355/ss2 to complete update
Nov 29 02:39:27.688: INFO: Waiting for Pod statefulset-4355/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Nov 29 02:39:27.688: INFO: Waiting for Pod statefulset-4355/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Nov 29 02:39:27.688: INFO: Waiting for Pod statefulset-4355/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Nov 29 02:39:37.707: INFO: Waiting for StatefulSet statefulset-4355/ss2 to complete update
Nov 29 02:39:37.707: INFO: Waiting for Pod statefulset-4355/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Nov 29 02:39:47.694: INFO: Deleting all statefulset in ns statefulset-4355
Nov 29 02:39:47.696: INFO: Scaling statefulset ss2 to 0
Nov 29 02:40:27.709: INFO: Waiting for statefulset status.replicas updated to 0
Nov 29 02:40:27.711: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:40:27.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4355" for this suite.

• [SLOW TEST:161.101 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":277,"completed":241,"skipped":3941,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:40:27.742: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7972
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-7972
STEP: creating replication controller externalsvc in namespace services-7972
I1129 02:40:27.848940      22 runners.go:190] Created replication controller with name: externalsvc, namespace: services-7972, replica count: 2
I1129 02:40:30.899327      22 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Nov 29 02:40:30.921: INFO: Creating new exec pod
Nov 29 02:40:34.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=services-7972 execpodc97sc -- /bin/sh -x -c nslookup clusterip-service'
Nov 29 02:40:35.182: INFO: stderr: "+ nslookup clusterip-service\n"
Nov 29 02:40:35.183: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-7972.svc.cluster.local\tcanonical name = externalsvc.services-7972.svc.cluster.local.\nName:\texternalsvc.services-7972.svc.cluster.local\nAddress: 10.103.135.130\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7972, will wait for the garbage collector to delete the pods
Nov 29 02:40:35.240: INFO: Deleting ReplicationController externalsvc took: 4.30607ms
Nov 29 02:40:35.340: INFO: Terminating ReplicationController externalsvc pods took: 100.191754ms
Nov 29 02:40:49.874: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:40:49.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7972" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:22.203 seconds]
[sig-network] Services
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":277,"completed":242,"skipped":3948,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:40:49.945: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 29 02:40:50.390: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 29 02:40:52.398: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742214449, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742214449, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742214449, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742214449, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 02:40:55.416: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:40:55.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1304" for this suite.
STEP: Destroying namespace "webhook-1304-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.925 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":277,"completed":243,"skipped":3948,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:40:55.870: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:41:12.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4610" for this suite.

• [SLOW TEST:16.162 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":277,"completed":244,"skipped":3954,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:41:12.037: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-upd-8dddd5c4-29db-4274-9285-4420faedbee0
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-8dddd5c4-29db-4274-9285-4420faedbee0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:42:36.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7451" for this suite.

• [SLOW TEST:84.485 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":245,"skipped":3992,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:42:36.523: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service externalname-service with the type=ExternalName in namespace services-616
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-616
I1129 02:42:36.615964      22 runners.go:190] Created replication controller with name: externalname-service, namespace: services-616, replica count: 2
I1129 02:42:39.670376      22 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 29 02:42:39.670: INFO: Creating new exec pod
Nov 29 02:42:44.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=services-616 execpodl2pn7 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Nov 29 02:42:44.881: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Nov 29 02:42:44.881: INFO: stdout: ""
Nov 29 02:42:44.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=services-616 execpodl2pn7 -- /bin/sh -x -c nc -zv -t -w 2 10.97.15.94 80'
Nov 29 02:42:45.084: INFO: stderr: "+ nc -zv -t -w 2 10.97.15.94 80\nConnection to 10.97.15.94 80 port [tcp/http] succeeded!\n"
Nov 29 02:42:45.084: INFO: stdout: ""
Nov 29 02:42:45.085: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:42:45.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-616" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:8.666 seconds]
[sig-network] Services
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":277,"completed":246,"skipped":4004,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:42:45.189: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: set up a multi version CRD
Nov 29 02:42:45.239: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:43:14.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-90" for this suite.

• [SLOW TEST:29.231 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":277,"completed":247,"skipped":4020,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:43:14.423: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 29 02:43:15.356: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 29 02:43:17.366: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742214594, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742214594, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742214594, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742214594, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 29 02:43:20.394: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:43:30.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2354" for this suite.
STEP: Destroying namespace "webhook-2354-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.233 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":277,"completed":248,"skipped":4043,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:43:30.658: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Nov 29 02:43:35.281: INFO: Successfully updated pod "pod-update-6520256d-e997-40c8-ae66-0f4b81677db5"
STEP: verifying the updated pod is in kubernetes
Nov 29 02:43:35.287: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:43:35.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5743" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":277,"completed":249,"skipped":4064,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:43:35.295: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-ca02102a-476c-42c2-8fbc-ebb9e1597c72
STEP: Creating a pod to test consume secrets
Nov 29 02:43:35.365: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-182a1d14-26a3-42d1-94ee-b91f7a0c519a" in namespace "projected-4974" to be "Succeeded or Failed"
Nov 29 02:43:35.369: INFO: Pod "pod-projected-secrets-182a1d14-26a3-42d1-94ee-b91f7a0c519a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.879405ms
Nov 29 02:43:37.383: INFO: Pod "pod-projected-secrets-182a1d14-26a3-42d1-94ee-b91f7a0c519a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017080612s
Nov 29 02:43:39.386: INFO: Pod "pod-projected-secrets-182a1d14-26a3-42d1-94ee-b91f7a0c519a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02026922s
STEP: Saw pod success
Nov 29 02:43:39.386: INFO: Pod "pod-projected-secrets-182a1d14-26a3-42d1-94ee-b91f7a0c519a" satisfied condition "Succeeded or Failed"
Nov 29 02:43:39.389: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod pod-projected-secrets-182a1d14-26a3-42d1-94ee-b91f7a0c519a container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 29 02:43:39.407: INFO: Waiting for pod pod-projected-secrets-182a1d14-26a3-42d1-94ee-b91f7a0c519a to disappear
Nov 29 02:43:39.415: INFO: Pod pod-projected-secrets-182a1d14-26a3-42d1-94ee-b91f7a0c519a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:43:39.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4974" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":250,"skipped":4068,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:43:39.429: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating Agnhost RC
Nov 29 02:43:39.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 create -f - --namespace=kubectl-7979'
Nov 29 02:43:39.802: INFO: stderr: ""
Nov 29 02:43:39.803: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Nov 29 02:43:40.807: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 29 02:43:40.807: INFO: Found 0 / 1
Nov 29 02:43:41.806: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 29 02:43:41.806: INFO: Found 0 / 1
Nov 29 02:43:42.807: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 29 02:43:42.807: INFO: Found 1 / 1
Nov 29 02:43:42.807: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Nov 29 02:43:42.811: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 29 02:43:42.811: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 29 02:43:42.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 patch pod agnhost-master-gbfx8 --namespace=kubectl-7979 -p {"metadata":{"annotations":{"x":"y"}}}'
Nov 29 02:43:42.914: INFO: stderr: ""
Nov 29 02:43:42.914: INFO: stdout: "pod/agnhost-master-gbfx8 patched\n"
STEP: checking annotations
Nov 29 02:43:42.917: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 29 02:43:42.917: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:43:42.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7979" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":277,"completed":251,"skipped":4191,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:43:42.925: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-map-cc4f6d38-68ca-4ccf-9794-9f9be8e36869
STEP: Creating a pod to test consume secrets
Nov 29 02:43:42.972: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2e6bf39d-a6b1-4fdf-a376-e656b1da2cfc" in namespace "projected-2591" to be "Succeeded or Failed"
Nov 29 02:43:42.975: INFO: Pod "pod-projected-secrets-2e6bf39d-a6b1-4fdf-a376-e656b1da2cfc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.405674ms
Nov 29 02:43:44.979: INFO: Pod "pod-projected-secrets-2e6bf39d-a6b1-4fdf-a376-e656b1da2cfc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006072447s
Nov 29 02:43:46.982: INFO: Pod "pod-projected-secrets-2e6bf39d-a6b1-4fdf-a376-e656b1da2cfc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009426244s
STEP: Saw pod success
Nov 29 02:43:46.982: INFO: Pod "pod-projected-secrets-2e6bf39d-a6b1-4fdf-a376-e656b1da2cfc" satisfied condition "Succeeded or Failed"
Nov 29 02:43:46.984: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod pod-projected-secrets-2e6bf39d-a6b1-4fdf-a376-e656b1da2cfc container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 29 02:43:47.003: INFO: Waiting for pod pod-projected-secrets-2e6bf39d-a6b1-4fdf-a376-e656b1da2cfc to disappear
Nov 29 02:43:47.011: INFO: Pod pod-projected-secrets-2e6bf39d-a6b1-4fdf-a376-e656b1da2cfc no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:43:47.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2591" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":277,"completed":252,"skipped":4195,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:43:47.026: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:44:03.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2611" for this suite.

• [SLOW TEST:16.150 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":277,"completed":253,"skipped":4217,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:44:03.176: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on node default medium
Nov 29 02:44:03.218: INFO: Waiting up to 5m0s for pod "pod-11edc90b-eb66-491b-9647-1e5240358119" in namespace "emptydir-9919" to be "Succeeded or Failed"
Nov 29 02:44:03.220: INFO: Pod "pod-11edc90b-eb66-491b-9647-1e5240358119": Phase="Pending", Reason="", readiness=false. Elapsed: 2.509051ms
Nov 29 02:44:05.224: INFO: Pod "pod-11edc90b-eb66-491b-9647-1e5240358119": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006124173s
Nov 29 02:44:07.227: INFO: Pod "pod-11edc90b-eb66-491b-9647-1e5240358119": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009358049s
STEP: Saw pod success
Nov 29 02:44:07.227: INFO: Pod "pod-11edc90b-eb66-491b-9647-1e5240358119" satisfied condition "Succeeded or Failed"
Nov 29 02:44:07.229: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod pod-11edc90b-eb66-491b-9647-1e5240358119 container test-container: <nil>
STEP: delete the pod
Nov 29 02:44:07.248: INFO: Waiting for pod pod-11edc90b-eb66-491b-9647-1e5240358119 to disappear
Nov 29 02:44:07.259: INFO: Pod pod-11edc90b-eb66-491b-9647-1e5240358119 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:44:07.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9919" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":254,"skipped":4255,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:44:07.271: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:44:18.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3464" for this suite.

• [SLOW TEST:11.128 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":277,"completed":255,"skipped":4279,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:44:18.400: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service nodeport-service with the type=NodePort in namespace services-1888
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1888
STEP: creating replication controller externalsvc in namespace services-1888
I1129 02:44:18.539627      22 runners.go:190] Created replication controller with name: externalsvc, namespace: services-1888, replica count: 2
I1129 02:44:21.590890      22 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Nov 29 02:44:21.623: INFO: Creating new exec pod
Nov 29 02:44:25.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 exec --namespace=services-1888 execpod8cb7b -- /bin/sh -x -c nslookup nodeport-service'
Nov 29 02:44:25.859: INFO: stderr: "+ nslookup nodeport-service\n"
Nov 29 02:44:25.859: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-1888.svc.cluster.local\tcanonical name = externalsvc.services-1888.svc.cluster.local.\nName:\texternalsvc.services-1888.svc.cluster.local\nAddress: 10.110.246.216\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1888, will wait for the garbage collector to delete the pods
Nov 29 02:44:25.918: INFO: Deleting ReplicationController externalsvc took: 5.317077ms
Nov 29 02:44:26.718: INFO: Terminating ReplicationController externalsvc pods took: 800.220971ms
Nov 29 02:44:39.945: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:44:39.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1888" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:21.592 seconds]
[sig-network] Services
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":277,"completed":256,"skipped":4307,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:44:39.992: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:44:40.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8457" for this suite.
STEP: Destroying namespace "nspatchtest-7340b308-e4d7-4ade-a68f-7a86149edc60-8805" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":277,"completed":257,"skipped":4358,"failed":0}
SSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:44:40.088: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating pod
Nov 29 02:44:44.152: INFO: Pod pod-hostip-33994e7f-cf4c-4758-8c71-9c4522d7dbfe has hostIP: 10.10.103.164
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:44:44.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4974" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":277,"completed":258,"skipped":4363,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:44:44.161: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating Agnhost RC
Nov 29 02:44:44.240: INFO: namespace kubectl-7439
Nov 29 02:44:44.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 create -f - --namespace=kubectl-7439'
Nov 29 02:44:44.489: INFO: stderr: ""
Nov 29 02:44:44.489: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Nov 29 02:44:45.495: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 29 02:44:45.495: INFO: Found 0 / 1
Nov 29 02:44:46.493: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 29 02:44:46.493: INFO: Found 0 / 1
Nov 29 02:44:47.493: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 29 02:44:47.493: INFO: Found 1 / 1
Nov 29 02:44:47.493: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 29 02:44:47.495: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 29 02:44:47.495: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 29 02:44:47.495: INFO: wait on agnhost-master startup in kubectl-7439 
Nov 29 02:44:47.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 logs agnhost-master-7h8kk agnhost-master --namespace=kubectl-7439'
Nov 29 02:44:47.618: INFO: stderr: ""
Nov 29 02:44:47.618: INFO: stdout: "Paused\n"
STEP: exposing RC
Nov 29 02:44:47.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 expose rc agnhost-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-7439'
Nov 29 02:44:47.764: INFO: stderr: ""
Nov 29 02:44:47.764: INFO: stdout: "service/rm2 exposed\n"
Nov 29 02:44:47.777: INFO: Service rm2 in namespace kubectl-7439 found.
STEP: exposing service
Nov 29 02:44:49.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-7439'
Nov 29 02:44:49.911: INFO: stderr: ""
Nov 29 02:44:49.911: INFO: stdout: "service/rm3 exposed\n"
Nov 29 02:44:49.919: INFO: Service rm3 in namespace kubectl-7439 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:44:51.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7439" for this suite.

• [SLOW TEST:7.775 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1119
    should create services for rc  [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":277,"completed":259,"skipped":4363,"failed":0}
SSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:44:51.937: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:44:55.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-577" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":277,"completed":260,"skipped":4366,"failed":0}
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:44:56.002: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-2075
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 29 02:44:56.052: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 29 02:44:56.101: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 29 02:44:58.105: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 29 02:45:00.105: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 02:45:02.104: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 02:45:04.105: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 02:45:06.105: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 02:45:08.105: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 02:45:10.104: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 02:45:12.105: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 29 02:45:14.105: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov 29 02:45:14.109: INFO: The status of Pod netserver-1 is Running (Ready = true)
Nov 29 02:45:14.113: INFO: The status of Pod netserver-2 is Running (Ready = false)
Nov 29 02:45:16.116: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Nov 29 02:45:20.144: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.93.243:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2075 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 02:45:20.144: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
Nov 29 02:45:20.255: INFO: Found all expected endpoints: [netserver-0]
Nov 29 02:45:20.257: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.191.174:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2075 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 02:45:20.257: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
Nov 29 02:45:20.375: INFO: Found all expected endpoints: [netserver-1]
Nov 29 02:45:20.377: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.36.195:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2075 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 02:45:20.377: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
Nov 29 02:45:20.481: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:45:20.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2075" for this suite.

• [SLOW TEST:24.490 seconds]
[sig-network] Networking
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":261,"skipped":4370,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:45:20.492: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Nov 29 02:45:26.558: INFO: 0 pods remaining
Nov 29 02:45:26.558: INFO: 0 pods has nil DeletionTimestamp
Nov 29 02:45:26.558: INFO: 
STEP: Gathering metrics
W1129 02:45:27.564798      22 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Nov 29 02:45:27.564: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:45:27.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8420" for this suite.

• [SLOW TEST:7.080 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":277,"completed":262,"skipped":4381,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:45:27.572: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on node default medium
Nov 29 02:45:27.617: INFO: Waiting up to 5m0s for pod "pod-fc9f72d6-da43-4cc1-af95-08be4812e03c" in namespace "emptydir-6772" to be "Succeeded or Failed"
Nov 29 02:45:27.625: INFO: Pod "pod-fc9f72d6-da43-4cc1-af95-08be4812e03c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.843357ms
Nov 29 02:45:29.629: INFO: Pod "pod-fc9f72d6-da43-4cc1-af95-08be4812e03c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011891521s
Nov 29 02:45:31.641: INFO: Pod "pod-fc9f72d6-da43-4cc1-af95-08be4812e03c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023951214s
STEP: Saw pod success
Nov 29 02:45:31.641: INFO: Pod "pod-fc9f72d6-da43-4cc1-af95-08be4812e03c" satisfied condition "Succeeded or Failed"
Nov 29 02:45:31.664: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod pod-fc9f72d6-da43-4cc1-af95-08be4812e03c container test-container: <nil>
STEP: delete the pod
Nov 29 02:45:31.692: INFO: Waiting for pod pod-fc9f72d6-da43-4cc1-af95-08be4812e03c to disappear
Nov 29 02:45:31.698: INFO: Pod pod-fc9f72d6-da43-4cc1-af95-08be4812e03c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:45:31.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6772" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":263,"skipped":4385,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:45:31.729: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Nov 29 02:45:35.796: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4607 PodName:pod-sharedvolume-e9d8038e-46d1-4be2-9757-d578bfdbcbe2 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 02:45:35.796: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
Nov 29 02:45:35.913: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:45:35.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4607" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":277,"completed":264,"skipped":4435,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:45:35.922: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl replace
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Nov 29 02:45:35.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-3647'
Nov 29 02:45:36.098: INFO: stderr: ""
Nov 29 02:45:36.098: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Nov 29 02:45:41.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 get pod e2e-test-httpd-pod --namespace=kubectl-3647 -o json'
Nov 29 02:45:41.259: INFO: stderr: ""
Nov 29 02:45:41.259: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"192.168.36.214/32\",\n            \"cni.projectcalico.org/podIPs\": \"192.168.36.214/32\"\n        },\n        \"creationTimestamp\": \"2020-11-29T02:45:35Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-11-29T02:45:35Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \".\": {},\n                            \"f:cni.projectcalico.org/podIP\": {},\n                            \"f:cni.projectcalico.org/podIPs\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-11-29T02:45:36Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"192.168.36.214\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-11-29T02:45:37Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3647\",\n        \"resourceVersion\": \"51596\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-3647/pods/e2e-test-httpd-pod\",\n        \"uid\": \"2f752fe8-503e-4921-bc05-87550759d3fb\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-8tthh\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"alex-cp1516-v3-vsp2-node-group-ea4104c57e\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-8tthh\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-8tthh\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-29T02:45:36Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-29T02:45:38Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-29T02:45:38Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-29T02:45:35Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://4f463ef1584ec398fd54ad1b1c5a2d0e4ae52008cb5d6d90b5f242153374417f\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-11-29T02:45:37Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.10.103.164\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.36.214\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.36.214\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-11-29T02:45:36Z\"\n    }\n}\n"
STEP: replace the image in the pod
Nov 29 02:45:41.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 replace -f - --namespace=kubectl-3647'
Nov 29 02:45:41.515: INFO: stderr: ""
Nov 29 02:45:41.515: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
Nov 29 02:45:41.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-195255472 delete pods e2e-test-httpd-pod --namespace=kubectl-3647'
Nov 29 02:45:49.874: INFO: stderr: ""
Nov 29 02:45:49.874: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:45:49.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3647" for this suite.

• [SLOW TEST:13.962 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1450
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":277,"completed":265,"skipped":4440,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:45:49.885: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 29 02:45:52.958: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:45:52.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9769" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":277,"completed":266,"skipped":4448,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:45:52.978: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod liveness-ed9559e9-63e5-470b-9f45-78a3ad7cafe2 in namespace container-probe-3010
Nov 29 02:45:57.047: INFO: Started pod liveness-ed9559e9-63e5-470b-9f45-78a3ad7cafe2 in namespace container-probe-3010
STEP: checking the pod's current state and verifying that restartCount is present
Nov 29 02:45:57.049: INFO: Initial restart count of pod liveness-ed9559e9-63e5-470b-9f45-78a3ad7cafe2 is 0
Nov 29 02:46:13.088: INFO: Restart count of pod container-probe-3010/liveness-ed9559e9-63e5-470b-9f45-78a3ad7cafe2 is now 1 (16.038333756s elapsed)
Nov 29 02:46:33.127: INFO: Restart count of pod container-probe-3010/liveness-ed9559e9-63e5-470b-9f45-78a3ad7cafe2 is now 2 (36.077678771s elapsed)
Nov 29 02:46:53.172: INFO: Restart count of pod container-probe-3010/liveness-ed9559e9-63e5-470b-9f45-78a3ad7cafe2 is now 3 (56.122415174s elapsed)
Nov 29 02:47:13.205: INFO: Restart count of pod container-probe-3010/liveness-ed9559e9-63e5-470b-9f45-78a3ad7cafe2 is now 4 (1m16.15563889s elapsed)
Nov 29 02:48:13.370: INFO: Restart count of pod container-probe-3010/liveness-ed9559e9-63e5-470b-9f45-78a3ad7cafe2 is now 5 (2m16.320429099s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:48:13.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3010" for this suite.

• [SLOW TEST:140.435 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":277,"completed":267,"skipped":4534,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:48:13.413: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Nov 29 02:48:13.457: INFO: Waiting up to 5m0s for pod "downwardapi-volume-744743ab-243e-4966-b2b9-2d535dbe5a64" in namespace "projected-1120" to be "Succeeded or Failed"
Nov 29 02:48:13.466: INFO: Pod "downwardapi-volume-744743ab-243e-4966-b2b9-2d535dbe5a64": Phase="Pending", Reason="", readiness=false. Elapsed: 8.408756ms
Nov 29 02:48:15.481: INFO: Pod "downwardapi-volume-744743ab-243e-4966-b2b9-2d535dbe5a64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023729474s
Nov 29 02:48:17.484: INFO: Pod "downwardapi-volume-744743ab-243e-4966-b2b9-2d535dbe5a64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026551266s
STEP: Saw pod success
Nov 29 02:48:17.484: INFO: Pod "downwardapi-volume-744743ab-243e-4966-b2b9-2d535dbe5a64" satisfied condition "Succeeded or Failed"
Nov 29 02:48:17.486: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod downwardapi-volume-744743ab-243e-4966-b2b9-2d535dbe5a64 container client-container: <nil>
STEP: delete the pod
Nov 29 02:48:17.514: INFO: Waiting for pod downwardapi-volume-744743ab-243e-4966-b2b9-2d535dbe5a64 to disappear
Nov 29 02:48:17.521: INFO: Pod downwardapi-volume-744743ab-243e-4966-b2b9-2d535dbe5a64 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:48:17.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1120" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":277,"completed":268,"skipped":4538,"failed":0}

------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:48:17.531: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 02:48:17.562: INFO: Creating deployment "test-recreate-deployment"
Nov 29 02:48:17.572: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Nov 29 02:48:17.585: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Nov 29 02:48:19.591: INFO: Waiting deployment "test-recreate-deployment" to complete
Nov 29 02:48:19.593: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742214896, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742214896, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63742214896, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63742214896, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-74d98b5f7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 29 02:48:21.596: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Nov 29 02:48:21.602: INFO: Updating deployment test-recreate-deployment
Nov 29 02:48:21.602: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Nov 29 02:48:21.738: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-2109 /apis/apps/v1/namespaces/deployment-2109/deployments/test-recreate-deployment 4d52b78c-044b-4969-a703-b467363a951c 52707 2 2020-11-29 02:48:16 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-11-29 02:48:20 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-11-29 02:48:21 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 110 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004d3e2d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-11-29 02:48:21 +0000 UTC,LastTransitionTime:2020-11-29 02:48:21 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-d5667d9c7" is progressing.,LastUpdateTime:2020-11-29 02:48:21 +0000 UTC,LastTransitionTime:2020-11-29 02:48:16 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Nov 29 02:48:21.741: INFO: New ReplicaSet "test-recreate-deployment-d5667d9c7" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-d5667d9c7  deployment-2109 /apis/apps/v1/namespaces/deployment-2109/replicasets/test-recreate-deployment-d5667d9c7 6ebafa09-8dd2-429b-8e50-ca00d674754e 52704 1 2020-11-29 02:48:20 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:d5667d9c7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 4d52b78c-044b-4969-a703-b467363a951c 0xc004d3e840 0xc004d3e841}] []  [{kube-controller-manager Update apps/v1 2020-11-29 02:48:21 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 52 100 53 50 98 55 56 99 45 48 52 52 98 45 52 57 54 57 45 97 55 48 51 45 98 52 54 55 51 54 51 97 57 53 49 99 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: d5667d9c7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:d5667d9c7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004d3e8b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 29 02:48:21.741: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Nov 29 02:48:21.742: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-74d98b5f7c  deployment-2109 /apis/apps/v1/namespaces/deployment-2109/replicasets/test-recreate-deployment-74d98b5f7c 108711e3-6c13-4b21-bc88-0c630ea2da3a 52695 2 2020-11-29 02:48:16 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:74d98b5f7c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 4d52b78c-044b-4969-a703-b467363a951c 0xc004d3e737 0xc004d3e738}] []  [{kube-controller-manager Update apps/v1 2020-11-29 02:48:20 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 52 100 53 50 98 55 56 99 45 48 52 52 98 45 52 57 54 57 45 97 55 48 51 45 98 52 54 55 51 54 51 97 57 53 49 99 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 74d98b5f7c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:74d98b5f7c] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004d3e7d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 29 02:48:21.745: INFO: Pod "test-recreate-deployment-d5667d9c7-v8bc9" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-d5667d9c7-v8bc9 test-recreate-deployment-d5667d9c7- deployment-2109 /api/v1/namespaces/deployment-2109/pods/test-recreate-deployment-d5667d9c7-v8bc9 8f17bf75-249e-4749-828d-f56eee4f2876 52706 0 2020-11-29 02:48:20 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:d5667d9c7] map[] [{apps/v1 ReplicaSet test-recreate-deployment-d5667d9c7 6ebafa09-8dd2-429b-8e50-ca00d674754e 0xc004d3edc0 0xc004d3edc1}] []  [{kube-controller-manager Update v1 2020-11-29 02:48:20 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 54 101 98 97 102 97 48 57 45 56 100 100 50 45 52 50 57 98 45 56 101 53 48 45 99 97 48 48 100 54 55 52 55 53 52 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-11-29 02:48:21 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tbprj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tbprj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tbprj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-cp1516-v3-vsp2-node-group-ea4104c57e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:48:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:48:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:48:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-29 02:48:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.103.164,PodIP:,StartTime:2020-11-29 02:48:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:48:21.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2109" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":277,"completed":269,"skipped":4538,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:48:21.753: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-8176
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-8176
STEP: Creating statefulset with conflicting port in namespace statefulset-8176
STEP: Waiting until pod test-pod will start running in namespace statefulset-8176
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8176
Nov 29 02:48:25.830: INFO: Observed stateful pod in namespace: statefulset-8176, name: ss-0, uid: 99bddd68-2aa1-48e7-9ebd-089bd59645a8, status phase: Pending. Waiting for statefulset controller to delete.
Nov 29 02:48:26.222: INFO: Observed stateful pod in namespace: statefulset-8176, name: ss-0, uid: 99bddd68-2aa1-48e7-9ebd-089bd59645a8, status phase: Failed. Waiting for statefulset controller to delete.
Nov 29 02:48:26.228: INFO: Observed stateful pod in namespace: statefulset-8176, name: ss-0, uid: 99bddd68-2aa1-48e7-9ebd-089bd59645a8, status phase: Failed. Waiting for statefulset controller to delete.
Nov 29 02:48:26.237: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8176
STEP: Removing pod with conflicting port in namespace statefulset-8176
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8176 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Nov 29 02:48:30.277: INFO: Deleting all statefulset in ns statefulset-8176
Nov 29 02:48:30.279: INFO: Scaling statefulset ss to 0
Nov 29 02:48:40.291: INFO: Waiting for statefulset status.replicas updated to 0
Nov 29 02:48:40.292: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:48:40.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8176" for this suite.

• [SLOW TEST:18.562 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":277,"completed":270,"skipped":4555,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:48:40.315: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Nov 29 02:48:41.884: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1129 02:48:41.884853      22 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:48:41.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8458" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":277,"completed":271,"skipped":4572,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:48:41.893: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name secret-emptykey-test-180cc89d-40cb-4bf4-8f6a-65dc564fbbad
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:48:41.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2338" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":277,"completed":272,"skipped":4585,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:48:41.948: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Nov 29 02:48:41.997: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ab4c8309-d314-4cc1-aa85-0aae1565b553" in namespace "projected-7878" to be "Succeeded or Failed"
Nov 29 02:48:42.019: INFO: Pod "downwardapi-volume-ab4c8309-d314-4cc1-aa85-0aae1565b553": Phase="Pending", Reason="", readiness=false. Elapsed: 21.404574ms
Nov 29 02:48:44.022: INFO: Pod "downwardapi-volume-ab4c8309-d314-4cc1-aa85-0aae1565b553": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024425373s
Nov 29 02:48:46.025: INFO: Pod "downwardapi-volume-ab4c8309-d314-4cc1-aa85-0aae1565b553": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02741766s
STEP: Saw pod success
Nov 29 02:48:46.025: INFO: Pod "downwardapi-volume-ab4c8309-d314-4cc1-aa85-0aae1565b553" satisfied condition "Succeeded or Failed"
Nov 29 02:48:46.028: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod downwardapi-volume-ab4c8309-d314-4cc1-aa85-0aae1565b553 container client-container: <nil>
STEP: delete the pod
Nov 29 02:48:46.050: INFO: Waiting for pod downwardapi-volume-ab4c8309-d314-4cc1-aa85-0aae1565b553 to disappear
Nov 29 02:48:46.062: INFO: Pod downwardapi-volume-ab4c8309-d314-4cc1-aa85-0aae1565b553 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:48:46.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7878" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":277,"completed":273,"skipped":4597,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:48:46.074: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Nov 29 02:48:50.639: INFO: Successfully updated pod "annotationupdateee9d58ed-3956-4fb3-87eb-4d0c90b81503"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:48:52.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9340" for this suite.

• [SLOW TEST:6.604 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":277,"completed":274,"skipped":4638,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] version v1
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:48:52.679: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 02:48:52.732: INFO: (0) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 11.804536ms)
Nov 29 02:48:52.736: INFO: (1) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 3.476739ms)
Nov 29 02:48:52.739: INFO: (2) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 3.120292ms)
Nov 29 02:48:52.742: INFO: (3) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 3.45329ms)
Nov 29 02:48:52.745: INFO: (4) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.454744ms)
Nov 29 02:48:52.748: INFO: (5) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 3.171314ms)
Nov 29 02:48:52.751: INFO: (6) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.792974ms)
Nov 29 02:48:52.754: INFO: (7) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.822064ms)
Nov 29 02:48:52.757: INFO: (8) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.896627ms)
Nov 29 02:48:52.760: INFO: (9) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 3.047601ms)
Nov 29 02:48:52.763: INFO: (10) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.98318ms)
Nov 29 02:48:52.766: INFO: (11) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 3.165083ms)
Nov 29 02:48:52.769: INFO: (12) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.729817ms)
Nov 29 02:48:52.772: INFO: (13) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.918536ms)
Nov 29 02:48:52.775: INFO: (14) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.844824ms)
Nov 29 02:48:52.778: INFO: (15) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 3.486055ms)
Nov 29 02:48:52.781: INFO: (16) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 3.156014ms)
Nov 29 02:48:52.785: INFO: (17) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 3.143754ms)
Nov 29 02:48:52.787: INFO: (18) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.538725ms)
Nov 29 02:48:52.792: INFO: (19) /api/v1/nodes/alex-cp1516-v3-vsp2-node-group-bc6233c429/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 4.822979ms)
[AfterEach] version v1
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:48:52.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9975" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]","total":277,"completed":275,"skipped":4662,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:48:52.803: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-2e8d824c-1d7a-4617-95f8-f30cc61eb6bf
STEP: Creating a pod to test consume configMaps
Nov 29 02:48:52.860: INFO: Waiting up to 5m0s for pod "pod-configmaps-19e36ed4-d08f-4ec2-8ac2-c95b4b6118c6" in namespace "configmap-2539" to be "Succeeded or Failed"
Nov 29 02:48:52.869: INFO: Pod "pod-configmaps-19e36ed4-d08f-4ec2-8ac2-c95b4b6118c6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.959405ms
Nov 29 02:48:54.872: INFO: Pod "pod-configmaps-19e36ed4-d08f-4ec2-8ac2-c95b4b6118c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012167312s
Nov 29 02:48:56.876: INFO: Pod "pod-configmaps-19e36ed4-d08f-4ec2-8ac2-c95b4b6118c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015274595s
STEP: Saw pod success
Nov 29 02:48:56.876: INFO: Pod "pod-configmaps-19e36ed4-d08f-4ec2-8ac2-c95b4b6118c6" satisfied condition "Succeeded or Failed"
Nov 29 02:48:56.878: INFO: Trying to get logs from node alex-cp1516-v3-vsp2-node-group-ea4104c57e pod pod-configmaps-19e36ed4-d08f-4ec2-8ac2-c95b4b6118c6 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 02:48:56.893: INFO: Waiting for pod pod-configmaps-19e36ed4-d08f-4ec2-8ac2-c95b4b6118c6 to disappear
Nov 29 02:48:56.903: INFO: Pod pod-configmaps-19e36ed4-d08f-4ec2-8ac2-c95b4b6118c6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:48:56.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2539" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":276,"skipped":4702,"failed":0}
SSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Nov 29 02:48:56.911: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Nov 29 02:48:56.947: INFO: >>> kubeConfig: /tmp/kubeconfig-195255472
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Nov 29 02:49:00.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-549" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":277,"completed":277,"skipped":4706,"failed":0}
SSSSSSSSSSNov 29 02:49:00.989: INFO: Running AfterSuite actions on all nodes
Nov 29 02:49:00.989: INFO: Running AfterSuite actions on node 1
Nov 29 02:49:00.989: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":277,"completed":277,"skipped":4716,"failed":0}

Ran 277 of 4993 Specs in 5604.426 seconds
SUCCESS! -- 277 Passed | 0 Failed | 0 Pending | 4716 Skipped
PASS

Ginkgo ran 1 suite in 1h33m26.473178751s
Test Suite Passed
