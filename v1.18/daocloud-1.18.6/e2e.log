I0927 02:39:58.501157      23 test_context.go:410] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-085554482
I0927 02:39:58.501188      23 test_context.go:423] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0927 02:39:58.501328      23 e2e.go:124] Starting e2e run "18bc0502-a78c-4c1b-9d82-fdd96d5bb095" on Ginkgo node 1
{"msg":"Test Suite starting","total":277,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1601174395 - Will randomize all specs
Will run 277 of 4992 specs

Sep 27 02:39:58.532: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
Sep 27 02:39:58.536: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Sep 27 02:39:58.556: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Sep 27 02:39:58.604: INFO: 31 / 31 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Sep 27 02:39:58.604: INFO: expected 8 pod replicas in namespace 'kube-system', 8 are Running and Ready.
Sep 27 02:39:58.604: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Sep 27 02:39:58.618: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Sep 27 02:39:58.618: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'dce-engine' (0 seconds elapsed)
Sep 27 02:39:58.618: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'dce-parcel-agent' (0 seconds elapsed)
Sep 27 02:39:58.618: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'dce-parcel-server' (0 seconds elapsed)
Sep 27 02:39:58.618: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Sep 27 02:39:58.618: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Sep 27 02:39:58.618: INFO: e2e test version: v1.18.6
Sep 27 02:39:58.621: INFO: kube-apiserver version: v1.18.6
Sep 27 02:39:58.621: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
Sep 27 02:39:58.630: INFO: Cluster IP family: ipv4
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:39:58.630: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename crd-publish-openapi
Sep 27 02:39:59.398: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Sep 27 02:39:59.489: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6656
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Sep 27 02:39:59.620: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Sep 27 02:40:18.770: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
Sep 27 02:40:23.623: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:40:42.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6656" for this suite.

â€¢ [SLOW TEST:43.729 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":277,"completed":1,"skipped":1,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] version v1
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:40:42.360: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-9532
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-9gwbb in namespace proxy-9532
I0927 02:40:46.444288      23 runners.go:190] Created replication controller with name: proxy-service-9gwbb, namespace: proxy-9532, replica count: 1
I0927 02:40:47.494813      23 runners.go:190] proxy-service-9gwbb Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 02:40:48.495102      23 runners.go:190] proxy-service-9gwbb Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 02:40:49.495266      23 runners.go:190] proxy-service-9gwbb Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 02:40:50.495499      23 runners.go:190] proxy-service-9gwbb Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 02:40:51.495793      23 runners.go:190] proxy-service-9gwbb Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 02:40:52.496018      23 runners.go:190] proxy-service-9gwbb Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 02:40:53.496217      23 runners.go:190] proxy-service-9gwbb Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0927 02:40:54.509371      23 runners.go:190] proxy-service-9gwbb Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0927 02:40:55.556176      23 runners.go:190] proxy-service-9gwbb Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0927 02:40:56.556333      23 runners.go:190] proxy-service-9gwbb Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0927 02:40:57.651059      23 runners.go:190] proxy-service-9gwbb Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0927 02:40:58.663256      23 runners.go:190] proxy-service-9gwbb Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0927 02:40:59.663418      23 runners.go:190] proxy-service-9gwbb Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0927 02:41:00.663620      23 runners.go:190] proxy-service-9gwbb Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 27 02:41:01.196: INFO: setup took 15.658554984s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Sep 27 02:41:01.264: INFO: (0) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/rewriteme">test</a> (200; 67.261622ms)
Sep 27 02:41:01.264: INFO: (0) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 67.219414ms)
Sep 27 02:41:01.264: INFO: (0) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname2/proxy/: bar (200; 67.884595ms)
Sep 27 02:41:01.264: INFO: (0) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">test<... (200; 68.22138ms)
Sep 27 02:41:01.264: INFO: (0) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname2/proxy/: bar (200; 67.203788ms)
Sep 27 02:41:01.264: INFO: (0) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">... (200; 67.1032ms)
Sep 27 02:41:01.264: INFO: (0) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 66.964737ms)
Sep 27 02:41:01.264: INFO: (0) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 67.11463ms)
Sep 27 02:41:01.269: INFO: (0) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname1/proxy/: foo (200; 71.504672ms)
Sep 27 02:41:01.269: INFO: (0) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 71.854581ms)
Sep 27 02:41:01.269: INFO: (0) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname1/proxy/: foo (200; 72.518955ms)
Sep 27 02:41:01.273: INFO: (0) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/tlsrewritem... (200; 77.066345ms)
Sep 27 02:41:01.276: INFO: (0) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:462/proxy/: tls qux (200; 79.121123ms)
Sep 27 02:41:01.276: INFO: (0) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname2/proxy/: tls qux (200; 79.86992ms)
Sep 27 02:41:01.280: INFO: (0) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:460/proxy/: tls baz (200; 83.345684ms)
Sep 27 02:41:01.280: INFO: (0) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname1/proxy/: tls baz (200; 82.386064ms)
Sep 27 02:41:01.299: INFO: (1) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:462/proxy/: tls qux (200; 18.813448ms)
Sep 27 02:41:01.300: INFO: (1) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">test<... (200; 19.109936ms)
Sep 27 02:41:01.300: INFO: (1) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 19.637032ms)
Sep 27 02:41:01.300: INFO: (1) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:460/proxy/: tls baz (200; 18.793847ms)
Sep 27 02:41:01.300: INFO: (1) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 19.790561ms)
Sep 27 02:41:01.300: INFO: (1) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">... (200; 19.932089ms)
Sep 27 02:41:01.300: INFO: (1) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 19.618574ms)
Sep 27 02:41:01.300: INFO: (1) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/tlsrewritem... (200; 19.045641ms)
Sep 27 02:41:01.300: INFO: (1) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 18.64256ms)
Sep 27 02:41:01.300: INFO: (1) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/rewriteme">test</a> (200; 18.75984ms)
Sep 27 02:41:01.530: INFO: (1) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname2/proxy/: bar (200; 249.011994ms)
Sep 27 02:41:01.802: INFO: (1) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname2/proxy/: bar (200; 521.571859ms)
Sep 27 02:41:01.802: INFO: (1) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname1/proxy/: foo (200; 521.064117ms)
Sep 27 02:41:01.802: INFO: (1) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname2/proxy/: tls qux (200; 520.618779ms)
Sep 27 02:41:01.802: INFO: (1) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname1/proxy/: tls baz (200; 520.977258ms)
Sep 27 02:41:01.802: INFO: (1) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname1/proxy/: foo (200; 520.481239ms)
Sep 27 02:41:01.823: INFO: (2) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 20.20386ms)
Sep 27 02:41:01.823: INFO: (2) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 20.346663ms)
Sep 27 02:41:01.823: INFO: (2) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/tlsrewritem... (200; 20.517068ms)
Sep 27 02:41:01.823: INFO: (2) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/rewriteme">test</a> (200; 20.641372ms)
Sep 27 02:41:01.823: INFO: (2) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">test<... (200; 20.917015ms)
Sep 27 02:41:01.823: INFO: (2) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">... (200; 20.482351ms)
Sep 27 02:41:01.823: INFO: (2) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 21.001637ms)
Sep 27 02:41:01.823: INFO: (2) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:462/proxy/: tls qux (200; 21.242359ms)
Sep 27 02:41:01.824: INFO: (2) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 21.126425ms)
Sep 27 02:41:01.824: INFO: (2) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:460/proxy/: tls baz (200; 21.464892ms)
Sep 27 02:41:01.828: INFO: (2) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname2/proxy/: tls qux (200; 25.856005ms)
Sep 27 02:41:01.828: INFO: (2) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname2/proxy/: bar (200; 25.602371ms)
Sep 27 02:41:01.828: INFO: (2) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname2/proxy/: bar (200; 25.963777ms)
Sep 27 02:41:01.828: INFO: (2) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname1/proxy/: foo (200; 25.841539ms)
Sep 27 02:41:01.828: INFO: (2) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname1/proxy/: foo (200; 25.59305ms)
Sep 27 02:41:01.828: INFO: (2) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname1/proxy/: tls baz (200; 25.566589ms)
Sep 27 02:41:01.954: INFO: (3) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">test<... (200; 124.87037ms)
Sep 27 02:41:01.954: INFO: (3) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:462/proxy/: tls qux (200; 125.886612ms)
Sep 27 02:41:01.954: INFO: (3) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/tlsrewritem... (200; 124.351562ms)
Sep 27 02:41:01.954: INFO: (3) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:460/proxy/: tls baz (200; 124.269741ms)
Sep 27 02:41:01.954: INFO: (3) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 125.602179ms)
Sep 27 02:41:01.954: INFO: (3) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 125.428846ms)
Sep 27 02:41:01.954: INFO: (3) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">... (200; 125.303271ms)
Sep 27 02:41:01.954: INFO: (3) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/rewriteme">test</a> (200; 125.835718ms)
Sep 27 02:41:01.954: INFO: (3) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 125.585767ms)
Sep 27 02:41:01.955: INFO: (3) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 126.166942ms)
Sep 27 02:41:01.968: INFO: (3) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname1/proxy/: foo (200; 139.325317ms)
Sep 27 02:41:01.968: INFO: (3) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname2/proxy/: bar (200; 137.841319ms)
Sep 27 02:41:01.968: INFO: (3) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname2/proxy/: tls qux (200; 139.942648ms)
Sep 27 02:41:01.968: INFO: (3) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname1/proxy/: foo (200; 138.349072ms)
Sep 27 02:41:01.968: INFO: (3) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname1/proxy/: tls baz (200; 138.362358ms)
Sep 27 02:41:01.970: INFO: (3) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname2/proxy/: bar (200; 140.642612ms)
Sep 27 02:41:01.978: INFO: (4) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:462/proxy/: tls qux (200; 7.942658ms)
Sep 27 02:41:01.978: INFO: (4) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">... (200; 7.281845ms)
Sep 27 02:41:01.978: INFO: (4) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">test<... (200; 7.214241ms)
Sep 27 02:41:01.978: INFO: (4) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/tlsrewritem... (200; 6.945174ms)
Sep 27 02:41:01.978: INFO: (4) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 7.68758ms)
Sep 27 02:41:01.978: INFO: (4) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 8.230148ms)
Sep 27 02:41:01.978: INFO: (4) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 7.607447ms)
Sep 27 02:41:01.979: INFO: (4) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/rewriteme">test</a> (200; 8.457548ms)
Sep 27 02:41:01.979: INFO: (4) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:460/proxy/: tls baz (200; 7.515818ms)
Sep 27 02:41:01.979: INFO: (4) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 8.649317ms)
Sep 27 02:41:01.981: INFO: (4) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname1/proxy/: foo (200; 9.983927ms)
Sep 27 02:41:01.981: INFO: (4) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname2/proxy/: bar (200; 9.628092ms)
Sep 27 02:41:01.981: INFO: (4) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname1/proxy/: tls baz (200; 9.962874ms)
Sep 27 02:41:01.981: INFO: (4) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname1/proxy/: foo (200; 10.908074ms)
Sep 27 02:41:01.981: INFO: (4) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname2/proxy/: bar (200; 10.434417ms)
Sep 27 02:41:01.981: INFO: (4) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname2/proxy/: tls qux (200; 11.39923ms)
Sep 27 02:41:01.990: INFO: (5) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">test<... (200; 7.85998ms)
Sep 27 02:41:01.990: INFO: (5) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 8.321268ms)
Sep 27 02:41:01.990: INFO: (5) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 8.41796ms)
Sep 27 02:41:01.990: INFO: (5) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 8.323659ms)
Sep 27 02:41:01.990: INFO: (5) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:462/proxy/: tls qux (200; 8.675702ms)
Sep 27 02:41:01.991: INFO: (5) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">... (200; 8.754003ms)
Sep 27 02:41:01.991: INFO: (5) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname2/proxy/: tls qux (200; 7.206215ms)
Sep 27 02:41:01.991: INFO: (5) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/tlsrewritem... (200; 7.115328ms)
Sep 27 02:41:01.991: INFO: (5) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/rewriteme">test</a> (200; 9.192167ms)
Sep 27 02:41:01.991: INFO: (5) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:460/proxy/: tls baz (200; 9.505517ms)
Sep 27 02:41:01.991: INFO: (5) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 9.125728ms)
Sep 27 02:41:01.991: INFO: (5) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname1/proxy/: tls baz (200; 8.544891ms)
Sep 27 02:41:01.992: INFO: (5) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname2/proxy/: bar (200; 9.745375ms)
Sep 27 02:41:01.992: INFO: (5) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname1/proxy/: foo (200; 10.182111ms)
Sep 27 02:41:01.992: INFO: (5) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname1/proxy/: foo (200; 9.462563ms)
Sep 27 02:41:01.992: INFO: (5) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname2/proxy/: bar (200; 9.168213ms)
Sep 27 02:41:01.996: INFO: (6) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 4.657788ms)
Sep 27 02:41:01.997: INFO: (6) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">test<... (200; 4.408714ms)
Sep 27 02:41:01.997: INFO: (6) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/tlsrewritem... (200; 4.134146ms)
Sep 27 02:41:02.147: INFO: (6) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/rewriteme">test</a> (200; 154.26459ms)
Sep 27 02:41:02.147: INFO: (6) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:462/proxy/: tls qux (200; 154.242246ms)
Sep 27 02:41:02.147: INFO: (6) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname2/proxy/: bar (200; 154.250942ms)
Sep 27 02:41:02.147: INFO: (6) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname2/proxy/: tls qux (200; 154.96249ms)
Sep 27 02:41:02.147: INFO: (6) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 154.152113ms)
Sep 27 02:41:02.147: INFO: (6) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">... (200; 154.287911ms)
Sep 27 02:41:02.147: INFO: (6) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname1/proxy/: foo (200; 154.599228ms)
Sep 27 02:41:02.147: INFO: (6) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:460/proxy/: tls baz (200; 154.690634ms)
Sep 27 02:41:02.147: INFO: (6) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname1/proxy/: tls baz (200; 155.378797ms)
Sep 27 02:41:02.147: INFO: (6) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname1/proxy/: foo (200; 155.520515ms)
Sep 27 02:41:02.147: INFO: (6) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 154.491722ms)
Sep 27 02:41:02.147: INFO: (6) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname2/proxy/: bar (200; 155.203136ms)
Sep 27 02:41:02.147: INFO: (6) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 154.346873ms)
Sep 27 02:41:02.155: INFO: (7) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 7.632963ms)
Sep 27 02:41:02.156: INFO: (7) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">... (200; 7.802169ms)
Sep 27 02:41:02.157: INFO: (7) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 8.80907ms)
Sep 27 02:41:02.157: INFO: (7) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:462/proxy/: tls qux (200; 9.218879ms)
Sep 27 02:41:02.157: INFO: (7) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 9.35152ms)
Sep 27 02:41:02.157: INFO: (7) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">test<... (200; 9.187175ms)
Sep 27 02:41:02.157: INFO: (7) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/rewriteme">test</a> (200; 9.261451ms)
Sep 27 02:41:02.157: INFO: (7) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 9.46403ms)
Sep 27 02:41:02.157: INFO: (7) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname2/proxy/: tls qux (200; 9.912916ms)
Sep 27 02:41:02.158: INFO: (7) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/tlsrewritem... (200; 10.73672ms)
Sep 27 02:41:02.158: INFO: (7) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:460/proxy/: tls baz (200; 10.723192ms)
Sep 27 02:41:02.158: INFO: (7) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname1/proxy/: tls baz (200; 10.977971ms)
Sep 27 02:41:02.159: INFO: (7) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname2/proxy/: bar (200; 11.2335ms)
Sep 27 02:41:02.159: INFO: (7) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname1/proxy/: foo (200; 11.048265ms)
Sep 27 02:41:02.159: INFO: (7) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname2/proxy/: bar (200; 11.426435ms)
Sep 27 02:41:02.159: INFO: (7) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname1/proxy/: foo (200; 11.447669ms)
Sep 27 02:41:02.169: INFO: (8) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:460/proxy/: tls baz (200; 8.308819ms)
Sep 27 02:41:02.169: INFO: (8) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:462/proxy/: tls qux (200; 8.015166ms)
Sep 27 02:41:02.169: INFO: (8) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/rewriteme">test</a> (200; 9.374483ms)
Sep 27 02:41:02.169: INFO: (8) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 9.061509ms)
Sep 27 02:41:02.169: INFO: (8) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 9.254497ms)
Sep 27 02:41:02.169: INFO: (8) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 8.188766ms)
Sep 27 02:41:02.169: INFO: (8) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/tlsrewritem... (200; 8.586812ms)
Sep 27 02:41:02.169: INFO: (8) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">test<... (200; 8.857383ms)
Sep 27 02:41:02.169: INFO: (8) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 9.127665ms)
Sep 27 02:41:02.169: INFO: (8) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">... (200; 9.190784ms)
Sep 27 02:41:02.170: INFO: (8) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname2/proxy/: bar (200; 9.456557ms)
Sep 27 02:41:02.170: INFO: (8) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname2/proxy/: bar (200; 10.296907ms)
Sep 27 02:41:02.170: INFO: (8) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname1/proxy/: foo (200; 10.110817ms)
Sep 27 02:41:02.170: INFO: (8) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname1/proxy/: tls baz (200; 10.055062ms)
Sep 27 02:41:02.170: INFO: (8) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname1/proxy/: foo (200; 9.528948ms)
Sep 27 02:41:02.170: INFO: (8) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname2/proxy/: tls qux (200; 9.815693ms)
Sep 27 02:41:02.180: INFO: (9) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/rewriteme">test</a> (200; 8.325317ms)
Sep 27 02:41:02.180: INFO: (9) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 9.262396ms)
Sep 27 02:41:02.180: INFO: (9) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/tlsrewritem... (200; 8.885856ms)
Sep 27 02:41:02.181: INFO: (9) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 9.688122ms)
Sep 27 02:41:02.181: INFO: (9) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 9.62332ms)
Sep 27 02:41:02.181: INFO: (9) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname2/proxy/: tls qux (200; 9.105882ms)
Sep 27 02:41:02.181: INFO: (9) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:462/proxy/: tls qux (200; 10.235797ms)
Sep 27 02:41:02.181: INFO: (9) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:460/proxy/: tls baz (200; 8.937761ms)
Sep 27 02:41:02.181: INFO: (9) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">... (200; 9.940728ms)
Sep 27 02:41:02.181: INFO: (9) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">test<... (200; 9.379684ms)
Sep 27 02:41:02.181: INFO: (9) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname1/proxy/: tls baz (200; 9.487422ms)
Sep 27 02:41:02.181: INFO: (9) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 8.735287ms)
Sep 27 02:41:02.181: INFO: (9) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname2/proxy/: bar (200; 10.207221ms)
Sep 27 02:41:02.181: INFO: (9) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname1/proxy/: foo (200; 8.999428ms)
Sep 27 02:41:02.181: INFO: (9) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname1/proxy/: foo (200; 10.358718ms)
Sep 27 02:41:02.183: INFO: (9) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname2/proxy/: bar (200; 11.192496ms)
Sep 27 02:41:02.195: INFO: (10) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 11.084739ms)
Sep 27 02:41:02.195: INFO: (10) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:462/proxy/: tls qux (200; 11.717715ms)
Sep 27 02:41:02.195: INFO: (10) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:460/proxy/: tls baz (200; 12.23208ms)
Sep 27 02:41:02.198: INFO: (10) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 14.13756ms)
Sep 27 02:41:02.198: INFO: (10) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname1/proxy/: tls baz (200; 13.993702ms)
Sep 27 02:41:02.198: INFO: (10) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname2/proxy/: tls qux (200; 13.778579ms)
Sep 27 02:41:02.198: INFO: (10) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname2/proxy/: bar (200; 14.243165ms)
Sep 27 02:41:02.198: INFO: (10) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname2/proxy/: bar (200; 15.10958ms)
Sep 27 02:41:02.198: INFO: (10) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname1/proxy/: foo (200; 14.588319ms)
Sep 27 02:41:02.198: INFO: (10) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 14.818327ms)
Sep 27 02:41:02.198: INFO: (10) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 15.42897ms)
Sep 27 02:41:02.198: INFO: (10) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">... (200; 15.076458ms)
Sep 27 02:41:02.198: INFO: (10) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">test<... (200; 14.486618ms)
Sep 27 02:41:02.199: INFO: (10) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname1/proxy/: foo (200; 15.727711ms)
Sep 27 02:41:02.199: INFO: (10) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/tlsrewritem... (200; 14.291704ms)
Sep 27 02:41:02.199: INFO: (10) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/rewriteme">test</a> (200; 15.976285ms)
Sep 27 02:41:02.206: INFO: (11) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/tlsrewritem... (200; 7.086567ms)
Sep 27 02:41:02.209: INFO: (11) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">test<... (200; 8.207508ms)
Sep 27 02:41:02.209: INFO: (11) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname1/proxy/: tls baz (200; 9.690511ms)
Sep 27 02:41:02.209: INFO: (11) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 9.663612ms)
Sep 27 02:41:02.209: INFO: (11) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 9.308691ms)
Sep 27 02:41:02.209: INFO: (11) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">... (200; 8.984125ms)
Sep 27 02:41:02.209: INFO: (11) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 9.261935ms)
Sep 27 02:41:02.209: INFO: (11) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 9.204358ms)
Sep 27 02:41:02.209: INFO: (11) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/rewriteme">test</a> (200; 9.636512ms)
Sep 27 02:41:02.210: INFO: (11) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:460/proxy/: tls baz (200; 10.873443ms)
Sep 27 02:41:02.210: INFO: (11) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname1/proxy/: foo (200; 10.368837ms)
Sep 27 02:41:02.210: INFO: (11) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname1/proxy/: foo (200; 9.66516ms)
Sep 27 02:41:02.210: INFO: (11) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname2/proxy/: bar (200; 10.024775ms)
Sep 27 02:41:02.211: INFO: (11) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:462/proxy/: tls qux (200; 11.219973ms)
Sep 27 02:41:02.211: INFO: (11) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname2/proxy/: bar (200; 11.847916ms)
Sep 27 02:41:02.211: INFO: (11) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname2/proxy/: tls qux (200; 11.788605ms)
Sep 27 02:41:02.217: INFO: (12) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 5.751234ms)
Sep 27 02:41:02.217: INFO: (12) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">test<... (200; 5.403286ms)
Sep 27 02:41:02.221: INFO: (12) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:460/proxy/: tls baz (200; 8.988115ms)
Sep 27 02:41:02.221: INFO: (12) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 8.305615ms)
Sep 27 02:41:02.221: INFO: (12) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/rewriteme">test</a> (200; 9.016606ms)
Sep 27 02:41:02.221: INFO: (12) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname2/proxy/: bar (200; 9.563535ms)
Sep 27 02:41:02.221: INFO: (12) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:462/proxy/: tls qux (200; 8.887578ms)
Sep 27 02:41:02.221: INFO: (12) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 8.495711ms)
Sep 27 02:41:02.221: INFO: (12) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname1/proxy/: tls baz (200; 9.836952ms)
Sep 27 02:41:02.221: INFO: (12) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">... (200; 8.740607ms)
Sep 27 02:41:02.221: INFO: (12) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 9.085561ms)
Sep 27 02:41:02.222: INFO: (12) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname1/proxy/: foo (200; 10.151022ms)
Sep 27 02:41:02.222: INFO: (12) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/tlsrewritem... (200; 10.183072ms)
Sep 27 02:41:02.223: INFO: (12) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname1/proxy/: foo (200; 11.016665ms)
Sep 27 02:41:02.223: INFO: (12) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname2/proxy/: tls qux (200; 11.492471ms)
Sep 27 02:41:02.223: INFO: (12) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname2/proxy/: bar (200; 10.775347ms)
Sep 27 02:41:02.232: INFO: (13) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">... (200; 8.72828ms)
Sep 27 02:41:02.233: INFO: (13) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:462/proxy/: tls qux (200; 9.560933ms)
Sep 27 02:41:02.234: INFO: (13) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">test<... (200; 9.429117ms)
Sep 27 02:41:02.234: INFO: (13) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 10.236052ms)
Sep 27 02:41:02.235: INFO: (13) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 9.554277ms)
Sep 27 02:41:02.235: INFO: (13) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 10.718015ms)
Sep 27 02:41:02.235: INFO: (13) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/rewriteme">test</a> (200; 9.728972ms)
Sep 27 02:41:02.235: INFO: (13) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:460/proxy/: tls baz (200; 9.924663ms)
Sep 27 02:41:02.235: INFO: (13) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 10.695119ms)
Sep 27 02:41:02.235: INFO: (13) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/tlsrewritem... (200; 10.040136ms)
Sep 27 02:41:02.235: INFO: (13) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname2/proxy/: bar (200; 11.039184ms)
Sep 27 02:41:02.236: INFO: (13) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname1/proxy/: tls baz (200; 12.025378ms)
Sep 27 02:41:02.237: INFO: (13) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname1/proxy/: foo (200; 12.968971ms)
Sep 27 02:41:02.237: INFO: (13) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname2/proxy/: bar (200; 12.943897ms)
Sep 27 02:41:02.237: INFO: (13) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname1/proxy/: foo (200; 12.641735ms)
Sep 27 02:41:02.237: INFO: (13) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname2/proxy/: tls qux (200; 12.924122ms)
Sep 27 02:41:02.244: INFO: (14) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:460/proxy/: tls baz (200; 6.004727ms)
Sep 27 02:41:02.244: INFO: (14) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 6.612423ms)
Sep 27 02:41:02.246: INFO: (14) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">... (200; 7.98865ms)
Sep 27 02:41:02.246: INFO: (14) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname1/proxy/: foo (200; 7.878381ms)
Sep 27 02:41:02.246: INFO: (14) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 8.281127ms)
Sep 27 02:41:02.247: INFO: (14) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/tlsrewritem... (200; 8.435122ms)
Sep 27 02:41:02.247: INFO: (14) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 8.128304ms)
Sep 27 02:41:02.247: INFO: (14) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">test<... (200; 8.74539ms)
Sep 27 02:41:02.247: INFO: (14) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/rewriteme">test</a> (200; 9.330828ms)
Sep 27 02:41:02.247: INFO: (14) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:462/proxy/: tls qux (200; 8.10785ms)
Sep 27 02:41:02.247: INFO: (14) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname2/proxy/: bar (200; 8.992355ms)
Sep 27 02:41:02.247: INFO: (14) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 9.308715ms)
Sep 27 02:41:02.247: INFO: (14) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname2/proxy/: bar (200; 8.796332ms)
Sep 27 02:41:02.247: INFO: (14) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname1/proxy/: foo (200; 8.493252ms)
Sep 27 02:41:02.247: INFO: (14) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname1/proxy/: tls baz (200; 9.116974ms)
Sep 27 02:41:02.249: INFO: (14) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname2/proxy/: tls qux (200; 10.764454ms)
Sep 27 02:41:02.256: INFO: (15) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 6.299664ms)
Sep 27 02:41:02.256: INFO: (15) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:462/proxy/: tls qux (200; 6.305543ms)
Sep 27 02:41:02.256: INFO: (15) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 5.780384ms)
Sep 27 02:41:02.256: INFO: (15) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 6.021874ms)
Sep 27 02:41:02.257: INFO: (15) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">... (200; 5.797955ms)
Sep 27 02:41:02.257: INFO: (15) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:460/proxy/: tls baz (200; 6.966303ms)
Sep 27 02:41:02.257: INFO: (15) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 6.15025ms)
Sep 27 02:41:02.257: INFO: (15) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/tlsrewritem... (200; 7.119529ms)
Sep 27 02:41:02.257: INFO: (15) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/rewriteme">test</a> (200; 6.413692ms)
Sep 27 02:41:02.257: INFO: (15) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">test<... (200; 5.84165ms)
Sep 27 02:41:02.259: INFO: (15) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname2/proxy/: bar (200; 8.364252ms)
Sep 27 02:41:02.259: INFO: (15) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname2/proxy/: bar (200; 9.35121ms)
Sep 27 02:41:02.260: INFO: (15) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname1/proxy/: tls baz (200; 10.231368ms)
Sep 27 02:41:02.260: INFO: (15) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname1/proxy/: foo (200; 9.485248ms)
Sep 27 02:41:02.260: INFO: (15) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname1/proxy/: foo (200; 8.761966ms)
Sep 27 02:41:02.263: INFO: (15) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname2/proxy/: tls qux (200; 12.927344ms)
Sep 27 02:41:02.272: INFO: (16) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">... (200; 7.91183ms)
Sep 27 02:41:02.272: INFO: (16) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 8.31252ms)
Sep 27 02:41:02.272: INFO: (16) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname2/proxy/: bar (200; 8.581318ms)
Sep 27 02:41:02.272: INFO: (16) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 8.396595ms)
Sep 27 02:41:02.272: INFO: (16) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/tlsrewritem... (200; 7.561917ms)
Sep 27 02:41:02.272: INFO: (16) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">test<... (200; 7.939198ms)
Sep 27 02:41:02.272: INFO: (16) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/rewriteme">test</a> (200; 9.023398ms)
Sep 27 02:41:02.272: INFO: (16) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 8.408896ms)
Sep 27 02:41:02.272: INFO: (16) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:460/proxy/: tls baz (200; 9.353398ms)
Sep 27 02:41:02.273: INFO: (16) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 9.134608ms)
Sep 27 02:41:02.273: INFO: (16) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname1/proxy/: foo (200; 9.236582ms)
Sep 27 02:41:02.273: INFO: (16) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname2/proxy/: tls qux (200; 8.834772ms)
Sep 27 02:41:02.273: INFO: (16) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname1/proxy/: tls baz (200; 9.197543ms)
Sep 27 02:41:02.273: INFO: (16) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:462/proxy/: tls qux (200; 9.936513ms)
Sep 27 02:41:02.273: INFO: (16) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname2/proxy/: bar (200; 9.146461ms)
Sep 27 02:41:02.274: INFO: (16) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname1/proxy/: foo (200; 10.493053ms)
Sep 27 02:41:02.280: INFO: (17) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 5.521754ms)
Sep 27 02:41:02.280: INFO: (17) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:460/proxy/: tls baz (200; 6.316249ms)
Sep 27 02:41:02.281: INFO: (17) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 6.133696ms)
Sep 27 02:41:02.281: INFO: (17) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 6.728162ms)
Sep 27 02:41:02.281: INFO: (17) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/rewriteme">test</a> (200; 6.979278ms)
Sep 27 02:41:02.281: INFO: (17) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">test<... (200; 7.734809ms)
Sep 27 02:41:02.285: INFO: (17) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/tlsrewritem... (200; 10.853091ms)
Sep 27 02:41:02.285: INFO: (17) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">... (200; 10.202007ms)
Sep 27 02:41:02.285: INFO: (17) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname2/proxy/: bar (200; 10.267547ms)
Sep 27 02:41:02.285: INFO: (17) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 10.558673ms)
Sep 27 02:41:02.285: INFO: (17) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname2/proxy/: tls qux (200; 11.086367ms)
Sep 27 02:41:02.285: INFO: (17) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:462/proxy/: tls qux (200; 10.365525ms)
Sep 27 02:41:02.286: INFO: (17) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname2/proxy/: bar (200; 11.872114ms)
Sep 27 02:41:02.286: INFO: (17) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname1/proxy/: tls baz (200; 10.856482ms)
Sep 27 02:41:02.286: INFO: (17) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname1/proxy/: foo (200; 11.489253ms)
Sep 27 02:41:02.286: INFO: (17) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname1/proxy/: foo (200; 10.950713ms)
Sep 27 02:41:02.293: INFO: (18) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname1/proxy/: tls baz (200; 5.767247ms)
Sep 27 02:41:02.293: INFO: (18) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">... (200; 6.338744ms)
Sep 27 02:41:02.293: INFO: (18) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 6.615945ms)
Sep 27 02:41:02.293: INFO: (18) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">test<... (200; 5.766584ms)
Sep 27 02:41:02.293: INFO: (18) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/rewriteme">test</a> (200; 6.740043ms)
Sep 27 02:41:02.293: INFO: (18) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 6.128887ms)
Sep 27 02:41:02.293: INFO: (18) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/tlsrewritem... (200; 5.542211ms)
Sep 27 02:41:02.293: INFO: (18) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 6.397002ms)
Sep 27 02:41:02.293: INFO: (18) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:462/proxy/: tls qux (200; 6.684897ms)
Sep 27 02:41:02.293: INFO: (18) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname1/proxy/: foo (200; 6.941263ms)
Sep 27 02:41:02.293: INFO: (18) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:460/proxy/: tls baz (200; 7.095601ms)
Sep 27 02:41:02.293: INFO: (18) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname2/proxy/: tls qux (200; 5.826863ms)
Sep 27 02:41:02.294: INFO: (18) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname1/proxy/: foo (200; 7.584158ms)
Sep 27 02:41:02.294: INFO: (18) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname2/proxy/: bar (200; 8.121821ms)
Sep 27 02:41:02.294: INFO: (18) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname2/proxy/: bar (200; 7.3973ms)
Sep 27 02:41:02.294: INFO: (18) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 7.977476ms)
Sep 27 02:41:02.308: INFO: (19) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname2/proxy/: bar (200; 12.730471ms)
Sep 27 02:41:02.308: INFO: (19) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 12.567004ms)
Sep 27 02:41:02.308: INFO: (19) /api/v1/namespaces/proxy-9532/services/proxy-service-9gwbb:portname1/proxy/: foo (200; 13.733845ms)
Sep 27 02:41:02.308: INFO: (19) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">... (200; 12.766003ms)
Sep 27 02:41:02.308: INFO: (19) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:443/proxy/tlsrewritem... (200; 13.3421ms)
Sep 27 02:41:02.309: INFO: (19) /api/v1/namespaces/proxy-9532/pods/http:proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 13.999788ms)
Sep 27 02:41:02.309: INFO: (19) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname2/proxy/: bar (200; 13.561731ms)
Sep 27 02:41:02.309: INFO: (19) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:462/proxy/: tls qux (200; 13.053037ms)
Sep 27 02:41:02.309: INFO: (19) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:1080/proxy/rewriteme">test<... (200; 13.683595ms)
Sep 27 02:41:02.309: INFO: (19) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname1/proxy/: tls baz (200; 13.798952ms)
Sep 27 02:41:02.309: INFO: (19) /api/v1/namespaces/proxy-9532/services/http:proxy-service-9gwbb:portname1/proxy/: foo (200; 13.314774ms)
Sep 27 02:41:02.309: INFO: (19) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:160/proxy/: foo (200; 12.861733ms)
Sep 27 02:41:02.309: INFO: (19) /api/v1/namespaces/proxy-9532/pods/https:proxy-service-9gwbb-tqkfr:460/proxy/: tls baz (200; 13.420561ms)
Sep 27 02:41:02.309: INFO: (19) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/: <a href="/api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr/proxy/rewriteme">test</a> (200; 13.292738ms)
Sep 27 02:41:02.309: INFO: (19) /api/v1/namespaces/proxy-9532/pods/proxy-service-9gwbb-tqkfr:162/proxy/: bar (200; 13.236347ms)
Sep 27 02:41:02.309: INFO: (19) /api/v1/namespaces/proxy-9532/services/https:proxy-service-9gwbb:tlsportname2/proxy/: tls qux (200; 13.628886ms)
STEP: deleting ReplicationController proxy-service-9gwbb in namespace proxy-9532, will wait for the garbage collector to delete the pods
Sep 27 02:41:02.372: INFO: Deleting ReplicationController proxy-service-9gwbb took: 10.626561ms
Sep 27 02:41:02.972: INFO: Terminating ReplicationController proxy-service-9gwbb pods took: 600.220021ms
[AfterEach] version v1
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:41:18.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9532" for this suite.

â€¢ [SLOW TEST:36.400 seconds]
[sig-network] Proxy
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":277,"completed":2,"skipped":13,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:41:18.761: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7685
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 27 02:41:20.114: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0af2416b-9bac-4055-acd7-edfda0214538" in namespace "downward-api-7685" to be "Succeeded or Failed"
Sep 27 02:41:20.196: INFO: Pod "downwardapi-volume-0af2416b-9bac-4055-acd7-edfda0214538": Phase="Pending", Reason="", readiness=false. Elapsed: 81.98134ms
Sep 27 02:41:22.199: INFO: Pod "downwardapi-volume-0af2416b-9bac-4055-acd7-edfda0214538": Phase="Pending", Reason="", readiness=false. Elapsed: 2.085186299s
Sep 27 02:41:24.230: INFO: Pod "downwardapi-volume-0af2416b-9bac-4055-acd7-edfda0214538": Phase="Pending", Reason="", readiness=false. Elapsed: 4.115936286s
Sep 27 02:41:26.354: INFO: Pod "downwardapi-volume-0af2416b-9bac-4055-acd7-edfda0214538": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.239601443s
STEP: Saw pod success
Sep 27 02:41:26.354: INFO: Pod "downwardapi-volume-0af2416b-9bac-4055-acd7-edfda0214538" satisfied condition "Succeeded or Failed"
Sep 27 02:41:26.358: INFO: Trying to get logs from node dce-10-6-171-86 pod downwardapi-volume-0af2416b-9bac-4055-acd7-edfda0214538 container client-container: <nil>
STEP: delete the pod
Sep 27 02:41:26.846: INFO: Waiting for pod downwardapi-volume-0af2416b-9bac-4055-acd7-edfda0214538 to disappear
Sep 27 02:41:26.849: INFO: Pod downwardapi-volume-0af2416b-9bac-4055-acd7-edfda0214538 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:41:26.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7685" for this suite.

â€¢ [SLOW TEST:8.100 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":3,"skipped":46,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:41:26.861: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9719
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-96260569-3b1d-47ea-840f-c934dd7f496f
STEP: Creating a pod to test consume secrets
Sep 27 02:41:27.371: INFO: Waiting up to 5m0s for pod "pod-secrets-26fbf35e-c389-46a6-b746-62f83fadf0e2" in namespace "secrets-9719" to be "Succeeded or Failed"
Sep 27 02:41:27.377: INFO: Pod "pod-secrets-26fbf35e-c389-46a6-b746-62f83fadf0e2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.932557ms
Sep 27 02:41:29.381: INFO: Pod "pod-secrets-26fbf35e-c389-46a6-b746-62f83fadf0e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009397858s
Sep 27 02:41:31.432: INFO: Pod "pod-secrets-26fbf35e-c389-46a6-b746-62f83fadf0e2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060166302s
Sep 27 02:41:33.561: INFO: Pod "pod-secrets-26fbf35e-c389-46a6-b746-62f83fadf0e2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.189960004s
Sep 27 02:41:35.673: INFO: Pod "pod-secrets-26fbf35e-c389-46a6-b746-62f83fadf0e2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.301573343s
Sep 27 02:41:37.677: INFO: Pod "pod-secrets-26fbf35e-c389-46a6-b746-62f83fadf0e2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.305682006s
Sep 27 02:41:39.682: INFO: Pod "pod-secrets-26fbf35e-c389-46a6-b746-62f83fadf0e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.310257231s
STEP: Saw pod success
Sep 27 02:41:39.682: INFO: Pod "pod-secrets-26fbf35e-c389-46a6-b746-62f83fadf0e2" satisfied condition "Succeeded or Failed"
Sep 27 02:41:39.685: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-secrets-26fbf35e-c389-46a6-b746-62f83fadf0e2 container secret-env-test: <nil>
STEP: delete the pod
Sep 27 02:41:39.711: INFO: Waiting for pod pod-secrets-26fbf35e-c389-46a6-b746-62f83fadf0e2 to disappear
Sep 27 02:41:39.726: INFO: Pod pod-secrets-26fbf35e-c389-46a6-b746-62f83fadf0e2 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:41:39.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9719" for this suite.

â€¢ [SLOW TEST:12.880 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:35
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":277,"completed":4,"skipped":50,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:41:39.741: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8341
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test override command
Sep 27 02:41:40.444: INFO: Waiting up to 5m0s for pod "client-containers-b104ce4b-a11a-4e5c-94d8-42cb9a0a84ee" in namespace "containers-8341" to be "Succeeded or Failed"
Sep 27 02:41:40.448: INFO: Pod "client-containers-b104ce4b-a11a-4e5c-94d8-42cb9a0a84ee": Phase="Pending", Reason="", readiness=false. Elapsed: 3.412799ms
Sep 27 02:41:42.820: INFO: Pod "client-containers-b104ce4b-a11a-4e5c-94d8-42cb9a0a84ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.376286196s
Sep 27 02:41:44.866: INFO: Pod "client-containers-b104ce4b-a11a-4e5c-94d8-42cb9a0a84ee": Phase="Pending", Reason="", readiness=false. Elapsed: 4.42156095s
Sep 27 02:41:47.130: INFO: Pod "client-containers-b104ce4b-a11a-4e5c-94d8-42cb9a0a84ee": Phase="Pending", Reason="", readiness=false. Elapsed: 6.686088514s
Sep 27 02:41:49.133: INFO: Pod "client-containers-b104ce4b-a11a-4e5c-94d8-42cb9a0a84ee": Phase="Pending", Reason="", readiness=false. Elapsed: 8.689365703s
Sep 27 02:41:51.137: INFO: Pod "client-containers-b104ce4b-a11a-4e5c-94d8-42cb9a0a84ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.692753413s
STEP: Saw pod success
Sep 27 02:41:51.137: INFO: Pod "client-containers-b104ce4b-a11a-4e5c-94d8-42cb9a0a84ee" satisfied condition "Succeeded or Failed"
Sep 27 02:41:51.140: INFO: Trying to get logs from node dce-10-6-171-86 pod client-containers-b104ce4b-a11a-4e5c-94d8-42cb9a0a84ee container test-container: <nil>
STEP: delete the pod
Sep 27 02:41:51.205: INFO: Waiting for pod client-containers-b104ce4b-a11a-4e5c-94d8-42cb9a0a84ee to disappear
Sep 27 02:41:51.211: INFO: Pod client-containers-b104ce4b-a11a-4e5c-94d8-42cb9a0a84ee no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:41:51.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8341" for this suite.

â€¢ [SLOW TEST:11.509 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":277,"completed":5,"skipped":61,"failed":0}
SS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:41:51.250: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-4139
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 02:41:51.699: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-e9136453-c30a-4af7-ac9c-cca6f2fe5579" in namespace "security-context-test-4139" to be "Succeeded or Failed"
Sep 27 02:41:51.704: INFO: Pod "busybox-privileged-false-e9136453-c30a-4af7-ac9c-cca6f2fe5579": Phase="Pending", Reason="", readiness=false. Elapsed: 5.824369ms
Sep 27 02:41:53.957: INFO: Pod "busybox-privileged-false-e9136453-c30a-4af7-ac9c-cca6f2fe5579": Phase="Pending", Reason="", readiness=false. Elapsed: 2.258840125s
Sep 27 02:41:55.989: INFO: Pod "busybox-privileged-false-e9136453-c30a-4af7-ac9c-cca6f2fe5579": Phase="Pending", Reason="", readiness=false. Elapsed: 4.289982078s
Sep 27 02:41:58.216: INFO: Pod "busybox-privileged-false-e9136453-c30a-4af7-ac9c-cca6f2fe5579": Phase="Pending", Reason="", readiness=false. Elapsed: 6.51753388s
Sep 27 02:42:00.220: INFO: Pod "busybox-privileged-false-e9136453-c30a-4af7-ac9c-cca6f2fe5579": Phase="Pending", Reason="", readiness=false. Elapsed: 8.521373109s
Sep 27 02:42:02.223: INFO: Pod "busybox-privileged-false-e9136453-c30a-4af7-ac9c-cca6f2fe5579": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.524736354s
Sep 27 02:42:02.223: INFO: Pod "busybox-privileged-false-e9136453-c30a-4af7-ac9c-cca6f2fe5579" satisfied condition "Succeeded or Failed"
Sep 27 02:42:02.230: INFO: Got logs for pod "busybox-privileged-false-e9136453-c30a-4af7-ac9c-cca6f2fe5579": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:42:02.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4139" for this suite.

â€¢ [SLOW TEST:10.989 seconds]
[k8s.io] Security Context
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  When creating a pod with privileged
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:227
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":6,"skipped":63,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:42:02.239: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4584
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-4584
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 27 02:42:02.548: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 27 02:42:02.591: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 02:42:04.616: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 02:42:06.600: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 02:42:08.781: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 02:42:10.594: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 02:42:12.595: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 02:42:14.594: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 02:42:16.659: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 02:42:18.595: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 02:42:20.698: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 02:42:22.618: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 02:42:24.595: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 02:42:26.595: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep 27 02:42:26.598: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Sep 27 02:42:36.616: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.29.108.66:8080/dial?request=hostname&protocol=udp&host=172.29.74.234&port=8081&tries=1'] Namespace:pod-network-test-4584 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 02:42:36.616: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
Sep 27 02:42:36.784: INFO: Waiting for responses: map[]
Sep 27 02:42:36.788: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.29.108.66:8080/dial?request=hostname&protocol=udp&host=172.29.108.68&port=8081&tries=1'] Namespace:pod-network-test-4584 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 02:42:36.788: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
Sep 27 02:42:36.942: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:42:36.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4584" for this suite.

â€¢ [SLOW TEST:34.715 seconds]
[sig-network] Networking
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":277,"completed":7,"skipped":84,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:42:36.954: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7267
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:42:37.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7267" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":277,"completed":8,"skipped":93,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:42:37.396: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2769
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 27 02:42:37.646: INFO: Waiting up to 5m0s for pod "downwardapi-volume-03f00125-3ae1-4aa0-972c-63d74f83c646" in namespace "projected-2769" to be "Succeeded or Failed"
Sep 27 02:42:37.662: INFO: Pod "downwardapi-volume-03f00125-3ae1-4aa0-972c-63d74f83c646": Phase="Pending", Reason="", readiness=false. Elapsed: 16.859016ms
Sep 27 02:42:39.666: INFO: Pod "downwardapi-volume-03f00125-3ae1-4aa0-972c-63d74f83c646": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020144042s
Sep 27 02:42:41.669: INFO: Pod "downwardapi-volume-03f00125-3ae1-4aa0-972c-63d74f83c646": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023182909s
Sep 27 02:42:44.446: INFO: Pod "downwardapi-volume-03f00125-3ae1-4aa0-972c-63d74f83c646": Phase="Pending", Reason="", readiness=false. Elapsed: 6.800136897s
Sep 27 02:42:46.518: INFO: Pod "downwardapi-volume-03f00125-3ae1-4aa0-972c-63d74f83c646": Phase="Pending", Reason="", readiness=false. Elapsed: 8.872701974s
Sep 27 02:42:48.526: INFO: Pod "downwardapi-volume-03f00125-3ae1-4aa0-972c-63d74f83c646": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.87991105s
STEP: Saw pod success
Sep 27 02:42:48.526: INFO: Pod "downwardapi-volume-03f00125-3ae1-4aa0-972c-63d74f83c646" satisfied condition "Succeeded or Failed"
Sep 27 02:42:48.529: INFO: Trying to get logs from node dce-10-6-171-86 pod downwardapi-volume-03f00125-3ae1-4aa0-972c-63d74f83c646 container client-container: <nil>
STEP: delete the pod
Sep 27 02:42:48.551: INFO: Waiting for pod downwardapi-volume-03f00125-3ae1-4aa0-972c-63d74f83c646 to disappear
Sep 27 02:42:48.558: INFO: Pod downwardapi-volume-03f00125-3ae1-4aa0-972c-63d74f83c646 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:42:48.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2769" for this suite.

â€¢ [SLOW TEST:11.173 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":277,"completed":9,"skipped":98,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:42:48.569: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4882
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-map-59e5bae1-45ce-4f4e-8696-efb3c22d8530
STEP: Creating a pod to test consume configMaps
Sep 27 02:42:48.769: INFO: Waiting up to 5m0s for pod "pod-configmaps-edf6213b-cebe-4da5-b3a6-e71c5d4f5f99" in namespace "configmap-4882" to be "Succeeded or Failed"
Sep 27 02:42:48.772: INFO: Pod "pod-configmaps-edf6213b-cebe-4da5-b3a6-e71c5d4f5f99": Phase="Pending", Reason="", readiness=false. Elapsed: 3.300279ms
Sep 27 02:42:50.778: INFO: Pod "pod-configmaps-edf6213b-cebe-4da5-b3a6-e71c5d4f5f99": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009319517s
Sep 27 02:42:52.783: INFO: Pod "pod-configmaps-edf6213b-cebe-4da5-b3a6-e71c5d4f5f99": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014650228s
Sep 27 02:42:54.935: INFO: Pod "pod-configmaps-edf6213b-cebe-4da5-b3a6-e71c5d4f5f99": Phase="Pending", Reason="", readiness=false. Elapsed: 6.16614923s
Sep 27 02:42:57.085: INFO: Pod "pod-configmaps-edf6213b-cebe-4da5-b3a6-e71c5d4f5f99": Phase="Pending", Reason="", readiness=false. Elapsed: 8.316541513s
Sep 27 02:42:59.089: INFO: Pod "pod-configmaps-edf6213b-cebe-4da5-b3a6-e71c5d4f5f99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.320681892s
STEP: Saw pod success
Sep 27 02:42:59.089: INFO: Pod "pod-configmaps-edf6213b-cebe-4da5-b3a6-e71c5d4f5f99" satisfied condition "Succeeded or Failed"
Sep 27 02:42:59.097: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-configmaps-edf6213b-cebe-4da5-b3a6-e71c5d4f5f99 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 27 02:42:59.140: INFO: Waiting for pod pod-configmaps-edf6213b-cebe-4da5-b3a6-e71c5d4f5f99 to disappear
Sep 27 02:42:59.183: INFO: Pod pod-configmaps-edf6213b-cebe-4da5-b3a6-e71c5d4f5f99 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:42:59.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4882" for this suite.

â€¢ [SLOW TEST:10.655 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":277,"completed":10,"skipped":100,"failed":0}
S
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:42:59.225: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1386
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:43:06.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1386" for this suite.

â€¢ [SLOW TEST:7.307 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":277,"completed":11,"skipped":101,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:43:06.532: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3795
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:43:24.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3795" for this suite.

â€¢ [SLOW TEST:17.594 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":277,"completed":12,"skipped":132,"failed":0}
SSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:43:24.126: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2006
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 02:43:24.649: INFO: The status of Pod test-webserver-f1695479-490f-48d5-a5f8-eeeedf0b97f4 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 02:43:26.656: INFO: The status of Pod test-webserver-f1695479-490f-48d5-a5f8-eeeedf0b97f4 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 02:43:28.731: INFO: The status of Pod test-webserver-f1695479-490f-48d5-a5f8-eeeedf0b97f4 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 02:43:30.653: INFO: The status of Pod test-webserver-f1695479-490f-48d5-a5f8-eeeedf0b97f4 is Running (Ready = false)
Sep 27 02:43:32.737: INFO: The status of Pod test-webserver-f1695479-490f-48d5-a5f8-eeeedf0b97f4 is Running (Ready = false)
Sep 27 02:43:34.654: INFO: The status of Pod test-webserver-f1695479-490f-48d5-a5f8-eeeedf0b97f4 is Running (Ready = false)
Sep 27 02:43:36.653: INFO: The status of Pod test-webserver-f1695479-490f-48d5-a5f8-eeeedf0b97f4 is Running (Ready = false)
Sep 27 02:43:38.653: INFO: The status of Pod test-webserver-f1695479-490f-48d5-a5f8-eeeedf0b97f4 is Running (Ready = false)
Sep 27 02:43:40.653: INFO: The status of Pod test-webserver-f1695479-490f-48d5-a5f8-eeeedf0b97f4 is Running (Ready = false)
Sep 27 02:43:42.653: INFO: The status of Pod test-webserver-f1695479-490f-48d5-a5f8-eeeedf0b97f4 is Running (Ready = false)
Sep 27 02:43:44.750: INFO: The status of Pod test-webserver-f1695479-490f-48d5-a5f8-eeeedf0b97f4 is Running (Ready = false)
Sep 27 02:43:46.654: INFO: The status of Pod test-webserver-f1695479-490f-48d5-a5f8-eeeedf0b97f4 is Running (Ready = false)
Sep 27 02:43:48.654: INFO: The status of Pod test-webserver-f1695479-490f-48d5-a5f8-eeeedf0b97f4 is Running (Ready = false)
Sep 27 02:43:50.653: INFO: The status of Pod test-webserver-f1695479-490f-48d5-a5f8-eeeedf0b97f4 is Running (Ready = true)
Sep 27 02:43:50.657: INFO: Container started at 2020-09-27 02:43:29 +0000 UTC, pod became ready at 2020-09-27 02:43:50 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:43:50.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2006" for this suite.

â€¢ [SLOW TEST:26.541 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":277,"completed":13,"skipped":136,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:43:50.668: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8073
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 02:43:50.954: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:44:05.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8073" for this suite.

â€¢ [SLOW TEST:14.816 seconds]
[k8s.io] Pods
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":277,"completed":14,"skipped":157,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:44:05.484: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1760
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-319c3933-ab98-4211-b70a-a713be961b8c
STEP: Creating a pod to test consume configMaps
Sep 27 02:44:06.042: INFO: Waiting up to 5m0s for pod "pod-configmaps-8f71a7bf-0d62-421c-9606-745073a50b09" in namespace "configmap-1760" to be "Succeeded or Failed"
Sep 27 02:44:06.110: INFO: Pod "pod-configmaps-8f71a7bf-0d62-421c-9606-745073a50b09": Phase="Pending", Reason="", readiness=false. Elapsed: 67.574885ms
Sep 27 02:44:08.744: INFO: Pod "pod-configmaps-8f71a7bf-0d62-421c-9606-745073a50b09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.702253903s
Sep 27 02:44:10.849: INFO: Pod "pod-configmaps-8f71a7bf-0d62-421c-9606-745073a50b09": Phase="Pending", Reason="", readiness=false. Elapsed: 4.806449963s
Sep 27 02:44:12.928: INFO: Pod "pod-configmaps-8f71a7bf-0d62-421c-9606-745073a50b09": Phase="Pending", Reason="", readiness=false. Elapsed: 6.885739688s
Sep 27 02:44:14.936: INFO: Pod "pod-configmaps-8f71a7bf-0d62-421c-9606-745073a50b09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.893855714s
STEP: Saw pod success
Sep 27 02:44:14.936: INFO: Pod "pod-configmaps-8f71a7bf-0d62-421c-9606-745073a50b09" satisfied condition "Succeeded or Failed"
Sep 27 02:44:14.940: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-configmaps-8f71a7bf-0d62-421c-9606-745073a50b09 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 27 02:44:15.100: INFO: Waiting for pod pod-configmaps-8f71a7bf-0d62-421c-9606-745073a50b09 to disappear
Sep 27 02:44:15.115: INFO: Pod pod-configmaps-8f71a7bf-0d62-421c-9606-745073a50b09 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:44:15.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1760" for this suite.

â€¢ [SLOW TEST:9.681 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":277,"completed":15,"skipped":168,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:44:15.165: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-785
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl replace
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 27 02:44:15.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-785'
Sep 27 02:44:23.825: INFO: stderr: ""
Sep 27 02:44:23.825: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Sep 27 02:44:33.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pod e2e-test-httpd-pod --namespace=kubectl-785 -o json'
Sep 27 02:44:33.983: INFO: stderr: ""
Sep 27 02:44:33.983: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/ipv4pools\": \"[\\\"default-ipv4-ippool\\\"]\",\n            \"dce.daocloud.io/parcel.egress.burst\": \"0\",\n            \"dce.daocloud.io/parcel.egress.rate\": \"0\",\n            \"dce.daocloud.io/parcel.ingress.burst\": \"0\",\n            \"dce.daocloud.io/parcel.ingress.rate\": \"0\",\n            \"kubernetes.io/psp\": \"dce-psp-allow-all\"\n        },\n        \"creationTimestamp\": \"2020-09-27T02:44:23Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-785\",\n        \"resourceVersion\": \"676124\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-785/pods/e2e-test-httpd-pod\",\n        \"uid\": \"74ad10cb-b332-495b-9545-4ed00c360652\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-ck8bh\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"dce-10-6-171-86\",\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-ck8bh\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-ck8bh\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-09-27T02:44:23Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-09-27T02:44:30Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-09-27T02:44:30Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-09-27T02:44:23Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://9ab83f41415d3304a80f857b5e830a95519374616eaf951f069f867ce67a2f70\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-09-27T02:44:28Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.6.171.86\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.29.108.73\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.29.108.73\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-09-27T02:44:23Z\"\n    }\n}\n"
STEP: replace the image in the pod
Sep 27 02:44:33.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 replace -f - --namespace=kubectl-785'
Sep 27 02:44:34.626: INFO: stderr: ""
Sep 27 02:44:34.626: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
Sep 27 02:44:34.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 delete pods e2e-test-httpd-pod --namespace=kubectl-785'
Sep 27 02:44:47.502: INFO: stderr: ""
Sep 27 02:44:47.502: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:44:47.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-785" for this suite.

â€¢ [SLOW TEST:32.357 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1450
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":277,"completed":16,"skipped":169,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:44:47.524: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8973
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 27 02:44:47.960: INFO: Waiting up to 5m0s for pod "downwardapi-volume-35a20df4-e3a4-4c0b-a98c-714b85bbc41e" in namespace "projected-8973" to be "Succeeded or Failed"
Sep 27 02:44:47.965: INFO: Pod "downwardapi-volume-35a20df4-e3a4-4c0b-a98c-714b85bbc41e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.142435ms
Sep 27 02:44:49.971: INFO: Pod "downwardapi-volume-35a20df4-e3a4-4c0b-a98c-714b85bbc41e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010825121s
Sep 27 02:44:51.983: INFO: Pod "downwardapi-volume-35a20df4-e3a4-4c0b-a98c-714b85bbc41e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02302735s
Sep 27 02:44:53.987: INFO: Pod "downwardapi-volume-35a20df4-e3a4-4c0b-a98c-714b85bbc41e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027437936s
STEP: Saw pod success
Sep 27 02:44:53.987: INFO: Pod "downwardapi-volume-35a20df4-e3a4-4c0b-a98c-714b85bbc41e" satisfied condition "Succeeded or Failed"
Sep 27 02:44:53.991: INFO: Trying to get logs from node dce-10-6-171-86 pod downwardapi-volume-35a20df4-e3a4-4c0b-a98c-714b85bbc41e container client-container: <nil>
STEP: delete the pod
Sep 27 02:44:54.011: INFO: Waiting for pod downwardapi-volume-35a20df4-e3a4-4c0b-a98c-714b85bbc41e to disappear
Sep 27 02:44:54.060: INFO: Pod downwardapi-volume-35a20df4-e3a4-4c0b-a98c-714b85bbc41e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:44:54.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8973" for this suite.

â€¢ [SLOW TEST:6.547 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":277,"completed":17,"skipped":199,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:44:54.072: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9868
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-upd-42650997-e557-4162-9dbc-e5e4b59767a4
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-42650997-e557-4162-9dbc-e5e4b59767a4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:45:02.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9868" for this suite.

â€¢ [SLOW TEST:8.301 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":18,"skipped":311,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:45:02.374: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-8481
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-8481, will wait for the garbage collector to delete the pods
Sep 27 02:45:19.546: INFO: Deleting Job.batch foo took: 342.63093ms
Sep 27 02:45:21.347: INFO: Terminating Job.batch foo pods took: 1.800175245s
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:46:07.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8481" for this suite.

â€¢ [SLOW TEST:65.315 seconds]
[sig-apps] Job
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":277,"completed":19,"skipped":324,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:46:07.689: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-50
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:46:08.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-50" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702
â€¢{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":277,"completed":20,"skipped":332,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:46:08.065: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9920
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-map-ff90ae96-bf74-4a68-876e-30e6d00464de
STEP: Creating a pod to test consume secrets
Sep 27 02:46:08.368: INFO: Waiting up to 5m0s for pod "pod-secrets-c9b11c8c-6983-4c05-b161-6f809b393d13" in namespace "secrets-9920" to be "Succeeded or Failed"
Sep 27 02:46:08.389: INFO: Pod "pod-secrets-c9b11c8c-6983-4c05-b161-6f809b393d13": Phase="Pending", Reason="", readiness=false. Elapsed: 20.746653ms
Sep 27 02:46:10.468: INFO: Pod "pod-secrets-c9b11c8c-6983-4c05-b161-6f809b393d13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.100198953s
Sep 27 02:46:12.491: INFO: Pod "pod-secrets-c9b11c8c-6983-4c05-b161-6f809b393d13": Phase="Pending", Reason="", readiness=false. Elapsed: 4.122703438s
Sep 27 02:46:14.537: INFO: Pod "pod-secrets-c9b11c8c-6983-4c05-b161-6f809b393d13": Phase="Pending", Reason="", readiness=false. Elapsed: 6.168449026s
Sep 27 02:46:16.542: INFO: Pod "pod-secrets-c9b11c8c-6983-4c05-b161-6f809b393d13": Phase="Pending", Reason="", readiness=false. Elapsed: 8.173640984s
Sep 27 02:46:18.756: INFO: Pod "pod-secrets-c9b11c8c-6983-4c05-b161-6f809b393d13": Phase="Pending", Reason="", readiness=false. Elapsed: 10.387658151s
Sep 27 02:46:20.877: INFO: Pod "pod-secrets-c9b11c8c-6983-4c05-b161-6f809b393d13": Phase="Pending", Reason="", readiness=false. Elapsed: 12.508670878s
Sep 27 02:46:22.881: INFO: Pod "pod-secrets-c9b11c8c-6983-4c05-b161-6f809b393d13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.512401775s
STEP: Saw pod success
Sep 27 02:46:22.881: INFO: Pod "pod-secrets-c9b11c8c-6983-4c05-b161-6f809b393d13" satisfied condition "Succeeded or Failed"
Sep 27 02:46:22.883: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-secrets-c9b11c8c-6983-4c05-b161-6f809b393d13 container secret-volume-test: <nil>
STEP: delete the pod
Sep 27 02:46:22.914: INFO: Waiting for pod pod-secrets-c9b11c8c-6983-4c05-b161-6f809b393d13 to disappear
Sep 27 02:46:22.919: INFO: Pod pod-secrets-c9b11c8c-6983-4c05-b161-6f809b393d13 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:46:22.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9920" for this suite.

â€¢ [SLOW TEST:14.865 seconds]
[sig-storage] Secrets
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":21,"skipped":334,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:46:22.930: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-1288
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Sep 27 02:46:24.758: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Sep 27 02:46:26.770: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771584, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771584, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771584, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771584, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-65c6cd5fdf\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 02:46:28.779: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771584, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771584, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771584, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771584, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-65c6cd5fdf\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 02:46:30.908: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771584, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771584, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771584, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771584, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-65c6cd5fdf\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 02:46:32.775: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771584, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771584, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771584, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771584, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-65c6cd5fdf\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 02:46:35.844: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 02:46:35.848: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:46:37.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1288" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

â€¢ [SLOW TEST:14.546 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":277,"completed":22,"skipped":351,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:46:37.477: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-87
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: set up a multi version CRD
Sep 27 02:46:37.685: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:47:09.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-87" for this suite.

â€¢ [SLOW TEST:32.780 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":277,"completed":23,"skipped":405,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:47:10.257: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-1103
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 02:47:11.060: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:47:19.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1103" for this suite.

â€¢ [SLOW TEST:9.599 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":277,"completed":24,"skipped":417,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:47:19.857: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8576
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:47:28.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8576" for this suite.

â€¢ [SLOW TEST:8.533 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":277,"completed":25,"skipped":443,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:47:28.391: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7201
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 02:47:30.001: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 27 02:47:32.261: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771650, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771650, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771650, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771649, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 02:47:34.280: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771650, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771650, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771650, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771649, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 02:47:36.312: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771650, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771650, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771650, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771649, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 02:47:38.265: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771650, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771650, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771650, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771649, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 02:47:40.265: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771650, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771650, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771650, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771649, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 02:47:43.425: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:47:54.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7201" for this suite.
STEP: Destroying namespace "webhook-7201-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:26.278 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":277,"completed":26,"skipped":524,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:47:54.670: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6343
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-70d3964c-3dcd-45ba-a5d9-74b84d5f79ae
STEP: Creating a pod to test consume secrets
Sep 27 02:47:55.209: INFO: Waiting up to 5m0s for pod "pod-secrets-86a7bcd3-a0a0-4f07-903c-617b69cbd628" in namespace "secrets-6343" to be "Succeeded or Failed"
Sep 27 02:47:55.227: INFO: Pod "pod-secrets-86a7bcd3-a0a0-4f07-903c-617b69cbd628": Phase="Pending", Reason="", readiness=false. Elapsed: 18.497415ms
Sep 27 02:47:57.324: INFO: Pod "pod-secrets-86a7bcd3-a0a0-4f07-903c-617b69cbd628": Phase="Pending", Reason="", readiness=false. Elapsed: 2.11479861s
Sep 27 02:47:59.343: INFO: Pod "pod-secrets-86a7bcd3-a0a0-4f07-903c-617b69cbd628": Phase="Pending", Reason="", readiness=false. Elapsed: 4.134173272s
Sep 27 02:48:01.659: INFO: Pod "pod-secrets-86a7bcd3-a0a0-4f07-903c-617b69cbd628": Phase="Pending", Reason="", readiness=false. Elapsed: 6.449949928s
Sep 27 02:48:03.669: INFO: Pod "pod-secrets-86a7bcd3-a0a0-4f07-903c-617b69cbd628": Phase="Pending", Reason="", readiness=false. Elapsed: 8.459661793s
Sep 27 02:48:06.124: INFO: Pod "pod-secrets-86a7bcd3-a0a0-4f07-903c-617b69cbd628": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.914709612s
STEP: Saw pod success
Sep 27 02:48:06.124: INFO: Pod "pod-secrets-86a7bcd3-a0a0-4f07-903c-617b69cbd628" satisfied condition "Succeeded or Failed"
Sep 27 02:48:06.172: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-secrets-86a7bcd3-a0a0-4f07-903c-617b69cbd628 container secret-volume-test: <nil>
STEP: delete the pod
Sep 27 02:48:07.124: INFO: Waiting for pod pod-secrets-86a7bcd3-a0a0-4f07-903c-617b69cbd628 to disappear
Sep 27 02:48:07.209: INFO: Pod pod-secrets-86a7bcd3-a0a0-4f07-903c-617b69cbd628 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:48:07.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6343" for this suite.

â€¢ [SLOW TEST:12.864 seconds]
[sig-storage] Secrets
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":277,"completed":27,"skipped":543,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:48:07.535: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-583
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 02:48:09.102: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 27 02:48:21.157: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Sep 27 02:48:21.176: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-583 /apis/apps/v1/namespaces/deployment-583/deployments/test-cleanup-deployment 859fb95b-b137-4660-8a25-5b939e0b31e6 677374 1 2020-09-27 02:48:21 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004cbe078 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Sep 27 02:48:21.180: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Sep 27 02:48:21.180: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Sep 27 02:48:21.180: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-583 /apis/apps/v1/namespaces/deployment-583/replicasets/test-cleanup-controller a32074b8-ecd9-4cb5-bf9d-2e44045ff7e4 677375 1 2020-09-27 02:48:09 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 859fb95b-b137-4660-8a25-5b939e0b31e6 0xc004cbe497 0xc004cbe498}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004cbe508 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 27 02:48:21.187: INFO: Pod "test-cleanup-controller-c5xln" is available:
&Pod{ObjectMeta:{test-cleanup-controller-c5xln test-cleanup-controller- deployment-583 /api/v1/namespaces/deployment-583/pods/test-cleanup-controller-c5xln 58fdbe51-8a58-4014-8d1d-04868f48fa43 677372 0 2020-09-27 02:48:09 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/ipv4pools:["default-ipv4-ippool"] dce.daocloud.io/parcel.egress.burst:0 dce.daocloud.io/parcel.egress.rate:0 dce.daocloud.io/parcel.ingress.burst:0 dce.daocloud.io/parcel.ingress.rate:0 kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet test-cleanup-controller a32074b8-ecd9-4cb5-bf9d-2e44045ff7e4 0xc004580e87 0xc004580e88}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p9cl8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p9cl8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p9cl8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 02:48:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 02:48:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 02:48:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 02:48:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.6.171.86,PodIP:172.29.108.88,StartTime:2020-09-27 02:48:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-27 02:48:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://dd30dadd24c134321929aebf8b8e06306797e00be850d0427827dc5474f448a6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.29.108.88,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:48:21.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-583" for this suite.

â€¢ [SLOW TEST:13.742 seconds]
[sig-apps] Deployment
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":277,"completed":28,"skipped":552,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:48:21.277: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3046
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-7a064d86-37a7-498c-a89f-9e51ef1ddbc0
STEP: Creating a pod to test consume configMaps
Sep 27 02:48:21.696: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cb0d6ddb-219a-467c-ac07-66c70525bfb2" in namespace "projected-3046" to be "Succeeded or Failed"
Sep 27 02:48:21.720: INFO: Pod "pod-projected-configmaps-cb0d6ddb-219a-467c-ac07-66c70525bfb2": Phase="Pending", Reason="", readiness=false. Elapsed: 23.82965ms
Sep 27 02:48:23.728: INFO: Pod "pod-projected-configmaps-cb0d6ddb-219a-467c-ac07-66c70525bfb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032377433s
Sep 27 02:48:25.734: INFO: Pod "pod-projected-configmaps-cb0d6ddb-219a-467c-ac07-66c70525bfb2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037868744s
Sep 27 02:48:27.738: INFO: Pod "pod-projected-configmaps-cb0d6ddb-219a-467c-ac07-66c70525bfb2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.042419665s
Sep 27 02:48:29.821: INFO: Pod "pod-projected-configmaps-cb0d6ddb-219a-467c-ac07-66c70525bfb2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.125395345s
Sep 27 02:48:31.857: INFO: Pod "pod-projected-configmaps-cb0d6ddb-219a-467c-ac07-66c70525bfb2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.16078051s
Sep 27 02:48:34.284: INFO: Pod "pod-projected-configmaps-cb0d6ddb-219a-467c-ac07-66c70525bfb2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.588024514s
Sep 27 02:48:36.326: INFO: Pod "pod-projected-configmaps-cb0d6ddb-219a-467c-ac07-66c70525bfb2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.629864077s
Sep 27 02:48:38.331: INFO: Pod "pod-projected-configmaps-cb0d6ddb-219a-467c-ac07-66c70525bfb2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.634571745s
Sep 27 02:48:40.335: INFO: Pod "pod-projected-configmaps-cb0d6ddb-219a-467c-ac07-66c70525bfb2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.638468354s
Sep 27 02:48:42.554: INFO: Pod "pod-projected-configmaps-cb0d6ddb-219a-467c-ac07-66c70525bfb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 20.857557996s
STEP: Saw pod success
Sep 27 02:48:42.554: INFO: Pod "pod-projected-configmaps-cb0d6ddb-219a-467c-ac07-66c70525bfb2" satisfied condition "Succeeded or Failed"
Sep 27 02:48:42.638: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-projected-configmaps-cb0d6ddb-219a-467c-ac07-66c70525bfb2 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 27 02:48:42.848: INFO: Waiting for pod pod-projected-configmaps-cb0d6ddb-219a-467c-ac07-66c70525bfb2 to disappear
Sep 27 02:48:42.868: INFO: Pod pod-projected-configmaps-cb0d6ddb-219a-467c-ac07-66c70525bfb2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:48:42.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3046" for this suite.

â€¢ [SLOW TEST:21.753 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":277,"completed":29,"skipped":553,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:48:43.030: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8376
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Sep 27 02:49:24.234: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:49:24.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0927 02:49:24.234761      23 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-8376" for this suite.

â€¢ [SLOW TEST:41.282 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":277,"completed":30,"skipped":568,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:49:24.343: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7957
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating Agnhost RC
Sep 27 02:49:25.095: INFO: namespace kubectl-7957
Sep 27 02:49:25.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 create -f - --namespace=kubectl-7957'
Sep 27 02:49:26.406: INFO: stderr: ""
Sep 27 02:49:26.406: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Sep 27 02:49:27.414: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 02:49:27.414: INFO: Found 0 / 1
Sep 27 02:49:28.412: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 02:49:28.412: INFO: Found 0 / 1
Sep 27 02:49:29.411: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 02:49:29.412: INFO: Found 0 / 1
Sep 27 02:49:30.413: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 02:49:30.413: INFO: Found 0 / 1
Sep 27 02:49:31.478: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 02:49:31.478: INFO: Found 0 / 1
Sep 27 02:49:32.414: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 02:49:32.414: INFO: Found 0 / 1
Sep 27 02:49:33.418: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 02:49:33.418: INFO: Found 0 / 1
Sep 27 02:49:34.424: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 02:49:34.424: INFO: Found 0 / 1
Sep 27 02:49:35.453: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 02:49:35.453: INFO: Found 0 / 1
Sep 27 02:49:36.410: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 02:49:36.411: INFO: Found 0 / 1
Sep 27 02:49:37.419: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 02:49:37.419: INFO: Found 0 / 1
Sep 27 02:49:38.417: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 02:49:38.417: INFO: Found 0 / 1
Sep 27 02:49:39.422: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 02:49:39.422: INFO: Found 0 / 1
Sep 27 02:49:40.415: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 02:49:40.415: INFO: Found 0 / 1
Sep 27 02:49:41.507: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 02:49:41.507: INFO: Found 0 / 1
Sep 27 02:49:42.521: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 02:49:42.521: INFO: Found 0 / 1
Sep 27 02:49:43.437: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 02:49:43.437: INFO: Found 0 / 1
Sep 27 02:49:44.484: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 02:49:44.484: INFO: Found 0 / 1
Sep 27 02:49:45.457: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 02:49:45.457: INFO: Found 0 / 1
Sep 27 02:49:46.551: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 02:49:46.551: INFO: Found 0 / 1
Sep 27 02:49:48.208: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 02:49:48.208: INFO: Found 1 / 1
Sep 27 02:49:48.208: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 27 02:49:48.260: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 02:49:48.260: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 27 02:49:48.260: INFO: wait on agnhost-master startup in kubectl-7957 
Sep 27 02:49:48.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 logs agnhost-master-s9t4b agnhost-master --namespace=kubectl-7957'
Sep 27 02:49:48.778: INFO: stderr: ""
Sep 27 02:49:48.778: INFO: stdout: "Paused\n"
STEP: exposing RC
Sep 27 02:49:48.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 expose rc agnhost-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-7957'
Sep 27 02:49:49.168: INFO: stderr: ""
Sep 27 02:49:49.168: INFO: stdout: "service/rm2 exposed\n"
Sep 27 02:49:49.181: INFO: Service rm2 in namespace kubectl-7957 found.
STEP: exposing service
Sep 27 02:49:51.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-7957'
Sep 27 02:49:51.382: INFO: stderr: ""
Sep 27 02:49:51.382: INFO: stdout: "service/rm3 exposed\n"
Sep 27 02:49:51.404: INFO: Service rm3 in namespace kubectl-7957 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:49:53.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7957" for this suite.

â€¢ [SLOW TEST:29.077 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1119
    should create services for rc  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":277,"completed":31,"skipped":590,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:49:53.422: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3124
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with configMap that has name projected-configmap-test-upd-b781f9c6-7b5a-4365-9c6e-f8678dbed636
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-b781f9c6-7b5a-4365-9c6e-f8678dbed636
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:50:13.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3124" for this suite.

â€¢ [SLOW TEST:19.683 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":32,"skipped":653,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:50:13.105: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2021
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep 27 02:50:13.347: INFO: Waiting up to 5m0s for pod "pod-043a528b-8571-4654-b24f-ceed80781682" in namespace "emptydir-2021" to be "Succeeded or Failed"
Sep 27 02:50:13.353: INFO: Pod "pod-043a528b-8571-4654-b24f-ceed80781682": Phase="Pending", Reason="", readiness=false. Elapsed: 5.510448ms
Sep 27 02:50:15.357: INFO: Pod "pod-043a528b-8571-4654-b24f-ceed80781682": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009574736s
Sep 27 02:50:17.964: INFO: Pod "pod-043a528b-8571-4654-b24f-ceed80781682": Phase="Pending", Reason="", readiness=false. Elapsed: 4.616915991s
Sep 27 02:50:19.974: INFO: Pod "pod-043a528b-8571-4654-b24f-ceed80781682": Phase="Pending", Reason="", readiness=false. Elapsed: 6.627443932s
Sep 27 02:50:21.992: INFO: Pod "pod-043a528b-8571-4654-b24f-ceed80781682": Phase="Pending", Reason="", readiness=false. Elapsed: 8.644528207s
Sep 27 02:50:23.996: INFO: Pod "pod-043a528b-8571-4654-b24f-ceed80781682": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.648564398s
STEP: Saw pod success
Sep 27 02:50:23.996: INFO: Pod "pod-043a528b-8571-4654-b24f-ceed80781682" satisfied condition "Succeeded or Failed"
Sep 27 02:50:23.999: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-043a528b-8571-4654-b24f-ceed80781682 container test-container: <nil>
STEP: delete the pod
Sep 27 02:50:24.022: INFO: Waiting for pod pod-043a528b-8571-4654-b24f-ceed80781682 to disappear
Sep 27 02:50:24.035: INFO: Pod pod-043a528b-8571-4654-b24f-ceed80781682 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:50:24.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2021" for this suite.

â€¢ [SLOW TEST:10.945 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":33,"skipped":669,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:50:24.050: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6655
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 02:50:25.090: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 27 02:50:27.167: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771825, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771825, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771825, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771825, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 02:50:29.265: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771825, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771825, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771825, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771825, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 02:50:31.252: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771825, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771825, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771825, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771825, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 02:50:33.263: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771825, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771825, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771825, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771825, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 02:50:35.172: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771825, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771825, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771825, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771825, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 02:50:37.173: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771825, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771825, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771825, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771825, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 02:50:40.550: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:50:43.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6655" for this suite.
STEP: Destroying namespace "webhook-6655-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:21.209 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":277,"completed":34,"skipped":669,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:50:45.259: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-8557
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Sep 27 02:50:47.180: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Sep 27 02:50:49.269: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771847, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771847, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771847, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771847, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-65c6cd5fdf\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 02:50:51.281: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771847, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771847, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771847, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771847, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-65c6cd5fdf\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 02:50:53.275: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771847, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771847, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771847, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771847, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-65c6cd5fdf\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 02:50:55.277: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771847, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771847, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771847, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736771847, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-65c6cd5fdf\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 02:50:58.284: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 02:50:58.294: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
Sep 27 02:51:17.104: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-4985-crd failed: Post https://e2e-test-crd-conversion-webhook.crd-webhook-8557.svc:9443/crdconvert?timeout=30s: dial tcp 172.31.36.120:9443: connect: connection timed out
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:51:19.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8557" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

â€¢ [SLOW TEST:35.390 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":277,"completed":35,"skipped":672,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:51:20.650: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6969
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep 27 02:51:31.155: INFO: Successfully updated pod "pod-update-activedeadlineseconds-d53b2664-9f02-4367-8c88-9e17656a08f9"
Sep 27 02:51:31.155: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-d53b2664-9f02-4367-8c88-9e17656a08f9" in namespace "pods-6969" to be "terminated due to deadline exceeded"
Sep 27 02:51:31.159: INFO: Pod "pod-update-activedeadlineseconds-d53b2664-9f02-4367-8c88-9e17656a08f9": Phase="Running", Reason="", readiness=true. Elapsed: 3.987103ms
Sep 27 02:51:33.179: INFO: Pod "pod-update-activedeadlineseconds-d53b2664-9f02-4367-8c88-9e17656a08f9": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.023276123s
Sep 27 02:51:33.179: INFO: Pod "pod-update-activedeadlineseconds-d53b2664-9f02-4367-8c88-9e17656a08f9" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:51:33.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6969" for this suite.

â€¢ [SLOW TEST:12.562 seconds]
[k8s.io] Pods
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":277,"completed":36,"skipped":712,"failed":0}
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:51:33.212: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8830
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 27 02:51:33.732: INFO: Waiting up to 5m0s for pod "downwardapi-volume-82a4439c-84ad-4a5d-bbbc-99dcc1f81b96" in namespace "projected-8830" to be "Succeeded or Failed"
Sep 27 02:51:33.811: INFO: Pod "downwardapi-volume-82a4439c-84ad-4a5d-bbbc-99dcc1f81b96": Phase="Pending", Reason="", readiness=false. Elapsed: 79.118852ms
Sep 27 02:51:35.816: INFO: Pod "downwardapi-volume-82a4439c-84ad-4a5d-bbbc-99dcc1f81b96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.084353235s
Sep 27 02:51:37.821: INFO: Pod "downwardapi-volume-82a4439c-84ad-4a5d-bbbc-99dcc1f81b96": Phase="Pending", Reason="", readiness=false. Elapsed: 4.089476232s
Sep 27 02:51:39.826: INFO: Pod "downwardapi-volume-82a4439c-84ad-4a5d-bbbc-99dcc1f81b96": Phase="Pending", Reason="", readiness=false. Elapsed: 6.093917087s
Sep 27 02:51:42.167: INFO: Pod "downwardapi-volume-82a4439c-84ad-4a5d-bbbc-99dcc1f81b96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.435132835s
STEP: Saw pod success
Sep 27 02:51:42.167: INFO: Pod "downwardapi-volume-82a4439c-84ad-4a5d-bbbc-99dcc1f81b96" satisfied condition "Succeeded or Failed"
Sep 27 02:51:42.418: INFO: Trying to get logs from node dce-10-6-171-86 pod downwardapi-volume-82a4439c-84ad-4a5d-bbbc-99dcc1f81b96 container client-container: <nil>
STEP: delete the pod
Sep 27 02:51:44.251: INFO: Waiting for pod downwardapi-volume-82a4439c-84ad-4a5d-bbbc-99dcc1f81b96 to disappear
Sep 27 02:51:44.289: INFO: Pod downwardapi-volume-82a4439c-84ad-4a5d-bbbc-99dcc1f81b96 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:51:44.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8830" for this suite.

â€¢ [SLOW TEST:11.356 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":37,"skipped":712,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:51:44.568: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6808
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 27 02:51:46.468: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3cae0396-c920-4a3a-aba7-40583d13c2f0" in namespace "projected-6808" to be "Succeeded or Failed"
Sep 27 02:51:46.987: INFO: Pod "downwardapi-volume-3cae0396-c920-4a3a-aba7-40583d13c2f0": Phase="Pending", Reason="", readiness=false. Elapsed: 518.840682ms
Sep 27 02:51:49.092: INFO: Pod "downwardapi-volume-3cae0396-c920-4a3a-aba7-40583d13c2f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.623414406s
Sep 27 02:51:51.148: INFO: Pod "downwardapi-volume-3cae0396-c920-4a3a-aba7-40583d13c2f0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.679920353s
Sep 27 02:51:53.165: INFO: Pod "downwardapi-volume-3cae0396-c920-4a3a-aba7-40583d13c2f0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.696488858s
Sep 27 02:51:55.390: INFO: Pod "downwardapi-volume-3cae0396-c920-4a3a-aba7-40583d13c2f0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.922098617s
Sep 27 02:51:57.483: INFO: Pod "downwardapi-volume-3cae0396-c920-4a3a-aba7-40583d13c2f0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.014579825s
Sep 27 02:51:59.488: INFO: Pod "downwardapi-volume-3cae0396-c920-4a3a-aba7-40583d13c2f0": Phase="Pending", Reason="", readiness=false. Elapsed: 13.01925698s
Sep 27 02:52:01.493: INFO: Pod "downwardapi-volume-3cae0396-c920-4a3a-aba7-40583d13c2f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 15.024803381s
STEP: Saw pod success
Sep 27 02:52:01.493: INFO: Pod "downwardapi-volume-3cae0396-c920-4a3a-aba7-40583d13c2f0" satisfied condition "Succeeded or Failed"
Sep 27 02:52:01.497: INFO: Trying to get logs from node dce-10-6-171-86 pod downwardapi-volume-3cae0396-c920-4a3a-aba7-40583d13c2f0 container client-container: <nil>
STEP: delete the pod
Sep 27 02:52:01.604: INFO: Waiting for pod downwardapi-volume-3cae0396-c920-4a3a-aba7-40583d13c2f0 to disappear
Sep 27 02:52:01.610: INFO: Pod downwardapi-volume-3cae0396-c920-4a3a-aba7-40583d13c2f0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:52:01.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6808" for this suite.

â€¢ [SLOW TEST:17.061 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":277,"completed":38,"skipped":754,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:52:01.630: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-1689
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 02:52:02.046: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1689
I0927 02:52:02.066326      23 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1689, replica count: 1
I0927 02:52:03.164835      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 02:52:04.165295      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 02:52:05.166134      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 02:52:06.172628      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 02:52:07.220194      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 02:52:08.512568      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 02:52:09.667432      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 02:52:10.681348      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 02:52:11.708890      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 02:52:12.960111      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 02:52:14.038404      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 02:52:15.059428      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 02:52:16.134354      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 02:52:17.134637      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 02:52:18.134963      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 27 02:52:19.046: INFO: Created: latency-svc-2wxp7
Sep 27 02:52:19.180: INFO: Got endpoints: latency-svc-2wxp7 [945.100625ms]
Sep 27 02:52:19.344: INFO: Created: latency-svc-r4566
Sep 27 02:52:19.367: INFO: Got endpoints: latency-svc-r4566 [186.445013ms]
Sep 27 02:52:19.425: INFO: Created: latency-svc-rj72d
Sep 27 02:52:19.464: INFO: Got endpoints: latency-svc-rj72d [284.151403ms]
Sep 27 02:52:19.485: INFO: Created: latency-svc-h9b46
Sep 27 02:52:19.557: INFO: Got endpoints: latency-svc-h9b46 [376.032892ms]
Sep 27 02:52:19.560: INFO: Created: latency-svc-5cv27
Sep 27 02:52:19.645: INFO: Got endpoints: latency-svc-5cv27 [463.628743ms]
Sep 27 02:52:19.684: INFO: Created: latency-svc-5tdnv
Sep 27 02:52:19.707: INFO: Got endpoints: latency-svc-5tdnv [525.465127ms]
Sep 27 02:52:19.711: INFO: Created: latency-svc-7jkl6
Sep 27 02:52:19.970: INFO: Got endpoints: latency-svc-7jkl6 [788.631376ms]
Sep 27 02:52:19.971: INFO: Created: latency-svc-bkccl
Sep 27 02:52:19.981: INFO: Got endpoints: latency-svc-bkccl [799.578232ms]
Sep 27 02:52:19.994: INFO: Created: latency-svc-q5jjj
Sep 27 02:52:20.008: INFO: Got endpoints: latency-svc-q5jjj [825.824157ms]
Sep 27 02:52:20.008: INFO: Created: latency-svc-bkmzb
Sep 27 02:52:20.024: INFO: Got endpoints: latency-svc-bkmzb [841.340579ms]
Sep 27 02:52:20.122: INFO: Created: latency-svc-hb95r
Sep 27 02:52:20.266: INFO: Got endpoints: latency-svc-hb95r [1.083247088s]
Sep 27 02:52:20.267: INFO: Created: latency-svc-t487v
Sep 27 02:52:20.280: INFO: Got endpoints: latency-svc-t487v [1.097505568s]
Sep 27 02:52:20.341: INFO: Created: latency-svc-7mtv2
Sep 27 02:52:20.386: INFO: Got endpoints: latency-svc-7mtv2 [1.20266701s]
Sep 27 02:52:20.423: INFO: Created: latency-svc-6p9p2
Sep 27 02:52:20.447: INFO: Got endpoints: latency-svc-6p9p2 [1.263777538s]
Sep 27 02:52:20.485: INFO: Created: latency-svc-zznd9
Sep 27 02:52:20.541: INFO: Got endpoints: latency-svc-zznd9 [1.357284619s]
Sep 27 02:52:20.646: INFO: Created: latency-svc-t8r9g
Sep 27 02:52:20.750: INFO: Got endpoints: latency-svc-t8r9g [1.566499421s]
Sep 27 02:52:20.751: INFO: Created: latency-svc-6gl4d
Sep 27 02:52:20.954: INFO: Created: latency-svc-495gw
Sep 27 02:52:20.954: INFO: Got endpoints: latency-svc-6gl4d [1.587116851s]
Sep 27 02:52:21.233: INFO: Created: latency-svc-nvcx6
Sep 27 02:52:21.234: INFO: Got endpoints: latency-svc-495gw [1.769066519s]
Sep 27 02:52:21.290: INFO: Got endpoints: latency-svc-nvcx6 [1.732977338s]
Sep 27 02:52:21.405: INFO: Created: latency-svc-hlrcm
Sep 27 02:52:21.533: INFO: Got endpoints: latency-svc-hlrcm [1.88808436s]
Sep 27 02:52:21.671: INFO: Created: latency-svc-kmm4n
Sep 27 02:52:21.914: INFO: Got endpoints: latency-svc-kmm4n [2.207110509s]
Sep 27 02:52:22.131: INFO: Created: latency-svc-5dknj
Sep 27 02:52:22.325: INFO: Created: latency-svc-hn4kf
Sep 27 02:52:22.325: INFO: Got endpoints: latency-svc-5dknj [2.354939319s]
Sep 27 02:52:22.650: INFO: Got endpoints: latency-svc-hn4kf [2.668595746s]
Sep 27 02:52:22.651: INFO: Created: latency-svc-47f6c
Sep 27 02:52:22.686: INFO: Got endpoints: latency-svc-47f6c [2.6782925s]
Sep 27 02:52:22.715: INFO: Created: latency-svc-w7t7m
Sep 27 02:52:22.745: INFO: Got endpoints: latency-svc-w7t7m [2.721316983s]
Sep 27 02:52:22.792: INFO: Created: latency-svc-w6lzn
Sep 27 02:52:22.871: INFO: Got endpoints: latency-svc-w6lzn [2.6053171s]
Sep 27 02:52:22.891: INFO: Created: latency-svc-6zzgn
Sep 27 02:52:22.937: INFO: Got endpoints: latency-svc-6zzgn [2.656841684s]
Sep 27 02:52:22.949: INFO: Created: latency-svc-t6g7s
Sep 27 02:52:22.990: INFO: Got endpoints: latency-svc-t6g7s [2.604235067s]
Sep 27 02:52:23.232: INFO: Created: latency-svc-pxck6
Sep 27 02:52:23.350: INFO: Got endpoints: latency-svc-pxck6 [2.902790199s]
Sep 27 02:52:23.351: INFO: Created: latency-svc-6zkc2
Sep 27 02:52:23.482: INFO: Got endpoints: latency-svc-6zkc2 [2.941120603s]
Sep 27 02:52:23.482: INFO: Created: latency-svc-m2w9j
Sep 27 02:52:23.678: INFO: Got endpoints: latency-svc-m2w9j [2.927838773s]
Sep 27 02:52:23.678: INFO: Created: latency-svc-92rc5
Sep 27 02:52:23.699: INFO: Got endpoints: latency-svc-92rc5 [2.74483126s]
Sep 27 02:52:23.795: INFO: Created: latency-svc-mq5hl
Sep 27 02:52:23.807: INFO: Got endpoints: latency-svc-mq5hl [2.57352675s]
Sep 27 02:52:23.854: INFO: Created: latency-svc-m4tqg
Sep 27 02:52:23.891: INFO: Got endpoints: latency-svc-m4tqg [2.601184274s]
Sep 27 02:52:23.920: INFO: Created: latency-svc-b7sjp
Sep 27 02:52:23.941: INFO: Got endpoints: latency-svc-b7sjp [2.407903639s]
Sep 27 02:52:23.952: INFO: Created: latency-svc-7t66s
Sep 27 02:52:23.961: INFO: Got endpoints: latency-svc-7t66s [2.046606175s]
Sep 27 02:52:23.991: INFO: Created: latency-svc-vh2br
Sep 27 02:52:24.036: INFO: Got endpoints: latency-svc-vh2br [1.710430469s]
Sep 27 02:52:24.069: INFO: Created: latency-svc-v4sf2
Sep 27 02:52:24.088: INFO: Got endpoints: latency-svc-v4sf2 [1.438227093s]
Sep 27 02:52:24.098: INFO: Created: latency-svc-27ltc
Sep 27 02:52:24.130: INFO: Got endpoints: latency-svc-27ltc [1.443781469s]
Sep 27 02:52:24.162: INFO: Created: latency-svc-lvz5n
Sep 27 02:52:24.189: INFO: Got endpoints: latency-svc-lvz5n [1.443636726s]
Sep 27 02:52:24.237: INFO: Created: latency-svc-l4g6m
Sep 27 02:52:24.280: INFO: Got endpoints: latency-svc-l4g6m [1.408450114s]
Sep 27 02:52:24.288: INFO: Created: latency-svc-8r92l
Sep 27 02:52:24.296: INFO: Got endpoints: latency-svc-8r92l [1.358566791s]
Sep 27 02:52:24.378: INFO: Created: latency-svc-7vfp9
Sep 27 02:52:24.399: INFO: Got endpoints: latency-svc-7vfp9 [1.408541475s]
Sep 27 02:52:24.399: INFO: Created: latency-svc-pvndr
Sep 27 02:52:24.407: INFO: Got endpoints: latency-svc-pvndr [1.056654222s]
Sep 27 02:52:24.421: INFO: Created: latency-svc-9gfpc
Sep 27 02:52:24.455: INFO: Got endpoints: latency-svc-9gfpc [972.667362ms]
Sep 27 02:52:24.472: INFO: Created: latency-svc-mvgs5
Sep 27 02:52:24.534: INFO: Got endpoints: latency-svc-mvgs5 [856.311428ms]
Sep 27 02:52:24.537: INFO: Created: latency-svc-blsbx
Sep 27 02:52:24.547: INFO: Got endpoints: latency-svc-blsbx [848.271851ms]
Sep 27 02:52:24.548: INFO: Created: latency-svc-2gwvp
Sep 27 02:52:24.555: INFO: Got endpoints: latency-svc-2gwvp [748.134017ms]
Sep 27 02:52:24.563: INFO: Created: latency-svc-fwk9z
Sep 27 02:52:24.572: INFO: Got endpoints: latency-svc-fwk9z [680.46052ms]
Sep 27 02:52:24.581: INFO: Created: latency-svc-k4hpq
Sep 27 02:52:24.605: INFO: Got endpoints: latency-svc-k4hpq [664.335616ms]
Sep 27 02:52:24.613: INFO: Created: latency-svc-v76t5
Sep 27 02:52:24.622: INFO: Got endpoints: latency-svc-v76t5 [661.073063ms]
Sep 27 02:52:24.680: INFO: Created: latency-svc-5p5b2
Sep 27 02:52:24.756: INFO: Got endpoints: latency-svc-5p5b2 [719.94681ms]
Sep 27 02:52:24.780: INFO: Created: latency-svc-s9bx9
Sep 27 02:52:24.789: INFO: Created: latency-svc-jxc2p
Sep 27 02:52:24.790: INFO: Got endpoints: latency-svc-s9bx9 [701.799699ms]
Sep 27 02:52:24.797: INFO: Got endpoints: latency-svc-jxc2p [667.072035ms]
Sep 27 02:52:24.834: INFO: Created: latency-svc-tkvdl
Sep 27 02:52:24.839: INFO: Got endpoints: latency-svc-tkvdl [650.2179ms]
Sep 27 02:52:24.849: INFO: Created: latency-svc-kp2v8
Sep 27 02:52:24.855: INFO: Got endpoints: latency-svc-kp2v8 [575.425658ms]
Sep 27 02:52:24.864: INFO: Created: latency-svc-jxjnm
Sep 27 02:52:24.872: INFO: Got endpoints: latency-svc-jxjnm [576.142671ms]
Sep 27 02:52:24.895: INFO: Created: latency-svc-qrjwm
Sep 27 02:52:24.940: INFO: Created: latency-svc-7trh9
Sep 27 02:52:24.940: INFO: Got endpoints: latency-svc-qrjwm [541.123419ms]
Sep 27 02:52:24.956: INFO: Got endpoints: latency-svc-7trh9 [549.298238ms]
Sep 27 02:52:24.964: INFO: Created: latency-svc-74zch
Sep 27 02:52:24.973: INFO: Got endpoints: latency-svc-74zch [518.239001ms]
Sep 27 02:52:25.018: INFO: Created: latency-svc-24cx7
Sep 27 02:52:25.022: INFO: Got endpoints: latency-svc-24cx7 [487.048921ms]
Sep 27 02:52:25.031: INFO: Created: latency-svc-v5dm2
Sep 27 02:52:25.064: INFO: Got endpoints: latency-svc-v5dm2 [516.383246ms]
Sep 27 02:52:25.147: INFO: Created: latency-svc-cfrcm
Sep 27 02:52:25.149: INFO: Got endpoints: latency-svc-cfrcm [594.171915ms]
Sep 27 02:52:25.165: INFO: Created: latency-svc-b27ws
Sep 27 02:52:25.178: INFO: Got endpoints: latency-svc-b27ws [606.412716ms]
Sep 27 02:52:25.211: INFO: Created: latency-svc-xrwh7
Sep 27 02:52:25.229: INFO: Got endpoints: latency-svc-xrwh7 [623.274653ms]
Sep 27 02:52:25.229: INFO: Created: latency-svc-zqwmp
Sep 27 02:52:25.238: INFO: Created: latency-svc-jdq95
Sep 27 02:52:25.239: INFO: Got endpoints: latency-svc-zqwmp [616.818395ms]
Sep 27 02:52:25.262: INFO: Created: latency-svc-4ts6j
Sep 27 02:52:25.262: INFO: Got endpoints: latency-svc-jdq95 [506.455084ms]
Sep 27 02:52:25.283: INFO: Got endpoints: latency-svc-4ts6j [493.025535ms]
Sep 27 02:52:25.333: INFO: Created: latency-svc-rgqts
Sep 27 02:52:25.513: INFO: Got endpoints: latency-svc-rgqts [715.59526ms]
Sep 27 02:52:25.514: INFO: Created: latency-svc-5j45c
Sep 27 02:52:25.554: INFO: Got endpoints: latency-svc-5j45c [714.669502ms]
Sep 27 02:52:25.775: INFO: Created: latency-svc-zvdb8
Sep 27 02:52:25.778: INFO: Created: latency-svc-h4kt4
Sep 27 02:52:25.808: INFO: Got endpoints: latency-svc-h4kt4 [935.9236ms]
Sep 27 02:52:25.809: INFO: Got endpoints: latency-svc-zvdb8 [953.244921ms]
Sep 27 02:52:25.817: INFO: Created: latency-svc-g5lts
Sep 27 02:52:25.825: INFO: Got endpoints: latency-svc-g5lts [885.149936ms]
Sep 27 02:52:25.900: INFO: Created: latency-svc-lmzk2
Sep 27 02:52:25.917: INFO: Got endpoints: latency-svc-lmzk2 [960.956134ms]
Sep 27 02:52:25.958: INFO: Created: latency-svc-tfmmq
Sep 27 02:52:26.022: INFO: Got endpoints: latency-svc-tfmmq [1.049512393s]
Sep 27 02:52:26.067: INFO: Created: latency-svc-bm5pj
Sep 27 02:52:26.101: INFO: Got endpoints: latency-svc-bm5pj [1.079215284s]
Sep 27 02:52:26.108: INFO: Created: latency-svc-vdb6m
Sep 27 02:52:26.118: INFO: Got endpoints: latency-svc-vdb6m [1.053741915s]
Sep 27 02:52:26.190: INFO: Created: latency-svc-2zkqj
Sep 27 02:52:26.209: INFO: Got endpoints: latency-svc-2zkqj [1.059708945s]
Sep 27 02:52:26.210: INFO: Created: latency-svc-spqfx
Sep 27 02:52:26.217: INFO: Got endpoints: latency-svc-spqfx [1.039001234s]
Sep 27 02:52:26.418: INFO: Created: latency-svc-d9b8l
Sep 27 02:52:26.543: INFO: Created: latency-svc-glmlf
Sep 27 02:52:26.543: INFO: Got endpoints: latency-svc-d9b8l [1.314316195s]
Sep 27 02:52:26.559: INFO: Got endpoints: latency-svc-glmlf [1.32010674s]
Sep 27 02:52:26.586: INFO: Created: latency-svc-4x6sb
Sep 27 02:52:26.609: INFO: Got endpoints: latency-svc-4x6sb [1.346494515s]
Sep 27 02:52:26.680: INFO: Created: latency-svc-jjsl9
Sep 27 02:52:26.688: INFO: Got endpoints: latency-svc-jjsl9 [1.404851198s]
Sep 27 02:52:26.692: INFO: Created: latency-svc-8pkcl
Sep 27 02:52:26.729: INFO: Got endpoints: latency-svc-8pkcl [1.215856202s]
Sep 27 02:52:26.898: INFO: Created: latency-svc-4t448
Sep 27 02:52:26.909: INFO: Created: latency-svc-9dqg7
Sep 27 02:52:26.911: INFO: Got endpoints: latency-svc-4t448 [1.357376737s]
Sep 27 02:52:26.977: INFO: Got endpoints: latency-svc-9dqg7 [1.168781082s]
Sep 27 02:52:26.993: INFO: Created: latency-svc-xbxrw
Sep 27 02:52:27.008: INFO: Got endpoints: latency-svc-xbxrw [1.199725726s]
Sep 27 02:52:27.021: INFO: Created: latency-svc-bhdw2
Sep 27 02:52:27.071: INFO: Created: latency-svc-nskmf
Sep 27 02:52:27.071: INFO: Got endpoints: latency-svc-bhdw2 [1.24603365s]
Sep 27 02:52:27.094: INFO: Got endpoints: latency-svc-nskmf [1.176945159s]
Sep 27 02:52:27.137: INFO: Created: latency-svc-cqfw8
Sep 27 02:52:27.162: INFO: Got endpoints: latency-svc-cqfw8 [1.1396105s]
Sep 27 02:52:27.282: INFO: Created: latency-svc-zdmlt
Sep 27 02:52:27.295: INFO: Got endpoints: latency-svc-zdmlt [1.194104958s]
Sep 27 02:52:27.296: INFO: Created: latency-svc-9hn2x
Sep 27 02:52:27.440: INFO: Created: latency-svc-2whl6
Sep 27 02:52:27.440: INFO: Got endpoints: latency-svc-9hn2x [1.322336174s]
Sep 27 02:52:27.453: INFO: Got endpoints: latency-svc-2whl6 [1.243507389s]
Sep 27 02:52:27.463: INFO: Created: latency-svc-szdvd
Sep 27 02:52:27.470: INFO: Got endpoints: latency-svc-szdvd [1.252418978s]
Sep 27 02:52:27.503: INFO: Created: latency-svc-w44l5
Sep 27 02:52:27.641: INFO: Created: latency-svc-bnrdq
Sep 27 02:52:27.641: INFO: Got endpoints: latency-svc-w44l5 [1.097702944s]
Sep 27 02:52:27.663: INFO: Created: latency-svc-pstn9
Sep 27 02:52:27.663: INFO: Got endpoints: latency-svc-bnrdq [1.104466029s]
Sep 27 02:52:27.679: INFO: Got endpoints: latency-svc-pstn9 [1.069723563s]
Sep 27 02:52:27.694: INFO: Created: latency-svc-4bhdj
Sep 27 02:52:27.737: INFO: Got endpoints: latency-svc-4bhdj [1.048508835s]
Sep 27 02:52:27.811: INFO: Created: latency-svc-89tnl
Sep 27 02:52:27.839: INFO: Created: latency-svc-mj6f8
Sep 27 02:52:27.840: INFO: Got endpoints: latency-svc-89tnl [1.110724796s]
Sep 27 02:52:27.862: INFO: Got endpoints: latency-svc-mj6f8 [950.705686ms]
Sep 27 02:52:27.904: INFO: Created: latency-svc-c59dg
Sep 27 02:52:27.981: INFO: Got endpoints: latency-svc-c59dg [1.003122085s]
Sep 27 02:52:27.986: INFO: Created: latency-svc-nzxc6
Sep 27 02:52:28.004: INFO: Got endpoints: latency-svc-nzxc6 [996.215618ms]
Sep 27 02:52:28.005: INFO: Created: latency-svc-5ldxf
Sep 27 02:52:28.028: INFO: Got endpoints: latency-svc-5ldxf [957.233482ms]
Sep 27 02:52:28.072: INFO: Created: latency-svc-jppkm
Sep 27 02:52:28.123: INFO: Got endpoints: latency-svc-jppkm [1.029016267s]
Sep 27 02:52:28.176: INFO: Created: latency-svc-zhsvr
Sep 27 02:52:28.214: INFO: Created: latency-svc-8xkhb
Sep 27 02:52:28.214: INFO: Got endpoints: latency-svc-zhsvr [1.052203247s]
Sep 27 02:52:28.259: INFO: Got endpoints: latency-svc-8xkhb [963.749631ms]
Sep 27 02:52:28.262: INFO: Created: latency-svc-b5rz2
Sep 27 02:52:28.353: INFO: Got endpoints: latency-svc-b5rz2 [912.983355ms]
Sep 27 02:52:28.354: INFO: Created: latency-svc-zpx8p
Sep 27 02:52:28.392: INFO: Got endpoints: latency-svc-zpx8p [939.233299ms]
Sep 27 02:52:28.475: INFO: Created: latency-svc-kxsnh
Sep 27 02:52:28.563: INFO: Got endpoints: latency-svc-kxsnh [1.092858923s]
Sep 27 02:52:28.676: INFO: Created: latency-svc-mn6js
Sep 27 02:52:28.755: INFO: Got endpoints: latency-svc-mn6js [1.113855051s]
Sep 27 02:52:28.755: INFO: Created: latency-svc-4zpqk
Sep 27 02:52:28.763: INFO: Got endpoints: latency-svc-4zpqk [1.099579044s]
Sep 27 02:52:28.772: INFO: Created: latency-svc-jl2dr
Sep 27 02:52:28.873: INFO: Got endpoints: latency-svc-jl2dr [1.193965821s]
Sep 27 02:52:28.929: INFO: Created: latency-svc-xpmkw
Sep 27 02:52:28.948: INFO: Got endpoints: latency-svc-xpmkw [1.211276437s]
Sep 27 02:52:29.014: INFO: Created: latency-svc-w2twq
Sep 27 02:52:29.021: INFO: Created: latency-svc-jknpl
Sep 27 02:52:29.021: INFO: Got endpoints: latency-svc-w2twq [1.181397543s]
Sep 27 02:52:29.073: INFO: Got endpoints: latency-svc-jknpl [1.211085075s]
Sep 27 02:52:29.108: INFO: Created: latency-svc-4zt6j
Sep 27 02:52:29.137: INFO: Got endpoints: latency-svc-4zt6j [1.156849497s]
Sep 27 02:52:29.155: INFO: Created: latency-svc-mcmwc
Sep 27 02:52:29.300: INFO: Created: latency-svc-xlppx
Sep 27 02:52:29.300: INFO: Created: latency-svc-96t82
Sep 27 02:52:29.300: INFO: Got endpoints: latency-svc-mcmwc [1.295695652s]
Sep 27 02:52:29.300: INFO: Got endpoints: latency-svc-96t82 [1.271460684s]
Sep 27 02:52:29.322: INFO: Got endpoints: latency-svc-xlppx [1.198601927s]
Sep 27 02:52:29.340: INFO: Created: latency-svc-792v8
Sep 27 02:52:29.341: INFO: Got endpoints: latency-svc-792v8 [1.126302459s]
Sep 27 02:52:29.617: INFO: Created: latency-svc-hgd8g
Sep 27 02:52:29.617: INFO: Got endpoints: latency-svc-hgd8g [1.358122066s]
Sep 27 02:52:29.657: INFO: Created: latency-svc-xjdp2
Sep 27 02:52:29.662: INFO: Got endpoints: latency-svc-xjdp2 [1.308978193s]
Sep 27 02:52:29.723: INFO: Created: latency-svc-p2phg
Sep 27 02:52:29.740: INFO: Got endpoints: latency-svc-p2phg [1.347872073s]
Sep 27 02:52:29.820: INFO: Created: latency-svc-9gvb6
Sep 27 02:52:29.821: INFO: Got endpoints: latency-svc-9gvb6 [1.258529689s]
Sep 27 02:52:29.939: INFO: Created: latency-svc-lqn2x
Sep 27 02:52:30.037: INFO: Created: latency-svc-gljgw
Sep 27 02:52:30.037: INFO: Got endpoints: latency-svc-gljgw [1.274175806s]
Sep 27 02:52:30.045: INFO: Got endpoints: latency-svc-lqn2x [1.290717513s]
Sep 27 02:52:30.056: INFO: Created: latency-svc-d5kxl
Sep 27 02:52:30.098: INFO: Got endpoints: latency-svc-d5kxl [1.225082513s]
Sep 27 02:52:30.220: INFO: Created: latency-svc-jzlpp
Sep 27 02:52:30.220: INFO: Created: latency-svc-h7rxv
Sep 27 02:52:30.220: INFO: Got endpoints: latency-svc-jzlpp [1.271920257s]
Sep 27 02:52:30.238: INFO: Created: latency-svc-ldz25
Sep 27 02:52:30.238: INFO: Got endpoints: latency-svc-h7rxv [1.216963562s]
Sep 27 02:52:30.489: INFO: Created: latency-svc-kftts
Sep 27 02:52:30.489: INFO: Got endpoints: latency-svc-ldz25 [1.415803129s]
Sep 27 02:52:30.578: INFO: Created: latency-svc-sg5d9
Sep 27 02:52:30.578: INFO: Got endpoints: latency-svc-kftts [1.440918117s]
Sep 27 02:52:30.812: INFO: Got endpoints: latency-svc-sg5d9 [1.512251291s]
Sep 27 02:52:30.813: INFO: Created: latency-svc-vxz5b
Sep 27 02:52:30.935: INFO: Got endpoints: latency-svc-vxz5b [1.635295309s]
Sep 27 02:52:30.956: INFO: Created: latency-svc-7xlls
Sep 27 02:52:31.035: INFO: Got endpoints: latency-svc-7xlls [1.713216013s]
Sep 27 02:52:31.070: INFO: Created: latency-svc-46fwn
Sep 27 02:52:31.170: INFO: Got endpoints: latency-svc-46fwn [1.829519362s]
Sep 27 02:52:31.270: INFO: Created: latency-svc-kh6k5
Sep 27 02:52:31.471: INFO: Created: latency-svc-kxlqq
Sep 27 02:52:31.471: INFO: Got endpoints: latency-svc-kh6k5 [1.854385926s]
Sep 27 02:52:31.546: INFO: Got endpoints: latency-svc-kxlqq [1.883605096s]
Sep 27 02:52:31.551: INFO: Created: latency-svc-drxgn
Sep 27 02:52:31.617: INFO: Got endpoints: latency-svc-drxgn [1.877061869s]
Sep 27 02:52:31.716: INFO: Created: latency-svc-lwqgr
Sep 27 02:52:31.813: INFO: Created: latency-svc-kcrws
Sep 27 02:52:31.813: INFO: Got endpoints: latency-svc-lwqgr [1.991792533s]
Sep 27 02:52:32.056: INFO: Got endpoints: latency-svc-kcrws [2.01879139s]
Sep 27 02:52:32.106: INFO: Created: latency-svc-dj77f
Sep 27 02:52:32.106: INFO: Got endpoints: latency-svc-dj77f [2.06047724s]
Sep 27 02:52:32.249: INFO: Created: latency-svc-zjssl
Sep 27 02:52:32.273: INFO: Got endpoints: latency-svc-zjssl [2.17492524s]
Sep 27 02:52:32.273: INFO: Created: latency-svc-7f75s
Sep 27 02:52:32.461: INFO: Created: latency-svc-d68kg
Sep 27 02:52:32.462: INFO: Got endpoints: latency-svc-7f75s [2.242148527s]
Sep 27 02:52:32.512: INFO: Got endpoints: latency-svc-d68kg [2.274023895s]
Sep 27 02:52:32.514: INFO: Created: latency-svc-kswfc
Sep 27 02:52:32.532: INFO: Got endpoints: latency-svc-kswfc [2.043357635s]
Sep 27 02:52:32.742: INFO: Created: latency-svc-cmlbg
Sep 27 02:52:32.934: INFO: Got endpoints: latency-svc-cmlbg [2.355943822s]
Sep 27 02:52:32.935: INFO: Created: latency-svc-zpwqn
Sep 27 02:52:32.935: INFO: Got endpoints: latency-svc-zpwqn [2.122373565s]
Sep 27 02:52:32.935: INFO: Created: latency-svc-d7nbp
Sep 27 02:52:32.975: INFO: Got endpoints: latency-svc-d7nbp [2.039427429s]
Sep 27 02:52:32.975: INFO: Created: latency-svc-nhz5p
Sep 27 02:52:33.111: INFO: Created: latency-svc-stphq
Sep 27 02:52:33.111: INFO: Got endpoints: latency-svc-stphq [1.940690339s]
Sep 27 02:52:33.111: INFO: Got endpoints: latency-svc-nhz5p [2.076092275s]
Sep 27 02:52:33.168: INFO: Created: latency-svc-rdk9r
Sep 27 02:52:33.292: INFO: Got endpoints: latency-svc-rdk9r [1.820160029s]
Sep 27 02:52:33.294: INFO: Created: latency-svc-bhd46
Sep 27 02:52:33.380: INFO: Created: latency-svc-59rk2
Sep 27 02:52:33.380: INFO: Got endpoints: latency-svc-bhd46 [1.834173092s]
Sep 27 02:52:33.422: INFO: Got endpoints: latency-svc-59rk2 [1.805085496s]
Sep 27 02:52:33.557: INFO: Created: latency-svc-pzrfc
Sep 27 02:52:33.791: INFO: Got endpoints: latency-svc-pzrfc [1.977480057s]
Sep 27 02:52:33.791: INFO: Created: latency-svc-x5nsx
Sep 27 02:52:33.990: INFO: Got endpoints: latency-svc-x5nsx [1.933582297s]
Sep 27 02:52:34.033: INFO: Created: latency-svc-4rzqq
Sep 27 02:52:34.122: INFO: Got endpoints: latency-svc-4rzqq [2.016142036s]
Sep 27 02:52:34.293: INFO: Created: latency-svc-xm8zx
Sep 27 02:52:34.344: INFO: Created: latency-svc-pvrz2
Sep 27 02:52:34.344: INFO: Got endpoints: latency-svc-xm8zx [2.071374358s]
Sep 27 02:52:34.426: INFO: Created: latency-svc-mk8sq
Sep 27 02:52:34.426: INFO: Got endpoints: latency-svc-pvrz2 [1.963599923s]
Sep 27 02:52:34.468: INFO: Got endpoints: latency-svc-mk8sq [1.956243929s]
Sep 27 02:52:34.544: INFO: Created: latency-svc-hzw4s
Sep 27 02:52:34.643: INFO: Got endpoints: latency-svc-hzw4s [2.110938689s]
Sep 27 02:52:34.809: INFO: Created: latency-svc-hhxgh
Sep 27 02:52:34.839: INFO: Got endpoints: latency-svc-hhxgh [1.903981952s]
Sep 27 02:52:34.911: INFO: Created: latency-svc-s5gh7
Sep 27 02:52:34.979: INFO: Created: latency-svc-lmjmb
Sep 27 02:52:34.979: INFO: Got endpoints: latency-svc-s5gh7 [2.044049714s]
Sep 27 02:52:35.081: INFO: Got endpoints: latency-svc-lmjmb [2.106334963s]
Sep 27 02:52:35.412: INFO: Created: latency-svc-t9l4g
Sep 27 02:52:35.454: INFO: Got endpoints: latency-svc-t9l4g [2.342600927s]
Sep 27 02:52:35.454: INFO: Created: latency-svc-mntw8
Sep 27 02:52:35.474: INFO: Got endpoints: latency-svc-mntw8 [2.362640582s]
Sep 27 02:52:35.546: INFO: Created: latency-svc-rzfg7
Sep 27 02:52:35.584: INFO: Got endpoints: latency-svc-rzfg7 [2.292746934s]
Sep 27 02:52:35.773: INFO: Created: latency-svc-gl26q
Sep 27 02:52:35.821: INFO: Created: latency-svc-kzgq2
Sep 27 02:52:35.821: INFO: Got endpoints: latency-svc-gl26q [2.441251494s]
Sep 27 02:52:35.935: INFO: Got endpoints: latency-svc-kzgq2 [2.512924433s]
Sep 27 02:52:35.936: INFO: Created: latency-svc-g4qpd
Sep 27 02:52:36.037: INFO: Got endpoints: latency-svc-g4qpd [2.246129587s]
Sep 27 02:52:36.103: INFO: Created: latency-svc-rhqfx
Sep 27 02:52:36.177: INFO: Got endpoints: latency-svc-rhqfx [2.187204997s]
Sep 27 02:52:36.246: INFO: Created: latency-svc-t6mq5
Sep 27 02:52:36.269: INFO: Got endpoints: latency-svc-t6mq5 [2.14679993s]
Sep 27 02:52:36.438: INFO: Created: latency-svc-8dp22
Sep 27 02:52:36.461: INFO: Got endpoints: latency-svc-8dp22 [2.116551702s]
Sep 27 02:52:36.502: INFO: Created: latency-svc-qvqx2
Sep 27 02:52:36.519: INFO: Got endpoints: latency-svc-qvqx2 [2.092838212s]
Sep 27 02:52:36.655: INFO: Created: latency-svc-5zbsz
Sep 27 02:52:36.794: INFO: Created: latency-svc-bmbsm
Sep 27 02:52:36.794: INFO: Got endpoints: latency-svc-5zbsz [2.325623045s]
Sep 27 02:52:36.843: INFO: Got endpoints: latency-svc-bmbsm [2.199675265s]
Sep 27 02:52:36.904: INFO: Created: latency-svc-tfjh4
Sep 27 02:52:36.951: INFO: Created: latency-svc-z92wz
Sep 27 02:52:36.952: INFO: Got endpoints: latency-svc-tfjh4 [2.113338328s]
Sep 27 02:52:36.976: INFO: Got endpoints: latency-svc-z92wz [1.997818546s]
Sep 27 02:52:36.985: INFO: Created: latency-svc-d8pt7
Sep 27 02:52:37.010: INFO: Got endpoints: latency-svc-d8pt7 [1.928796992s]
Sep 27 02:52:37.026: INFO: Created: latency-svc-pzgnv
Sep 27 02:52:37.052: INFO: Got endpoints: latency-svc-pzgnv [1.598310712s]
Sep 27 02:52:37.169: INFO: Created: latency-svc-sndw9
Sep 27 02:52:37.186: INFO: Got endpoints: latency-svc-sndw9 [1.711992884s]
Sep 27 02:52:37.271: INFO: Created: latency-svc-rjz68
Sep 27 02:52:37.475: INFO: Got endpoints: latency-svc-rjz68 [1.890287015s]
Sep 27 02:52:37.478: INFO: Created: latency-svc-k9t2t
Sep 27 02:52:37.830: INFO: Got endpoints: latency-svc-k9t2t [2.008610624s]
Sep 27 02:52:37.832: INFO: Created: latency-svc-7dszk
Sep 27 02:52:38.351: INFO: Got endpoints: latency-svc-7dszk [2.415665116s]
Sep 27 02:52:38.415: INFO: Created: latency-svc-jvvx4
Sep 27 02:52:38.595: INFO: Got endpoints: latency-svc-jvvx4 [2.557889395s]
Sep 27 02:52:38.618: INFO: Created: latency-svc-bkr4f
Sep 27 02:52:38.971: INFO: Got endpoints: latency-svc-bkr4f [2.794530895s]
Sep 27 02:52:38.979: INFO: Created: latency-svc-88fmq
Sep 27 02:52:39.010: INFO: Got endpoints: latency-svc-88fmq [2.740808597s]
Sep 27 02:52:39.238: INFO: Created: latency-svc-fx55h
Sep 27 02:52:39.416: INFO: Created: latency-svc-djdqc
Sep 27 02:52:39.417: INFO: Got endpoints: latency-svc-fx55h [2.956239916s]
Sep 27 02:52:39.436: INFO: Got endpoints: latency-svc-djdqc [2.917639389s]
Sep 27 02:52:39.683: INFO: Created: latency-svc-58fnl
Sep 27 02:52:40.234: INFO: Created: latency-svc-dqqkw
Sep 27 02:52:40.234: INFO: Got endpoints: latency-svc-58fnl [3.439984219s]
Sep 27 02:52:40.814: INFO: Got endpoints: latency-svc-dqqkw [3.97061612s]
Sep 27 02:52:40.986: INFO: Created: latency-svc-4lk4x
Sep 27 02:52:41.164: INFO: Got endpoints: latency-svc-4lk4x [4.211958301s]
Sep 27 02:52:41.298: INFO: Created: latency-svc-zq8bk
Sep 27 02:52:41.311: INFO: Got endpoints: latency-svc-zq8bk [4.334352991s]
Sep 27 02:52:41.311: INFO: Created: latency-svc-w2ft4
Sep 27 02:52:41.395: INFO: Got endpoints: latency-svc-w2ft4 [4.385239515s]
Sep 27 02:52:41.560: INFO: Created: latency-svc-96m5p
Sep 27 02:52:41.583: INFO: Created: latency-svc-kjr89
Sep 27 02:52:41.584: INFO: Got endpoints: latency-svc-96m5p [4.531910887s]
Sep 27 02:52:41.619: INFO: Got endpoints: latency-svc-kjr89 [4.433350336s]
Sep 27 02:52:41.679: INFO: Created: latency-svc-x7bsw
Sep 27 02:52:41.712: INFO: Got endpoints: latency-svc-x7bsw [4.237258215s]
Sep 27 02:52:41.744: INFO: Created: latency-svc-p84vj
Sep 27 02:52:41.830: INFO: Got endpoints: latency-svc-p84vj [4.000084272s]
Sep 27 02:52:41.860: INFO: Created: latency-svc-lbprs
Sep 27 02:52:41.904: INFO: Got endpoints: latency-svc-lbprs [3.55299171s]
Sep 27 02:52:41.979: INFO: Created: latency-svc-2hmt8
Sep 27 02:52:42.010: INFO: Got endpoints: latency-svc-2hmt8 [3.415355603s]
Sep 27 02:52:42.275: INFO: Created: latency-svc-z6jz6
Sep 27 02:52:42.322: INFO: Got endpoints: latency-svc-z6jz6 [3.350857787s]
Sep 27 02:52:42.323: INFO: Created: latency-svc-nwqnz
Sep 27 02:52:42.370: INFO: Got endpoints: latency-svc-nwqnz [3.360171059s]
Sep 27 02:52:42.370: INFO: Latencies: [186.445013ms 284.151403ms 376.032892ms 463.628743ms 487.048921ms 493.025535ms 506.455084ms 516.383246ms 518.239001ms 525.465127ms 541.123419ms 549.298238ms 575.425658ms 576.142671ms 594.171915ms 606.412716ms 616.818395ms 623.274653ms 650.2179ms 661.073063ms 664.335616ms 667.072035ms 680.46052ms 701.799699ms 714.669502ms 715.59526ms 719.94681ms 748.134017ms 788.631376ms 799.578232ms 825.824157ms 841.340579ms 848.271851ms 856.311428ms 885.149936ms 912.983355ms 935.9236ms 939.233299ms 950.705686ms 953.244921ms 957.233482ms 960.956134ms 963.749631ms 972.667362ms 996.215618ms 1.003122085s 1.029016267s 1.039001234s 1.048508835s 1.049512393s 1.052203247s 1.053741915s 1.056654222s 1.059708945s 1.069723563s 1.079215284s 1.083247088s 1.092858923s 1.097505568s 1.097702944s 1.099579044s 1.104466029s 1.110724796s 1.113855051s 1.126302459s 1.1396105s 1.156849497s 1.168781082s 1.176945159s 1.181397543s 1.193965821s 1.194104958s 1.198601927s 1.199725726s 1.20266701s 1.211085075s 1.211276437s 1.215856202s 1.216963562s 1.225082513s 1.243507389s 1.24603365s 1.252418978s 1.258529689s 1.263777538s 1.271460684s 1.271920257s 1.274175806s 1.290717513s 1.295695652s 1.308978193s 1.314316195s 1.32010674s 1.322336174s 1.346494515s 1.347872073s 1.357284619s 1.357376737s 1.358122066s 1.358566791s 1.404851198s 1.408450114s 1.408541475s 1.415803129s 1.438227093s 1.440918117s 1.443636726s 1.443781469s 1.512251291s 1.566499421s 1.587116851s 1.598310712s 1.635295309s 1.710430469s 1.711992884s 1.713216013s 1.732977338s 1.769066519s 1.805085496s 1.820160029s 1.829519362s 1.834173092s 1.854385926s 1.877061869s 1.883605096s 1.88808436s 1.890287015s 1.903981952s 1.928796992s 1.933582297s 1.940690339s 1.956243929s 1.963599923s 1.977480057s 1.991792533s 1.997818546s 2.008610624s 2.016142036s 2.01879139s 2.039427429s 2.043357635s 2.044049714s 2.046606175s 2.06047724s 2.071374358s 2.076092275s 2.092838212s 2.106334963s 2.110938689s 2.113338328s 2.116551702s 2.122373565s 2.14679993s 2.17492524s 2.187204997s 2.199675265s 2.207110509s 2.242148527s 2.246129587s 2.274023895s 2.292746934s 2.325623045s 2.342600927s 2.354939319s 2.355943822s 2.362640582s 2.407903639s 2.415665116s 2.441251494s 2.512924433s 2.557889395s 2.57352675s 2.601184274s 2.604235067s 2.6053171s 2.656841684s 2.668595746s 2.6782925s 2.721316983s 2.740808597s 2.74483126s 2.794530895s 2.902790199s 2.917639389s 2.927838773s 2.941120603s 2.956239916s 3.350857787s 3.360171059s 3.415355603s 3.439984219s 3.55299171s 3.97061612s 4.000084272s 4.211958301s 4.237258215s 4.334352991s 4.385239515s 4.433350336s 4.531910887s]
Sep 27 02:52:42.370: INFO: 50 %ile: 1.404851198s
Sep 27 02:52:42.370: INFO: 90 %ile: 2.74483126s
Sep 27 02:52:42.370: INFO: 99 %ile: 4.433350336s
Sep 27 02:52:42.370: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:52:42.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-1689" for this suite.

â€¢ [SLOW TEST:40.822 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":277,"completed":39,"skipped":773,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:52:42.453: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-3081
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 02:52:43.241: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-63f4c0be-60cc-4025-86e1-4b6d2b1748f6" in namespace "security-context-test-3081" to be "Succeeded or Failed"
Sep 27 02:52:43.244: INFO: Pod "alpine-nnp-false-63f4c0be-60cc-4025-86e1-4b6d2b1748f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.997402ms
Sep 27 02:52:45.248: INFO: Pod "alpine-nnp-false-63f4c0be-60cc-4025-86e1-4b6d2b1748f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006909937s
Sep 27 02:52:47.255: INFO: Pod "alpine-nnp-false-63f4c0be-60cc-4025-86e1-4b6d2b1748f6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013910888s
Sep 27 02:52:49.366: INFO: Pod "alpine-nnp-false-63f4c0be-60cc-4025-86e1-4b6d2b1748f6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.124619612s
Sep 27 02:52:51.475: INFO: Pod "alpine-nnp-false-63f4c0be-60cc-4025-86e1-4b6d2b1748f6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.233336168s
Sep 27 02:52:53.545: INFO: Pod "alpine-nnp-false-63f4c0be-60cc-4025-86e1-4b6d2b1748f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.303518203s
Sep 27 02:52:53.545: INFO: Pod "alpine-nnp-false-63f4c0be-60cc-4025-86e1-4b6d2b1748f6" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:52:53.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3081" for this suite.

â€¢ [SLOW TEST:11.315 seconds]
[k8s.io] Security Context
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:291
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":40,"skipped":830,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:52:53.768: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4608
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl label
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1206
STEP: creating the pod
Sep 27 02:52:54.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 create -f - --namespace=kubectl-4608'
Sep 27 02:52:59.685: INFO: stderr: ""
Sep 27 02:52:59.685: INFO: stdout: "pod/pause created\n"
Sep 27 02:52:59.685: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Sep 27 02:52:59.685: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4608" to be "running and ready"
Sep 27 02:52:59.757: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 72.063346ms
Sep 27 02:53:01.851: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.16618558s
Sep 27 02:53:03.919: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.234228891s
Sep 27 02:53:06.129: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.44448105s
Sep 27 02:53:08.414: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 8.729319694s
Sep 27 02:53:10.496: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 10.811588817s
Sep 27 02:53:12.565: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 12.880461716s
Sep 27 02:53:14.674: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 14.989674941s
Sep 27 02:53:14.674: INFO: Pod "pause" satisfied condition "running and ready"
Sep 27 02:53:14.675: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: adding the label testing-label with value testing-label-value to a pod
Sep 27 02:53:14.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 label pods pause testing-label=testing-label-value --namespace=kubectl-4608'
Sep 27 02:53:15.162: INFO: stderr: ""
Sep 27 02:53:15.162: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Sep 27 02:53:15.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pod pause -L testing-label --namespace=kubectl-4608'
Sep 27 02:53:15.633: INFO: stderr: ""
Sep 27 02:53:15.633: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          16s   testing-label-value\n"
STEP: removing the label testing-label of a pod
Sep 27 02:53:15.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 label pods pause testing-label- --namespace=kubectl-4608'
Sep 27 02:53:15.944: INFO: stderr: ""
Sep 27 02:53:15.944: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Sep 27 02:53:15.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pod pause -L testing-label --namespace=kubectl-4608'
Sep 27 02:53:16.095: INFO: stderr: ""
Sep 27 02:53:16.095: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          17s   \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1213
STEP: using delete to clean up resources
Sep 27 02:53:16.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 delete --grace-period=0 --force -f - --namespace=kubectl-4608'
Sep 27 02:53:16.367: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 27 02:53:16.367: INFO: stdout: "pod \"pause\" force deleted\n"
Sep 27 02:53:16.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get rc,svc -l name=pause --no-headers --namespace=kubectl-4608'
Sep 27 02:53:16.557: INFO: stderr: "No resources found in kubectl-4608 namespace.\n"
Sep 27 02:53:16.557: INFO: stdout: ""
Sep 27 02:53:16.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods -l name=pause --namespace=kubectl-4608 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 27 02:53:16.757: INFO: stderr: ""
Sep 27 02:53:16.757: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:53:16.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4608" for this suite.

â€¢ [SLOW TEST:23.252 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1203
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":277,"completed":41,"skipped":835,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:53:17.021: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1987
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 02:53:17.746: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 27 02:53:25.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 --namespace=crd-publish-openapi-1987 create -f -'
Sep 27 02:53:29.569: INFO: stderr: ""
Sep 27 02:53:29.569: INFO: stdout: "e2e-test-crd-publish-openapi-2528-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep 27 02:53:29.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 --namespace=crd-publish-openapi-1987 delete e2e-test-crd-publish-openapi-2528-crds test-cr'
Sep 27 02:53:29.852: INFO: stderr: ""
Sep 27 02:53:29.852: INFO: stdout: "e2e-test-crd-publish-openapi-2528-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Sep 27 02:53:29.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 --namespace=crd-publish-openapi-1987 apply -f -'
Sep 27 02:53:30.403: INFO: stderr: ""
Sep 27 02:53:30.403: INFO: stdout: "e2e-test-crd-publish-openapi-2528-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep 27 02:53:30.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 --namespace=crd-publish-openapi-1987 delete e2e-test-crd-publish-openapi-2528-crds test-cr'
Sep 27 02:53:30.536: INFO: stderr: ""
Sep 27 02:53:30.536: INFO: stdout: "e2e-test-crd-publish-openapi-2528-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Sep 27 02:53:30.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 explain e2e-test-crd-publish-openapi-2528-crds'
Sep 27 02:53:30.966: INFO: stderr: ""
Sep 27 02:53:30.966: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2528-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:53:36.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1987" for this suite.

â€¢ [SLOW TEST:20.003 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":277,"completed":42,"skipped":881,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:53:37.025: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7981
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep 27 02:53:37.383: INFO: Waiting up to 5m0s for pod "pod-40bd4dee-dccb-4b8b-89dc-7f8a1bb61d8a" in namespace "emptydir-7981" to be "Succeeded or Failed"
Sep 27 02:53:37.569: INFO: Pod "pod-40bd4dee-dccb-4b8b-89dc-7f8a1bb61d8a": Phase="Pending", Reason="", readiness=false. Elapsed: 186.860046ms
Sep 27 02:53:39.674: INFO: Pod "pod-40bd4dee-dccb-4b8b-89dc-7f8a1bb61d8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.291457282s
Sep 27 02:53:41.796: INFO: Pod "pod-40bd4dee-dccb-4b8b-89dc-7f8a1bb61d8a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.413552515s
Sep 27 02:53:44.282: INFO: Pod "pod-40bd4dee-dccb-4b8b-89dc-7f8a1bb61d8a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.899246508s
Sep 27 02:53:46.527: INFO: Pod "pod-40bd4dee-dccb-4b8b-89dc-7f8a1bb61d8a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.144322203s
Sep 27 02:53:49.011: INFO: Pod "pod-40bd4dee-dccb-4b8b-89dc-7f8a1bb61d8a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.628148745s
Sep 27 02:53:51.017: INFO: Pod "pod-40bd4dee-dccb-4b8b-89dc-7f8a1bb61d8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 13.634108995s
STEP: Saw pod success
Sep 27 02:53:51.017: INFO: Pod "pod-40bd4dee-dccb-4b8b-89dc-7f8a1bb61d8a" satisfied condition "Succeeded or Failed"
Sep 27 02:53:51.020: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-40bd4dee-dccb-4b8b-89dc-7f8a1bb61d8a container test-container: <nil>
STEP: delete the pod
Sep 27 02:53:51.083: INFO: Waiting for pod pod-40bd4dee-dccb-4b8b-89dc-7f8a1bb61d8a to disappear
Sep 27 02:53:51.088: INFO: Pod pod-40bd4dee-dccb-4b8b-89dc-7f8a1bb61d8a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 02:53:51.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7981" for this suite.

â€¢ [SLOW TEST:14.075 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":43,"skipped":958,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 02:53:51.101: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6169
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-6169
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a new StatefulSet
Sep 27 02:53:51.752: INFO: Found 0 stateful pods, waiting for 3
Sep 27 02:54:01.867: INFO: Found 1 stateful pods, waiting for 3
Sep 27 02:54:11.757: INFO: Found 1 stateful pods, waiting for 3
Sep 27 02:54:21.763: INFO: Found 2 stateful pods, waiting for 3
Sep 27 02:54:32.073: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 02:54:32.073: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 02:54:32.073: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep 27 02:54:41.757: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 02:54:41.757: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 02:54:41.757: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep 27 02:54:51.758: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 02:54:51.758: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 02:54:51.758: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep 27 02:55:01.763: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 02:55:01.763: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 02:55:01.763: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep 27 02:55:11.759: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 02:55:11.759: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 02:55:11.759: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep 27 02:55:21.759: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 02:55:21.759: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 02:55:21.759: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep 27 02:55:31.761: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 02:55:31.761: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 02:55:31.761: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep 27 02:55:41.757: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 02:55:41.757: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 02:55:41.757: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep 27 02:55:51.763: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 02:55:51.763: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 02:55:51.763: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep 27 02:56:01.757: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 02:56:01.757: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 02:56:01.757: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep 27 02:56:11.757: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 02:56:11.757: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 02:56:11.757: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep 27 02:56:21.758: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 02:56:21.758: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 02:56:21.758: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep 27 02:56:31.762: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 02:56:31.762: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 02:56:31.762: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep 27 02:56:41.758: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 02:56:41.758: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 02:56:41.758: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep 27 02:56:51.956: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 02:56:51.956: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 02:56:51.956: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep 27 02:57:01.756: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 02:57:01.756: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 02:57:01.756: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 02:57:01.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-6169 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 27 02:57:02.079: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 27 02:57:02.079: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 27 02:57:02.079: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Sep 27 02:57:12.115: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Sep 27 02:57:22.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-6169 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 02:57:22.408: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 27 02:57:22.408: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 27 02:57:22.408: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 27 02:57:32.449: INFO: Waiting for StatefulSet statefulset-6169/ss2 to complete update
Sep 27 02:57:32.449: INFO: Waiting for Pod statefulset-6169/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 02:57:32.449: INFO: Waiting for Pod statefulset-6169/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 02:57:32.449: INFO: Waiting for Pod statefulset-6169/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 02:57:42.457: INFO: Waiting for StatefulSet statefulset-6169/ss2 to complete update
Sep 27 02:57:42.457: INFO: Waiting for Pod statefulset-6169/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 02:57:42.457: INFO: Waiting for Pod statefulset-6169/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 02:57:52.518: INFO: Waiting for StatefulSet statefulset-6169/ss2 to complete update
Sep 27 02:57:52.518: INFO: Waiting for Pod statefulset-6169/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 02:57:52.518: INFO: Waiting for Pod statefulset-6169/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 02:58:02.458: INFO: Waiting for StatefulSet statefulset-6169/ss2 to complete update
Sep 27 02:58:02.458: INFO: Waiting for Pod statefulset-6169/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 02:58:02.458: INFO: Waiting for Pod statefulset-6169/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 02:58:12.457: INFO: Waiting for StatefulSet statefulset-6169/ss2 to complete update
Sep 27 02:58:12.457: INFO: Waiting for Pod statefulset-6169/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 02:58:12.457: INFO: Waiting for Pod statefulset-6169/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 02:58:22.460: INFO: Waiting for StatefulSet statefulset-6169/ss2 to complete update
Sep 27 02:58:22.460: INFO: Waiting for Pod statefulset-6169/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 02:58:22.460: INFO: Waiting for Pod statefulset-6169/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 02:58:32.666: INFO: Waiting for StatefulSet statefulset-6169/ss2 to complete update
Sep 27 02:58:32.666: INFO: Waiting for Pod statefulset-6169/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 02:58:32.666: INFO: Waiting for Pod statefulset-6169/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 02:58:42.458: INFO: Waiting for StatefulSet statefulset-6169/ss2 to complete update
Sep 27 02:58:42.458: INFO: Waiting for Pod statefulset-6169/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 02:58:42.458: INFO: Waiting for Pod statefulset-6169/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 02:58:52.456: INFO: Waiting for StatefulSet statefulset-6169/ss2 to complete update
Sep 27 02:58:52.456: INFO: Waiting for Pod statefulset-6169/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 02:58:52.456: INFO: Waiting for Pod statefulset-6169/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 02:59:02.459: INFO: Waiting for StatefulSet statefulset-6169/ss2 to complete update
Sep 27 02:59:02.459: INFO: Waiting for Pod statefulset-6169/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 02:59:02.459: INFO: Waiting for Pod statefulset-6169/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 02:59:12.459: INFO: Waiting for StatefulSet statefulset-6169/ss2 to complete update
Sep 27 02:59:12.460: INFO: Waiting for Pod statefulset-6169/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 02:59:12.460: INFO: Waiting for Pod statefulset-6169/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 02:59:22.462: INFO: Waiting for StatefulSet statefulset-6169/ss2 to complete update
Sep 27 02:59:22.462: INFO: Waiting for Pod statefulset-6169/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 02:59:22.462: INFO: Waiting for Pod statefulset-6169/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 02:59:32.459: INFO: Waiting for StatefulSet statefulset-6169/ss2 to complete update
Sep 27 02:59:32.459: INFO: Waiting for Pod statefulset-6169/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 02:59:32.459: INFO: Waiting for Pod statefulset-6169/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 02:59:42.457: INFO: Waiting for StatefulSet statefulset-6169/ss2 to complete update
Sep 27 02:59:42.457: INFO: Waiting for Pod statefulset-6169/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 02:59:42.457: INFO: Waiting for Pod statefulset-6169/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 02:59:52.680: INFO: Waiting for StatefulSet statefulset-6169/ss2 to complete update
Sep 27 02:59:52.680: INFO: Waiting for Pod statefulset-6169/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 02:59:52.680: INFO: Waiting for Pod statefulset-6169/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 03:00:02.467: INFO: Waiting for StatefulSet statefulset-6169/ss2 to complete update
Sep 27 03:00:02.467: INFO: Waiting for Pod statefulset-6169/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 03:00:02.467: INFO: Waiting for Pod statefulset-6169/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 03:00:12.458: INFO: Waiting for StatefulSet statefulset-6169/ss2 to complete update
Sep 27 03:00:12.458: INFO: Waiting for Pod statefulset-6169/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 03:00:12.458: INFO: Waiting for Pod statefulset-6169/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 03:00:22.610: INFO: Waiting for StatefulSet statefulset-6169/ss2 to complete update
Sep 27 03:00:22.610: INFO: Waiting for Pod statefulset-6169/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 03:00:22.610: INFO: Waiting for Pod statefulset-6169/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 03:00:32.458: INFO: Waiting for StatefulSet statefulset-6169/ss2 to complete update
Sep 27 03:00:32.459: INFO: Waiting for Pod statefulset-6169/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 03:00:32.459: INFO: Waiting for Pod statefulset-6169/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 03:00:42.459: INFO: Waiting for StatefulSet statefulset-6169/ss2 to complete update
Sep 27 03:00:42.459: INFO: Waiting for Pod statefulset-6169/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 03:00:52.510: INFO: Waiting for StatefulSet statefulset-6169/ss2 to complete update
Sep 27 03:00:52.510: INFO: Waiting for Pod statefulset-6169/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 03:01:02.457: INFO: Waiting for StatefulSet statefulset-6169/ss2 to complete update
Sep 27 03:01:02.457: INFO: Waiting for Pod statefulset-6169/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 03:01:12.457: INFO: Waiting for StatefulSet statefulset-6169/ss2 to complete update
STEP: Rolling back to a previous revision
Sep 27 03:01:22.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-6169 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 27 03:01:23.958: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 27 03:01:23.958: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 27 03:01:23.958: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 27 03:01:34.286: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Sep 27 03:01:44.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-6169 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:01:44.989: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 27 03:01:44.990: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 27 03:01:44.990: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 27 03:01:55.021: INFO: Waiting for StatefulSet statefulset-6169/ss2 to complete update
Sep 27 03:01:55.021: INFO: Waiting for Pod statefulset-6169/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Sep 27 03:01:55.021: INFO: Waiting for Pod statefulset-6169/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Sep 27 03:01:55.021: INFO: Waiting for Pod statefulset-6169/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Sep 27 03:02:05.029: INFO: Waiting for StatefulSet statefulset-6169/ss2 to complete update
Sep 27 03:02:05.029: INFO: Waiting for Pod statefulset-6169/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Sep 27 03:02:05.029: INFO: Waiting for Pod statefulset-6169/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Sep 27 03:02:15.157: INFO: Waiting for StatefulSet statefulset-6169/ss2 to complete update
Sep 27 03:02:15.157: INFO: Waiting for Pod statefulset-6169/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Sep 27 03:02:25.047: INFO: Waiting for StatefulSet statefulset-6169/ss2 to complete update
Sep 27 03:02:25.047: INFO: Waiting for Pod statefulset-6169/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Sep 27 03:02:35.028: INFO: Waiting for StatefulSet statefulset-6169/ss2 to complete update
Sep 27 03:02:35.028: INFO: Waiting for Pod statefulset-6169/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Sep 27 03:02:45.058: INFO: Waiting for StatefulSet statefulset-6169/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Sep 27 03:02:55.112: INFO: Deleting all statefulset in ns statefulset-6169
Sep 27 03:02:55.118: INFO: Scaling statefulset ss2 to 0
Sep 27 03:03:35.163: INFO: Waiting for statefulset status.replicas updated to 0
Sep 27 03:03:35.166: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:03:35.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6169" for this suite.

â€¢ [SLOW TEST:584.315 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":277,"completed":44,"skipped":1014,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:03:35.416: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1265
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Sep 27 03:03:36.179: INFO: Waiting up to 5m0s for pod "downward-api-5ec4cc75-e813-4510-bfc2-fa59b1a71daf" in namespace "downward-api-1265" to be "Succeeded or Failed"
Sep 27 03:03:36.188: INFO: Pod "downward-api-5ec4cc75-e813-4510-bfc2-fa59b1a71daf": Phase="Pending", Reason="", readiness=false. Elapsed: 9.465244ms
Sep 27 03:03:38.196: INFO: Pod "downward-api-5ec4cc75-e813-4510-bfc2-fa59b1a71daf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017082304s
Sep 27 03:03:40.285: INFO: Pod "downward-api-5ec4cc75-e813-4510-bfc2-fa59b1a71daf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.106719502s
Sep 27 03:03:42.427: INFO: Pod "downward-api-5ec4cc75-e813-4510-bfc2-fa59b1a71daf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.248677642s
Sep 27 03:03:44.495: INFO: Pod "downward-api-5ec4cc75-e813-4510-bfc2-fa59b1a71daf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.316848754s
Sep 27 03:03:46.610: INFO: Pod "downward-api-5ec4cc75-e813-4510-bfc2-fa59b1a71daf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.431400198s
STEP: Saw pod success
Sep 27 03:03:46.610: INFO: Pod "downward-api-5ec4cc75-e813-4510-bfc2-fa59b1a71daf" satisfied condition "Succeeded or Failed"
Sep 27 03:03:46.701: INFO: Trying to get logs from node dce-10-6-171-86 pod downward-api-5ec4cc75-e813-4510-bfc2-fa59b1a71daf container dapi-container: <nil>
STEP: delete the pod
Sep 27 03:03:48.218: INFO: Waiting for pod downward-api-5ec4cc75-e813-4510-bfc2-fa59b1a71daf to disappear
Sep 27 03:03:48.255: INFO: Pod downward-api-5ec4cc75-e813-4510-bfc2-fa59b1a71daf no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:03:48.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1265" for this suite.

â€¢ [SLOW TEST:12.885 seconds]
[sig-node] Downward API
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":277,"completed":45,"skipped":1060,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:03:48.302: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9011
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Sep 27 03:03:49.111: INFO: Waiting up to 5m0s for pod "downward-api-b3e164c8-1271-4bbd-8268-6f22765081ef" in namespace "downward-api-9011" to be "Succeeded or Failed"
Sep 27 03:03:49.204: INFO: Pod "downward-api-b3e164c8-1271-4bbd-8268-6f22765081ef": Phase="Pending", Reason="", readiness=false. Elapsed: 93.250655ms
Sep 27 03:03:51.223: INFO: Pod "downward-api-b3e164c8-1271-4bbd-8268-6f22765081ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.111606461s
Sep 27 03:03:53.383: INFO: Pod "downward-api-b3e164c8-1271-4bbd-8268-6f22765081ef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.272133501s
Sep 27 03:03:55.694: INFO: Pod "downward-api-b3e164c8-1271-4bbd-8268-6f22765081ef": Phase="Pending", Reason="", readiness=false. Elapsed: 6.582607817s
Sep 27 03:03:58.029: INFO: Pod "downward-api-b3e164c8-1271-4bbd-8268-6f22765081ef": Phase="Pending", Reason="", readiness=false. Elapsed: 8.917496793s
Sep 27 03:04:00.753: INFO: Pod "downward-api-b3e164c8-1271-4bbd-8268-6f22765081ef": Phase="Pending", Reason="", readiness=false. Elapsed: 11.641729248s
Sep 27 03:04:02.791: INFO: Pod "downward-api-b3e164c8-1271-4bbd-8268-6f22765081ef": Phase="Pending", Reason="", readiness=false. Elapsed: 13.679788721s
Sep 27 03:04:04.795: INFO: Pod "downward-api-b3e164c8-1271-4bbd-8268-6f22765081ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 15.683973374s
STEP: Saw pod success
Sep 27 03:04:04.795: INFO: Pod "downward-api-b3e164c8-1271-4bbd-8268-6f22765081ef" satisfied condition "Succeeded or Failed"
Sep 27 03:04:04.799: INFO: Trying to get logs from node dce-10-6-171-86 pod downward-api-b3e164c8-1271-4bbd-8268-6f22765081ef container dapi-container: <nil>
STEP: delete the pod
Sep 27 03:04:04.969: INFO: Waiting for pod downward-api-b3e164c8-1271-4bbd-8268-6f22765081ef to disappear
Sep 27 03:04:04.976: INFO: Pod downward-api-b3e164c8-1271-4bbd-8268-6f22765081ef no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:04:04.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9011" for this suite.

â€¢ [SLOW TEST:16.692 seconds]
[sig-node] Downward API
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":277,"completed":46,"skipped":1114,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:04:04.994: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6267
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 27 03:04:08.261: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c9100cea-16c3-460f-b9ec-85f8b797d385" in namespace "downward-api-6267" to be "Succeeded or Failed"
Sep 27 03:04:08.270: INFO: Pod "downwardapi-volume-c9100cea-16c3-460f-b9ec-85f8b797d385": Phase="Pending", Reason="", readiness=false. Elapsed: 8.430314ms
Sep 27 03:04:10.545: INFO: Pod "downwardapi-volume-c9100cea-16c3-460f-b9ec-85f8b797d385": Phase="Pending", Reason="", readiness=false. Elapsed: 2.283512849s
Sep 27 03:04:12.614: INFO: Pod "downwardapi-volume-c9100cea-16c3-460f-b9ec-85f8b797d385": Phase="Pending", Reason="", readiness=false. Elapsed: 4.35244675s
Sep 27 03:04:14.926: INFO: Pod "downwardapi-volume-c9100cea-16c3-460f-b9ec-85f8b797d385": Phase="Pending", Reason="", readiness=false. Elapsed: 6.664330147s
Sep 27 03:04:17.092: INFO: Pod "downwardapi-volume-c9100cea-16c3-460f-b9ec-85f8b797d385": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.830510734s
STEP: Saw pod success
Sep 27 03:04:17.092: INFO: Pod "downwardapi-volume-c9100cea-16c3-460f-b9ec-85f8b797d385" satisfied condition "Succeeded or Failed"
Sep 27 03:04:17.124: INFO: Trying to get logs from node dce-10-6-171-86 pod downwardapi-volume-c9100cea-16c3-460f-b9ec-85f8b797d385 container client-container: <nil>
STEP: delete the pod
Sep 27 03:04:18.098: INFO: Waiting for pod downwardapi-volume-c9100cea-16c3-460f-b9ec-85f8b797d385 to disappear
Sep 27 03:04:18.426: INFO: Pod downwardapi-volume-c9100cea-16c3-460f-b9ec-85f8b797d385 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:04:18.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6267" for this suite.

â€¢ [SLOW TEST:14.086 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":277,"completed":47,"skipped":1120,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:04:19.080: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3112
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 03:04:22.812: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 27 03:04:25.235: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772662, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772662, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772664, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772662, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 03:04:27.277: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772662, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772662, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772664, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772662, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 03:04:29.239: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772662, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772662, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772664, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772662, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 03:04:31.239: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772662, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772662, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772664, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772662, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 03:04:34.714: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 03:04:34.717: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:04:35.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3112" for this suite.
STEP: Destroying namespace "webhook-3112-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:17.206 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":277,"completed":48,"skipped":1126,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:04:36.286: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1614
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:04:41.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1614" for this suite.

â€¢ [SLOW TEST:5.039 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":277,"completed":49,"skipped":1131,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:04:41.325: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-429
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name cm-test-opt-del-26f5101a-0ed3-47b9-b16e-2eaa3fa32d2e
STEP: Creating configMap with name cm-test-opt-upd-cb70c1e8-0162-4964-a064-48e02d43cd7c
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-26f5101a-0ed3-47b9-b16e-2eaa3fa32d2e
STEP: Updating configmap cm-test-opt-upd-cb70c1e8-0162-4964-a064-48e02d43cd7c
STEP: Creating configMap with name cm-test-opt-create-c408074a-dad6-4fde-88dd-b33bd0faa85b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:06:03.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-429" for this suite.

â€¢ [SLOW TEST:82.542 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":50,"skipped":1136,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:06:03.867: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8510
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:06:16.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8510" for this suite.

â€¢ [SLOW TEST:12.223 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":277,"completed":51,"skipped":1165,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:06:16.090: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-902
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-upd-369ac02b-73cd-41af-bf15-d57a2a335c28
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:06:30.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-902" for this suite.

â€¢ [SLOW TEST:14.398 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":52,"skipped":1172,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:06:30.488: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1211
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 03:06:31.340: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 27 03:06:33.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772791, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772791, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772791, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772791, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 03:06:35.367: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772791, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772791, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772791, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772791, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 03:06:37.360: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772791, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772791, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772791, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772791, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 03:06:39.363: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772791, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772791, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772791, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772791, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 03:06:41.410: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772791, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772791, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772791, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736772791, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 03:06:44.443: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:06:44.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1211" for this suite.
STEP: Destroying namespace "webhook-1211-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:16.126 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":277,"completed":53,"skipped":1216,"failed":0}
SSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] version v1
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:06:46.614: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-2137
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 03:06:47.768: INFO: (0) /api/v1/nodes/dce-10-6-171-85:10250/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 41.50307ms)
Sep 27 03:06:47.791: INFO: (1) /api/v1/nodes/dce-10-6-171-85:10250/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 23.080454ms)
Sep 27 03:06:47.796: INFO: (2) /api/v1/nodes/dce-10-6-171-85:10250/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 4.992768ms)
Sep 27 03:06:47.802: INFO: (3) /api/v1/nodes/dce-10-6-171-85:10250/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 5.778522ms)
Sep 27 03:06:47.810: INFO: (4) /api/v1/nodes/dce-10-6-171-85:10250/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 8.361156ms)
Sep 27 03:06:47.822: INFO: (5) /api/v1/nodes/dce-10-6-171-85:10250/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 11.188628ms)
Sep 27 03:06:48.027: INFO: (6) /api/v1/nodes/dce-10-6-171-85:10250/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 205.540054ms)
Sep 27 03:06:48.034: INFO: (7) /api/v1/nodes/dce-10-6-171-85:10250/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 6.943137ms)
Sep 27 03:06:48.038: INFO: (8) /api/v1/nodes/dce-10-6-171-85:10250/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 4.28145ms)
Sep 27 03:06:48.043: INFO: (9) /api/v1/nodes/dce-10-6-171-85:10250/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 4.602224ms)
Sep 27 03:06:48.047: INFO: (10) /api/v1/nodes/dce-10-6-171-85:10250/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 4.180097ms)
Sep 27 03:06:48.051: INFO: (11) /api/v1/nodes/dce-10-6-171-85:10250/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 4.074442ms)
Sep 27 03:06:48.059: INFO: (12) /api/v1/nodes/dce-10-6-171-85:10250/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 7.146244ms)
Sep 27 03:06:48.067: INFO: (13) /api/v1/nodes/dce-10-6-171-85:10250/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 7.862476ms)
Sep 27 03:06:48.074: INFO: (14) /api/v1/nodes/dce-10-6-171-85:10250/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 7.137898ms)
Sep 27 03:06:48.080: INFO: (15) /api/v1/nodes/dce-10-6-171-85:10250/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 6.456421ms)
Sep 27 03:06:48.085: INFO: (16) /api/v1/nodes/dce-10-6-171-85:10250/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 4.300049ms)
Sep 27 03:06:48.089: INFO: (17) /api/v1/nodes/dce-10-6-171-85:10250/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 4.367771ms)
Sep 27 03:06:48.094: INFO: (18) /api/v1/nodes/dce-10-6-171-85:10250/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 5.397502ms)
Sep 27 03:06:48.100: INFO: (19) /api/v1/nodes/dce-10-6-171-85:10250/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 5.658879ms)
[AfterEach] version v1
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:06:48.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2137" for this suite.
â€¢{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]","total":277,"completed":54,"skipped":1221,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:06:48.130: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-1827
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:171
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating server pod server in namespace prestop-1827
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-1827
STEP: Deleting pre-stop pod
Sep 27 03:07:18.597: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:07:18.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1827" for this suite.

â€¢ [SLOW TEST:30.819 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":277,"completed":55,"skipped":1230,"failed":0}
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:07:18.949: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9227
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-map-6bfac523-5b71-4243-a911-def874441c17
STEP: Creating a pod to test consume secrets
Sep 27 03:07:19.930: INFO: Waiting up to 5m0s for pod "pod-secrets-7f04a382-95e3-44c5-a8b3-d7fb10957a41" in namespace "secrets-9227" to be "Succeeded or Failed"
Sep 27 03:07:19.947: INFO: Pod "pod-secrets-7f04a382-95e3-44c5-a8b3-d7fb10957a41": Phase="Pending", Reason="", readiness=false. Elapsed: 17.120649ms
Sep 27 03:07:21.977: INFO: Pod "pod-secrets-7f04a382-95e3-44c5-a8b3-d7fb10957a41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047133058s
Sep 27 03:07:23.986: INFO: Pod "pod-secrets-7f04a382-95e3-44c5-a8b3-d7fb10957a41": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055911664s
Sep 27 03:07:26.142: INFO: Pod "pod-secrets-7f04a382-95e3-44c5-a8b3-d7fb10957a41": Phase="Pending", Reason="", readiness=false. Elapsed: 6.212194552s
Sep 27 03:07:28.149: INFO: Pod "pod-secrets-7f04a382-95e3-44c5-a8b3-d7fb10957a41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.218607691s
STEP: Saw pod success
Sep 27 03:07:28.149: INFO: Pod "pod-secrets-7f04a382-95e3-44c5-a8b3-d7fb10957a41" satisfied condition "Succeeded or Failed"
Sep 27 03:07:28.154: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-secrets-7f04a382-95e3-44c5-a8b3-d7fb10957a41 container secret-volume-test: <nil>
STEP: delete the pod
Sep 27 03:07:28.226: INFO: Waiting for pod pod-secrets-7f04a382-95e3-44c5-a8b3-d7fb10957a41 to disappear
Sep 27 03:07:28.263: INFO: Pod pod-secrets-7f04a382-95e3-44c5-a8b3-d7fb10957a41 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:07:28.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9227" for this suite.

â€¢ [SLOW TEST:9.481 seconds]
[sig-storage] Secrets
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":277,"completed":56,"skipped":1230,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:07:28.431: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6228
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Sep 27 03:07:29.768: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:07:29.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0927 03:07:29.768135      23 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-6228" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":277,"completed":57,"skipped":1244,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:07:29.815: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3616
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir volume type on tmpfs
Sep 27 03:07:30.403: INFO: Waiting up to 5m0s for pod "pod-3630db79-b154-42ca-9d59-f7e79c29d641" in namespace "emptydir-3616" to be "Succeeded or Failed"
Sep 27 03:07:30.414: INFO: Pod "pod-3630db79-b154-42ca-9d59-f7e79c29d641": Phase="Pending", Reason="", readiness=false. Elapsed: 11.468912ms
Sep 27 03:07:32.749: INFO: Pod "pod-3630db79-b154-42ca-9d59-f7e79c29d641": Phase="Pending", Reason="", readiness=false. Elapsed: 2.345794107s
Sep 27 03:07:34.812: INFO: Pod "pod-3630db79-b154-42ca-9d59-f7e79c29d641": Phase="Pending", Reason="", readiness=false. Elapsed: 4.409481247s
Sep 27 03:07:36.830: INFO: Pod "pod-3630db79-b154-42ca-9d59-f7e79c29d641": Phase="Pending", Reason="", readiness=false. Elapsed: 6.426732049s
Sep 27 03:07:38.834: INFO: Pod "pod-3630db79-b154-42ca-9d59-f7e79c29d641": Phase="Pending", Reason="", readiness=false. Elapsed: 8.431125004s
Sep 27 03:07:41.064: INFO: Pod "pod-3630db79-b154-42ca-9d59-f7e79c29d641": Phase="Pending", Reason="", readiness=false. Elapsed: 10.661572667s
Sep 27 03:07:44.861: INFO: Pod "pod-3630db79-b154-42ca-9d59-f7e79c29d641": Phase="Pending", Reason="", readiness=false. Elapsed: 14.458041082s
Sep 27 03:07:47.062: INFO: Pod "pod-3630db79-b154-42ca-9d59-f7e79c29d641": Phase="Pending", Reason="", readiness=false. Elapsed: 16.659560547s
Sep 27 03:07:49.110: INFO: Pod "pod-3630db79-b154-42ca-9d59-f7e79c29d641": Phase="Pending", Reason="", readiness=false. Elapsed: 18.706722385s
Sep 27 03:07:52.167: INFO: Pod "pod-3630db79-b154-42ca-9d59-f7e79c29d641": Phase="Succeeded", Reason="", readiness=false. Elapsed: 21.764477032s
STEP: Saw pod success
Sep 27 03:07:52.167: INFO: Pod "pod-3630db79-b154-42ca-9d59-f7e79c29d641" satisfied condition "Succeeded or Failed"
Sep 27 03:07:52.204: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-3630db79-b154-42ca-9d59-f7e79c29d641 container test-container: <nil>
STEP: delete the pod
Sep 27 03:07:52.665: INFO: Waiting for pod pod-3630db79-b154-42ca-9d59-f7e79c29d641 to disappear
Sep 27 03:07:52.669: INFO: Pod pod-3630db79-b154-42ca-9d59-f7e79c29d641 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:07:52.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3616" for this suite.

â€¢ [SLOW TEST:23.180 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":58,"skipped":1254,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:07:52.996: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1123
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-ae7a6461-47f8-46e8-93cb-b7ab7ea9a420
STEP: Creating a pod to test consume secrets
Sep 27 03:07:56.427: INFO: Waiting up to 5m0s for pod "pod-secrets-caf5f284-a53c-4899-b02a-54a3f2205cfd" in namespace "secrets-1123" to be "Succeeded or Failed"
Sep 27 03:07:56.524: INFO: Pod "pod-secrets-caf5f284-a53c-4899-b02a-54a3f2205cfd": Phase="Pending", Reason="", readiness=false. Elapsed: 97.185331ms
Sep 27 03:07:58.532: INFO: Pod "pod-secrets-caf5f284-a53c-4899-b02a-54a3f2205cfd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.104541113s
Sep 27 03:08:00.564: INFO: Pod "pod-secrets-caf5f284-a53c-4899-b02a-54a3f2205cfd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.137134367s
Sep 27 03:08:02.569: INFO: Pod "pod-secrets-caf5f284-a53c-4899-b02a-54a3f2205cfd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.141822666s
Sep 27 03:08:04.590: INFO: Pod "pod-secrets-caf5f284-a53c-4899-b02a-54a3f2205cfd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.162946399s
Sep 27 03:08:06.595: INFO: Pod "pod-secrets-caf5f284-a53c-4899-b02a-54a3f2205cfd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.168082103s
STEP: Saw pod success
Sep 27 03:08:06.595: INFO: Pod "pod-secrets-caf5f284-a53c-4899-b02a-54a3f2205cfd" satisfied condition "Succeeded or Failed"
Sep 27 03:08:06.599: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-secrets-caf5f284-a53c-4899-b02a-54a3f2205cfd container secret-volume-test: <nil>
STEP: delete the pod
Sep 27 03:08:06.628: INFO: Waiting for pod pod-secrets-caf5f284-a53c-4899-b02a-54a3f2205cfd to disappear
Sep 27 03:08:06.702: INFO: Pod pod-secrets-caf5f284-a53c-4899-b02a-54a3f2205cfd no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:08:06.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1123" for this suite.

â€¢ [SLOW TEST:13.722 seconds]
[sig-storage] Secrets
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":59,"skipped":1263,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:08:06.718: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7006
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-7006
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-7006
STEP: Creating statefulset with conflicting port in namespace statefulset-7006
STEP: Waiting until pod test-pod will start running in namespace statefulset-7006
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7006
Sep 27 03:08:17.164: INFO: Observed stateful pod in namespace: statefulset-7006, name: ss-0, uid: dfce5fa1-0138-4f50-9c2c-a573547fbd4f, status phase: Pending. Waiting for statefulset controller to delete.
Sep 27 03:08:17.904: INFO: Observed stateful pod in namespace: statefulset-7006, name: ss-0, uid: dfce5fa1-0138-4f50-9c2c-a573547fbd4f, status phase: Failed. Waiting for statefulset controller to delete.
Sep 27 03:08:17.904: INFO: Observed stateful pod in namespace: statefulset-7006, name: ss-0, uid: dfce5fa1-0138-4f50-9c2c-a573547fbd4f, status phase: Failed. Waiting for statefulset controller to delete.
Sep 27 03:08:17.905: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7006
STEP: Removing pod with conflicting port in namespace statefulset-7006
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7006 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Sep 27 03:08:32.593: INFO: Deleting all statefulset in ns statefulset-7006
Sep 27 03:08:32.627: INFO: Scaling statefulset ss to 0
Sep 27 03:08:42.771: INFO: Waiting for statefulset status.replicas updated to 0
Sep 27 03:08:42.795: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:08:42.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7006" for this suite.

â€¢ [SLOW TEST:36.324 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":277,"completed":60,"skipped":1273,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:08:43.044: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3552
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:08:47.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3552" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":277,"completed":61,"skipped":1327,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:08:47.632: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2752
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Sep 27 03:08:47.890: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:09:07.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2752" for this suite.

â€¢ [SLOW TEST:20.349 seconds]
[k8s.io] Pods
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":277,"completed":62,"skipped":1338,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:09:07.982: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7831
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 03:09:08.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 create -f - --namespace=kubectl-7831'
Sep 27 03:09:16.984: INFO: stderr: ""
Sep 27 03:09:16.984: INFO: stdout: "replicationcontroller/agnhost-master created\n"
Sep 27 03:09:16.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 create -f - --namespace=kubectl-7831'
Sep 27 03:09:17.325: INFO: stderr: ""
Sep 27 03:09:17.325: INFO: stdout: "service/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Sep 27 03:09:18.724: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 03:09:18.724: INFO: Found 0 / 1
Sep 27 03:09:19.458: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 03:09:19.458: INFO: Found 0 / 1
Sep 27 03:09:20.706: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 03:09:20.706: INFO: Found 0 / 1
Sep 27 03:09:21.893: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 03:09:21.893: INFO: Found 0 / 1
Sep 27 03:09:22.669: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 03:09:22.669: INFO: Found 0 / 1
Sep 27 03:09:23.341: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 03:09:23.341: INFO: Found 0 / 1
Sep 27 03:09:24.587: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 03:09:24.587: INFO: Found 0 / 1
Sep 27 03:09:25.912: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 03:09:25.912: INFO: Found 0 / 1
Sep 27 03:09:26.950: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 03:09:26.950: INFO: Found 0 / 1
Sep 27 03:09:27.345: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 03:09:27.345: INFO: Found 0 / 1
Sep 27 03:09:28.533: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 03:09:28.533: INFO: Found 0 / 1
Sep 27 03:09:29.493: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 03:09:29.493: INFO: Found 0 / 1
Sep 27 03:09:30.432: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 03:09:30.432: INFO: Found 1 / 1
Sep 27 03:09:30.432: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 27 03:09:30.446: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 03:09:30.446: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 27 03:09:30.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 describe pod agnhost-master-c7vjz --namespace=kubectl-7831'
Sep 27 03:09:33.605: INFO: stderr: ""
Sep 27 03:09:33.605: INFO: stdout: "Name:         agnhost-master-c7vjz\nNamespace:    kubectl-7831\nNode:         dce-10-6-171-86/10.6.171.86\nStart Time:   Sun, 27 Sep 2020 03:09:17 +0000\nLabels:       app=agnhost\n              role=master\nAnnotations:  cni.projectcalico.org/ipv4pools: [\"default-ipv4-ippool\"]\n              dce.daocloud.io/parcel.egress.burst: 0\n              dce.daocloud.io/parcel.egress.rate: 0\n              dce.daocloud.io/parcel.ingress.burst: 0\n              dce.daocloud.io/parcel.ingress.rate: 0\n              kubernetes.io/psp: dce-psp-allow-all\nStatus:       Running\nIP:           172.29.108.124\nIPs:\n  IP:           172.29.108.124\nControlled By:  ReplicationController/agnhost-master\nContainers:\n  agnhost-master:\n    Container ID:   docker://546b3c18702d9bc6a91fa28e42d9fa065eaf0f6a861419a75d11245d986d8f11\n    Image:          us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12\n    Image ID:       docker-pullable://us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sun, 27 Sep 2020 03:09:29 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qpwj5 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-qpwj5:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-qpwj5\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                      Message\n  ----    ------     ----  ----                      -------\n  Normal  Scheduled  16s   default-scheduler         Successfully assigned kubectl-7831/agnhost-master-c7vjz to dce-10-6-171-86\n  Normal  Pulled     5s    kubelet, dce-10-6-171-86  Container image \"us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12\" already present on machine\n  Normal  Created    5s    kubelet, dce-10-6-171-86  Created container agnhost-master\n  Normal  Started    4s    kubelet, dce-10-6-171-86  Started container agnhost-master\n"
Sep 27 03:09:33.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 describe rc agnhost-master --namespace=kubectl-7831'
Sep 27 03:09:34.180: INFO: stderr: ""
Sep 27 03:09:34.180: INFO: stdout: "Name:         agnhost-master\nNamespace:    kubectl-7831\nSelector:     app=agnhost,role=master\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=master\n  Containers:\n   agnhost-master:\n    Image:        us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  17s   replication-controller  Created pod: agnhost-master-c7vjz\n"
Sep 27 03:09:34.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 describe service agnhost-master --namespace=kubectl-7831'
Sep 27 03:09:34.318: INFO: stderr: ""
Sep 27 03:09:34.318: INFO: stdout: "Name:              agnhost-master\nNamespace:         kubectl-7831\nLabels:            app=agnhost\n                   role=master\nAnnotations:       <none>\nSelector:          app=agnhost,role=master\nType:              ClusterIP\nIP:                172.31.182.178\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.29.108.124:6379\nSession Affinity:  None\nEvents:            <none>\n"
Sep 27 03:09:34.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 describe node dce-10-6-171-84'
Sep 27 03:09:34.642: INFO: stderr: ""
Sep 27 03:09:34.642: INFO: stdout: "Name:               dce-10-6-171-84\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    io.daocloud.dce.dns=\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=dce-10-6-171-84\n                    kubernetes.io/os=linux\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 24 Sep 2020 09:40:00 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  dce-10-6-171-84\n  AcquireTime:     <unset>\n  RenewTime:       Sun, 27 Sep 2020 03:09:28 +0000\nConditions:\n  Type                            Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                            ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable              False   Thu, 24 Sep 2020 09:49:42 +0000   Thu, 24 Sep 2020 09:49:42 +0000   CalicoIsUp                   Calico is running on this node\n  DCEKubeApiServerProxyNotReady   False   Sun, 27 Sep 2020 03:08:59 +0000   Thu, 24 Sep 2020 10:37:34 +0000   DCEKubeApiServerProxyReady   DCE kube apiserver proxy is posting ready status.\n  TimeNotSynchronized             False   Sun, 27 Sep 2020 03:08:59 +0000   Thu, 24 Sep 2020 10:37:34 +0000   TimeSynchronized             The time of the node is synchronized\n  DCEEngineNotReady               False   Sun, 27 Sep 2020 03:08:59 +0000   Thu, 24 Sep 2020 10:37:34 +0000   DCEEngineReady               DCE engine is posting ready status.\n  DockerDiskPressure              False   Sun, 27 Sep 2020 03:08:59 +0000   Thu, 24 Sep 2020 10:37:34 +0000   DockerHasNoDiskPressure      docker has no disk pressure\n  MemoryPressure                  False   Sun, 27 Sep 2020 03:09:28 +0000   Thu, 24 Sep 2020 09:40:00 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure                    False   Sun, 27 Sep 2020 03:09:28 +0000   Thu, 24 Sep 2020 09:40:00 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure                     False   Sun, 27 Sep 2020 03:09:28 +0000   Thu, 24 Sep 2020 09:40:00 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                           True    Sun, 27 Sep 2020 03:09:28 +0000   Thu, 24 Sep 2020 09:41:51 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.6.171.84\n  Hostname:    dce-10-6-171-84\nCapacity:\n  cpu:                8\n  ephemeral-storage:  37320904Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             12125148Ki\n  pods:               110\nAllocatable:\n  cpu:                7500m\n  ephemeral-storage:  34394945070\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             11498460Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 0deb38c7b3c5463fa65d450c81cad922\n  System UUID:                533e3442-9d89-166e-a31f-1c849d0910f1\n  Boot ID:                    2a931502-9dc1-492f-8ed2-44550e195857\n  Kernel Version:             4.18.0-193.el8.x86_64\n  OS Image:                   Oracle Linux Server 8.2\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://19.3.8\n  Kubelet Version:            v1.18.6\n  Kube-Proxy Version:         v1.18.6\nPodCIDR:                      172.30.2.0/24\nPodCIDRs:                     172.30.2.0/24\nNon-terminated Pods:          (14 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits   AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------   ---\n  dce-system                  dce-system-dnsservice-74c694858b-c7b2w                     300m (4%)     300m (4%)   314572800 (2%)   314572800 (2%)  2d1h\n  default                     aaa-dao-2048-7897cf64bc-krl9t                              50m (0%)      50m (0%)    50M (0%)         50M (0%)        41h\n  default                     dao-2048-dao-2048-db8cd6864-g8txm                          64m (0%)      128m (1%)   268435456 (2%)   268435456 (2%)  41h\n  default                     release-2048-dao-2048-6ff5c774d9-txzdv                     50m (0%)      50m (0%)    50M (0%)         50M (0%)        41h\n  kube-system                 calico-node-7lk8x                                          200m (2%)     800m (10%)  200Mi (1%)       400Mi (3%)      2d17h\n  kube-system                 coredns-coredns-7899d777b7-d9xwj                           250m (3%)     500m (6%)   250M (2%)        250M (2%)       41h\n  kube-system                 coredns-coredns-7899d777b7-tpxhd                           250m (3%)     500m (6%)   250M (2%)        250M (2%)       2d\n  kube-system                 dce-engine-8sng6                                           100m (1%)     400m (5%)   200Mi (1%)       400Mi (3%)      2d17h\n  kube-system                 dce-kube-apiserver-proxy-dce-10-6-171-84                   100m (1%)     400m (5%)   150Mi (1%)       300Mi (2%)      2d17h\n  kube-system                 dce-parcel-agent-85gws                                     250m (3%)     1 (13%)     150Mi (1%)       300Mi (2%)      2d17h\n  kube-system                 kube-proxy-6qqrv                                           100m (1%)     400m (5%)   300Mi (2%)       600Mi (5%)      2d17h\n  kube-system                 node-local-dns-hh96g                                       250m (3%)     1 (13%)     250M (2%)        250M (2%)       2d16h\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-da170140ad3f4ead-glbd8    0 (0%)        0 (0%)      0 (0%)           0 (0%)          29m\n  storage-system              uds-host-metrics-dirver-r7msd                              50m (0%)      150m (2%)   50Mi (0%)        150Mi (1%)      2d1h\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests          Limits\n  --------           --------          ------\n  cpu                2014m (26%)       5678m (75%)\n  memory             2534013056 (21%)  3687446656 (31%)\n  ephemeral-storage  0 (0%)            0 (0%)\n  hugepages-1Gi      0 (0%)            0 (0%)\n  hugepages-2Mi      0 (0%)            0 (0%)\nEvents:              <none>\n"
Sep 27 03:09:34.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 describe namespace kubectl-7831'
Sep 27 03:09:34.836: INFO: stderr: ""
Sep 27 03:09:34.836: INFO: stdout: "Name:         kubectl-7831\nLabels:       e2e-framework=kubectl\n              e2e-run=18bc0502-a78c-4c1b-9d82-fdd96d5bb095\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:09:34.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7831" for this suite.

â€¢ [SLOW TEST:26.868 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:978
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":277,"completed":63,"skipped":1373,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:09:34.850: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2481
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:09:36.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2481" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":277,"completed":64,"skipped":1392,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:09:36.100: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8755
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep 27 03:09:36.814: INFO: Waiting up to 5m0s for pod "pod-7fe6f921-da24-4d0b-97d1-9dc3405fa837" in namespace "emptydir-8755" to be "Succeeded or Failed"
Sep 27 03:09:36.817: INFO: Pod "pod-7fe6f921-da24-4d0b-97d1-9dc3405fa837": Phase="Pending", Reason="", readiness=false. Elapsed: 3.130841ms
Sep 27 03:09:38.821: INFO: Pod "pod-7fe6f921-da24-4d0b-97d1-9dc3405fa837": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007045484s
Sep 27 03:09:41.420: INFO: Pod "pod-7fe6f921-da24-4d0b-97d1-9dc3405fa837": Phase="Pending", Reason="", readiness=false. Elapsed: 4.606206638s
Sep 27 03:09:43.544: INFO: Pod "pod-7fe6f921-da24-4d0b-97d1-9dc3405fa837": Phase="Pending", Reason="", readiness=false. Elapsed: 6.729710214s
Sep 27 03:09:46.382: INFO: Pod "pod-7fe6f921-da24-4d0b-97d1-9dc3405fa837": Phase="Pending", Reason="", readiness=false. Elapsed: 9.568342722s
Sep 27 03:09:48.414: INFO: Pod "pod-7fe6f921-da24-4d0b-97d1-9dc3405fa837": Phase="Pending", Reason="", readiness=false. Elapsed: 11.600008098s
Sep 27 03:09:52.014: INFO: Pod "pod-7fe6f921-da24-4d0b-97d1-9dc3405fa837": Phase="Pending", Reason="", readiness=false. Elapsed: 15.199860597s
Sep 27 03:09:54.145: INFO: Pod "pod-7fe6f921-da24-4d0b-97d1-9dc3405fa837": Phase="Pending", Reason="", readiness=false. Elapsed: 17.331107709s
Sep 27 03:09:56.952: INFO: Pod "pod-7fe6f921-da24-4d0b-97d1-9dc3405fa837": Phase="Pending", Reason="", readiness=false. Elapsed: 20.137937526s
Sep 27 03:09:59.055: INFO: Pod "pod-7fe6f921-da24-4d0b-97d1-9dc3405fa837": Phase="Pending", Reason="", readiness=false. Elapsed: 22.240743873s
Sep 27 03:10:01.134: INFO: Pod "pod-7fe6f921-da24-4d0b-97d1-9dc3405fa837": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.319696116s
STEP: Saw pod success
Sep 27 03:10:01.134: INFO: Pod "pod-7fe6f921-da24-4d0b-97d1-9dc3405fa837" satisfied condition "Succeeded or Failed"
Sep 27 03:10:01.227: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-7fe6f921-da24-4d0b-97d1-9dc3405fa837 container test-container: <nil>
STEP: delete the pod
Sep 27 03:10:01.463: INFO: Waiting for pod pod-7fe6f921-da24-4d0b-97d1-9dc3405fa837 to disappear
Sep 27 03:10:01.639: INFO: Pod pod-7fe6f921-da24-4d0b-97d1-9dc3405fa837 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:10:01.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8755" for this suite.

â€¢ [SLOW TEST:25.669 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":65,"skipped":1399,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:10:01.791: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7887
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod liveness-a398387b-2341-41d5-a1e6-6c0c5a55d40a in namespace container-probe-7887
Sep 27 03:10:14.324: INFO: Started pod liveness-a398387b-2341-41d5-a1e6-6c0c5a55d40a in namespace container-probe-7887
STEP: checking the pod's current state and verifying that restartCount is present
Sep 27 03:10:14.329: INFO: Initial restart count of pod liveness-a398387b-2341-41d5-a1e6-6c0c5a55d40a is 0
Sep 27 03:10:33.134: INFO: Restart count of pod container-probe-7887/liveness-a398387b-2341-41d5-a1e6-6c0c5a55d40a is now 1 (18.804371807s elapsed)
Sep 27 03:10:51.433: INFO: Restart count of pod container-probe-7887/liveness-a398387b-2341-41d5-a1e6-6c0c5a55d40a is now 2 (37.103506056s elapsed)
Sep 27 03:11:11.851: INFO: Restart count of pod container-probe-7887/liveness-a398387b-2341-41d5-a1e6-6c0c5a55d40a is now 3 (57.521423295s elapsed)
Sep 27 03:11:29.991: INFO: Restart count of pod container-probe-7887/liveness-a398387b-2341-41d5-a1e6-6c0c5a55d40a is now 4 (1m15.661445965s elapsed)
Sep 27 03:12:31.390: INFO: Restart count of pod container-probe-7887/liveness-a398387b-2341-41d5-a1e6-6c0c5a55d40a is now 5 (2m17.060424978s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:12:31.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7887" for this suite.

â€¢ [SLOW TEST:150.029 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":277,"completed":66,"skipped":1401,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:12:31.820: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1411
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep 27 03:12:45.544: INFO: Successfully updated pod "pod-update-0e8d46f0-9da7-479d-81d4-2b881b0fb1f3"
STEP: verifying the updated pod is in kubernetes
Sep 27 03:12:45.724: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:12:45.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1411" for this suite.

â€¢ [SLOW TEST:13.939 seconds]
[k8s.io] Pods
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":277,"completed":67,"skipped":1413,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:12:45.760: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2918
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-2918
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 27 03:12:46.182: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 27 03:12:46.671: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 03:12:48.682: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 03:12:50.675: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 03:12:52.825: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 03:12:55.681: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 03:12:58.965: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 03:13:00.738: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 03:13:02.926: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 03:13:04.695: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 03:13:06.674: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 03:13:08.675: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 03:13:10.994: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 03:13:12.675: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 03:13:14.675: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 03:13:16.894: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 03:13:18.674: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 03:13:20.772: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep 27 03:13:20.780: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep 27 03:13:23.138: INFO: The status of Pod netserver-1 is Running (Ready = true)
Sep 27 03:13:23.380: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Sep 27 03:13:35.504: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.29.108.82:8080/dial?request=hostname&protocol=http&host=172.29.74.244&port=8080&tries=1'] Namespace:pod-network-test-2918 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 03:13:35.504: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
Sep 27 03:13:35.715: INFO: Waiting for responses: map[]
Sep 27 03:13:35.720: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.29.108.82:8080/dial?request=hostname&protocol=http&host=172.29.146.25&port=8080&tries=1'] Namespace:pod-network-test-2918 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 03:13:35.720: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
Sep 27 03:13:35.923: INFO: Waiting for responses: map[]
Sep 27 03:13:35.926: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.29.108.82:8080/dial?request=hostname&protocol=http&host=172.29.108.75&port=8080&tries=1'] Namespace:pod-network-test-2918 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 03:13:35.926: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
Sep 27 03:13:36.192: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:13:36.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2918" for this suite.

â€¢ [SLOW TEST:50.491 seconds]
[sig-network] Networking
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":277,"completed":68,"skipped":1447,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:13:36.251: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-27
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap configmap-27/configmap-test-43d9747b-b0f7-46d0-81b5-9f5491bbcbb9
STEP: Creating a pod to test consume configMaps
Sep 27 03:13:36.805: INFO: Waiting up to 5m0s for pod "pod-configmaps-8d7048cb-2622-4c43-aeef-7979e6a47a40" in namespace "configmap-27" to be "Succeeded or Failed"
Sep 27 03:13:36.853: INFO: Pod "pod-configmaps-8d7048cb-2622-4c43-aeef-7979e6a47a40": Phase="Pending", Reason="", readiness=false. Elapsed: 47.67736ms
Sep 27 03:13:38.861: INFO: Pod "pod-configmaps-8d7048cb-2622-4c43-aeef-7979e6a47a40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055084423s
Sep 27 03:13:40.914: INFO: Pod "pod-configmaps-8d7048cb-2622-4c43-aeef-7979e6a47a40": Phase="Pending", Reason="", readiness=false. Elapsed: 4.108271592s
Sep 27 03:13:43.082: INFO: Pod "pod-configmaps-8d7048cb-2622-4c43-aeef-7979e6a47a40": Phase="Pending", Reason="", readiness=false. Elapsed: 6.276729405s
Sep 27 03:13:45.192: INFO: Pod "pod-configmaps-8d7048cb-2622-4c43-aeef-7979e6a47a40": Phase="Pending", Reason="", readiness=false. Elapsed: 8.387030473s
Sep 27 03:13:47.402: INFO: Pod "pod-configmaps-8d7048cb-2622-4c43-aeef-7979e6a47a40": Phase="Pending", Reason="", readiness=false. Elapsed: 10.596268137s
Sep 27 03:13:49.651: INFO: Pod "pod-configmaps-8d7048cb-2622-4c43-aeef-7979e6a47a40": Phase="Pending", Reason="", readiness=false. Elapsed: 12.845404724s
Sep 27 03:13:52.240: INFO: Pod "pod-configmaps-8d7048cb-2622-4c43-aeef-7979e6a47a40": Phase="Pending", Reason="", readiness=false. Elapsed: 15.434901191s
Sep 27 03:13:54.898: INFO: Pod "pod-configmaps-8d7048cb-2622-4c43-aeef-7979e6a47a40": Phase="Pending", Reason="", readiness=false. Elapsed: 18.092825266s
Sep 27 03:13:56.917: INFO: Pod "pod-configmaps-8d7048cb-2622-4c43-aeef-7979e6a47a40": Phase="Pending", Reason="", readiness=false. Elapsed: 20.111459133s
Sep 27 03:13:58.927: INFO: Pod "pod-configmaps-8d7048cb-2622-4c43-aeef-7979e6a47a40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.121639102s
STEP: Saw pod success
Sep 27 03:13:58.927: INFO: Pod "pod-configmaps-8d7048cb-2622-4c43-aeef-7979e6a47a40" satisfied condition "Succeeded or Failed"
Sep 27 03:13:58.934: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-configmaps-8d7048cb-2622-4c43-aeef-7979e6a47a40 container env-test: <nil>
STEP: delete the pod
Sep 27 03:13:59.108: INFO: Waiting for pod pod-configmaps-8d7048cb-2622-4c43-aeef-7979e6a47a40 to disappear
Sep 27 03:13:59.131: INFO: Pod pod-configmaps-8d7048cb-2622-4c43-aeef-7979e6a47a40 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:13:59.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-27" for this suite.

â€¢ [SLOW TEST:22.901 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:34
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":277,"completed":69,"skipped":1481,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:13:59.153: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1062
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:14:16.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1062" for this suite.

â€¢ [SLOW TEST:17.504 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":277,"completed":70,"skipped":1499,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:14:16.657: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-9169
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 03:14:17.384: INFO: Waiting up to 5m0s for pod "busybox-user-65534-48598d7f-2969-46ab-a4a0-c3a23cf94216" in namespace "security-context-test-9169" to be "Succeeded or Failed"
Sep 27 03:14:17.476: INFO: Pod "busybox-user-65534-48598d7f-2969-46ab-a4a0-c3a23cf94216": Phase="Pending", Reason="", readiness=false. Elapsed: 92.101912ms
Sep 27 03:14:19.606: INFO: Pod "busybox-user-65534-48598d7f-2969-46ab-a4a0-c3a23cf94216": Phase="Pending", Reason="", readiness=false. Elapsed: 2.221950383s
Sep 27 03:14:23.827: INFO: Pod "busybox-user-65534-48598d7f-2969-46ab-a4a0-c3a23cf94216": Phase="Pending", Reason="", readiness=false. Elapsed: 6.442789617s
Sep 27 03:14:25.894: INFO: Pod "busybox-user-65534-48598d7f-2969-46ab-a4a0-c3a23cf94216": Phase="Pending", Reason="", readiness=false. Elapsed: 8.509772785s
Sep 27 03:14:28.011: INFO: Pod "busybox-user-65534-48598d7f-2969-46ab-a4a0-c3a23cf94216": Phase="Pending", Reason="", readiness=false. Elapsed: 10.627148445s
Sep 27 03:14:30.019: INFO: Pod "busybox-user-65534-48598d7f-2969-46ab-a4a0-c3a23cf94216": Phase="Pending", Reason="", readiness=false. Elapsed: 12.635237224s
Sep 27 03:14:32.299: INFO: Pod "busybox-user-65534-48598d7f-2969-46ab-a4a0-c3a23cf94216": Phase="Pending", Reason="", readiness=false. Elapsed: 14.915294062s
Sep 27 03:14:34.838: INFO: Pod "busybox-user-65534-48598d7f-2969-46ab-a4a0-c3a23cf94216": Phase="Pending", Reason="", readiness=false. Elapsed: 17.454328947s
Sep 27 03:14:36.843: INFO: Pod "busybox-user-65534-48598d7f-2969-46ab-a4a0-c3a23cf94216": Phase="Succeeded", Reason="", readiness=false. Elapsed: 19.459020454s
Sep 27 03:14:36.843: INFO: Pod "busybox-user-65534-48598d7f-2969-46ab-a4a0-c3a23cf94216" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:14:36.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9169" for this suite.

â€¢ [SLOW TEST:20.204 seconds]
[k8s.io] Security Context
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  When creating a container with runAsUser
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:45
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":71,"skipped":1515,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:14:36.861: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3516
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Sep 27 03:14:37.380: INFO: Waiting up to 5m0s for pod "downward-api-b6b7a55d-8322-48ac-a72f-77c873462217" in namespace "downward-api-3516" to be "Succeeded or Failed"
Sep 27 03:14:37.387: INFO: Pod "downward-api-b6b7a55d-8322-48ac-a72f-77c873462217": Phase="Pending", Reason="", readiness=false. Elapsed: 6.872059ms
Sep 27 03:14:39.391: INFO: Pod "downward-api-b6b7a55d-8322-48ac-a72f-77c873462217": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011109061s
Sep 27 03:14:41.838: INFO: Pod "downward-api-b6b7a55d-8322-48ac-a72f-77c873462217": Phase="Pending", Reason="", readiness=false. Elapsed: 4.457776245s
Sep 27 03:14:43.944: INFO: Pod "downward-api-b6b7a55d-8322-48ac-a72f-77c873462217": Phase="Pending", Reason="", readiness=false. Elapsed: 6.564123958s
Sep 27 03:14:46.136: INFO: Pod "downward-api-b6b7a55d-8322-48ac-a72f-77c873462217": Phase="Pending", Reason="", readiness=false. Elapsed: 8.756132629s
Sep 27 03:14:48.190: INFO: Pod "downward-api-b6b7a55d-8322-48ac-a72f-77c873462217": Phase="Pending", Reason="", readiness=false. Elapsed: 10.810031605s
Sep 27 03:14:50.196: INFO: Pod "downward-api-b6b7a55d-8322-48ac-a72f-77c873462217": Phase="Pending", Reason="", readiness=false. Elapsed: 12.815921749s
Sep 27 03:14:52.346: INFO: Pod "downward-api-b6b7a55d-8322-48ac-a72f-77c873462217": Phase="Pending", Reason="", readiness=false. Elapsed: 14.965961036s
Sep 27 03:14:54.456: INFO: Pod "downward-api-b6b7a55d-8322-48ac-a72f-77c873462217": Phase="Succeeded", Reason="", readiness=false. Elapsed: 17.076136318s
STEP: Saw pod success
Sep 27 03:14:54.456: INFO: Pod "downward-api-b6b7a55d-8322-48ac-a72f-77c873462217" satisfied condition "Succeeded or Failed"
Sep 27 03:14:54.508: INFO: Trying to get logs from node dce-10-6-171-86 pod downward-api-b6b7a55d-8322-48ac-a72f-77c873462217 container dapi-container: <nil>
STEP: delete the pod
Sep 27 03:14:54.849: INFO: Waiting for pod downward-api-b6b7a55d-8322-48ac-a72f-77c873462217 to disappear
Sep 27 03:14:55.032: INFO: Pod downward-api-b6b7a55d-8322-48ac-a72f-77c873462217 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:14:55.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3516" for this suite.

â€¢ [SLOW TEST:18.438 seconds]
[sig-node] Downward API
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":277,"completed":72,"skipped":1530,"failed":0}
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:14:55.300: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3753
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Sep 27 03:14:57.990: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 27 03:14:59.141: INFO: Waiting for terminating namespaces to be deleted...
Sep 27 03:14:59.600: INFO: 
Logging pods the kubelet thinks is on node dce-10-6-171-84 before test
Sep 27 03:15:00.380: INFO: kube-proxy-6qqrv from kube-system started at 2020-09-24 09:40:05 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.380: INFO: 	Container kube-proxy ready: true, restart count 1
Sep 27 03:15:00.380: INFO: dao-2048-dao-2048-db8cd6864-g8txm from default started at 2020-09-25 10:06:04 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.380: INFO: 	Container dao-2048-dao-2048 ready: true, restart count 0
Sep 27 03:15:00.380: INFO: dce-parcel-agent-85gws from kube-system started at 2020-09-24 09:40:06 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.380: INFO: 	Container dce-parcel-agent ready: true, restart count 4
Sep 27 03:15:00.380: INFO: node-local-dns-hh96g from kube-system started at 2020-09-24 10:25:26 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.380: INFO: 	Container node-cache ready: true, restart count 0
Sep 27 03:15:00.380: INFO: coredns-coredns-7899d777b7-tpxhd from kube-system started at 2020-09-25 02:24:32 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.380: INFO: 	Container coredns-coredns ready: true, restart count 0
Sep 27 03:15:00.380: INFO: uds-host-metrics-dirver-r7msd from storage-system started at 2020-09-25 01:46:24 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.380: INFO: 	Container metrics-collector ready: true, restart count 0
Sep 27 03:15:00.380: INFO: aaa-dao-2048-7897cf64bc-krl9t from default started at 2020-09-25 10:06:04 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.380: INFO: 	Container dao-2048 ready: true, restart count 0
Sep 27 03:15:00.380: INFO: calico-node-7lk8x from kube-system started at 2020-09-24 09:40:07 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.380: INFO: 	Container calico-node ready: true, restart count 1
Sep 27 03:15:00.380: INFO: sonobuoy-systemd-logs-daemon-set-da170140ad3f4ead-glbd8 from sonobuoy started at 2020-09-27 02:39:45 +0000 UTC (2 container statuses recorded)
Sep 27 03:15:00.380: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 03:15:00.380: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 03:15:00.380: INFO: release-2048-dao-2048-6ff5c774d9-txzdv from default started at 2020-09-25 10:06:04 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.380: INFO: 	Container dao-2048 ready: true, restart count 0
Sep 27 03:15:00.380: INFO: coredns-coredns-7899d777b7-d9xwj from kube-system started at 2020-09-25 10:06:04 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.380: INFO: 	Container coredns-coredns ready: true, restart count 0
Sep 27 03:15:00.380: INFO: dce-kube-apiserver-proxy-dce-10-6-171-84 from kube-system started at 2020-09-24 09:48:25 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.380: INFO: 	Container dce-kube-apiserver-proxy ready: true, restart count 2
Sep 27 03:15:00.380: INFO: dce-engine-8sng6 from kube-system started at 2020-09-24 09:40:03 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.380: INFO: 	Container dce-engine ready: true, restart count 1
Sep 27 03:15:00.380: INFO: dce-system-dnsservice-74c694858b-c7b2w from dce-system started at 2020-09-25 01:45:36 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.381: INFO: 	Container dce-system-dnsservice ready: true, restart count 0
Sep 27 03:15:00.381: INFO: 
Logging pods the kubelet thinks is on node dce-10-6-171-85 before test
Sep 27 03:15:00.691: INFO: dce-kube-scheduler-dce-10-6-171-85 from kube-system started at 2020-09-24 10:36:45 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.692: INFO: 	Container dce-kube-scheduler ready: true, restart count 4
Sep 27 03:15:00.692: INFO: kube-proxy-mbrkq from kube-system started at 2020-09-24 09:12:07 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.692: INFO: 	Container kube-proxy ready: true, restart count 3
Sep 27 03:15:00.692: INFO: dce-parcel-agent-ndwd9 from kube-system started at 2020-09-24 09:12:07 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.692: INFO: 	Container dce-parcel-agent ready: true, restart count 5
Sep 27 03:15:00.692: INFO: calico-kube-controllers-7449c6cbb8-bq8n5 from kube-system started at 2020-09-24 09:15:06 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.692: INFO: 	Container calico-kube-controllers ready: true, restart count 5
Sep 27 03:15:00.692: INFO: uds-host-metrics-dirver-884nn from storage-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.692: INFO: 	Container metrics-collector ready: true, restart count 3
Sep 27 03:15:00.692: INFO: dce-kube-controller-manager-dce-10-6-171-85 from kube-system started at 2020-09-24 10:36:44 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.692: INFO: 	Container dce-kube-controller-manager ready: true, restart count 4
Sep 27 03:15:00.692: INFO: calico-node-pcdmf from kube-system started at 2020-09-24 09:15:05 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.692: INFO: 	Container calico-node ready: true, restart count 3
Sep 27 03:15:00.692: INFO: dce-system-uds-58c4c68dbf-bbmdk from dce-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.692: INFO: 	Container dce-system-uds ready: false, restart count 5
Sep 27 03:15:00.692: INFO: dce-chart-manager-68fbf6bb4c-7288c from kube-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.692: INFO: 	Container chart-manager ready: true, restart count 3
Sep 27 03:15:00.692: INFO: dce-engine-hfc8k from kube-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.692: INFO: 	Container dce-engine ready: true, restart count 3
Sep 27 03:15:00.692: INFO: dce-core-keepalived-59dd799677-m42wb from kube-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.692: INFO: 	Container dce-core-keepalived ready: true, restart count 3
Sep 27 03:15:00.692: INFO: uds-storage-server-5f7b5f6fdc-q7kmm from storage-system started at 2020-09-25 02:32:02 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.692: INFO: 	Container main ready: false, restart count 2
Sep 27 03:15:00.692: INFO: dce-kube-apiserver-dce-10-6-171-85 from kube-system started at 2020-09-24 10:36:42 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.692: INFO: 	Container dce-kube-apiserver ready: true, restart count 0
Sep 27 03:15:00.692: INFO: dce-etcd-dce-10-6-171-85 from kube-system started at 2020-09-24 10:36:41 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.692: INFO: 	Container dce-etcd ready: true, restart count 3
Sep 27 03:15:00.692: INFO: dce-kube-apiserver-proxy-dce-10-6-171-85 from kube-system started at 2020-09-24 10:36:43 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.692: INFO: 	Container dce-kube-apiserver-proxy ready: true, restart count 3
Sep 27 03:15:00.692: INFO: dce-parcel-server-rwgb8 from kube-system started at 2020-09-24 09:12:07 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.692: INFO: 	Container dce-parcel-server ready: true, restart count 5
Sep 27 03:15:00.692: INFO: dce-system-uds-58c4c68dbf-g8fb6 from dce-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.692: INFO: 	Container dce-system-uds ready: true, restart count 3
Sep 27 03:15:00.692: INFO: uds-storage-server-5f7b5f6fdc-r46qt from storage-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.692: INFO: 	Container main ready: true, restart count 3
Sep 27 03:15:00.692: INFO: node-local-dns-9lnth from kube-system started at 2020-09-24 10:25:26 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.692: INFO: 	Container node-cache ready: true, restart count 1
Sep 27 03:15:00.692: INFO: sonobuoy-systemd-logs-daemon-set-da170140ad3f4ead-2mrkt from sonobuoy started at 2020-09-27 02:39:45 +0000 UTC (2 container statuses recorded)
Sep 27 03:15:00.692: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 03:15:00.692: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 03:15:00.692: INFO: metrics-server-6768fdf9db-56ks5 from kube-system started at 2020-09-24 09:15:28 +0000 UTC (2 container statuses recorded)
Sep 27 03:15:00.692: INFO: 	Container metrics-server ready: true, restart count 3
Sep 27 03:15:00.692: INFO: 	Container metrics-server-nanny ready: true, restart count 3
Sep 27 03:15:00.692: INFO: dce-registry-6cc79fc8c-fpp82 from kube-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.692: INFO: 	Container dce-registry ready: true, restart count 3
Sep 27 03:15:00.692: INFO: dce-prometheus-5dc44cf4dd-jjsjl from kube-system started at 2020-09-24 09:15:28 +0000 UTC (2 container statuses recorded)
Sep 27 03:15:00.692: INFO: 	Container dce-metrics-server ready: true, restart count 3
Sep 27 03:15:00.692: INFO: 	Container dce-prometheus ready: true, restart count 3
Sep 27 03:15:00.692: INFO: 
Logging pods the kubelet thinks is on node dce-10-6-171-86 before test
Sep 27 03:15:00.717: INFO: sonobuoy-systemd-logs-daemon-set-da170140ad3f4ead-fbqzn from sonobuoy started at 2020-09-27 02:39:45 +0000 UTC (2 container statuses recorded)
Sep 27 03:15:00.717: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 03:15:00.717: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 03:15:00.717: INFO: dce-parcel-agent-qrv24 from kube-system started at 2020-09-25 10:51:48 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.717: INFO: 	Container dce-parcel-agent ready: true, restart count 0
Sep 27 03:15:00.717: INFO: dce-engine-dvcxl from kube-system started at 2020-09-24 09:35:13 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.717: INFO: 	Container dce-engine ready: true, restart count 1
Sep 27 03:15:00.717: INFO: sonobuoy-e2e-job-f515d88659da49be from sonobuoy started at 2020-09-27 02:39:45 +0000 UTC (2 container statuses recorded)
Sep 27 03:15:00.717: INFO: 	Container e2e ready: true, restart count 0
Sep 27 03:15:00.717: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 03:15:00.717: INFO: kube-proxy-wjt5k from kube-system started at 2020-09-24 09:35:13 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.717: INFO: 	Container kube-proxy ready: true, restart count 1
Sep 27 03:15:00.717: INFO: calico-node-hsm9f from kube-system started at 2020-09-24 09:35:13 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.717: INFO: 	Container calico-node ready: true, restart count 1
Sep 27 03:15:00.717: INFO: dce-kube-apiserver-proxy-dce-10-6-171-86 from kube-system started at 2020-09-24 09:47:03 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.717: INFO: 	Container dce-kube-apiserver-proxy ready: true, restart count 1
Sep 27 03:15:00.717: INFO: node-local-dns-qwp6t from kube-system started at 2020-09-25 10:52:22 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.717: INFO: 	Container node-cache ready: true, restart count 0
Sep 27 03:15:00.717: INFO: sonobuoy from sonobuoy started at 2020-09-27 02:39:34 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.717: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 27 03:15:00.717: INFO: uds-host-metrics-dirver-48m2w from storage-system started at 2020-09-25 01:46:33 +0000 UTC (1 container statuses recorded)
Sep 27 03:15:00.717: INFO: 	Container metrics-collector ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-c284c1d3-b12d-483d-8262-dc76e1cf9f03 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-c284c1d3-b12d-483d-8262-dc76e1cf9f03 off the node dce-10-6-171-86
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c284c1d3-b12d-483d-8262-dc76e1cf9f03
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:20:35.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3753" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

â€¢ [SLOW TEST:340.154 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":277,"completed":73,"skipped":1540,"failed":0}
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:20:35.454: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9376
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 27 03:20:36.144: INFO: Waiting up to 5m0s for pod "downwardapi-volume-65bc6a66-bfcc-45d1-a30f-66fc077a5e67" in namespace "projected-9376" to be "Succeeded or Failed"
Sep 27 03:20:36.193: INFO: Pod "downwardapi-volume-65bc6a66-bfcc-45d1-a30f-66fc077a5e67": Phase="Pending", Reason="", readiness=false. Elapsed: 49.353843ms
Sep 27 03:20:38.198: INFO: Pod "downwardapi-volume-65bc6a66-bfcc-45d1-a30f-66fc077a5e67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053747664s
Sep 27 03:20:40.272: INFO: Pod "downwardapi-volume-65bc6a66-bfcc-45d1-a30f-66fc077a5e67": Phase="Pending", Reason="", readiness=false. Elapsed: 4.127695027s
Sep 27 03:20:42.312: INFO: Pod "downwardapi-volume-65bc6a66-bfcc-45d1-a30f-66fc077a5e67": Phase="Pending", Reason="", readiness=false. Elapsed: 6.16754213s
Sep 27 03:20:44.428: INFO: Pod "downwardapi-volume-65bc6a66-bfcc-45d1-a30f-66fc077a5e67": Phase="Pending", Reason="", readiness=false. Elapsed: 8.284435226s
Sep 27 03:20:47.076: INFO: Pod "downwardapi-volume-65bc6a66-bfcc-45d1-a30f-66fc077a5e67": Phase="Pending", Reason="", readiness=false. Elapsed: 10.931888225s
Sep 27 03:20:49.083: INFO: Pod "downwardapi-volume-65bc6a66-bfcc-45d1-a30f-66fc077a5e67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.939191669s
STEP: Saw pod success
Sep 27 03:20:49.083: INFO: Pod "downwardapi-volume-65bc6a66-bfcc-45d1-a30f-66fc077a5e67" satisfied condition "Succeeded or Failed"
Sep 27 03:20:49.090: INFO: Trying to get logs from node dce-10-6-171-86 pod downwardapi-volume-65bc6a66-bfcc-45d1-a30f-66fc077a5e67 container client-container: <nil>
STEP: delete the pod
Sep 27 03:20:49.215: INFO: Waiting for pod downwardapi-volume-65bc6a66-bfcc-45d1-a30f-66fc077a5e67 to disappear
Sep 27 03:20:49.238: INFO: Pod downwardapi-volume-65bc6a66-bfcc-45d1-a30f-66fc077a5e67 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:20:49.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9376" for this suite.

â€¢ [SLOW TEST:13.856 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":277,"completed":74,"skipped":1540,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:20:49.310: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9615
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep 27 03:20:49.927: INFO: Waiting up to 5m0s for pod "pod-4cdd9e0a-ce70-4211-ba71-03094d5a3318" in namespace "emptydir-9615" to be "Succeeded or Failed"
Sep 27 03:20:49.934: INFO: Pod "pod-4cdd9e0a-ce70-4211-ba71-03094d5a3318": Phase="Pending", Reason="", readiness=false. Elapsed: 7.235312ms
Sep 27 03:20:51.940: INFO: Pod "pod-4cdd9e0a-ce70-4211-ba71-03094d5a3318": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013822205s
Sep 27 03:20:53.945: INFO: Pod "pod-4cdd9e0a-ce70-4211-ba71-03094d5a3318": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018702934s
Sep 27 03:20:56.582: INFO: Pod "pod-4cdd9e0a-ce70-4211-ba71-03094d5a3318": Phase="Pending", Reason="", readiness=false. Elapsed: 6.655417072s
Sep 27 03:20:58.879: INFO: Pod "pod-4cdd9e0a-ce70-4211-ba71-03094d5a3318": Phase="Pending", Reason="", readiness=false. Elapsed: 8.951869476s
Sep 27 03:21:01.099: INFO: Pod "pod-4cdd9e0a-ce70-4211-ba71-03094d5a3318": Phase="Pending", Reason="", readiness=false. Elapsed: 11.172501875s
Sep 27 03:21:03.104: INFO: Pod "pod-4cdd9e0a-ce70-4211-ba71-03094d5a3318": Phase="Succeeded", Reason="", readiness=false. Elapsed: 13.177352535s
STEP: Saw pod success
Sep 27 03:21:03.104: INFO: Pod "pod-4cdd9e0a-ce70-4211-ba71-03094d5a3318" satisfied condition "Succeeded or Failed"
Sep 27 03:21:03.108: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-4cdd9e0a-ce70-4211-ba71-03094d5a3318 container test-container: <nil>
STEP: delete the pod
Sep 27 03:21:03.134: INFO: Waiting for pod pod-4cdd9e0a-ce70-4211-ba71-03094d5a3318 to disappear
Sep 27 03:21:03.186: INFO: Pod pod-4cdd9e0a-ce70-4211-ba71-03094d5a3318 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:21:03.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9615" for this suite.

â€¢ [SLOW TEST:13.891 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":75,"skipped":1542,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:21:03.202: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7740
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Sep 27 03:21:03.542: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 27 03:21:03.567: INFO: Waiting for terminating namespaces to be deleted...
Sep 27 03:21:03.572: INFO: 
Logging pods the kubelet thinks is on node dce-10-6-171-84 before test
Sep 27 03:21:03.727: INFO: coredns-coredns-7899d777b7-tpxhd from kube-system started at 2020-09-25 02:24:32 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.727: INFO: 	Container coredns-coredns ready: true, restart count 0
Sep 27 03:21:03.727: INFO: uds-host-metrics-dirver-r7msd from storage-system started at 2020-09-25 01:46:24 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.727: INFO: 	Container metrics-collector ready: true, restart count 0
Sep 27 03:21:03.727: INFO: node-local-dns-hh96g from kube-system started at 2020-09-24 10:25:26 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.727: INFO: 	Container node-cache ready: true, restart count 0
Sep 27 03:21:03.727: INFO: calico-node-7lk8x from kube-system started at 2020-09-24 09:40:07 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.727: INFO: 	Container calico-node ready: true, restart count 1
Sep 27 03:21:03.727: INFO: sonobuoy-systemd-logs-daemon-set-da170140ad3f4ead-glbd8 from sonobuoy started at 2020-09-27 02:39:45 +0000 UTC (2 container statuses recorded)
Sep 27 03:21:03.727: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 03:21:03.727: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 03:21:03.727: INFO: aaa-dao-2048-7897cf64bc-krl9t from default started at 2020-09-25 10:06:04 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.727: INFO: 	Container dao-2048 ready: true, restart count 0
Sep 27 03:21:03.727: INFO: dce-kube-apiserver-proxy-dce-10-6-171-84 from kube-system started at 2020-09-24 09:48:25 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.727: INFO: 	Container dce-kube-apiserver-proxy ready: true, restart count 2
Sep 27 03:21:03.727: INFO: dce-engine-8sng6 from kube-system started at 2020-09-24 09:40:03 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.727: INFO: 	Container dce-engine ready: true, restart count 1
Sep 27 03:21:03.727: INFO: release-2048-dao-2048-6ff5c774d9-txzdv from default started at 2020-09-25 10:06:04 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.727: INFO: 	Container dao-2048 ready: true, restart count 0
Sep 27 03:21:03.727: INFO: coredns-coredns-7899d777b7-d9xwj from kube-system started at 2020-09-25 10:06:04 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.727: INFO: 	Container coredns-coredns ready: true, restart count 0
Sep 27 03:21:03.727: INFO: dce-system-dnsservice-74c694858b-c7b2w from dce-system started at 2020-09-25 01:45:36 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.727: INFO: 	Container dce-system-dnsservice ready: true, restart count 0
Sep 27 03:21:03.727: INFO: kube-proxy-6qqrv from kube-system started at 2020-09-24 09:40:05 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.727: INFO: 	Container kube-proxy ready: true, restart count 1
Sep 27 03:21:03.727: INFO: dao-2048-dao-2048-db8cd6864-g8txm from default started at 2020-09-25 10:06:04 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.727: INFO: 	Container dao-2048-dao-2048 ready: true, restart count 0
Sep 27 03:21:03.727: INFO: dce-parcel-agent-85gws from kube-system started at 2020-09-24 09:40:06 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.727: INFO: 	Container dce-parcel-agent ready: true, restart count 4
Sep 27 03:21:03.727: INFO: 
Logging pods the kubelet thinks is on node dce-10-6-171-85 before test
Sep 27 03:21:03.786: INFO: sonobuoy-systemd-logs-daemon-set-da170140ad3f4ead-2mrkt from sonobuoy started at 2020-09-27 02:39:45 +0000 UTC (2 container statuses recorded)
Sep 27 03:21:03.786: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 03:21:03.786: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 03:21:03.786: INFO: metrics-server-6768fdf9db-56ks5 from kube-system started at 2020-09-24 09:15:28 +0000 UTC (2 container statuses recorded)
Sep 27 03:21:03.786: INFO: 	Container metrics-server ready: true, restart count 3
Sep 27 03:21:03.786: INFO: 	Container metrics-server-nanny ready: true, restart count 3
Sep 27 03:21:03.786: INFO: dce-registry-6cc79fc8c-fpp82 from kube-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.786: INFO: 	Container dce-registry ready: true, restart count 3
Sep 27 03:21:03.786: INFO: dce-prometheus-5dc44cf4dd-jjsjl from kube-system started at 2020-09-24 09:15:28 +0000 UTC (2 container statuses recorded)
Sep 27 03:21:03.786: INFO: 	Container dce-metrics-server ready: true, restart count 3
Sep 27 03:21:03.786: INFO: 	Container dce-prometheus ready: true, restart count 3
Sep 27 03:21:03.786: INFO: dce-kube-scheduler-dce-10-6-171-85 from kube-system started at 2020-09-24 10:36:45 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.786: INFO: 	Container dce-kube-scheduler ready: true, restart count 4
Sep 27 03:21:03.786: INFO: kube-proxy-mbrkq from kube-system started at 2020-09-24 09:12:07 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.786: INFO: 	Container kube-proxy ready: true, restart count 3
Sep 27 03:21:03.786: INFO: dce-parcel-agent-ndwd9 from kube-system started at 2020-09-24 09:12:07 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.786: INFO: 	Container dce-parcel-agent ready: true, restart count 5
Sep 27 03:21:03.786: INFO: calico-kube-controllers-7449c6cbb8-bq8n5 from kube-system started at 2020-09-24 09:15:06 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.786: INFO: 	Container calico-kube-controllers ready: true, restart count 5
Sep 27 03:21:03.786: INFO: uds-host-metrics-dirver-884nn from storage-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.786: INFO: 	Container metrics-collector ready: true, restart count 3
Sep 27 03:21:03.786: INFO: dce-kube-controller-manager-dce-10-6-171-85 from kube-system started at 2020-09-24 10:36:44 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.786: INFO: 	Container dce-kube-controller-manager ready: true, restart count 4
Sep 27 03:21:03.786: INFO: calico-node-pcdmf from kube-system started at 2020-09-24 09:15:05 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.786: INFO: 	Container calico-node ready: true, restart count 3
Sep 27 03:21:03.786: INFO: dce-system-uds-58c4c68dbf-bbmdk from dce-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.786: INFO: 	Container dce-system-uds ready: false, restart count 5
Sep 27 03:21:03.786: INFO: dce-chart-manager-68fbf6bb4c-7288c from kube-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.786: INFO: 	Container chart-manager ready: true, restart count 3
Sep 27 03:21:03.786: INFO: dce-engine-hfc8k from kube-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.786: INFO: 	Container dce-engine ready: true, restart count 3
Sep 27 03:21:03.786: INFO: dce-core-keepalived-59dd799677-m42wb from kube-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.786: INFO: 	Container dce-core-keepalived ready: true, restart count 3
Sep 27 03:21:03.786: INFO: uds-storage-server-5f7b5f6fdc-q7kmm from storage-system started at 2020-09-25 02:32:02 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.786: INFO: 	Container main ready: false, restart count 2
Sep 27 03:21:03.786: INFO: dce-kube-apiserver-dce-10-6-171-85 from kube-system started at 2020-09-24 10:36:42 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.786: INFO: 	Container dce-kube-apiserver ready: true, restart count 0
Sep 27 03:21:03.786: INFO: dce-etcd-dce-10-6-171-85 from kube-system started at 2020-09-24 10:36:41 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.786: INFO: 	Container dce-etcd ready: true, restart count 3
Sep 27 03:21:03.786: INFO: dce-kube-apiserver-proxy-dce-10-6-171-85 from kube-system started at 2020-09-24 10:36:43 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.786: INFO: 	Container dce-kube-apiserver-proxy ready: true, restart count 3
Sep 27 03:21:03.786: INFO: dce-parcel-server-rwgb8 from kube-system started at 2020-09-24 09:12:07 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.786: INFO: 	Container dce-parcel-server ready: true, restart count 5
Sep 27 03:21:03.786: INFO: dce-system-uds-58c4c68dbf-g8fb6 from dce-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.786: INFO: 	Container dce-system-uds ready: true, restart count 3
Sep 27 03:21:03.786: INFO: uds-storage-server-5f7b5f6fdc-r46qt from storage-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.786: INFO: 	Container main ready: true, restart count 3
Sep 27 03:21:03.786: INFO: node-local-dns-9lnth from kube-system started at 2020-09-24 10:25:26 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.786: INFO: 	Container node-cache ready: true, restart count 1
Sep 27 03:21:03.786: INFO: 
Logging pods the kubelet thinks is on node dce-10-6-171-86 before test
Sep 27 03:21:03.804: INFO: dce-parcel-agent-qrv24 from kube-system started at 2020-09-25 10:51:48 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.804: INFO: 	Container dce-parcel-agent ready: true, restart count 0
Sep 27 03:21:03.804: INFO: sonobuoy-systemd-logs-daemon-set-da170140ad3f4ead-fbqzn from sonobuoy started at 2020-09-27 02:39:45 +0000 UTC (2 container statuses recorded)
Sep 27 03:21:03.804: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 03:21:03.804: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 03:21:03.804: INFO: kube-proxy-wjt5k from kube-system started at 2020-09-24 09:35:13 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.804: INFO: 	Container kube-proxy ready: true, restart count 1
Sep 27 03:21:03.804: INFO: dce-engine-dvcxl from kube-system started at 2020-09-24 09:35:13 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.804: INFO: 	Container dce-engine ready: true, restart count 1
Sep 27 03:21:03.805: INFO: sonobuoy-e2e-job-f515d88659da49be from sonobuoy started at 2020-09-27 02:39:45 +0000 UTC (2 container statuses recorded)
Sep 27 03:21:03.805: INFO: 	Container e2e ready: true, restart count 0
Sep 27 03:21:03.805: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 03:21:03.805: INFO: calico-node-hsm9f from kube-system started at 2020-09-24 09:35:13 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.805: INFO: 	Container calico-node ready: true, restart count 1
Sep 27 03:21:03.805: INFO: dce-kube-apiserver-proxy-dce-10-6-171-86 from kube-system started at 2020-09-24 09:47:03 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.805: INFO: 	Container dce-kube-apiserver-proxy ready: true, restart count 1
Sep 27 03:21:03.805: INFO: sonobuoy from sonobuoy started at 2020-09-27 02:39:34 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.805: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 27 03:21:03.805: INFO: node-local-dns-qwp6t from kube-system started at 2020-09-25 10:52:22 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.805: INFO: 	Container node-cache ready: true, restart count 0
Sep 27 03:21:03.805: INFO: uds-host-metrics-dirver-48m2w from storage-system started at 2020-09-25 01:46:33 +0000 UTC (1 container statuses recorded)
Sep 27 03:21:03.805: INFO: 	Container metrics-collector ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.163885dfd093595f], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:21:04.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7740" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82
â€¢{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":277,"completed":76,"skipped":1562,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:21:05.189: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3822
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 27 03:21:09.336: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f4cc9905-73c6-495a-8734-181eccf444aa" in namespace "downward-api-3822" to be "Succeeded or Failed"
Sep 27 03:21:09.356: INFO: Pod "downwardapi-volume-f4cc9905-73c6-495a-8734-181eccf444aa": Phase="Pending", Reason="", readiness=false. Elapsed: 20.186073ms
Sep 27 03:21:11.360: INFO: Pod "downwardapi-volume-f4cc9905-73c6-495a-8734-181eccf444aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024089383s
Sep 27 03:21:13.368: INFO: Pod "downwardapi-volume-f4cc9905-73c6-495a-8734-181eccf444aa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032214296s
Sep 27 03:21:15.639: INFO: Pod "downwardapi-volume-f4cc9905-73c6-495a-8734-181eccf444aa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.302657122s
Sep 27 03:21:17.645: INFO: Pod "downwardapi-volume-f4cc9905-73c6-495a-8734-181eccf444aa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.308288871s
Sep 27 03:21:19.798: INFO: Pod "downwardapi-volume-f4cc9905-73c6-495a-8734-181eccf444aa": Phase="Pending", Reason="", readiness=false. Elapsed: 10.462281164s
Sep 27 03:21:22.938: INFO: Pod "downwardapi-volume-f4cc9905-73c6-495a-8734-181eccf444aa": Phase="Pending", Reason="", readiness=false. Elapsed: 13.601991507s
Sep 27 03:21:25.138: INFO: Pod "downwardapi-volume-f4cc9905-73c6-495a-8734-181eccf444aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 15.802006722s
STEP: Saw pod success
Sep 27 03:21:25.138: INFO: Pod "downwardapi-volume-f4cc9905-73c6-495a-8734-181eccf444aa" satisfied condition "Succeeded or Failed"
Sep 27 03:21:25.576: INFO: Trying to get logs from node dce-10-6-171-86 pod downwardapi-volume-f4cc9905-73c6-495a-8734-181eccf444aa container client-container: <nil>
STEP: delete the pod
Sep 27 03:21:26.183: INFO: Waiting for pod downwardapi-volume-f4cc9905-73c6-495a-8734-181eccf444aa to disappear
Sep 27 03:21:26.205: INFO: Pod downwardapi-volume-f4cc9905-73c6-495a-8734-181eccf444aa no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:21:26.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3822" for this suite.

â€¢ [SLOW TEST:21.061 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":277,"completed":77,"skipped":1563,"failed":0}
SSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:21:26.251: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-808
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod liveness-516e4c86-6626-475b-94be-ccbdaec8bd05 in namespace container-probe-808
Sep 27 03:21:44.816: INFO: Started pod liveness-516e4c86-6626-475b-94be-ccbdaec8bd05 in namespace container-probe-808
STEP: checking the pod's current state and verifying that restartCount is present
Sep 27 03:21:45.209: INFO: Initial restart count of pod liveness-516e4c86-6626-475b-94be-ccbdaec8bd05 is 0
Sep 27 03:22:00.269: INFO: Restart count of pod container-probe-808/liveness-516e4c86-6626-475b-94be-ccbdaec8bd05 is now 1 (15.059824181s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:22:02.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-808" for this suite.

â€¢ [SLOW TEST:36.165 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":277,"completed":78,"skipped":1569,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:22:02.416: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3599
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep 27 03:22:02.989: INFO: Waiting up to 5m0s for pod "pod-cfb6899b-c3a5-4346-aa41-5ef7a0c74d0d" in namespace "emptydir-3599" to be "Succeeded or Failed"
Sep 27 03:22:02.993: INFO: Pod "pod-cfb6899b-c3a5-4346-aa41-5ef7a0c74d0d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.371622ms
Sep 27 03:22:05.051: INFO: Pod "pod-cfb6899b-c3a5-4346-aa41-5ef7a0c74d0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06216082s
Sep 27 03:22:07.257: INFO: Pod "pod-cfb6899b-c3a5-4346-aa41-5ef7a0c74d0d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.268269762s
Sep 27 03:22:09.453: INFO: Pod "pod-cfb6899b-c3a5-4346-aa41-5ef7a0c74d0d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.464547031s
Sep 27 03:22:11.682: INFO: Pod "pod-cfb6899b-c3a5-4346-aa41-5ef7a0c74d0d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.693533297s
Sep 27 03:22:13.851: INFO: Pod "pod-cfb6899b-c3a5-4346-aa41-5ef7a0c74d0d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.862313702s
Sep 27 03:22:15.967: INFO: Pod "pod-cfb6899b-c3a5-4346-aa41-5ef7a0c74d0d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.978185753s
Sep 27 03:22:17.972: INFO: Pod "pod-cfb6899b-c3a5-4346-aa41-5ef7a0c74d0d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.983363404s
Sep 27 03:22:20.089: INFO: Pod "pod-cfb6899b-c3a5-4346-aa41-5ef7a0c74d0d": Phase="Pending", Reason="", readiness=false. Elapsed: 17.100293395s
Sep 27 03:22:22.093: INFO: Pod "pod-cfb6899b-c3a5-4346-aa41-5ef7a0c74d0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 19.104460175s
STEP: Saw pod success
Sep 27 03:22:22.093: INFO: Pod "pod-cfb6899b-c3a5-4346-aa41-5ef7a0c74d0d" satisfied condition "Succeeded or Failed"
Sep 27 03:22:22.118: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-cfb6899b-c3a5-4346-aa41-5ef7a0c74d0d container test-container: <nil>
STEP: delete the pod
Sep 27 03:22:22.573: INFO: Waiting for pod pod-cfb6899b-c3a5-4346-aa41-5ef7a0c74d0d to disappear
Sep 27 03:22:22.601: INFO: Pod pod-cfb6899b-c3a5-4346-aa41-5ef7a0c74d0d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:22:22.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3599" for this suite.

â€¢ [SLOW TEST:20.245 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":79,"skipped":1573,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:22:22.661: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6717
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service nodeport-service with the type=NodePort in namespace services-6717
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6717
STEP: creating replication controller externalsvc in namespace services-6717
I0927 03:22:23.543509      23 runners.go:190] Created replication controller with name: externalsvc, namespace: services-6717, replica count: 2
I0927 03:22:26.656999      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 03:22:29.683504      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 03:22:32.684084      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 03:22:35.770333      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 03:22:38.770594      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Sep 27 03:22:38.867: INFO: Creating new exec pod
Sep 27 03:22:48.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=services-6717 execpoddxlxr -- /bin/sh -x -c nslookup nodeport-service'
Sep 27 03:22:51.893: INFO: stderr: "+ nslookup nodeport-service\n"
Sep 27 03:22:51.893: INFO: stdout: "Server:\t\t172.31.0.10\nAddress:\t172.31.0.10#53\n\nnodeport-service.services-6717.svc.cluster.local\tcanonical name = externalsvc.services-6717.svc.cluster.local.\nName:\texternalsvc.services-6717.svc.cluster.local\nAddress: 172.31.10.251\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6717, will wait for the garbage collector to delete the pods
Sep 27 03:22:51.973: INFO: Deleting ReplicationController externalsvc took: 5.181195ms
Sep 27 03:22:52.574: INFO: Terminating ReplicationController externalsvc pods took: 600.261671ms
Sep 27 03:23:04.816: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:23:04.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6717" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

â€¢ [SLOW TEST:42.324 seconds]
[sig-network] Services
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":277,"completed":80,"skipped":1574,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:23:04.985: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8094
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 03:23:05.951: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 27 03:23:07.988: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773786, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773786, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773786, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773785, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 03:23:10.281: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773786, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773786, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773786, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773785, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 03:23:11.992: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773786, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773786, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773786, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773785, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 03:23:13.992: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773786, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773786, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773786, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773785, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 03:23:15.992: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773786, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773786, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773786, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773785, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 03:23:18.102: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773786, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773786, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773786, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773785, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 03:23:20.079: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773786, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773786, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773786, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773785, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 03:23:22.998: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 03:23:23.002: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8795-crds.webhook.example.com via the AdmissionRegistration API
Sep 27 03:23:33.523: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:23:34.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8094" for this suite.
STEP: Destroying namespace "webhook-8094-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:29.495 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":277,"completed":81,"skipped":1582,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:23:34.481: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5792
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Sep 27 03:23:34.874: INFO: PodSpec: initContainers in spec.initContainers
Sep 27 03:24:24.480: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-ee817ad4-2b3c-4cc7-b437-5d76bad68399", GenerateName:"", Namespace:"init-container-5792", SelfLink:"/api/v1/namespaces/init-container-5792/pods/pod-init-ee817ad4-2b3c-4cc7-b437-5d76bad68399", UID:"1c51ba44-f17e-49e8-b92f-5a51bec9b607", ResourceVersion:"688764", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63736773814, loc:(*time.Location)(0x7b51220)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"874441915"}, Annotations:map[string]string{"cni.projectcalico.org/ipv4pools":"[\"default-ipv4-ippool\"]", "dce.daocloud.io/parcel.egress.burst":"0", "dce.daocloud.io/parcel.egress.rate":"0", "dce.daocloud.io/parcel.ingress.burst":"0", "dce.daocloud.io/parcel.ingress.rate":"0", "kubernetes.io/psp":"dce-psp-allow-all"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-274cr", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc006b8c000), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-274cr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-274cr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-274cr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0008100b8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"dce-10-6-171-86", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002f98000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000810320)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000810340)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc000810348), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773814, loc:(*time.Location)(0x7b51220)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773814, loc:(*time.Location)(0x7b51220)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773814, loc:(*time.Location)(0x7b51220)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773814, loc:(*time.Location)(0x7b51220)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.6.171.86", PodIP:"172.29.108.102", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.29.108.102"}}, StartTime:(*v1.Time)(0xc000f584c0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002f980e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002f98150)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://e8b1d4bccab8bdf5a55ce7e6e1893fd1c8b0954c32a675a317f20686154ca929", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000f58b00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000f58620), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc00081050f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:24:24.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5792" for this suite.

â€¢ [SLOW TEST:50.085 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":277,"completed":82,"skipped":1621,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:24:24.566: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7549
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:24:36.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7549" for this suite.

â€¢ [SLOW TEST:12.326 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when scheduling a read only busybox container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:188
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":83,"skipped":1634,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:24:36.893: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-4180
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 03:24:37.203: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:24:37.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4180" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":277,"completed":84,"skipped":1666,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:24:37.894: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8534
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 03:24:38.153: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Sep 27 03:24:38.162: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep 27 03:24:43.321: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 27 03:24:49.396: INFO: Creating deployment "test-rolling-update-deployment"
Sep 27 03:24:49.399: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Sep 27 03:24:49.407: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Sep 27 03:24:51.426: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Sep 27 03:24:51.445: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773889, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773889, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773889, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773889, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-59d5cb45c7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 03:24:53.469: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773889, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773889, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773889, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773889, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-59d5cb45c7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 03:24:55.449: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773889, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773889, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773889, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773889, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-59d5cb45c7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 03:24:57.450: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Sep 27 03:24:57.461: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-8534 /apis/apps/v1/namespaces/deployment-8534/deployments/test-rolling-update-deployment f0955de4-699f-4cc7-b037-36f853eaee82 688989 1 2020-09-27 03:24:49 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00459e218 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-09-27 03:24:49 +0000 UTC,LastTransitionTime:2020-09-27 03:24:49 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-59d5cb45c7" has successfully progressed.,LastUpdateTime:2020-09-27 03:24:55 +0000 UTC,LastTransitionTime:2020-09-27 03:24:49 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 27 03:24:57.465: INFO: New ReplicaSet "test-rolling-update-deployment-59d5cb45c7" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-59d5cb45c7  deployment-8534 /apis/apps/v1/namespaces/deployment-8534/replicasets/test-rolling-update-deployment-59d5cb45c7 a541898d-889d-4dda-b871-1165f4e58847 688978 1 2020-09-27 03:24:49 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:59d5cb45c7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment f0955de4-699f-4cc7-b037-36f853eaee82 0xc006001587 0xc006001588}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 59d5cb45c7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:59d5cb45c7] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0060015f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 27 03:24:57.465: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Sep 27 03:24:57.465: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-8534 /apis/apps/v1/namespaces/deployment-8534/replicasets/test-rolling-update-controller a00438cd-1549-4133-8476-2a866736bf61 688988 2 2020-09-27 03:24:38 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment f0955de4-699f-4cc7-b037-36f853eaee82 0xc0060014b7 0xc0060014b8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006001518 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 27 03:24:57.469: INFO: Pod "test-rolling-update-deployment-59d5cb45c7-4wwr5" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-59d5cb45c7-4wwr5 test-rolling-update-deployment-59d5cb45c7- deployment-8534 /api/v1/namespaces/deployment-8534/pods/test-rolling-update-deployment-59d5cb45c7-4wwr5 09e17118-68f1-401e-b3d8-49014f728aaf 688977 0 2020-09-27 03:24:49 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:59d5cb45c7] map[cni.projectcalico.org/ipv4pools:["default-ipv4-ippool"] dce.daocloud.io/parcel.egress.burst:0 dce.daocloud.io/parcel.egress.rate:0 dce.daocloud.io/parcel.ingress.burst:0 dce.daocloud.io/parcel.ingress.rate:0 kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet test-rolling-update-deployment-59d5cb45c7 a541898d-889d-4dda-b871-1165f4e58847 0xc006001b67 0xc006001b68}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wrkwn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wrkwn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wrkwn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 03:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 03:24:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 03:24:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 03:24:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.6.171.86,PodIP:172.29.108.101,StartTime:2020-09-27 03:24:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-27 03:24:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,ImageID:docker-pullable://us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c,ContainerID:docker://b4ab71dda8df38525c3c5c126530de19474d30088d7e04c9562b0d87b4121f42,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.29.108.101,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:24:57.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8534" for this suite.

â€¢ [SLOW TEST:19.586 seconds]
[sig-apps] Deployment
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":277,"completed":85,"skipped":1697,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:24:57.481: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4279
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 27 03:24:57.850: INFO: Waiting up to 5m0s for pod "downwardapi-volume-168ee807-b0bb-466b-83cb-41f31f029e19" in namespace "projected-4279" to be "Succeeded or Failed"
Sep 27 03:24:57.862: INFO: Pod "downwardapi-volume-168ee807-b0bb-466b-83cb-41f31f029e19": Phase="Pending", Reason="", readiness=false. Elapsed: 11.920593ms
Sep 27 03:24:59.870: INFO: Pod "downwardapi-volume-168ee807-b0bb-466b-83cb-41f31f029e19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020033098s
Sep 27 03:25:02.253: INFO: Pod "downwardapi-volume-168ee807-b0bb-466b-83cb-41f31f029e19": Phase="Pending", Reason="", readiness=false. Elapsed: 4.402221195s
Sep 27 03:25:04.263: INFO: Pod "downwardapi-volume-168ee807-b0bb-466b-83cb-41f31f029e19": Phase="Pending", Reason="", readiness=false. Elapsed: 6.41230683s
Sep 27 03:25:06.477: INFO: Pod "downwardapi-volume-168ee807-b0bb-466b-83cb-41f31f029e19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.626879146s
STEP: Saw pod success
Sep 27 03:25:06.477: INFO: Pod "downwardapi-volume-168ee807-b0bb-466b-83cb-41f31f029e19" satisfied condition "Succeeded or Failed"
Sep 27 03:25:06.493: INFO: Trying to get logs from node dce-10-6-171-86 pod downwardapi-volume-168ee807-b0bb-466b-83cb-41f31f029e19 container client-container: <nil>
STEP: delete the pod
Sep 27 03:25:06.932: INFO: Waiting for pod downwardapi-volume-168ee807-b0bb-466b-83cb-41f31f029e19 to disappear
Sep 27 03:25:06.949: INFO: Pod downwardapi-volume-168ee807-b0bb-466b-83cb-41f31f029e19 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:25:06.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4279" for this suite.

â€¢ [SLOW TEST:9.872 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":277,"completed":86,"skipped":1709,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:25:07.353: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-300
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 03:25:10.658: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 27 03:25:13.162: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773910, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773910, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773910, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773910, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 03:25:15.185: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773910, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773910, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773910, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773910, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 03:25:17.310: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773910, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773910, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773910, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773910, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 03:25:19.229: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773910, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773910, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773910, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773910, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 03:25:21.166: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773910, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773910, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773910, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736773910, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 03:25:24.210: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:25:36.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-300" for this suite.
STEP: Destroying namespace "webhook-300-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:29.496 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":277,"completed":87,"skipped":1717,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:25:36.850: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2838
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Update Demo
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:271
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a replication controller
Sep 27 03:25:37.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 create -f - --namespace=kubectl-2838'
Sep 27 03:25:37.832: INFO: stderr: ""
Sep 27 03:25:37.832: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 27 03:25:37.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2838'
Sep 27 03:25:37.966: INFO: stderr: ""
Sep 27 03:25:37.966: INFO: stdout: "update-demo-nautilus-429sb update-demo-nautilus-pmdzl "
Sep 27 03:25:37.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods update-demo-nautilus-429sb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2838'
Sep 27 03:25:38.071: INFO: stderr: ""
Sep 27 03:25:38.071: INFO: stdout: ""
Sep 27 03:25:38.071: INFO: update-demo-nautilus-429sb is created but not running
Sep 27 03:25:43.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2838'
Sep 27 03:25:44.684: INFO: stderr: ""
Sep 27 03:25:44.684: INFO: stdout: "update-demo-nautilus-429sb update-demo-nautilus-pmdzl "
Sep 27 03:25:44.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods update-demo-nautilus-429sb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2838'
Sep 27 03:25:46.311: INFO: stderr: ""
Sep 27 03:25:46.311: INFO: stdout: ""
Sep 27 03:25:46.311: INFO: update-demo-nautilus-429sb is created but not running
Sep 27 03:25:51.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2838'
Sep 27 03:25:51.510: INFO: stderr: ""
Sep 27 03:25:51.510: INFO: stdout: "update-demo-nautilus-429sb update-demo-nautilus-pmdzl "
Sep 27 03:25:51.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods update-demo-nautilus-429sb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2838'
Sep 27 03:25:51.667: INFO: stderr: ""
Sep 27 03:25:51.667: INFO: stdout: ""
Sep 27 03:25:51.667: INFO: update-demo-nautilus-429sb is created but not running
Sep 27 03:25:56.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2838'
Sep 27 03:25:57.391: INFO: stderr: ""
Sep 27 03:25:57.391: INFO: stdout: "update-demo-nautilus-429sb update-demo-nautilus-pmdzl "
Sep 27 03:25:57.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods update-demo-nautilus-429sb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2838'
Sep 27 03:25:57.750: INFO: stderr: ""
Sep 27 03:25:57.750: INFO: stdout: "true"
Sep 27 03:25:57.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods update-demo-nautilus-429sb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2838'
Sep 27 03:25:57.854: INFO: stderr: ""
Sep 27 03:25:57.854: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 27 03:25:57.854: INFO: validating pod update-demo-nautilus-429sb
Sep 27 03:25:57.862: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 27 03:25:57.862: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 27 03:25:57.862: INFO: update-demo-nautilus-429sb is verified up and running
Sep 27 03:25:57.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods update-demo-nautilus-pmdzl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2838'
Sep 27 03:25:57.975: INFO: stderr: ""
Sep 27 03:25:57.975: INFO: stdout: "true"
Sep 27 03:25:57.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods update-demo-nautilus-pmdzl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2838'
Sep 27 03:25:58.081: INFO: stderr: ""
Sep 27 03:25:58.081: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 27 03:25:58.081: INFO: validating pod update-demo-nautilus-pmdzl
Sep 27 03:25:58.088: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 27 03:25:58.088: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 27 03:25:58.088: INFO: update-demo-nautilus-pmdzl is verified up and running
STEP: using delete to clean up resources
Sep 27 03:25:58.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 delete --grace-period=0 --force -f - --namespace=kubectl-2838'
Sep 27 03:25:58.213: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 27 03:25:58.213: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep 27 03:25:58.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2838'
Sep 27 03:25:58.334: INFO: stderr: "No resources found in kubectl-2838 namespace.\n"
Sep 27 03:25:58.334: INFO: stdout: ""
Sep 27 03:25:58.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods -l name=update-demo --namespace=kubectl-2838 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 27 03:25:58.447: INFO: stderr: ""
Sep 27 03:25:58.447: INFO: stdout: "update-demo-nautilus-429sb\nupdate-demo-nautilus-pmdzl\n"
Sep 27 03:25:58.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2838'
Sep 27 03:25:59.271: INFO: stderr: "No resources found in kubectl-2838 namespace.\n"
Sep 27 03:25:59.271: INFO: stdout: ""
Sep 27 03:25:59.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods -l name=update-demo --namespace=kubectl-2838 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 27 03:25:59.379: INFO: stderr: ""
Sep 27 03:25:59.379: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:25:59.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2838" for this suite.

â€¢ [SLOW TEST:22.542 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:269
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":277,"completed":88,"skipped":1721,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:25:59.392: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3024
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-3024
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-3024
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3024
Sep 27 03:26:00.002: INFO: Found 0 stateful pods, waiting for 1
Sep 27 03:26:10.006: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Sep 27 03:26:20.006: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Sep 27 03:26:20.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-3024 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 27 03:26:20.341: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 27 03:26:20.341: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 27 03:26:20.341: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 27 03:26:20.345: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep 27 03:26:30.382: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 27 03:26:30.383: INFO: Waiting for statefulset status.replicas updated to 0
Sep 27 03:26:30.398: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999109s
Sep 27 03:26:31.495: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.965776278s
Sep 27 03:26:32.766: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.896633028s
Sep 27 03:26:33.953: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.625702831s
Sep 27 03:26:35.058: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.439591531s
Sep 27 03:26:36.140: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.333776735s
Sep 27 03:26:37.146: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.25171381s
Sep 27 03:26:38.152: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.244506964s
Sep 27 03:26:39.157: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.239912153s
Sep 27 03:26:40.162: INFO: Verifying statefulset ss doesn't scale past 1 for another 234.864806ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3024
Sep 27 03:26:41.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-3024 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:26:41.523: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 27 03:26:41.523: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 27 03:26:41.523: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 27 03:26:41.527: INFO: Found 1 stateful pods, waiting for 3
Sep 27 03:26:51.532: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 03:26:51.532: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 03:26:51.532: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep 27 03:27:01.532: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 03:27:01.532: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 03:27:01.532: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Sep 27 03:27:01.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-3024 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 27 03:27:01.863: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 27 03:27:01.863: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 27 03:27:01.863: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 27 03:27:01.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-3024 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 27 03:27:03.101: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 27 03:27:03.101: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 27 03:27:03.101: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 27 03:27:03.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-3024 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 27 03:27:03.493: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 27 03:27:03.493: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 27 03:27:03.493: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 27 03:27:03.493: INFO: Waiting for statefulset status.replicas updated to 0
Sep 27 03:27:03.497: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Sep 27 03:27:13.509: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 27 03:27:13.509: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep 27 03:27:13.509: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep 27 03:27:13.522: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999143s
Sep 27 03:27:14.527: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994554091s
Sep 27 03:27:15.532: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989805274s
Sep 27 03:27:16.565: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98447471s
Sep 27 03:27:17.571: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.951749149s
Sep 27 03:27:18.619: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.930781063s
Sep 27 03:27:19.808: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.725404909s
Sep 27 03:27:20.854: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.680362164s
Sep 27 03:27:21.861: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.662751292s
Sep 27 03:27:22.870: INFO: Verifying statefulset ss doesn't scale past 3 for another 655.758635ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3024
Sep 27 03:27:23.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-3024 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:27:24.193: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 27 03:27:24.193: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 27 03:27:24.193: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 27 03:27:24.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-3024 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:27:24.669: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 27 03:27:24.669: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 27 03:27:24.669: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 27 03:27:24.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-3024 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:27:25.080: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 27 03:27:25.080: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 27 03:27:25.080: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 27 03:27:25.080: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Sep 27 03:27:55.170: INFO: Deleting all statefulset in ns statefulset-3024
Sep 27 03:27:55.286: INFO: Scaling statefulset ss to 0
Sep 27 03:27:55.548: INFO: Waiting for statefulset status.replicas updated to 0
Sep 27 03:27:55.713: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:27:55.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3024" for this suite.

â€¢ [SLOW TEST:116.828 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":277,"completed":89,"skipped":1742,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:27:56.220: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1540
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-map-ec0b6142-a1d9-4607-b4ad-e12c11ca8d1a
STEP: Creating a pod to test consume secrets
Sep 27 03:27:57.691: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3057f190-5d73-47fb-b9bb-50e9803197ea" in namespace "projected-1540" to be "Succeeded or Failed"
Sep 27 03:27:57.786: INFO: Pod "pod-projected-secrets-3057f190-5d73-47fb-b9bb-50e9803197ea": Phase="Pending", Reason="", readiness=false. Elapsed: 94.982042ms
Sep 27 03:27:59.863: INFO: Pod "pod-projected-secrets-3057f190-5d73-47fb-b9bb-50e9803197ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.172798595s
Sep 27 03:28:01.867: INFO: Pod "pod-projected-secrets-3057f190-5d73-47fb-b9bb-50e9803197ea": Phase="Pending", Reason="", readiness=false. Elapsed: 4.176026235s
Sep 27 03:28:03.873: INFO: Pod "pod-projected-secrets-3057f190-5d73-47fb-b9bb-50e9803197ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.182287401s
STEP: Saw pod success
Sep 27 03:28:03.873: INFO: Pod "pod-projected-secrets-3057f190-5d73-47fb-b9bb-50e9803197ea" satisfied condition "Succeeded or Failed"
Sep 27 03:28:03.879: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-projected-secrets-3057f190-5d73-47fb-b9bb-50e9803197ea container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 27 03:28:03.949: INFO: Waiting for pod pod-projected-secrets-3057f190-5d73-47fb-b9bb-50e9803197ea to disappear
Sep 27 03:28:03.956: INFO: Pod pod-projected-secrets-3057f190-5d73-47fb-b9bb-50e9803197ea no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:28:03.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1540" for this suite.

â€¢ [SLOW TEST:7.752 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":277,"completed":90,"skipped":1747,"failed":0}
S
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:28:03.973: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-5437
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Sep 27 03:28:24.974: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5437 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 03:28:24.974: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
Sep 27 03:28:25.132: INFO: Exec stderr: ""
Sep 27 03:28:25.132: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5437 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 03:28:25.132: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
Sep 27 03:28:25.344: INFO: Exec stderr: ""
Sep 27 03:28:25.344: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5437 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 03:28:25.344: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
Sep 27 03:28:25.525: INFO: Exec stderr: ""
Sep 27 03:28:25.525: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5437 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 03:28:25.526: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
Sep 27 03:28:25.688: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Sep 27 03:28:25.688: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5437 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 03:28:25.688: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
Sep 27 03:28:25.871: INFO: Exec stderr: ""
Sep 27 03:28:25.871: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5437 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 03:28:25.871: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
Sep 27 03:28:26.037: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Sep 27 03:28:26.037: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5437 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 03:28:26.037: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
Sep 27 03:28:26.202: INFO: Exec stderr: ""
Sep 27 03:28:26.202: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5437 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 03:28:26.202: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
Sep 27 03:28:26.378: INFO: Exec stderr: ""
Sep 27 03:28:26.378: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5437 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 03:28:26.378: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
Sep 27 03:28:26.556: INFO: Exec stderr: ""
Sep 27 03:28:26.556: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5437 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 03:28:26.556: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
Sep 27 03:28:26.712: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:28:26.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-5437" for this suite.

â€¢ [SLOW TEST:22.752 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":91,"skipped":1748,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:28:26.725: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7124
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:28:26.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7124" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702
â€¢{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":277,"completed":92,"skipped":1790,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:28:26.950: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-3667
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 03:28:27.187: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:28:28.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3667" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":277,"completed":93,"skipped":1824,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:28:28.301: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2135
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: set up a multi version CRD
Sep 27 03:28:29.410: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:28:58.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2135" for this suite.

â€¢ [SLOW TEST:30.447 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":277,"completed":94,"skipped":1855,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:28:58.748: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5275
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-7f5b6932-39cb-45c8-9978-a730159e5822
STEP: Creating a pod to test consume configMaps
Sep 27 03:28:59.008: INFO: Waiting up to 5m0s for pod "pod-configmaps-0e17803a-d153-4d05-979a-607689007282" in namespace "configmap-5275" to be "Succeeded or Failed"
Sep 27 03:28:59.013: INFO: Pod "pod-configmaps-0e17803a-d153-4d05-979a-607689007282": Phase="Pending", Reason="", readiness=false. Elapsed: 4.944882ms
Sep 27 03:29:01.019: INFO: Pod "pod-configmaps-0e17803a-d153-4d05-979a-607689007282": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010759524s
Sep 27 03:29:03.023: INFO: Pod "pod-configmaps-0e17803a-d153-4d05-979a-607689007282": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015068831s
Sep 27 03:29:05.031: INFO: Pod "pod-configmaps-0e17803a-d153-4d05-979a-607689007282": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022803251s
Sep 27 03:29:07.035: INFO: Pod "pod-configmaps-0e17803a-d153-4d05-979a-607689007282": Phase="Pending", Reason="", readiness=false. Elapsed: 8.026674499s
Sep 27 03:29:09.146: INFO: Pod "pod-configmaps-0e17803a-d153-4d05-979a-607689007282": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.137482405s
STEP: Saw pod success
Sep 27 03:29:09.146: INFO: Pod "pod-configmaps-0e17803a-d153-4d05-979a-607689007282" satisfied condition "Succeeded or Failed"
Sep 27 03:29:09.180: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-configmaps-0e17803a-d153-4d05-979a-607689007282 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 27 03:29:09.599: INFO: Waiting for pod pod-configmaps-0e17803a-d153-4d05-979a-607689007282 to disappear
Sep 27 03:29:09.618: INFO: Pod pod-configmaps-0e17803a-d153-4d05-979a-607689007282 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:29:09.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5275" for this suite.

â€¢ [SLOW TEST:10.890 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":277,"completed":95,"skipped":1889,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:29:09.639: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2116
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Sep 27 03:29:09.943: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2116 /api/v1/namespaces/watch-2116/configmaps/e2e-watch-test-watch-closed 5fe4ea88-e0fd-4583-b58d-7618148ff82d 690403 0 2020-09-27 03:29:09 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 03:29:09.943: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2116 /api/v1/namespaces/watch-2116/configmaps/e2e-watch-test-watch-closed 5fe4ea88-e0fd-4583-b58d-7618148ff82d 690404 0 2020-09-27 03:29:09 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Sep 27 03:29:10.100: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2116 /api/v1/namespaces/watch-2116/configmaps/e2e-watch-test-watch-closed 5fe4ea88-e0fd-4583-b58d-7618148ff82d 690405 0 2020-09-27 03:29:09 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 03:29:10.100: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2116 /api/v1/namespaces/watch-2116/configmaps/e2e-watch-test-watch-closed 5fe4ea88-e0fd-4583-b58d-7618148ff82d 690407 0 2020-09-27 03:29:09 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:29:10.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2116" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":277,"completed":96,"skipped":1899,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:29:10.146: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7884
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 03:29:10.454: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 27 03:29:14.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 --namespace=crd-publish-openapi-7884 create -f -'
Sep 27 03:29:21.150: INFO: stderr: ""
Sep 27 03:29:21.150: INFO: stdout: "e2e-test-crd-publish-openapi-2628-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep 27 03:29:21.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 --namespace=crd-publish-openapi-7884 delete e2e-test-crd-publish-openapi-2628-crds test-cr'
Sep 27 03:29:21.262: INFO: stderr: ""
Sep 27 03:29:21.262: INFO: stdout: "e2e-test-crd-publish-openapi-2628-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Sep 27 03:29:21.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 --namespace=crd-publish-openapi-7884 apply -f -'
Sep 27 03:29:21.634: INFO: stderr: ""
Sep 27 03:29:21.634: INFO: stdout: "e2e-test-crd-publish-openapi-2628-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep 27 03:29:21.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 --namespace=crd-publish-openapi-7884 delete e2e-test-crd-publish-openapi-2628-crds test-cr'
Sep 27 03:29:21.753: INFO: stderr: ""
Sep 27 03:29:21.753: INFO: stdout: "e2e-test-crd-publish-openapi-2628-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Sep 27 03:29:21.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 explain e2e-test-crd-publish-openapi-2628-crds'
Sep 27 03:29:21.981: INFO: stderr: ""
Sep 27 03:29:21.981: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2628-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:29:26.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7884" for this suite.

â€¢ [SLOW TEST:16.445 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":277,"completed":97,"skipped":1920,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:29:26.592: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4141
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep 27 03:29:51.040: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 27 03:29:51.064: INFO: Pod pod-with-prestop-http-hook still exists
Sep 27 03:29:53.064: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 27 03:29:53.069: INFO: Pod pod-with-prestop-http-hook still exists
Sep 27 03:29:55.101: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 27 03:29:55.121: INFO: Pod pod-with-prestop-http-hook still exists
Sep 27 03:29:57.064: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 27 03:29:57.110: INFO: Pod pod-with-prestop-http-hook still exists
Sep 27 03:29:59.064: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 27 03:29:59.097: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:29:59.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4141" for this suite.

â€¢ [SLOW TEST:32.564 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":277,"completed":98,"skipped":1938,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:29:59.157: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3435
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Sep 27 03:29:59.408: INFO: Created pod &Pod{ObjectMeta:{dns-3435  dns-3435 /api/v1/namespaces/dns-3435/pods/dns-3435 948666b6-30e8-446e-b037-3ceb00871809 690647 0 2020-09-27 03:29:59 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:dce-psp-allow-all] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tg8nf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tg8nf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tg8nf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 03:29:59.441: INFO: The status of Pod dns-3435 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 03:30:01.444: INFO: The status of Pod dns-3435 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 03:30:03.454: INFO: The status of Pod dns-3435 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 03:30:05.482: INFO: The status of Pod dns-3435 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 03:30:07.445: INFO: The status of Pod dns-3435 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Sep 27 03:30:07.445: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3435 PodName:dns-3435 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 03:30:07.445: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Verifying customized DNS server is configured on pod...
Sep 27 03:30:07.657: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3435 PodName:dns-3435 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 03:30:07.657: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
Sep 27 03:30:07.947: INFO: Deleting pod dns-3435...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:30:08.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3435" for this suite.

â€¢ [SLOW TEST:9.079 seconds]
[sig-network] DNS
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":277,"completed":99,"skipped":1987,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:30:08.237: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2830
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 03:30:11.112: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 27 03:30:13.122: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774211, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774211, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774211, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774210, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 03:30:15.126: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774211, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774211, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774211, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774210, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 03:30:17.130: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774211, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774211, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774211, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774210, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 03:30:20.486: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 03:30:20.490: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4146-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:30:22.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2830" for this suite.
STEP: Destroying namespace "webhook-2830-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:14.677 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":277,"completed":100,"skipped":2000,"failed":0}
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:30:22.914: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5761
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-5761
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 27 03:30:23.469: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 27 03:30:23.812: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 03:30:25.817: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 03:30:27.815: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 03:30:31.144: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 03:30:32.543: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 03:30:33.911: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 03:30:35.935: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 03:30:37.816: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 03:30:39.816: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 03:30:41.817: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 03:30:43.998: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 03:30:45.984: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 03:30:47.816: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep 27 03:30:47.822: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep 27 03:30:49.827: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep 27 03:30:51.827: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep 27 03:30:53.827: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep 27 03:30:55.937: INFO: The status of Pod netserver-1 is Running (Ready = true)
Sep 27 03:30:55.946: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Sep 27 03:31:06.194: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.29.74.241:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5761 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 03:31:06.194: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
Sep 27 03:31:06.377: INFO: Found all expected endpoints: [netserver-0]
Sep 27 03:31:06.380: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.29.146.27:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5761 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 03:31:06.380: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
Sep 27 03:31:06.556: INFO: Found all expected endpoints: [netserver-1]
Sep 27 03:31:06.559: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.29.108.90:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5761 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 03:31:06.559: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
Sep 27 03:31:06.839: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:31:06.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5761" for this suite.

â€¢ [SLOW TEST:44.163 seconds]
[sig-network] Networking
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":101,"skipped":2004,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:31:07.078: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename limitrange
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in limitrange-7401
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Sep 27 03:31:08.110: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Sep 27 03:31:08.115: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep 27 03:31:08.115: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Sep 27 03:31:08.130: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep 27 03:31:08.130: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Sep 27 03:31:08.231: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Sep 27 03:31:08.232: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Sep 27 03:31:15.822: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:31:15.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-7401" for this suite.

â€¢ [SLOW TEST:8.899 seconds]
[sig-scheduling] LimitRange
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":277,"completed":102,"skipped":2016,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:31:15.978: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9688
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep 27 03:31:17.055: INFO: Waiting up to 5m0s for pod "pod-3dec677d-aad5-40e3-ac40-baf96d43dab3" in namespace "emptydir-9688" to be "Succeeded or Failed"
Sep 27 03:31:17.148: INFO: Pod "pod-3dec677d-aad5-40e3-ac40-baf96d43dab3": Phase="Pending", Reason="", readiness=false. Elapsed: 93.00156ms
Sep 27 03:31:19.247: INFO: Pod "pod-3dec677d-aad5-40e3-ac40-baf96d43dab3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.191839797s
Sep 27 03:31:21.429: INFO: Pod "pod-3dec677d-aad5-40e3-ac40-baf96d43dab3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.374236563s
Sep 27 03:31:23.564: INFO: Pod "pod-3dec677d-aad5-40e3-ac40-baf96d43dab3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.509246118s
Sep 27 03:31:25.642: INFO: Pod "pod-3dec677d-aad5-40e3-ac40-baf96d43dab3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.586977655s
Sep 27 03:31:27.646: INFO: Pod "pod-3dec677d-aad5-40e3-ac40-baf96d43dab3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.590719883s
Sep 27 03:31:29.724: INFO: Pod "pod-3dec677d-aad5-40e3-ac40-baf96d43dab3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.669168716s
Sep 27 03:31:31.790: INFO: Pod "pod-3dec677d-aad5-40e3-ac40-baf96d43dab3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.735510097s
Sep 27 03:31:33.802: INFO: Pod "pod-3dec677d-aad5-40e3-ac40-baf96d43dab3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.747167496s
STEP: Saw pod success
Sep 27 03:31:33.802: INFO: Pod "pod-3dec677d-aad5-40e3-ac40-baf96d43dab3" satisfied condition "Succeeded or Failed"
Sep 27 03:31:33.807: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-3dec677d-aad5-40e3-ac40-baf96d43dab3 container test-container: <nil>
STEP: delete the pod
Sep 27 03:31:33.917: INFO: Waiting for pod pod-3dec677d-aad5-40e3-ac40-baf96d43dab3 to disappear
Sep 27 03:31:33.921: INFO: Pod pod-3dec677d-aad5-40e3-ac40-baf96d43dab3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:31:33.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9688" for this suite.

â€¢ [SLOW TEST:17.965 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":103,"skipped":2030,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:31:33.943: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3825
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep 27 03:31:35.370: INFO: Waiting up to 5m0s for pod "pod-09cbe20f-52c6-42bc-ba84-88e7897ddafb" in namespace "emptydir-3825" to be "Succeeded or Failed"
Sep 27 03:31:35.703: INFO: Pod "pod-09cbe20f-52c6-42bc-ba84-88e7897ddafb": Phase="Pending", Reason="", readiness=false. Elapsed: 333.263569ms
Sep 27 03:31:37.818: INFO: Pod "pod-09cbe20f-52c6-42bc-ba84-88e7897ddafb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.448419643s
Sep 27 03:31:39.926: INFO: Pod "pod-09cbe20f-52c6-42bc-ba84-88e7897ddafb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.55589861s
Sep 27 03:31:41.992: INFO: Pod "pod-09cbe20f-52c6-42bc-ba84-88e7897ddafb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.622339976s
Sep 27 03:31:44.062: INFO: Pod "pod-09cbe20f-52c6-42bc-ba84-88e7897ddafb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.69173199s
STEP: Saw pod success
Sep 27 03:31:44.062: INFO: Pod "pod-09cbe20f-52c6-42bc-ba84-88e7897ddafb" satisfied condition "Succeeded or Failed"
Sep 27 03:31:44.418: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-09cbe20f-52c6-42bc-ba84-88e7897ddafb container test-container: <nil>
STEP: delete the pod
Sep 27 03:31:44.996: INFO: Waiting for pod pod-09cbe20f-52c6-42bc-ba84-88e7897ddafb to disappear
Sep 27 03:31:45.014: INFO: Pod pod-09cbe20f-52c6-42bc-ba84-88e7897ddafb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:31:45.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3825" for this suite.

â€¢ [SLOW TEST:11.139 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":104,"skipped":2064,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:31:45.082: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7378
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:157
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:31:45.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7378" for this suite.
â€¢{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":277,"completed":105,"skipped":2084,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:31:46.083: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1115
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 03:31:47.300: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 27 03:31:49.313: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774307, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774307, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774307, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774307, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 03:31:51.365: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774307, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774307, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774307, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774307, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 03:31:53.435: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774307, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774307, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774307, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774307, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 03:31:55.793: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774307, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774307, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774307, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774307, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 03:31:57.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774307, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774307, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774307, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774307, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 03:31:59.390: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774307, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774307, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774307, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774307, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 03:32:01.329: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774307, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774307, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774307, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736774307, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 03:32:04.799: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the webhook via the AdmissionRegistration API
Sep 27 03:32:06.267: INFO: Waiting for webhook configuration to be ready...
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Sep 27 03:32:14.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 attach --namespace=webhook-1115 to-be-attached-pod -i -c=container1'
Sep 27 03:32:14.602: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:32:14.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1115" for this suite.
STEP: Destroying namespace "webhook-1115-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:28.895 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":277,"completed":106,"skipped":2091,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:32:14.978: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5539
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 03:32:15.231: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Sep 27 03:32:17.416: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:32:17.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5539" for this suite.
â€¢{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":277,"completed":107,"skipped":2101,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:32:17.542: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3411
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 27 03:32:18.154: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1dde8adf-6bb3-4445-97c0-a91e853f9ba5" in namespace "downward-api-3411" to be "Succeeded or Failed"
Sep 27 03:32:18.171: INFO: Pod "downwardapi-volume-1dde8adf-6bb3-4445-97c0-a91e853f9ba5": Phase="Pending", Reason="", readiness=false. Elapsed: 17.12604ms
Sep 27 03:32:20.832: INFO: Pod "downwardapi-volume-1dde8adf-6bb3-4445-97c0-a91e853f9ba5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.677608759s
Sep 27 03:32:22.982: INFO: Pod "downwardapi-volume-1dde8adf-6bb3-4445-97c0-a91e853f9ba5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.828446287s
Sep 27 03:32:25.143: INFO: Pod "downwardapi-volume-1dde8adf-6bb3-4445-97c0-a91e853f9ba5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.988971267s
Sep 27 03:32:27.212: INFO: Pod "downwardapi-volume-1dde8adf-6bb3-4445-97c0-a91e853f9ba5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.057674607s
Sep 27 03:32:29.336: INFO: Pod "downwardapi-volume-1dde8adf-6bb3-4445-97c0-a91e853f9ba5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.181721792s
Sep 27 03:32:31.357: INFO: Pod "downwardapi-volume-1dde8adf-6bb3-4445-97c0-a91e853f9ba5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.202533065s
Sep 27 03:32:33.372: INFO: Pod "downwardapi-volume-1dde8adf-6bb3-4445-97c0-a91e853f9ba5": Phase="Pending", Reason="", readiness=false. Elapsed: 15.217739592s
Sep 27 03:32:35.521: INFO: Pod "downwardapi-volume-1dde8adf-6bb3-4445-97c0-a91e853f9ba5": Phase="Pending", Reason="", readiness=false. Elapsed: 17.366745345s
Sep 27 03:32:37.849: INFO: Pod "downwardapi-volume-1dde8adf-6bb3-4445-97c0-a91e853f9ba5": Phase="Pending", Reason="", readiness=false. Elapsed: 19.694948925s
Sep 27 03:32:39.855: INFO: Pod "downwardapi-volume-1dde8adf-6bb3-4445-97c0-a91e853f9ba5": Phase="Pending", Reason="", readiness=false. Elapsed: 21.700639894s
Sep 27 03:32:41.938: INFO: Pod "downwardapi-volume-1dde8adf-6bb3-4445-97c0-a91e853f9ba5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 23.784330769s
STEP: Saw pod success
Sep 27 03:32:41.938: INFO: Pod "downwardapi-volume-1dde8adf-6bb3-4445-97c0-a91e853f9ba5" satisfied condition "Succeeded or Failed"
Sep 27 03:32:42.041: INFO: Trying to get logs from node dce-10-6-171-86 pod downwardapi-volume-1dde8adf-6bb3-4445-97c0-a91e853f9ba5 container client-container: <nil>
STEP: delete the pod
Sep 27 03:32:42.712: INFO: Waiting for pod downwardapi-volume-1dde8adf-6bb3-4445-97c0-a91e853f9ba5 to disappear
Sep 27 03:32:42.938: INFO: Pod downwardapi-volume-1dde8adf-6bb3-4445-97c0-a91e853f9ba5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:32:42.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3411" for this suite.

â€¢ [SLOW TEST:25.474 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":277,"completed":108,"skipped":2109,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:32:43.017: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9996
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-configmap-ktdf
STEP: Creating a pod to test atomic-volume-subpath
Sep 27 03:32:44.218: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-ktdf" in namespace "subpath-9996" to be "Succeeded or Failed"
Sep 27 03:32:44.250: INFO: Pod "pod-subpath-test-configmap-ktdf": Phase="Pending", Reason="", readiness=false. Elapsed: 32.413391ms
Sep 27 03:32:46.551: INFO: Pod "pod-subpath-test-configmap-ktdf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.333871841s
Sep 27 03:32:48.613: INFO: Pod "pod-subpath-test-configmap-ktdf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.394959639s
Sep 27 03:32:50.761: INFO: Pod "pod-subpath-test-configmap-ktdf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.543098338s
Sep 27 03:32:52.946: INFO: Pod "pod-subpath-test-configmap-ktdf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.728483181s
Sep 27 03:32:55.536: INFO: Pod "pod-subpath-test-configmap-ktdf": Phase="Pending", Reason="", readiness=false. Elapsed: 11.318572206s
Sep 27 03:32:57.727: INFO: Pod "pod-subpath-test-configmap-ktdf": Phase="Pending", Reason="", readiness=false. Elapsed: 13.508993984s
Sep 27 03:32:59.877: INFO: Pod "pod-subpath-test-configmap-ktdf": Phase="Pending", Reason="", readiness=false. Elapsed: 15.659197607s
Sep 27 03:33:01.883: INFO: Pod "pod-subpath-test-configmap-ktdf": Phase="Running", Reason="", readiness=true. Elapsed: 17.665399406s
Sep 27 03:33:03.886: INFO: Pod "pod-subpath-test-configmap-ktdf": Phase="Running", Reason="", readiness=true. Elapsed: 19.668806692s
Sep 27 03:33:06.091: INFO: Pod "pod-subpath-test-configmap-ktdf": Phase="Running", Reason="", readiness=true. Elapsed: 21.873635289s
Sep 27 03:33:08.165: INFO: Pod "pod-subpath-test-configmap-ktdf": Phase="Running", Reason="", readiness=true. Elapsed: 23.947254363s
Sep 27 03:33:10.173: INFO: Pod "pod-subpath-test-configmap-ktdf": Phase="Running", Reason="", readiness=true. Elapsed: 25.955284572s
Sep 27 03:33:12.177: INFO: Pod "pod-subpath-test-configmap-ktdf": Phase="Running", Reason="", readiness=true. Elapsed: 27.959816139s
Sep 27 03:33:14.182: INFO: Pod "pod-subpath-test-configmap-ktdf": Phase="Running", Reason="", readiness=true. Elapsed: 29.964250586s
Sep 27 03:33:16.188: INFO: Pod "pod-subpath-test-configmap-ktdf": Phase="Running", Reason="", readiness=true. Elapsed: 31.970580615s
Sep 27 03:33:18.265: INFO: Pod "pod-subpath-test-configmap-ktdf": Phase="Running", Reason="", readiness=true. Elapsed: 34.047225616s
Sep 27 03:33:20.447: INFO: Pod "pod-subpath-test-configmap-ktdf": Phase="Running", Reason="", readiness=true. Elapsed: 36.229362865s
Sep 27 03:33:22.671: INFO: Pod "pod-subpath-test-configmap-ktdf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 38.453533434s
STEP: Saw pod success
Sep 27 03:33:22.671: INFO: Pod "pod-subpath-test-configmap-ktdf" satisfied condition "Succeeded or Failed"
Sep 27 03:33:22.692: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-subpath-test-configmap-ktdf container test-container-subpath-configmap-ktdf: <nil>
STEP: delete the pod
Sep 27 03:33:23.123: INFO: Waiting for pod pod-subpath-test-configmap-ktdf to disappear
Sep 27 03:33:23.137: INFO: Pod pod-subpath-test-configmap-ktdf no longer exists
STEP: Deleting pod pod-subpath-test-configmap-ktdf
Sep 27 03:33:23.137: INFO: Deleting pod "pod-subpath-test-configmap-ktdf" in namespace "subpath-9996"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:33:23.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9996" for this suite.

â€¢ [SLOW TEST:40.305 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":277,"completed":109,"skipped":2132,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:33:23.322: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-3955
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: getting the auto-created API token
Sep 27 03:33:24.365: INFO: created pod pod-service-account-defaultsa
Sep 27 03:33:24.365: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Sep 27 03:33:24.372: INFO: created pod pod-service-account-mountsa
Sep 27 03:33:24.372: INFO: pod pod-service-account-mountsa service account token volume mount: true
Sep 27 03:33:24.382: INFO: created pod pod-service-account-nomountsa
Sep 27 03:33:24.382: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Sep 27 03:33:24.403: INFO: created pod pod-service-account-defaultsa-mountspec
Sep 27 03:33:24.403: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Sep 27 03:33:24.439: INFO: created pod pod-service-account-mountsa-mountspec
Sep 27 03:33:24.440: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Sep 27 03:33:24.513: INFO: created pod pod-service-account-nomountsa-mountspec
Sep 27 03:33:24.513: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Sep 27 03:33:24.539: INFO: created pod pod-service-account-defaultsa-nomountspec
Sep 27 03:33:24.539: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Sep 27 03:33:24.549: INFO: created pod pod-service-account-mountsa-nomountspec
Sep 27 03:33:24.549: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Sep 27 03:33:24.579: INFO: created pod pod-service-account-nomountsa-nomountspec
Sep 27 03:33:24.579: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:33:24.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3955" for this suite.
â€¢{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":277,"completed":110,"skipped":2161,"failed":0}
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:33:24.713: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9512
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep 27 03:34:07.092: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 27 03:34:07.095: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 27 03:34:09.095: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 27 03:34:09.103: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 27 03:34:11.095: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 27 03:34:11.182: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 27 03:34:13.095: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 27 03:34:13.111: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 27 03:34:15.095: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 27 03:34:15.137: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 27 03:34:17.095: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 27 03:34:17.100: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 27 03:34:19.243: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 27 03:34:19.275: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:34:19.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9512" for this suite.

â€¢ [SLOW TEST:55.196 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":277,"completed":111,"skipped":2162,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:34:19.910: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-9111
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a Namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nspatchtest-d6bf0eb0-dfb6-406e-8c8a-52de934e4cf2-1614
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:34:22.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9111" for this suite.
STEP: Destroying namespace "nspatchtest-d6bf0eb0-dfb6-406e-8c8a-52de934e4cf2-1614" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":277,"completed":112,"skipped":2199,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:34:23.042: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-3007
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 27 03:34:32.785: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:34:32.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3007" for this suite.

â€¢ [SLOW TEST:9.992 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  blackbox test
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
    on terminated container
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:133
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":277,"completed":113,"skipped":2206,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:34:33.034: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5940
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating Agnhost RC
Sep 27 03:34:33.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 create -f - --namespace=kubectl-5940'
Sep 27 03:34:36.391: INFO: stderr: ""
Sep 27 03:34:36.391: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Sep 27 03:34:37.604: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 03:34:37.604: INFO: Found 0 / 1
Sep 27 03:34:38.897: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 03:34:38.897: INFO: Found 0 / 1
Sep 27 03:34:39.407: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 03:34:39.408: INFO: Found 0 / 1
Sep 27 03:34:40.677: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 03:34:40.678: INFO: Found 0 / 1
Sep 27 03:34:41.428: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 03:34:41.428: INFO: Found 0 / 1
Sep 27 03:34:42.395: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 03:34:42.395: INFO: Found 0 / 1
Sep 27 03:34:43.397: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 03:34:43.397: INFO: Found 0 / 1
Sep 27 03:34:44.396: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 03:34:44.396: INFO: Found 0 / 1
Sep 27 03:34:45.400: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 03:34:45.400: INFO: Found 0 / 1
Sep 27 03:34:46.400: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 03:34:46.400: INFO: Found 0 / 1
Sep 27 03:34:47.396: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 03:34:47.396: INFO: Found 1 / 1
Sep 27 03:34:47.396: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Sep 27 03:34:47.402: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 03:34:47.402: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 27 03:34:47.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 patch pod agnhost-master-q2sk5 --namespace=kubectl-5940 -p {"metadata":{"annotations":{"x":"y"}}}'
Sep 27 03:34:47.545: INFO: stderr: ""
Sep 27 03:34:47.545: INFO: stdout: "pod/agnhost-master-q2sk5 patched\n"
STEP: checking annotations
Sep 27 03:34:47.550: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 03:34:47.550: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:34:47.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5940" for this suite.

â€¢ [SLOW TEST:14.526 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1363
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":277,"completed":114,"skipped":2213,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:34:47.560: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8057
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-secret-rw8h
STEP: Creating a pod to test atomic-volume-subpath
Sep 27 03:34:47.922: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-rw8h" in namespace "subpath-8057" to be "Succeeded or Failed"
Sep 27 03:34:47.947: INFO: Pod "pod-subpath-test-secret-rw8h": Phase="Pending", Reason="", readiness=false. Elapsed: 25.197387ms
Sep 27 03:34:49.955: INFO: Pod "pod-subpath-test-secret-rw8h": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032701633s
Sep 27 03:34:51.958: INFO: Pod "pod-subpath-test-secret-rw8h": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036446233s
Sep 27 03:34:54.093: INFO: Pod "pod-subpath-test-secret-rw8h": Phase="Pending", Reason="", readiness=false. Elapsed: 6.171124772s
Sep 27 03:34:56.104: INFO: Pod "pod-subpath-test-secret-rw8h": Phase="Pending", Reason="", readiness=false. Elapsed: 8.182034994s
Sep 27 03:34:58.108: INFO: Pod "pod-subpath-test-secret-rw8h": Phase="Running", Reason="", readiness=true. Elapsed: 10.185953943s
Sep 27 03:35:00.112: INFO: Pod "pod-subpath-test-secret-rw8h": Phase="Running", Reason="", readiness=true. Elapsed: 12.190026334s
Sep 27 03:35:02.116: INFO: Pod "pod-subpath-test-secret-rw8h": Phase="Running", Reason="", readiness=true. Elapsed: 14.194493022s
Sep 27 03:35:04.120: INFO: Pod "pod-subpath-test-secret-rw8h": Phase="Running", Reason="", readiness=true. Elapsed: 16.198317931s
Sep 27 03:35:06.381: INFO: Pod "pod-subpath-test-secret-rw8h": Phase="Running", Reason="", readiness=true. Elapsed: 18.459412656s
Sep 27 03:35:08.386: INFO: Pod "pod-subpath-test-secret-rw8h": Phase="Running", Reason="", readiness=true. Elapsed: 20.46421356s
Sep 27 03:35:10.715: INFO: Pod "pod-subpath-test-secret-rw8h": Phase="Running", Reason="", readiness=true. Elapsed: 22.792898844s
Sep 27 03:35:12.720: INFO: Pod "pod-subpath-test-secret-rw8h": Phase="Running", Reason="", readiness=true. Elapsed: 24.797926475s
Sep 27 03:35:14.724: INFO: Pod "pod-subpath-test-secret-rw8h": Phase="Running", Reason="", readiness=true. Elapsed: 26.801569462s
Sep 27 03:35:16.801: INFO: Pod "pod-subpath-test-secret-rw8h": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.878811107s
STEP: Saw pod success
Sep 27 03:35:16.801: INFO: Pod "pod-subpath-test-secret-rw8h" satisfied condition "Succeeded or Failed"
Sep 27 03:35:16.807: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-subpath-test-secret-rw8h container test-container-subpath-secret-rw8h: <nil>
STEP: delete the pod
Sep 27 03:35:16.937: INFO: Waiting for pod pod-subpath-test-secret-rw8h to disappear
Sep 27 03:35:17.051: INFO: Pod pod-subpath-test-secret-rw8h no longer exists
STEP: Deleting pod pod-subpath-test-secret-rw8h
Sep 27 03:35:17.051: INFO: Deleting pod "pod-subpath-test-secret-rw8h" in namespace "subpath-8057"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:35:17.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8057" for this suite.

â€¢ [SLOW TEST:29.626 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":277,"completed":115,"skipped":2269,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:35:17.187: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-3785
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:35:17.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-3785" for this suite.
â€¢{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":277,"completed":116,"skipped":2308,"failed":0}
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:35:17.934: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7312
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-7312
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating stateful set ss in namespace statefulset-7312
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7312
Sep 27 03:35:19.399: INFO: Found 0 stateful pods, waiting for 1
Sep 27 03:35:29.403: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Sep 27 03:35:39.408: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Sep 27 03:35:39.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 27 03:35:39.835: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 27 03:35:39.835: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 27 03:35:39.836: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 27 03:35:39.841: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep 27 03:35:49.846: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 27 03:35:49.846: INFO: Waiting for statefulset status.replicas updated to 0
Sep 27 03:35:49.868: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Sep 27 03:35:49.868: INFO: ss-0  dce-10-6-171-86  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:19 +0000 UTC  }]
Sep 27 03:35:49.868: INFO: 
Sep 27 03:35:49.868: INFO: StatefulSet ss has not reached scale 3, at 1
Sep 27 03:35:50.874: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995233973s
Sep 27 03:35:51.879: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989108035s
Sep 27 03:35:52.884: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.983869316s
Sep 27 03:35:53.890: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.977739778s
Sep 27 03:35:55.011: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.972891674s
Sep 27 03:35:56.083: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.852514185s
Sep 27 03:35:57.090: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.779825574s
Sep 27 03:35:58.096: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.773007751s
Sep 27 03:35:59.102: INFO: Verifying statefulset ss doesn't scale past 3 for another 767.448703ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7312
Sep 27 03:36:00.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:36:00.411: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 27 03:36:00.411: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 27 03:36:00.411: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 27 03:36:00.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:36:00.759: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep 27 03:36:00.759: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 27 03:36:00.759: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 27 03:36:00.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:36:01.537: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep 27 03:36:01.823: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 27 03:36:01.823: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 27 03:36:02.143: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 03:36:02.143: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 03:36:02.143: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Sep 27 03:36:03.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 27 03:36:03.673: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 27 03:36:03.673: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 27 03:36:03.673: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 27 03:36:03.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 27 03:36:04.123: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 27 03:36:04.123: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 27 03:36:04.123: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 27 03:36:04.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 27 03:36:04.618: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 27 03:36:04.618: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 27 03:36:04.618: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 27 03:36:04.618: INFO: Waiting for statefulset status.replicas updated to 0
Sep 27 03:36:04.705: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Sep 27 03:36:14.712: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 27 03:36:14.712: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep 27 03:36:14.713: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep 27 03:36:14.723: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Sep 27 03:36:14.723: INFO: ss-0  dce-10-6-171-86  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:19 +0000 UTC  }]
Sep 27 03:36:14.723: INFO: ss-1  dce-10-6-171-84  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:49 +0000 UTC  }]
Sep 27 03:36:14.723: INFO: ss-2  dce-10-6-171-85  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:50 +0000 UTC  }]
Sep 27 03:36:14.723: INFO: 
Sep 27 03:36:14.723: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 27 03:36:15.729: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Sep 27 03:36:15.729: INFO: ss-0  dce-10-6-171-86  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:19 +0000 UTC  }]
Sep 27 03:36:15.729: INFO: ss-1  dce-10-6-171-84  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:49 +0000 UTC  }]
Sep 27 03:36:15.729: INFO: ss-2  dce-10-6-171-85  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:50 +0000 UTC  }]
Sep 27 03:36:15.729: INFO: 
Sep 27 03:36:15.729: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 27 03:36:16.735: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Sep 27 03:36:16.735: INFO: ss-0  dce-10-6-171-86  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:19 +0000 UTC  }]
Sep 27 03:36:16.735: INFO: ss-1  dce-10-6-171-84  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:49 +0000 UTC  }]
Sep 27 03:36:16.735: INFO: ss-2  dce-10-6-171-85  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:50 +0000 UTC  }]
Sep 27 03:36:16.735: INFO: 
Sep 27 03:36:16.735: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 27 03:36:18.256: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Sep 27 03:36:18.256: INFO: ss-0  dce-10-6-171-86  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:19 +0000 UTC  }]
Sep 27 03:36:18.256: INFO: ss-1  dce-10-6-171-84  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:49 +0000 UTC  }]
Sep 27 03:36:18.256: INFO: ss-2  dce-10-6-171-85  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:50 +0000 UTC  }]
Sep 27 03:36:18.256: INFO: 
Sep 27 03:36:18.256: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 27 03:36:19.361: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Sep 27 03:36:19.361: INFO: ss-0  dce-10-6-171-86  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:19 +0000 UTC  }]
Sep 27 03:36:19.361: INFO: ss-1  dce-10-6-171-84  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:49 +0000 UTC  }]
Sep 27 03:36:19.361: INFO: ss-2  dce-10-6-171-85  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:50 +0000 UTC  }]
Sep 27 03:36:19.361: INFO: 
Sep 27 03:36:19.361: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 27 03:36:20.431: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Sep 27 03:36:20.431: INFO: ss-0  dce-10-6-171-86  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:19 +0000 UTC  }]
Sep 27 03:36:20.431: INFO: ss-1  dce-10-6-171-84  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:49 +0000 UTC  }]
Sep 27 03:36:20.431: INFO: ss-2  dce-10-6-171-85  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:50 +0000 UTC  }]
Sep 27 03:36:20.431: INFO: 
Sep 27 03:36:20.431: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 27 03:36:21.459: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Sep 27 03:36:21.459: INFO: ss-0  dce-10-6-171-86  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:19 +0000 UTC  }]
Sep 27 03:36:21.460: INFO: ss-1  dce-10-6-171-84  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:49 +0000 UTC  }]
Sep 27 03:36:21.460: INFO: ss-2  dce-10-6-171-85  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:50 +0000 UTC  }]
Sep 27 03:36:21.460: INFO: 
Sep 27 03:36:21.460: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 27 03:36:22.468: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Sep 27 03:36:22.468: INFO: ss-0  dce-10-6-171-86  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:19 +0000 UTC  }]
Sep 27 03:36:22.468: INFO: ss-1  dce-10-6-171-84  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:49 +0000 UTC  }]
Sep 27 03:36:22.468: INFO: ss-2  dce-10-6-171-85  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:50 +0000 UTC  }]
Sep 27 03:36:22.468: INFO: 
Sep 27 03:36:22.468: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 27 03:36:23.473: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Sep 27 03:36:23.473: INFO: ss-0  dce-10-6-171-86  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:19 +0000 UTC  }]
Sep 27 03:36:23.473: INFO: ss-1  dce-10-6-171-84  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:49 +0000 UTC  }]
Sep 27 03:36:23.473: INFO: ss-2  dce-10-6-171-85  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:50 +0000 UTC  }]
Sep 27 03:36:23.474: INFO: 
Sep 27 03:36:23.474: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 27 03:36:24.478: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Sep 27 03:36:24.478: INFO: ss-1  dce-10-6-171-84  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:49 +0000 UTC  }]
Sep 27 03:36:24.478: INFO: ss-2  dce-10-6-171-85  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:36:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-27 03:35:50 +0000 UTC  }]
Sep 27 03:36:24.478: INFO: 
Sep 27 03:36:24.478: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7312
Sep 27 03:36:25.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:36:25.715: INFO: rc: 1
Sep 27 03:36:25.715: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Sep 27 03:36:35.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:36:35.828: INFO: rc: 1
Sep 27 03:36:35.828: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 03:36:45.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:36:45.943: INFO: rc: 1
Sep 27 03:36:45.943: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 03:36:55.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:36:56.211: INFO: rc: 1
Sep 27 03:36:56.211: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 03:37:06.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:37:07.060: INFO: rc: 1
Sep 27 03:37:07.060: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 03:37:17.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:37:17.165: INFO: rc: 1
Sep 27 03:37:17.165: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 03:37:27.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:37:27.278: INFO: rc: 1
Sep 27 03:37:27.278: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 03:37:37.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:37:37.400: INFO: rc: 1
Sep 27 03:37:37.400: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 03:37:47.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:37:47.563: INFO: rc: 1
Sep 27 03:37:47.563: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 03:37:57.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:37:57.692: INFO: rc: 1
Sep 27 03:37:57.692: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 03:38:07.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:38:07.861: INFO: rc: 1
Sep 27 03:38:07.861: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 03:38:17.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:38:18.931: INFO: rc: 1
Sep 27 03:38:18.931: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 03:38:28.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:38:29.403: INFO: rc: 1
Sep 27 03:38:29.403: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 03:38:39.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:38:39.549: INFO: rc: 1
Sep 27 03:38:39.549: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 03:38:49.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:38:49.686: INFO: rc: 1
Sep 27 03:38:49.686: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 03:38:59.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:39:01.023: INFO: rc: 1
Sep 27 03:39:01.023: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 03:39:11.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:39:11.134: INFO: rc: 1
Sep 27 03:39:11.134: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 03:39:21.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:39:25.685: INFO: rc: 1
Sep 27 03:39:25.685: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 03:39:35.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:39:35.822: INFO: rc: 1
Sep 27 03:39:35.822: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 03:39:45.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:39:46.432: INFO: rc: 1
Sep 27 03:39:46.432: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 03:39:56.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:39:56.589: INFO: rc: 1
Sep 27 03:39:56.589: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 03:40:06.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:40:06.907: INFO: rc: 1
Sep 27 03:40:06.907: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 03:40:16.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:40:17.024: INFO: rc: 1
Sep 27 03:40:17.024: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 03:40:27.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:40:27.151: INFO: rc: 1
Sep 27 03:40:27.151: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 03:40:37.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:40:37.288: INFO: rc: 1
Sep 27 03:40:37.288: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 03:40:47.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:40:47.408: INFO: rc: 1
Sep 27 03:40:47.408: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 03:40:57.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:40:57.540: INFO: rc: 1
Sep 27 03:40:57.540: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 03:41:07.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:41:09.197: INFO: rc: 1
Sep 27 03:41:09.197: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 03:41:19.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:41:19.561: INFO: rc: 1
Sep 27 03:41:19.561: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 03:41:29.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=statefulset-7312 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 03:41:29.774: INFO: rc: 1
Sep 27 03:41:29.774: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: 
Sep 27 03:41:29.774: INFO: Scaling statefulset ss to 0
Sep 27 03:41:30.053: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Sep 27 03:41:30.406: INFO: Deleting all statefulset in ns statefulset-7312
Sep 27 03:41:30.451: INFO: Scaling statefulset ss to 0
Sep 27 03:41:30.861: INFO: Waiting for statefulset status.replicas updated to 0
Sep 27 03:41:30.871: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:41:30.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7312" for this suite.

â€¢ [SLOW TEST:372.973 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":277,"completed":117,"skipped":2315,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:41:30.907: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9138
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:42:31.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9138" for this suite.

â€¢ [SLOW TEST:61.179 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":277,"completed":118,"skipped":2317,"failed":0}
S
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:42:32.087: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-5473
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Sep 27 03:42:33.103: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 27 03:43:33.330: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 03:43:33.386: INFO: Starting informer...
STEP: Starting pod...
Sep 27 03:43:33.604: INFO: Pod is running on dce-10-6-171-86. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Sep 27 03:43:33.618: INFO: Pod wasn't evicted. Proceeding
Sep 27 03:43:33.618: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Sep 27 03:44:48.869: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:44:48.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-5473" for this suite.

â€¢ [SLOW TEST:136.796 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":277,"completed":119,"skipped":2318,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:44:48.883: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9992
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep 27 03:44:49.229: INFO: Waiting up to 5m0s for pod "pod-04dd5a18-48f2-4a7d-b125-9afb2d689980" in namespace "emptydir-9992" to be "Succeeded or Failed"
Sep 27 03:44:49.243: INFO: Pod "pod-04dd5a18-48f2-4a7d-b125-9afb2d689980": Phase="Pending", Reason="", readiness=false. Elapsed: 13.355617ms
Sep 27 03:44:51.249: INFO: Pod "pod-04dd5a18-48f2-4a7d-b125-9afb2d689980": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020034387s
Sep 27 03:44:53.263: INFO: Pod "pod-04dd5a18-48f2-4a7d-b125-9afb2d689980": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033813186s
Sep 27 03:44:55.490: INFO: Pod "pod-04dd5a18-48f2-4a7d-b125-9afb2d689980": Phase="Pending", Reason="", readiness=false. Elapsed: 6.260891009s
Sep 27 03:44:57.496: INFO: Pod "pod-04dd5a18-48f2-4a7d-b125-9afb2d689980": Phase="Pending", Reason="", readiness=false. Elapsed: 8.26672126s
Sep 27 03:44:59.533: INFO: Pod "pod-04dd5a18-48f2-4a7d-b125-9afb2d689980": Phase="Pending", Reason="", readiness=false. Elapsed: 10.303846295s
Sep 27 03:45:01.538: INFO: Pod "pod-04dd5a18-48f2-4a7d-b125-9afb2d689980": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.308278099s
STEP: Saw pod success
Sep 27 03:45:01.538: INFO: Pod "pod-04dd5a18-48f2-4a7d-b125-9afb2d689980" satisfied condition "Succeeded or Failed"
Sep 27 03:45:01.541: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-04dd5a18-48f2-4a7d-b125-9afb2d689980 container test-container: <nil>
STEP: delete the pod
Sep 27 03:45:01.592: INFO: Waiting for pod pod-04dd5a18-48f2-4a7d-b125-9afb2d689980 to disappear
Sep 27 03:45:01.646: INFO: Pod pod-04dd5a18-48f2-4a7d-b125-9afb2d689980 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:45:01.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9992" for this suite.

â€¢ [SLOW TEST:12.774 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":120,"skipped":2335,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:45:01.657: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3265
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-3265
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a new StatefulSet
Sep 27 03:45:01.861: INFO: Found 0 stateful pods, waiting for 3
Sep 27 03:45:11.913: INFO: Found 1 stateful pods, waiting for 3
Sep 27 03:45:21.865: INFO: Found 2 stateful pods, waiting for 3
Sep 27 03:45:31.867: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 03:45:31.867: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 03:45:31.867: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Sep 27 03:45:31.905: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Sep 27 03:45:42.038: INFO: Updating stateful set ss2
Sep 27 03:45:42.084: INFO: Waiting for Pod statefulset-3265/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 03:45:52.094: INFO: Waiting for Pod statefulset-3265/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Sep 27 03:46:02.355: INFO: Found 2 stateful pods, waiting for 3
Sep 27 03:46:12.407: INFO: Found 2 stateful pods, waiting for 3
Sep 27 03:46:22.363: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 03:46:22.363: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 03:46:22.363: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep 27 03:46:32.363: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 03:46:32.363: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 03:46:32.363: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Sep 27 03:46:32.389: INFO: Updating stateful set ss2
Sep 27 03:46:32.403: INFO: Waiting for Pod statefulset-3265/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 03:46:42.490: INFO: Waiting for Pod statefulset-3265/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 03:47:02.512: INFO: Updating stateful set ss2
Sep 27 03:47:02.531: INFO: Waiting for StatefulSet statefulset-3265/ss2 to complete update
Sep 27 03:47:02.531: INFO: Waiting for Pod statefulset-3265/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 27 03:47:12.654: INFO: Waiting for StatefulSet statefulset-3265/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Sep 27 03:47:22.541: INFO: Deleting all statefulset in ns statefulset-3265
Sep 27 03:47:22.544: INFO: Scaling statefulset ss2 to 0
Sep 27 03:48:02.560: INFO: Waiting for statefulset status.replicas updated to 0
Sep 27 03:48:02.569: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:48:02.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3265" for this suite.

â€¢ [SLOW TEST:180.941 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":277,"completed":121,"skipped":2343,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:48:02.599: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6212
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Sep 27 03:48:15.534: INFO: Successfully updated pod "annotationupdate19aac77a-9bb2-4f83-a22f-bbf45cc63e6f"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:48:19.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6212" for this suite.

â€¢ [SLOW TEST:17.435 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":277,"completed":122,"skipped":2350,"failed":0}
S
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:48:20.034: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-1402
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: getting the auto-created API token
STEP: reading a file in the container
Sep 27 03:48:33.225: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1402 pod-service-account-0c363f22-4b42-4124-84ca-ecc0f85f83a4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Sep 27 03:48:33.509: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1402 pod-service-account-0c363f22-4b42-4124-84ca-ecc0f85f83a4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Sep 27 03:48:33.826: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1402 pod-service-account-0c363f22-4b42-4124-84ca-ecc0f85f83a4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:48:34.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1402" for this suite.

â€¢ [SLOW TEST:14.079 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":277,"completed":123,"skipped":2351,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:48:34.114: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-8672
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7080
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-106
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:49:11.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8672" for this suite.
STEP: Destroying namespace "nsdeletetest-7080" for this suite.
Sep 27 03:49:11.134: INFO: Namespace nsdeletetest-7080 was already deleted
STEP: Destroying namespace "nsdeletetest-106" for this suite.

â€¢ [SLOW TEST:37.026 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":277,"completed":124,"skipped":2413,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:49:11.140: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2845
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Sep 27 03:49:12.614: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
Sep 27 03:49:18.406: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:49:37.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2845" for this suite.

â€¢ [SLOW TEST:26.044 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":277,"completed":125,"skipped":2421,"failed":0}
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:49:37.184: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8937
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test override arguments
Sep 27 03:49:38.626: INFO: Waiting up to 5m0s for pod "client-containers-180cdc6c-001f-45d4-bfc4-3d15cab58b88" in namespace "containers-8937" to be "Succeeded or Failed"
Sep 27 03:49:38.765: INFO: Pod "client-containers-180cdc6c-001f-45d4-bfc4-3d15cab58b88": Phase="Pending", Reason="", readiness=false. Elapsed: 138.410879ms
Sep 27 03:49:40.829: INFO: Pod "client-containers-180cdc6c-001f-45d4-bfc4-3d15cab58b88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.20255044s
Sep 27 03:49:42.832: INFO: Pod "client-containers-180cdc6c-001f-45d4-bfc4-3d15cab58b88": Phase="Pending", Reason="", readiness=false. Elapsed: 4.205466841s
Sep 27 03:49:44.836: INFO: Pod "client-containers-180cdc6c-001f-45d4-bfc4-3d15cab58b88": Phase="Pending", Reason="", readiness=false. Elapsed: 6.2096881s
Sep 27 03:49:46.848: INFO: Pod "client-containers-180cdc6c-001f-45d4-bfc4-3d15cab58b88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.221483509s
STEP: Saw pod success
Sep 27 03:49:46.848: INFO: Pod "client-containers-180cdc6c-001f-45d4-bfc4-3d15cab58b88" satisfied condition "Succeeded or Failed"
Sep 27 03:49:46.853: INFO: Trying to get logs from node dce-10-6-171-86 pod client-containers-180cdc6c-001f-45d4-bfc4-3d15cab58b88 container test-container: <nil>
STEP: delete the pod
Sep 27 03:49:46.892: INFO: Waiting for pod client-containers-180cdc6c-001f-45d4-bfc4-3d15cab58b88 to disappear
Sep 27 03:49:46.916: INFO: Pod client-containers-180cdc6c-001f-45d4-bfc4-3d15cab58b88 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:49:46.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8937" for this suite.

â€¢ [SLOW TEST:9.746 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":277,"completed":126,"skipped":2424,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:49:46.930: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9749
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Sep 27 03:50:02.336: INFO: Successfully updated pod "annotationupdatef4b3e9df-0ab5-4818-bdf4-fb23929fba57"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:50:06.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9749" for this suite.

â€¢ [SLOW TEST:19.583 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":277,"completed":127,"skipped":2429,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:50:06.514: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4576
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-3f5dc2e6-1260-4105-855b-f5a2e1f4935c
STEP: Creating a pod to test consume secrets
Sep 27 03:50:07.224: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-50969273-e5fd-4ac8-9dbc-2eebb6359f2e" in namespace "projected-4576" to be "Succeeded or Failed"
Sep 27 03:50:07.348: INFO: Pod "pod-projected-secrets-50969273-e5fd-4ac8-9dbc-2eebb6359f2e": Phase="Pending", Reason="", readiness=false. Elapsed: 123.232827ms
Sep 27 03:50:09.351: INFO: Pod "pod-projected-secrets-50969273-e5fd-4ac8-9dbc-2eebb6359f2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.126663112s
Sep 27 03:50:11.355: INFO: Pod "pod-projected-secrets-50969273-e5fd-4ac8-9dbc-2eebb6359f2e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.130615066s
Sep 27 03:50:13.591: INFO: Pod "pod-projected-secrets-50969273-e5fd-4ac8-9dbc-2eebb6359f2e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.366492212s
Sep 27 03:50:15.595: INFO: Pod "pod-projected-secrets-50969273-e5fd-4ac8-9dbc-2eebb6359f2e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.370288328s
Sep 27 03:50:17.599: INFO: Pod "pod-projected-secrets-50969273-e5fd-4ac8-9dbc-2eebb6359f2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.374296429s
STEP: Saw pod success
Sep 27 03:50:17.599: INFO: Pod "pod-projected-secrets-50969273-e5fd-4ac8-9dbc-2eebb6359f2e" satisfied condition "Succeeded or Failed"
Sep 27 03:50:17.608: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-projected-secrets-50969273-e5fd-4ac8-9dbc-2eebb6359f2e container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 27 03:50:17.708: INFO: Waiting for pod pod-projected-secrets-50969273-e5fd-4ac8-9dbc-2eebb6359f2e to disappear
Sep 27 03:50:17.711: INFO: Pod pod-projected-secrets-50969273-e5fd-4ac8-9dbc-2eebb6359f2e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:50:17.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4576" for this suite.

â€¢ [SLOW TEST:11.213 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":128,"skipped":2446,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:50:17.727: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2893
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 27 03:50:18.506: INFO: Waiting up to 5m0s for pod "downwardapi-volume-40f1e02e-07ce-4546-ac40-a4f102c9c200" in namespace "projected-2893" to be "Succeeded or Failed"
Sep 27 03:50:18.533: INFO: Pod "downwardapi-volume-40f1e02e-07ce-4546-ac40-a4f102c9c200": Phase="Pending", Reason="", readiness=false. Elapsed: 27.297083ms
Sep 27 03:50:20.588: INFO: Pod "downwardapi-volume-40f1e02e-07ce-4546-ac40-a4f102c9c200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.082107586s
Sep 27 03:50:22.614: INFO: Pod "downwardapi-volume-40f1e02e-07ce-4546-ac40-a4f102c9c200": Phase="Pending", Reason="", readiness=false. Elapsed: 4.107593816s
Sep 27 03:50:24.640: INFO: Pod "downwardapi-volume-40f1e02e-07ce-4546-ac40-a4f102c9c200": Phase="Pending", Reason="", readiness=false. Elapsed: 6.133542384s
Sep 27 03:50:26.652: INFO: Pod "downwardapi-volume-40f1e02e-07ce-4546-ac40-a4f102c9c200": Phase="Pending", Reason="", readiness=false. Elapsed: 8.145716983s
Sep 27 03:50:28.903: INFO: Pod "downwardapi-volume-40f1e02e-07ce-4546-ac40-a4f102c9c200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.397274889s
STEP: Saw pod success
Sep 27 03:50:28.903: INFO: Pod "downwardapi-volume-40f1e02e-07ce-4546-ac40-a4f102c9c200" satisfied condition "Succeeded or Failed"
Sep 27 03:50:29.006: INFO: Trying to get logs from node dce-10-6-171-86 pod downwardapi-volume-40f1e02e-07ce-4546-ac40-a4f102c9c200 container client-container: <nil>
STEP: delete the pod
Sep 27 03:50:29.650: INFO: Waiting for pod downwardapi-volume-40f1e02e-07ce-4546-ac40-a4f102c9c200 to disappear
Sep 27 03:50:30.036: INFO: Pod downwardapi-volume-40f1e02e-07ce-4546-ac40-a4f102c9c200 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:50:30.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2893" for this suite.

â€¢ [SLOW TEST:12.466 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":277,"completed":129,"skipped":2474,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:50:30.193: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9902
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep 27 03:50:59.407: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 27 03:50:59.454: INFO: Pod pod-with-poststart-http-hook still exists
Sep 27 03:51:01.454: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 27 03:51:01.464: INFO: Pod pod-with-poststart-http-hook still exists
Sep 27 03:51:03.454: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 27 03:51:03.459: INFO: Pod pod-with-poststart-http-hook still exists
Sep 27 03:51:05.454: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 27 03:51:05.460: INFO: Pod pod-with-poststart-http-hook still exists
Sep 27 03:51:07.454: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 27 03:51:07.459: INFO: Pod pod-with-poststart-http-hook still exists
Sep 27 03:51:09.454: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 27 03:51:09.470: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:51:09.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9902" for this suite.

â€¢ [SLOW TEST:39.493 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":277,"completed":130,"skipped":2508,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:51:09.687: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4233
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-configmap-94sl
STEP: Creating a pod to test atomic-volume-subpath
Sep 27 03:51:10.421: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-94sl" in namespace "subpath-4233" to be "Succeeded or Failed"
Sep 27 03:51:10.424: INFO: Pod "pod-subpath-test-configmap-94sl": Phase="Pending", Reason="", readiness=false. Elapsed: 3.063483ms
Sep 27 03:51:13.477: INFO: Pod "pod-subpath-test-configmap-94sl": Phase="Pending", Reason="", readiness=false. Elapsed: 3.05620044s
Sep 27 03:51:15.638: INFO: Pod "pod-subpath-test-configmap-94sl": Phase="Pending", Reason="", readiness=false. Elapsed: 5.21636653s
Sep 27 03:51:17.730: INFO: Pod "pod-subpath-test-configmap-94sl": Phase="Pending", Reason="", readiness=false. Elapsed: 7.308637294s
Sep 27 03:51:19.782: INFO: Pod "pod-subpath-test-configmap-94sl": Phase="Running", Reason="", readiness=true. Elapsed: 9.360540349s
Sep 27 03:51:21.785: INFO: Pod "pod-subpath-test-configmap-94sl": Phase="Running", Reason="", readiness=true. Elapsed: 11.363733961s
Sep 27 03:51:23.789: INFO: Pod "pod-subpath-test-configmap-94sl": Phase="Running", Reason="", readiness=true. Elapsed: 13.36790594s
Sep 27 03:51:25.794: INFO: Pod "pod-subpath-test-configmap-94sl": Phase="Running", Reason="", readiness=true. Elapsed: 15.372621027s
Sep 27 03:51:27.797: INFO: Pod "pod-subpath-test-configmap-94sl": Phase="Running", Reason="", readiness=true. Elapsed: 17.375903936s
Sep 27 03:51:29.802: INFO: Pod "pod-subpath-test-configmap-94sl": Phase="Running", Reason="", readiness=true. Elapsed: 19.380871463s
Sep 27 03:51:31.807: INFO: Pod "pod-subpath-test-configmap-94sl": Phase="Running", Reason="", readiness=true. Elapsed: 21.386237s
Sep 27 03:51:33.901: INFO: Pod "pod-subpath-test-configmap-94sl": Phase="Running", Reason="", readiness=true. Elapsed: 23.479997924s
Sep 27 03:51:35.959: INFO: Pod "pod-subpath-test-configmap-94sl": Phase="Running", Reason="", readiness=true. Elapsed: 25.537795535s
Sep 27 03:51:38.152: INFO: Pod "pod-subpath-test-configmap-94sl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 27.730881094s
STEP: Saw pod success
Sep 27 03:51:38.152: INFO: Pod "pod-subpath-test-configmap-94sl" satisfied condition "Succeeded or Failed"
Sep 27 03:51:38.230: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-subpath-test-configmap-94sl container test-container-subpath-configmap-94sl: <nil>
STEP: delete the pod
Sep 27 03:51:38.909: INFO: Waiting for pod pod-subpath-test-configmap-94sl to disappear
Sep 27 03:51:38.927: INFO: Pod pod-subpath-test-configmap-94sl no longer exists
STEP: Deleting pod pod-subpath-test-configmap-94sl
Sep 27 03:51:38.927: INFO: Deleting pod "pod-subpath-test-configmap-94sl" in namespace "subpath-4233"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:51:38.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4233" for this suite.

â€¢ [SLOW TEST:29.362 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":277,"completed":131,"skipped":2526,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:51:39.049: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3401
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:51:50.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3401" for this suite.

â€¢ [SLOW TEST:11.551 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":277,"completed":132,"skipped":2565,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:51:50.600: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1390
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap configmap-1390/configmap-test-6aa95b8c-81f9-4d27-911b-d953bf6085bb
STEP: Creating a pod to test consume configMaps
Sep 27 03:51:51.649: INFO: Waiting up to 5m0s for pod "pod-configmaps-a319c0a9-05bc-4d6f-be48-aa2846146a2e" in namespace "configmap-1390" to be "Succeeded or Failed"
Sep 27 03:51:51.768: INFO: Pod "pod-configmaps-a319c0a9-05bc-4d6f-be48-aa2846146a2e": Phase="Pending", Reason="", readiness=false. Elapsed: 118.310759ms
Sep 27 03:51:53.960: INFO: Pod "pod-configmaps-a319c0a9-05bc-4d6f-be48-aa2846146a2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.310657267s
Sep 27 03:51:56.315: INFO: Pod "pod-configmaps-a319c0a9-05bc-4d6f-be48-aa2846146a2e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.665481047s
Sep 27 03:51:58.364: INFO: Pod "pod-configmaps-a319c0a9-05bc-4d6f-be48-aa2846146a2e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.714433616s
Sep 27 03:52:00.367: INFO: Pod "pod-configmaps-a319c0a9-05bc-4d6f-be48-aa2846146a2e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.717878759s
Sep 27 03:52:02.371: INFO: Pod "pod-configmaps-a319c0a9-05bc-4d6f-be48-aa2846146a2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.721293235s
STEP: Saw pod success
Sep 27 03:52:02.371: INFO: Pod "pod-configmaps-a319c0a9-05bc-4d6f-be48-aa2846146a2e" satisfied condition "Succeeded or Failed"
Sep 27 03:52:02.374: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-configmaps-a319c0a9-05bc-4d6f-be48-aa2846146a2e container env-test: <nil>
STEP: delete the pod
Sep 27 03:52:02.474: INFO: Waiting for pod pod-configmaps-a319c0a9-05bc-4d6f-be48-aa2846146a2e to disappear
Sep 27 03:52:02.493: INFO: Pod pod-configmaps-a319c0a9-05bc-4d6f-be48-aa2846146a2e no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:52:02.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1390" for this suite.

â€¢ [SLOW TEST:11.901 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:34
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":277,"completed":133,"skipped":2574,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:52:02.502: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5322
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Sep 27 03:52:15.100: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-5322 PodName:pod-sharedvolume-a4cd804b-e8ce-40ee-a536-0acae7bf6212 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 03:52:15.100: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
Sep 27 03:52:15.328: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:52:15.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5322" for this suite.

â€¢ [SLOW TEST:12.839 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":277,"completed":134,"skipped":2578,"failed":0}
SSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:52:15.341: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8194
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod busybox-d9fb39a2-eebb-4ca2-9b18-8933eee7ff8e in namespace container-probe-8194
Sep 27 03:52:25.777: INFO: Started pod busybox-d9fb39a2-eebb-4ca2-9b18-8933eee7ff8e in namespace container-probe-8194
STEP: checking the pod's current state and verifying that restartCount is present
Sep 27 03:52:25.782: INFO: Initial restart count of pod busybox-d9fb39a2-eebb-4ca2-9b18-8933eee7ff8e is 0
Sep 27 03:53:12.296: INFO: Restart count of pod container-probe-8194/busybox-d9fb39a2-eebb-4ca2-9b18-8933eee7ff8e is now 1 (46.513355837s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:53:12.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8194" for this suite.

â€¢ [SLOW TEST:56.991 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":277,"completed":135,"skipped":2584,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:53:12.332: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3860
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating service endpoint-test2 in namespace services-3860
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3860 to expose endpoints map[]
Sep 27 03:53:13.386: INFO: successfully validated that service endpoint-test2 in namespace services-3860 exposes endpoints map[] (140.235144ms elapsed)
STEP: Creating pod pod1 in namespace services-3860
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3860 to expose endpoints map[pod1:[80]]
Sep 27 03:53:17.564: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]] (4.141189878s elapsed, will retry)
Sep 27 03:53:23.243: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]] (9.820194906s elapsed, will retry)
Sep 27 03:53:24.264: INFO: successfully validated that service endpoint-test2 in namespace services-3860 exposes endpoints map[pod1:[80]] (10.841999296s elapsed)
STEP: Creating pod pod2 in namespace services-3860
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3860 to expose endpoints map[pod1:[80] pod2:[80]]
Sep 27 03:53:28.359: INFO: Unexpected endpoints: found map[f889fbd8-59e8-43b7-a457-7a4c0245b296:[80]], expected map[pod1:[80] pod2:[80]] (4.085435062s elapsed, will retry)
Sep 27 03:53:31.982: INFO: successfully validated that service endpoint-test2 in namespace services-3860 exposes endpoints map[pod1:[80] pod2:[80]] (7.707831092s elapsed)
STEP: Deleting pod pod1 in namespace services-3860
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3860 to expose endpoints map[pod2:[80]]
Sep 27 03:53:32.091: INFO: successfully validated that service endpoint-test2 in namespace services-3860 exposes endpoints map[pod2:[80]] (96.179248ms elapsed)
STEP: Deleting pod pod2 in namespace services-3860
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3860 to expose endpoints map[]
Sep 27 03:53:32.254: INFO: successfully validated that service endpoint-test2 in namespace services-3860 exposes endpoints map[] (123.168618ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:53:32.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3860" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

â€¢ [SLOW TEST:20.349 seconds]
[sig-network] Services
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":277,"completed":136,"skipped":2610,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:53:32.681: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7905
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-map-71e09007-8ccb-4c6d-a987-6f6498bcd254
STEP: Creating a pod to test consume configMaps
Sep 27 03:53:32.981: INFO: Waiting up to 5m0s for pod "pod-configmaps-2e8c7dca-5432-4357-98cb-1c9cc0ddf5ec" in namespace "configmap-7905" to be "Succeeded or Failed"
Sep 27 03:53:32.986: INFO: Pod "pod-configmaps-2e8c7dca-5432-4357-98cb-1c9cc0ddf5ec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.61135ms
Sep 27 03:53:35.031: INFO: Pod "pod-configmaps-2e8c7dca-5432-4357-98cb-1c9cc0ddf5ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050192501s
Sep 27 03:53:37.072: INFO: Pod "pod-configmaps-2e8c7dca-5432-4357-98cb-1c9cc0ddf5ec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.090322591s
Sep 27 03:53:39.242: INFO: Pod "pod-configmaps-2e8c7dca-5432-4357-98cb-1c9cc0ddf5ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.260657213s
STEP: Saw pod success
Sep 27 03:53:39.242: INFO: Pod "pod-configmaps-2e8c7dca-5432-4357-98cb-1c9cc0ddf5ec" satisfied condition "Succeeded or Failed"
Sep 27 03:53:39.247: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-configmaps-2e8c7dca-5432-4357-98cb-1c9cc0ddf5ec container configmap-volume-test: <nil>
STEP: delete the pod
Sep 27 03:53:39.586: INFO: Waiting for pod pod-configmaps-2e8c7dca-5432-4357-98cb-1c9cc0ddf5ec to disappear
Sep 27 03:53:39.744: INFO: Pod pod-configmaps-2e8c7dca-5432-4357-98cb-1c9cc0ddf5ec no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:53:39.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7905" for this suite.

â€¢ [SLOW TEST:7.080 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":277,"completed":137,"skipped":2615,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:53:39.761: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4198
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:53:56.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4198" for this suite.

â€¢ [SLOW TEST:16.872 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:79
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":277,"completed":138,"skipped":2646,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:53:56.634: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7386
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 03:53:58.184: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 27 03:54:00.215: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736775638, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736775638, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736775638, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736775638, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 03:54:02.229: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736775638, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736775638, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736775638, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736775638, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 03:54:04.231: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736775638, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736775638, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736775638, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736775638, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 03:54:07.267: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:54:11.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7386" for this suite.
STEP: Destroying namespace "webhook-7386-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:15.584 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":277,"completed":139,"skipped":2654,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:54:12.218: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3743
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-74c1d9a1-e65f-4830-965d-b19330aa5086
STEP: Creating a pod to test consume secrets
Sep 27 03:54:13.188: INFO: Waiting up to 5m0s for pod "pod-secrets-39af8c20-7ace-4d44-8065-cc6eb9944f4b" in namespace "secrets-3743" to be "Succeeded or Failed"
Sep 27 03:54:13.243: INFO: Pod "pod-secrets-39af8c20-7ace-4d44-8065-cc6eb9944f4b": Phase="Pending", Reason="", readiness=false. Elapsed: 54.725809ms
Sep 27 03:54:15.246: INFO: Pod "pod-secrets-39af8c20-7ace-4d44-8065-cc6eb9944f4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058434966s
Sep 27 03:54:17.335: INFO: Pod "pod-secrets-39af8c20-7ace-4d44-8065-cc6eb9944f4b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.146940332s
Sep 27 03:54:20.416: INFO: Pod "pod-secrets-39af8c20-7ace-4d44-8065-cc6eb9944f4b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.228231004s
Sep 27 03:54:22.704: INFO: Pod "pod-secrets-39af8c20-7ace-4d44-8065-cc6eb9944f4b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.515803456s
Sep 27 03:54:24.707: INFO: Pod "pod-secrets-39af8c20-7ace-4d44-8065-cc6eb9944f4b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.519347961s
Sep 27 03:54:26.711: INFO: Pod "pod-secrets-39af8c20-7ace-4d44-8065-cc6eb9944f4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 13.523007181s
STEP: Saw pod success
Sep 27 03:54:26.711: INFO: Pod "pod-secrets-39af8c20-7ace-4d44-8065-cc6eb9944f4b" satisfied condition "Succeeded or Failed"
Sep 27 03:54:26.715: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-secrets-39af8c20-7ace-4d44-8065-cc6eb9944f4b container secret-volume-test: <nil>
STEP: delete the pod
Sep 27 03:54:26.770: INFO: Waiting for pod pod-secrets-39af8c20-7ace-4d44-8065-cc6eb9944f4b to disappear
Sep 27 03:54:26.778: INFO: Pod pod-secrets-39af8c20-7ace-4d44-8065-cc6eb9944f4b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:54:26.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3743" for this suite.

â€¢ [SLOW TEST:14.639 seconds]
[sig-storage] Secrets
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":277,"completed":140,"skipped":2668,"failed":0}
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:54:26.858: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4128
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Sep 27 03:54:27.940: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4128 /api/v1/namespaces/watch-4128/configmaps/e2e-watch-test-resource-version 1d5a4feb-bc99-47a8-9dc1-9ba2831c3992 697432 0 2020-09-27 03:54:27 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 03:54:27.940: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4128 /api/v1/namespaces/watch-4128/configmaps/e2e-watch-test-resource-version 1d5a4feb-bc99-47a8-9dc1-9ba2831c3992 697434 0 2020-09-27 03:54:27 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:54:27.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4128" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":277,"completed":141,"skipped":2669,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:54:27.961: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3853
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep 27 03:54:28.343: INFO: Waiting up to 5m0s for pod "pod-c75fb030-274c-4ead-a820-efd57b140292" in namespace "emptydir-3853" to be "Succeeded or Failed"
Sep 27 03:54:28.392: INFO: Pod "pod-c75fb030-274c-4ead-a820-efd57b140292": Phase="Pending", Reason="", readiness=false. Elapsed: 48.28919ms
Sep 27 03:54:30.545: INFO: Pod "pod-c75fb030-274c-4ead-a820-efd57b140292": Phase="Pending", Reason="", readiness=false. Elapsed: 2.201423035s
Sep 27 03:54:32.639: INFO: Pod "pod-c75fb030-274c-4ead-a820-efd57b140292": Phase="Pending", Reason="", readiness=false. Elapsed: 4.295570467s
Sep 27 03:54:34.815: INFO: Pod "pod-c75fb030-274c-4ead-a820-efd57b140292": Phase="Pending", Reason="", readiness=false. Elapsed: 6.471021577s
Sep 27 03:54:37.139: INFO: Pod "pod-c75fb030-274c-4ead-a820-efd57b140292": Phase="Pending", Reason="", readiness=false. Elapsed: 8.795292274s
Sep 27 03:54:39.497: INFO: Pod "pod-c75fb030-274c-4ead-a820-efd57b140292": Phase="Pending", Reason="", readiness=false. Elapsed: 11.153143164s
Sep 27 03:54:41.503: INFO: Pod "pod-c75fb030-274c-4ead-a820-efd57b140292": Phase="Succeeded", Reason="", readiness=false. Elapsed: 13.159545654s
STEP: Saw pod success
Sep 27 03:54:41.503: INFO: Pod "pod-c75fb030-274c-4ead-a820-efd57b140292" satisfied condition "Succeeded or Failed"
Sep 27 03:54:41.515: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-c75fb030-274c-4ead-a820-efd57b140292 container test-container: <nil>
STEP: delete the pod
Sep 27 03:54:41.671: INFO: Waiting for pod pod-c75fb030-274c-4ead-a820-efd57b140292 to disappear
Sep 27 03:54:41.723: INFO: Pod pod-c75fb030-274c-4ead-a820-efd57b140292 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:54:41.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3853" for this suite.

â€¢ [SLOW TEST:13.792 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":142,"skipped":2676,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:54:41.753: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-685
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:54:42.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-685" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":277,"completed":143,"skipped":2682,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:54:42.082: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4858
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 03:54:43.355: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 27 03:54:45.507: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736775683, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736775683, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736775684, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736775683, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 03:54:47.812: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736775683, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736775683, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736775684, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736775683, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 03:54:49.518: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736775683, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736775683, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736775684, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736775683, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 03:54:52.547: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:54:52.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4858" for this suite.
STEP: Destroying namespace "webhook-4858-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:11.016 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":277,"completed":144,"skipped":2707,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:54:53.098: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1920
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-2cc4137a-73a7-4d86-ac35-3ef198b20dde
STEP: Creating a pod to test consume configMaps
Sep 27 03:54:54.583: INFO: Waiting up to 5m0s for pod "pod-configmaps-272db97e-5c47-4101-98c0-ec0751f5e701" in namespace "configmap-1920" to be "Succeeded or Failed"
Sep 27 03:54:54.658: INFO: Pod "pod-configmaps-272db97e-5c47-4101-98c0-ec0751f5e701": Phase="Pending", Reason="", readiness=false. Elapsed: 75.192043ms
Sep 27 03:54:56.662: INFO: Pod "pod-configmaps-272db97e-5c47-4101-98c0-ec0751f5e701": Phase="Pending", Reason="", readiness=false. Elapsed: 2.079230714s
Sep 27 03:54:58.698: INFO: Pod "pod-configmaps-272db97e-5c47-4101-98c0-ec0751f5e701": Phase="Pending", Reason="", readiness=false. Elapsed: 4.115212465s
Sep 27 03:55:00.702: INFO: Pod "pod-configmaps-272db97e-5c47-4101-98c0-ec0751f5e701": Phase="Pending", Reason="", readiness=false. Elapsed: 6.119393381s
Sep 27 03:55:02.709: INFO: Pod "pod-configmaps-272db97e-5c47-4101-98c0-ec0751f5e701": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.126424213s
STEP: Saw pod success
Sep 27 03:55:02.709: INFO: Pod "pod-configmaps-272db97e-5c47-4101-98c0-ec0751f5e701" satisfied condition "Succeeded or Failed"
Sep 27 03:55:02.713: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-configmaps-272db97e-5c47-4101-98c0-ec0751f5e701 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 27 03:55:02.740: INFO: Waiting for pod pod-configmaps-272db97e-5c47-4101-98c0-ec0751f5e701 to disappear
Sep 27 03:55:02.774: INFO: Pod pod-configmaps-272db97e-5c47-4101-98c0-ec0751f5e701 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:55:02.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1920" for this suite.

â€¢ [SLOW TEST:9.688 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":145,"skipped":2724,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:55:02.786: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9929
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:55:02.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9929" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":277,"completed":146,"skipped":2725,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:55:02.978: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8633
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-eeb45ea9-3e2a-4e1e-8d73-5bad7aa7ddb2
STEP: Creating a pod to test consume configMaps
Sep 27 03:55:03.252: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c9ed2b23-eb1c-4ad1-a3b6-37b332bb78cd" in namespace "projected-8633" to be "Succeeded or Failed"
Sep 27 03:55:03.260: INFO: Pod "pod-projected-configmaps-c9ed2b23-eb1c-4ad1-a3b6-37b332bb78cd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.755152ms
Sep 27 03:55:05.338: INFO: Pod "pod-projected-configmaps-c9ed2b23-eb1c-4ad1-a3b6-37b332bb78cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.086441677s
Sep 27 03:55:07.381: INFO: Pod "pod-projected-configmaps-c9ed2b23-eb1c-4ad1-a3b6-37b332bb78cd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.129583531s
Sep 27 03:55:09.392: INFO: Pod "pod-projected-configmaps-c9ed2b23-eb1c-4ad1-a3b6-37b332bb78cd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.140587871s
Sep 27 03:55:11.448: INFO: Pod "pod-projected-configmaps-c9ed2b23-eb1c-4ad1-a3b6-37b332bb78cd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.196509844s
Sep 27 03:55:13.492: INFO: Pod "pod-projected-configmaps-c9ed2b23-eb1c-4ad1-a3b6-37b332bb78cd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.23965536s
Sep 27 03:55:15.498: INFO: Pod "pod-projected-configmaps-c9ed2b23-eb1c-4ad1-a3b6-37b332bb78cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.246514031s
STEP: Saw pod success
Sep 27 03:55:15.498: INFO: Pod "pod-projected-configmaps-c9ed2b23-eb1c-4ad1-a3b6-37b332bb78cd" satisfied condition "Succeeded or Failed"
Sep 27 03:55:15.501: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-projected-configmaps-c9ed2b23-eb1c-4ad1-a3b6-37b332bb78cd container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 27 03:55:15.570: INFO: Waiting for pod pod-projected-configmaps-c9ed2b23-eb1c-4ad1-a3b6-37b332bb78cd to disappear
Sep 27 03:55:15.595: INFO: Pod pod-projected-configmaps-c9ed2b23-eb1c-4ad1-a3b6-37b332bb78cd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:55:15.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8633" for this suite.

â€¢ [SLOW TEST:12.652 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":277,"completed":147,"skipped":2727,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:55:15.631: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8159
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-projected-8648
STEP: Creating a pod to test atomic-volume-subpath
Sep 27 03:55:16.134: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-8648" in namespace "subpath-8159" to be "Succeeded or Failed"
Sep 27 03:55:16.181: INFO: Pod "pod-subpath-test-projected-8648": Phase="Pending", Reason="", readiness=false. Elapsed: 46.525055ms
Sep 27 03:55:18.275: INFO: Pod "pod-subpath-test-projected-8648": Phase="Pending", Reason="", readiness=false. Elapsed: 2.140372619s
Sep 27 03:55:20.312: INFO: Pod "pod-subpath-test-projected-8648": Phase="Pending", Reason="", readiness=false. Elapsed: 4.177412813s
Sep 27 03:55:22.327: INFO: Pod "pod-subpath-test-projected-8648": Phase="Pending", Reason="", readiness=false. Elapsed: 6.192375635s
Sep 27 03:55:24.350: INFO: Pod "pod-subpath-test-projected-8648": Phase="Pending", Reason="", readiness=false. Elapsed: 8.215632365s
Sep 27 03:55:26.371: INFO: Pod "pod-subpath-test-projected-8648": Phase="Pending", Reason="", readiness=false. Elapsed: 10.236441133s
Sep 27 03:55:28.401: INFO: Pod "pod-subpath-test-projected-8648": Phase="Running", Reason="", readiness=true. Elapsed: 12.266541373s
Sep 27 03:55:30.405: INFO: Pod "pod-subpath-test-projected-8648": Phase="Running", Reason="", readiness=true. Elapsed: 14.270735282s
Sep 27 03:55:32.412: INFO: Pod "pod-subpath-test-projected-8648": Phase="Running", Reason="", readiness=true. Elapsed: 16.277595012s
Sep 27 03:55:34.417: INFO: Pod "pod-subpath-test-projected-8648": Phase="Running", Reason="", readiness=true. Elapsed: 18.282478349s
Sep 27 03:55:36.433: INFO: Pod "pod-subpath-test-projected-8648": Phase="Running", Reason="", readiness=true. Elapsed: 20.298538562s
Sep 27 03:55:38.442: INFO: Pod "pod-subpath-test-projected-8648": Phase="Running", Reason="", readiness=true. Elapsed: 22.307362639s
Sep 27 03:55:40.447: INFO: Pod "pod-subpath-test-projected-8648": Phase="Running", Reason="", readiness=true. Elapsed: 24.312629801s
Sep 27 03:55:42.468: INFO: Pod "pod-subpath-test-projected-8648": Phase="Running", Reason="", readiness=true. Elapsed: 26.334172185s
Sep 27 03:55:44.599: INFO: Pod "pod-subpath-test-projected-8648": Phase="Running", Reason="", readiness=true. Elapsed: 28.464398595s
Sep 27 03:55:46.603: INFO: Pod "pod-subpath-test-projected-8648": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.468546036s
STEP: Saw pod success
Sep 27 03:55:46.603: INFO: Pod "pod-subpath-test-projected-8648" satisfied condition "Succeeded or Failed"
Sep 27 03:55:46.609: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-subpath-test-projected-8648 container test-container-subpath-projected-8648: <nil>
STEP: delete the pod
Sep 27 03:55:46.641: INFO: Waiting for pod pod-subpath-test-projected-8648 to disappear
Sep 27 03:55:46.645: INFO: Pod pod-subpath-test-projected-8648 no longer exists
STEP: Deleting pod pod-subpath-test-projected-8648
Sep 27 03:55:46.645: INFO: Deleting pod "pod-subpath-test-projected-8648" in namespace "subpath-8159"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:55:46.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8159" for this suite.

â€¢ [SLOW TEST:31.040 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":277,"completed":148,"skipped":2731,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:55:46.671: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1076
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name projected-secret-test-33cc73da-9260-485c-92dd-a9f9ff36f7e7
STEP: Creating a pod to test consume secrets
Sep 27 03:55:46.900: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d52dc74c-7051-4910-bda4-99ae8f6014b4" in namespace "projected-1076" to be "Succeeded or Failed"
Sep 27 03:55:46.912: INFO: Pod "pod-projected-secrets-d52dc74c-7051-4910-bda4-99ae8f6014b4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.178841ms
Sep 27 03:55:48.952: INFO: Pod "pod-projected-secrets-d52dc74c-7051-4910-bda4-99ae8f6014b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052461673s
Sep 27 03:55:51.007: INFO: Pod "pod-projected-secrets-d52dc74c-7051-4910-bda4-99ae8f6014b4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.10755884s
Sep 27 03:55:53.132: INFO: Pod "pod-projected-secrets-d52dc74c-7051-4910-bda4-99ae8f6014b4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.232378706s
Sep 27 03:55:55.212: INFO: Pod "pod-projected-secrets-d52dc74c-7051-4910-bda4-99ae8f6014b4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.312767516s
Sep 27 03:55:57.233: INFO: Pod "pod-projected-secrets-d52dc74c-7051-4910-bda4-99ae8f6014b4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.333167167s
Sep 27 03:55:59.237: INFO: Pod "pod-projected-secrets-d52dc74c-7051-4910-bda4-99ae8f6014b4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.337886013s
Sep 27 03:56:01.241: INFO: Pod "pod-projected-secrets-d52dc74c-7051-4910-bda4-99ae8f6014b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.341624194s
STEP: Saw pod success
Sep 27 03:56:01.241: INFO: Pod "pod-projected-secrets-d52dc74c-7051-4910-bda4-99ae8f6014b4" satisfied condition "Succeeded or Failed"
Sep 27 03:56:01.245: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-projected-secrets-d52dc74c-7051-4910-bda4-99ae8f6014b4 container secret-volume-test: <nil>
STEP: delete the pod
Sep 27 03:56:01.307: INFO: Waiting for pod pod-projected-secrets-d52dc74c-7051-4910-bda4-99ae8f6014b4 to disappear
Sep 27 03:56:01.312: INFO: Pod pod-projected-secrets-d52dc74c-7051-4910-bda4-99ae8f6014b4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:56:01.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1076" for this suite.

â€¢ [SLOW TEST:14.649 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":277,"completed":149,"skipped":2742,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:56:01.320: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8282
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep 27 03:56:01.736: INFO: Waiting up to 5m0s for pod "pod-9b187720-7188-4813-a649-3003a0aaeaa7" in namespace "emptydir-8282" to be "Succeeded or Failed"
Sep 27 03:56:01.742: INFO: Pod "pod-9b187720-7188-4813-a649-3003a0aaeaa7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.462559ms
Sep 27 03:56:03.750: INFO: Pod "pod-9b187720-7188-4813-a649-3003a0aaeaa7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013863832s
Sep 27 03:56:05.758: INFO: Pod "pod-9b187720-7188-4813-a649-3003a0aaeaa7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021905211s
Sep 27 03:56:07.761: INFO: Pod "pod-9b187720-7188-4813-a649-3003a0aaeaa7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025419929s
Sep 27 03:56:10.210: INFO: Pod "pod-9b187720-7188-4813-a649-3003a0aaeaa7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.474145723s
STEP: Saw pod success
Sep 27 03:56:10.210: INFO: Pod "pod-9b187720-7188-4813-a649-3003a0aaeaa7" satisfied condition "Succeeded or Failed"
Sep 27 03:56:10.638: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-9b187720-7188-4813-a649-3003a0aaeaa7 container test-container: <nil>
STEP: delete the pod
Sep 27 03:56:10.913: INFO: Waiting for pod pod-9b187720-7188-4813-a649-3003a0aaeaa7 to disappear
Sep 27 03:56:10.918: INFO: Pod pod-9b187720-7188-4813-a649-3003a0aaeaa7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 03:56:10.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8282" for this suite.

â€¢ [SLOW TEST:9.616 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":150,"skipped":2756,"failed":0}
SSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 03:56:10.937: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6742
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod busybox-4306c651-fe33-4c82-9b43-571108e75c13 in namespace container-probe-6742
Sep 27 03:56:19.548: INFO: Started pod busybox-4306c651-fe33-4c82-9b43-571108e75c13 in namespace container-probe-6742
STEP: checking the pod's current state and verifying that restartCount is present
Sep 27 03:56:19.608: INFO: Initial restart count of pod busybox-4306c651-fe33-4c82-9b43-571108e75c13 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:00:20.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6742" for this suite.

â€¢ [SLOW TEST:249.695 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":277,"completed":151,"skipped":2762,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:00:20.633: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9923
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Sep 27 04:00:23.430: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
W0927 04:00:23.430244      23 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep 27 04:00:23.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9923" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":277,"completed":152,"skipped":2768,"failed":0}
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:00:23.449: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-8949
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Sep 27 04:00:23.667: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:00:47.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8949" for this suite.

â€¢ [SLOW TEST:24.516 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":277,"completed":153,"skipped":2770,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:00:47.964: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-9290
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:00:59.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9290" for this suite.

â€¢ [SLOW TEST:11.252 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":277,"completed":154,"skipped":2779,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:00:59.216: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7105
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 04:01:00.022: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 27 04:01:02.266: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776060, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776060, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776060, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776059, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:01:04.269: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776060, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776060, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776060, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776059, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:01:06.330: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776060, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776060, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776060, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776059, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 04:01:09.292: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:01:09.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7105" for this suite.
STEP: Destroying namespace "webhook-7105-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:10.579 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":277,"completed":155,"skipped":2786,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:01:09.796: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8082
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 04:01:10.442: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"8db5ff92-a6ea-4c84-a8db-3646df021fb6", Controller:(*bool)(0xc006cb503a), BlockOwnerDeletion:(*bool)(0xc006cb503b)}}
Sep 27 04:01:10.476: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"2ca55d63-b606-420c-9ec0-ef8d6948521d", Controller:(*bool)(0xc0065af60a), BlockOwnerDeletion:(*bool)(0xc0065af60b)}}
Sep 27 04:01:10.525: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"a658e5e8-d486-44d5-a7eb-8b937912fe92", Controller:(*bool)(0xc006cb521a), BlockOwnerDeletion:(*bool)(0xc006cb521b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:01:15.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8082" for this suite.

â€¢ [SLOW TEST:5.795 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":277,"completed":156,"skipped":2806,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:01:15.591: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2259
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 04:01:15.927: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 27 04:01:23.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 --namespace=crd-publish-openapi-2259 create -f -'
Sep 27 04:01:27.605: INFO: stderr: ""
Sep 27 04:01:27.605: INFO: stdout: "e2e-test-crd-publish-openapi-421-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep 27 04:01:27.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 --namespace=crd-publish-openapi-2259 delete e2e-test-crd-publish-openapi-421-crds test-cr'
Sep 27 04:01:27.726: INFO: stderr: ""
Sep 27 04:01:27.726: INFO: stdout: "e2e-test-crd-publish-openapi-421-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Sep 27 04:01:27.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 --namespace=crd-publish-openapi-2259 apply -f -'
Sep 27 04:01:28.056: INFO: stderr: ""
Sep 27 04:01:28.056: INFO: stdout: "e2e-test-crd-publish-openapi-421-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep 27 04:01:28.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 --namespace=crd-publish-openapi-2259 delete e2e-test-crd-publish-openapi-421-crds test-cr'
Sep 27 04:01:28.148: INFO: stderr: ""
Sep 27 04:01:28.148: INFO: stdout: "e2e-test-crd-publish-openapi-421-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Sep 27 04:01:28.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 explain e2e-test-crd-publish-openapi-421-crds'
Sep 27 04:01:28.374: INFO: stderr: ""
Sep 27 04:01:28.374: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-421-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:01:32.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2259" for this suite.

â€¢ [SLOW TEST:17.056 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":277,"completed":157,"skipped":2828,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:01:32.647: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5055
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir volume type on node default medium
Sep 27 04:01:32.978: INFO: Waiting up to 5m0s for pod "pod-384ab638-5b8c-4438-8258-bb7a5731cf8b" in namespace "emptydir-5055" to be "Succeeded or Failed"
Sep 27 04:01:32.982: INFO: Pod "pod-384ab638-5b8c-4438-8258-bb7a5731cf8b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.757596ms
Sep 27 04:01:34.985: INFO: Pod "pod-384ab638-5b8c-4438-8258-bb7a5731cf8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007150208s
Sep 27 04:01:36.989: INFO: Pod "pod-384ab638-5b8c-4438-8258-bb7a5731cf8b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011028569s
Sep 27 04:01:39.017: INFO: Pod "pod-384ab638-5b8c-4438-8258-bb7a5731cf8b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.039100745s
Sep 27 04:01:41.025: INFO: Pod "pod-384ab638-5b8c-4438-8258-bb7a5731cf8b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.04672725s
Sep 27 04:01:43.028: INFO: Pod "pod-384ab638-5b8c-4438-8258-bb7a5731cf8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.050211315s
STEP: Saw pod success
Sep 27 04:01:43.028: INFO: Pod "pod-384ab638-5b8c-4438-8258-bb7a5731cf8b" satisfied condition "Succeeded or Failed"
Sep 27 04:01:43.031: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-384ab638-5b8c-4438-8258-bb7a5731cf8b container test-container: <nil>
STEP: delete the pod
Sep 27 04:01:43.083: INFO: Waiting for pod pod-384ab638-5b8c-4438-8258-bb7a5731cf8b to disappear
Sep 27 04:01:43.101: INFO: Pod pod-384ab638-5b8c-4438-8258-bb7a5731cf8b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:01:43.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5055" for this suite.

â€¢ [SLOW TEST:10.464 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":158,"skipped":2829,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:01:43.112: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7801
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 04:01:44.123: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 27 04:01:46.193: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776104, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776104, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776104, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776104, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:01:48.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776104, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776104, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776104, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776104, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:01:50.198: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776104, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776104, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776104, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776104, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 04:01:53.242: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 04:01:53.278: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2010-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:01:54.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7801" for this suite.
STEP: Destroying namespace "webhook-7801-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:12.086 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":277,"completed":159,"skipped":2838,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:01:55.198: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-20
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test hostPath mode
Sep 27 04:01:55.553: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-20" to be "Succeeded or Failed"
Sep 27 04:01:55.587: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 33.723288ms
Sep 27 04:01:57.593: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040122411s
Sep 27 04:01:59.600: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046559321s
Sep 27 04:02:01.666: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 6.113111907s
Sep 27 04:02:03.719: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 8.165590636s
Sep 27 04:02:05.787: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 10.233209877s
Sep 27 04:02:07.790: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 12.23660165s
Sep 27 04:02:09.795: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.242143808s
STEP: Saw pod success
Sep 27 04:02:09.796: INFO: Pod "pod-host-path-test" satisfied condition "Succeeded or Failed"
Sep 27 04:02:09.799: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Sep 27 04:02:09.822: INFO: Waiting for pod pod-host-path-test to disappear
Sep 27 04:02:09.866: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:02:09.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-20" for this suite.

â€¢ [SLOW TEST:14.704 seconds]
[sig-storage] HostPath
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":160,"skipped":2852,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:02:09.902: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4526
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Sep 27 04:02:20.386: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:02:20.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0927 04:02:20.386291      23 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-4526" for this suite.

â€¢ [SLOW TEST:10.504 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":277,"completed":161,"skipped":2855,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:02:20.406: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6030
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Sep 27 04:02:21.523: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6030 /api/v1/namespaces/watch-6030/configmaps/e2e-watch-test-label-changed dd9c0b0b-359b-493f-9b4d-fb771364a7c5 699691 0 2020-09-27 04:02:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 04:02:21.523: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6030 /api/v1/namespaces/watch-6030/configmaps/e2e-watch-test-label-changed dd9c0b0b-359b-493f-9b4d-fb771364a7c5 699692 0 2020-09-27 04:02:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 04:02:21.523: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6030 /api/v1/namespaces/watch-6030/configmaps/e2e-watch-test-label-changed dd9c0b0b-359b-493f-9b4d-fb771364a7c5 699693 0 2020-09-27 04:02:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Sep 27 04:02:31.852: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6030 /api/v1/namespaces/watch-6030/configmaps/e2e-watch-test-label-changed dd9c0b0b-359b-493f-9b4d-fb771364a7c5 699739 0 2020-09-27 04:02:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 04:02:31.852: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6030 /api/v1/namespaces/watch-6030/configmaps/e2e-watch-test-label-changed dd9c0b0b-359b-493f-9b4d-fb771364a7c5 699740 0 2020-09-27 04:02:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 04:02:31.852: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6030 /api/v1/namespaces/watch-6030/configmaps/e2e-watch-test-label-changed dd9c0b0b-359b-493f-9b4d-fb771364a7c5 699741 0 2020-09-27 04:02:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:02:31.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6030" for this suite.

â€¢ [SLOW TEST:11.531 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":277,"completed":162,"skipped":2873,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:02:31.937: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1978
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 04:02:32.538: INFO: Creating deployment "webserver-deployment"
Sep 27 04:02:32.542: INFO: Waiting for observed generation 1
Sep 27 04:02:34.928: INFO: Waiting for all required pods to come up
Sep 27 04:02:34.995: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Sep 27 04:03:03.297: INFO: Waiting for deployment "webserver-deployment" to complete
Sep 27 04:03:03.382: INFO: Updating deployment "webserver-deployment" with a non-existent image
Sep 27 04:03:03.397: INFO: Updating deployment webserver-deployment
Sep 27 04:03:03.397: INFO: Waiting for observed generation 2
Sep 27 04:03:05.665: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Sep 27 04:03:05.835: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Sep 27 04:03:05.886: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep 27 04:03:05.898: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Sep 27 04:03:05.898: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Sep 27 04:03:05.953: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep 27 04:03:05.962: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Sep 27 04:03:05.962: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Sep 27 04:03:06.149: INFO: Updating deployment webserver-deployment
Sep 27 04:03:06.149: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Sep 27 04:03:06.349: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Sep 27 04:03:06.413: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Sep 27 04:03:06.538: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-1978 /apis/apps/v1/namespaces/deployment-1978/deployments/webserver-deployment 23e8efea-5910-4fd8-9b72-072970024313 700132 3 2020-09-27 04:02:32 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00420a4e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-09-27 04:03:06 +0000 UTC,LastTransitionTime:2020-09-27 04:03:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-6676bcd6d4" is progressing.,LastUpdateTime:2020-09-27 04:03:06 +0000 UTC,LastTransitionTime:2020-09-27 04:02:32 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Sep 27 04:03:06.598: INFO: New ReplicaSet "webserver-deployment-6676bcd6d4" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-6676bcd6d4  deployment-1978 /apis/apps/v1/namespaces/deployment-1978/replicasets/webserver-deployment-6676bcd6d4 10e3326a-618b-4067-9d8a-e343898ba1d9 700126 3 2020-09-27 04:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 23e8efea-5910-4fd8-9b72-072970024313 0xc004272f17 0xc004272f18}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 6676bcd6d4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004272fb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 27 04:03:06.598: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Sep 27 04:03:06.598: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-84855cf797  deployment-1978 /apis/apps/v1/namespaces/deployment-1978/replicasets/webserver-deployment-84855cf797 ce653f3b-9af9-42fe-b6b7-ac24d59ede71 700123 3 2020-09-27 04:02:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 23e8efea-5910-4fd8-9b72-072970024313 0xc004273027 0xc004273028}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 84855cf797,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0042730a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Sep 27 04:03:06.631: INFO: Pod "webserver-deployment-6676bcd6d4-7wxpj" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-7wxpj webserver-deployment-6676bcd6d4- deployment-1978 /api/v1/namespaces/deployment-1978/pods/webserver-deployment-6676bcd6d4-7wxpj 4e4234dc-ffba-4a2e-a0c9-0104141e901b 700061 0 2020-09-27 04:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 10e3326a-618b-4067-9d8a-e343898ba1d9 0xc0041e2117 0xc0041e2118}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7dghj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7dghj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7dghj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.6.171.86,PodIP:,StartTime:2020-09-27 04:03:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 04:03:06.631: INFO: Pod "webserver-deployment-6676bcd6d4-8smlq" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-8smlq webserver-deployment-6676bcd6d4- deployment-1978 /api/v1/namespaces/deployment-1978/pods/webserver-deployment-6676bcd6d4-8smlq 418f4b01-c01c-4b25-b1e1-84ddea13debe 700051 0 2020-09-27 04:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 10e3326a-618b-4067-9d8a-e343898ba1d9 0xc0041e22e0 0xc0041e22e1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7dghj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7dghj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7dghj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-84,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.6.171.84,PodIP:,StartTime:2020-09-27 04:03:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 04:03:06.631: INFO: Pod "webserver-deployment-6676bcd6d4-8wh26" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-8wh26 webserver-deployment-6676bcd6d4- deployment-1978 /api/v1/namespaces/deployment-1978/pods/webserver-deployment-6676bcd6d4-8wh26 40d09e79-2433-4180-9b22-7fb226a5265d 700117 0 2020-09-27 04:03:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 10e3326a-618b-4067-9d8a-e343898ba1d9 0xc0041e2450 0xc0041e2451}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7dghj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7dghj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7dghj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-84,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 04:03:06.632: INFO: Pod "webserver-deployment-6676bcd6d4-9p745" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-9p745 webserver-deployment-6676bcd6d4- deployment-1978 /api/v1/namespaces/deployment-1978/pods/webserver-deployment-6676bcd6d4-9p745 aca74efd-b7f4-492a-9056-ed09804e7dcf 700063 0 2020-09-27 04:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 10e3326a-618b-4067-9d8a-e343898ba1d9 0xc0041e2560 0xc0041e2561}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7dghj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7dghj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7dghj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.6.171.86,PodIP:,StartTime:2020-09-27 04:03:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 04:03:06.632: INFO: Pod "webserver-deployment-6676bcd6d4-b4w5m" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-b4w5m webserver-deployment-6676bcd6d4- deployment-1978 /api/v1/namespaces/deployment-1978/pods/webserver-deployment-6676bcd6d4-b4w5m a8b97810-7e11-4e6b-986d-3a00329a70d3 700097 0 2020-09-27 04:03:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 10e3326a-618b-4067-9d8a-e343898ba1d9 0xc0041e2700 0xc0041e2701}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7dghj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7dghj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7dghj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-84,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 04:03:06.632: INFO: Pod "webserver-deployment-6676bcd6d4-dcnfg" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-dcnfg webserver-deployment-6676bcd6d4- deployment-1978 /api/v1/namespaces/deployment-1978/pods/webserver-deployment-6676bcd6d4-dcnfg c216f96c-88b4-4f27-84f5-f67ec7a4e94c 700146 0 2020-09-27 04:03:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 10e3326a-618b-4067-9d8a-e343898ba1d9 0xc0041e28b0 0xc0041e28b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7dghj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7dghj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7dghj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.6.171.86,PodIP:,StartTime:2020-09-27 04:03:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 04:03:06.632: INFO: Pod "webserver-deployment-6676bcd6d4-gr7tz" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-gr7tz webserver-deployment-6676bcd6d4- deployment-1978 /api/v1/namespaces/deployment-1978/pods/webserver-deployment-6676bcd6d4-gr7tz cc547459-6cb5-4105-9806-b26c5974888e 700064 0 2020-09-27 04:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 10e3326a-618b-4067-9d8a-e343898ba1d9 0xc0041e2a40 0xc0041e2a41}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7dghj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7dghj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7dghj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.6.171.86,PodIP:,StartTime:2020-09-27 04:03:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 04:03:06.632: INFO: Pod "webserver-deployment-6676bcd6d4-kcm8b" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-kcm8b webserver-deployment-6676bcd6d4- deployment-1978 /api/v1/namespaces/deployment-1978/pods/webserver-deployment-6676bcd6d4-kcm8b e363abca-11fc-44ea-8dae-466b1290336e 700107 0 2020-09-27 04:03:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 10e3326a-618b-4067-9d8a-e343898ba1d9 0xc0041e2c20 0xc0041e2c21}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7dghj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7dghj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7dghj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 04:03:06.633: INFO: Pod "webserver-deployment-6676bcd6d4-ljcff" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-ljcff webserver-deployment-6676bcd6d4- deployment-1978 /api/v1/namespaces/deployment-1978/pods/webserver-deployment-6676bcd6d4-ljcff 9320539a-7fe6-4c3a-bd63-7ec0573a9c4b 700114 0 2020-09-27 04:03:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 10e3326a-618b-4067-9d8a-e343898ba1d9 0xc0041e2d70 0xc0041e2d71}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7dghj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7dghj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7dghj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 04:03:06.633: INFO: Pod "webserver-deployment-6676bcd6d4-nbbrh" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-nbbrh webserver-deployment-6676bcd6d4- deployment-1978 /api/v1/namespaces/deployment-1978/pods/webserver-deployment-6676bcd6d4-nbbrh 687b642c-baf4-4869-9a05-844ea3bca6e4 700118 0 2020-09-27 04:03:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 10e3326a-618b-4067-9d8a-e343898ba1d9 0xc0041e2ea0 0xc0041e2ea1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7dghj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7dghj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7dghj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 04:03:06.633: INFO: Pod "webserver-deployment-6676bcd6d4-nsdj7" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-nsdj7 webserver-deployment-6676bcd6d4- deployment-1978 /api/v1/namespaces/deployment-1978/pods/webserver-deployment-6676bcd6d4-nsdj7 eb7f8b6f-5825-4594-8340-40621535f889 700125 0 2020-09-27 04:03:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 10e3326a-618b-4067-9d8a-e343898ba1d9 0xc0041e2ff0 0xc0041e2ff1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7dghj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7dghj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7dghj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 04:03:06.633: INFO: Pod "webserver-deployment-6676bcd6d4-qjn2j" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-qjn2j webserver-deployment-6676bcd6d4- deployment-1978 /api/v1/namespaces/deployment-1978/pods/webserver-deployment-6676bcd6d4-qjn2j 7aeeb3c1-21cb-4259-bd91-0a7e96e77606 700035 0 2020-09-27 04:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 10e3326a-618b-4067-9d8a-e343898ba1d9 0xc0041e3110 0xc0041e3111}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7dghj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7dghj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7dghj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-84,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.6.171.84,PodIP:,StartTime:2020-09-27 04:03:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 04:03:06.633: INFO: Pod "webserver-deployment-6676bcd6d4-wq6cx" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-wq6cx webserver-deployment-6676bcd6d4- deployment-1978 /api/v1/namespaces/deployment-1978/pods/webserver-deployment-6676bcd6d4-wq6cx ba97e897-204a-410b-b186-3280bb057b70 700121 0 2020-09-27 04:03:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 10e3326a-618b-4067-9d8a-e343898ba1d9 0xc0041e32a0 0xc0041e32a1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7dghj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7dghj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7dghj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-84,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 04:03:06.668: INFO: Pod "webserver-deployment-84855cf797-5jndr" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-5jndr webserver-deployment-84855cf797- deployment-1978 /api/v1/namespaces/deployment-1978/pods/webserver-deployment-84855cf797-5jndr e8d9807a-b805-49bd-bd00-6a2eec6271f7 700134 0 2020-09-27 04:03:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ce653f3b-9af9-42fe-b6b7-ac24d59ede71 0xc0041e3490 0xc0041e3491}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7dghj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7dghj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7dghj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.6.171.86,PodIP:,StartTime:2020-09-27 04:03:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 04:03:06.668: INFO: Pod "webserver-deployment-84855cf797-5zlx6" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-5zlx6 webserver-deployment-84855cf797- deployment-1978 /api/v1/namespaces/deployment-1978/pods/webserver-deployment-84855cf797-5zlx6 c40c6fe9-c0bf-4c4f-89cc-f09b895c0c3c 700116 0 2020-09-27 04:03:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ce653f3b-9af9-42fe-b6b7-ac24d59ede71 0xc0041e3687 0xc0041e3688}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7dghj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7dghj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7dghj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-84,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 04:03:06.668: INFO: Pod "webserver-deployment-84855cf797-6sznr" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-6sznr webserver-deployment-84855cf797- deployment-1978 /api/v1/namespaces/deployment-1978/pods/webserver-deployment-84855cf797-6sznr 4e2c7b05-554b-4dbd-b0f4-4d9f5edbb9d1 699940 0 2020-09-27 04:02:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/ipv4pools:["default-ipv4-ippool"] dce.daocloud.io/parcel.egress.burst:0 dce.daocloud.io/parcel.egress.rate:0 dce.daocloud.io/parcel.ingress.burst:0 dce.daocloud.io/parcel.ingress.rate:0 kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ce653f3b-9af9-42fe-b6b7-ac24d59ede71 0xc0041e37f0 0xc0041e37f1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7dghj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7dghj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7dghj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:02:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:02:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:02:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:02:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.6.171.86,PodIP:172.29.108.69,StartTime:2020-09-27 04:02:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-27 04:02:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://c8433056bc1621671b8108aada56229ef9b0e40d67f73ab9c1c18282f1013d49,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.29.108.69,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 04:03:06.668: INFO: Pod "webserver-deployment-84855cf797-7hqcs" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-7hqcs webserver-deployment-84855cf797- deployment-1978 /api/v1/namespaces/deployment-1978/pods/webserver-deployment-84855cf797-7hqcs 33f128b1-154d-4b3e-9ef8-d5849af6ebcf 700095 0 2020-09-27 04:03:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ce653f3b-9af9-42fe-b6b7-ac24d59ede71 0xc0041e3a37 0xc0041e3a38}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7dghj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7dghj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7dghj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 04:03:06.672: INFO: Pod "webserver-deployment-84855cf797-9l8bv" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-9l8bv webserver-deployment-84855cf797- deployment-1978 /api/v1/namespaces/deployment-1978/pods/webserver-deployment-84855cf797-9l8bv 0fa0934d-398b-41eb-b2d9-75c6c7641f2e 700102 0 2020-09-27 04:03:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ce653f3b-9af9-42fe-b6b7-ac24d59ede71 0xc0041e3bd0 0xc0041e3bd1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7dghj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7dghj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7dghj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 04:03:06.672: INFO: Pod "webserver-deployment-84855cf797-cg9wt" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-cg9wt webserver-deployment-84855cf797- deployment-1978 /api/v1/namespaces/deployment-1978/pods/webserver-deployment-84855cf797-cg9wt b4aac18a-8bdb-4e24-8753-c4f4bf75b798 699953 0 2020-09-27 04:02:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/ipv4pools:["default-ipv4-ippool"] dce.daocloud.io/parcel.egress.burst:0 dce.daocloud.io/parcel.egress.rate:0 dce.daocloud.io/parcel.ingress.burst:0 dce.daocloud.io/parcel.ingress.rate:0 kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ce653f3b-9af9-42fe-b6b7-ac24d59ede71 0xc0041e3d50 0xc0041e3d51}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7dghj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7dghj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7dghj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:02:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:02:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:02:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:02:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.6.171.86,PodIP:172.29.108.71,StartTime:2020-09-27 04:02:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-27 04:02:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://4ced9c21bb4bdfc0a972b640edcf0781152b494777fe23c8248ead4d567a8217,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.29.108.71,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 04:03:06.672: INFO: Pod "webserver-deployment-84855cf797-cq9nc" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-cq9nc webserver-deployment-84855cf797- deployment-1978 /api/v1/namespaces/deployment-1978/pods/webserver-deployment-84855cf797-cq9nc 796d5d7a-3d9e-49bb-bb6a-6531742d9778 700002 0 2020-09-27 04:02:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/ipv4pools:["default-ipv4-ippool"] dce.daocloud.io/parcel.egress.burst:0 dce.daocloud.io/parcel.egress.rate:0 dce.daocloud.io/parcel.ingress.burst:0 dce.daocloud.io/parcel.ingress.rate:0 kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ce653f3b-9af9-42fe-b6b7-ac24d59ede71 0xc0041e3ed7 0xc0041e3ed8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7dghj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7dghj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7dghj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:02:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:02:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.6.171.86,PodIP:172.29.108.127,StartTime:2020-09-27 04:02:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-27 04:03:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://50292c6aceb2134329933293d9cc210f6714b2f6c38f93a7c2d9262a6d3f5100,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.29.108.127,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 04:03:06.673: INFO: Pod "webserver-deployment-84855cf797-fj6jk" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-fj6jk webserver-deployment-84855cf797- deployment-1978 /api/v1/namespaces/deployment-1978/pods/webserver-deployment-84855cf797-fj6jk f875335b-7dfb-45f7-a596-b180bb6e1c82 700119 0 2020-09-27 04:03:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ce653f3b-9af9-42fe-b6b7-ac24d59ede71 0xc0041ba0a7 0xc0041ba0a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7dghj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7dghj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7dghj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 04:03:06.673: INFO: Pod "webserver-deployment-84855cf797-kbgpk" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-kbgpk webserver-deployment-84855cf797- deployment-1978 /api/v1/namespaces/deployment-1978/pods/webserver-deployment-84855cf797-kbgpk 74f0c285-472c-4cd6-a6d0-70a898468ae1 700120 0 2020-09-27 04:03:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ce653f3b-9af9-42fe-b6b7-ac24d59ede71 0xc0041ba220 0xc0041ba221}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7dghj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7dghj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7dghj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 04:03:06.673: INFO: Pod "webserver-deployment-84855cf797-mtgb2" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-mtgb2 webserver-deployment-84855cf797- deployment-1978 /api/v1/namespaces/deployment-1978/pods/webserver-deployment-84855cf797-mtgb2 0e253976-2d13-4050-a411-67f8f509d147 700111 0 2020-09-27 04:03:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ce653f3b-9af9-42fe-b6b7-ac24d59ede71 0xc0041ba360 0xc0041ba361}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7dghj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7dghj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7dghj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-84,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 04:03:06.673: INFO: Pod "webserver-deployment-84855cf797-nsc2m" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-nsc2m webserver-deployment-84855cf797- deployment-1978 /api/v1/namespaces/deployment-1978/pods/webserver-deployment-84855cf797-nsc2m 167e806a-f722-4bb3-b408-4d36619a099e 700115 0 2020-09-27 04:03:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ce653f3b-9af9-42fe-b6b7-ac24d59ede71 0xc0041ba4b0 0xc0041ba4b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7dghj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7dghj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7dghj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 04:03:06.673: INFO: Pod "webserver-deployment-84855cf797-p2kfw" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-p2kfw webserver-deployment-84855cf797- deployment-1978 /api/v1/namespaces/deployment-1978/pods/webserver-deployment-84855cf797-p2kfw e2718c4b-dbb2-4f75-8c07-b63fb36c3891 700092 0 2020-09-27 04:03:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ce653f3b-9af9-42fe-b6b7-ac24d59ede71 0xc0041ba5d0 0xc0041ba5d1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7dghj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7dghj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7dghj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-84,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 04:03:06.674: INFO: Pod "webserver-deployment-84855cf797-phtph" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-phtph webserver-deployment-84855cf797- deployment-1978 /api/v1/namespaces/deployment-1978/pods/webserver-deployment-84855cf797-phtph e911274f-a764-4f06-8fc8-6f8848a4fbb7 699850 0 2020-09-27 04:02:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/ipv4pools:["default-ipv4-ippool"] dce.daocloud.io/parcel.egress.burst:0 dce.daocloud.io/parcel.egress.rate:0 dce.daocloud.io/parcel.ingress.burst:0 dce.daocloud.io/parcel.ingress.rate:0 kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ce653f3b-9af9-42fe-b6b7-ac24d59ede71 0xc0041ba6d0 0xc0041ba6d1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7dghj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7dghj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7dghj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-84,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:02:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:02:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:02:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:02:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.6.171.84,PodIP:172.29.74.250,StartTime:2020-09-27 04:02:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-27 04:02:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://c78e893d40b3daae0bc70fa619ee2f30028989be96b2c7c104c3bad0b4f2758e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.29.74.250,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 04:03:06.674: INFO: Pod "webserver-deployment-84855cf797-pvwk2" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-pvwk2 webserver-deployment-84855cf797- deployment-1978 /api/v1/namespaces/deployment-1978/pods/webserver-deployment-84855cf797-pvwk2 31deaf0d-946c-4fae-82c4-baab3381cde6 699943 0 2020-09-27 04:02:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/ipv4pools:["default-ipv4-ippool"] dce.daocloud.io/parcel.egress.burst:0 dce.daocloud.io/parcel.egress.rate:0 dce.daocloud.io/parcel.ingress.burst:0 dce.daocloud.io/parcel.ingress.rate:0 kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ce653f3b-9af9-42fe-b6b7-ac24d59ede71 0xc0041ba8c7 0xc0041ba8c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7dghj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7dghj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7dghj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:02:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:02:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:02:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:02:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.6.171.86,PodIP:172.29.108.122,StartTime:2020-09-27 04:02:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-27 04:02:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://3a8be3facade877cd65601c5f99ccf319c514efc4cf23fd74168aaa49d855de3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.29.108.122,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 04:03:06.674: INFO: Pod "webserver-deployment-84855cf797-pz6m7" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-pz6m7 webserver-deployment-84855cf797- deployment-1978 /api/v1/namespaces/deployment-1978/pods/webserver-deployment-84855cf797-pz6m7 0baaf17d-37ed-4990-8df8-edc0f72ef9a2 700098 0 2020-09-27 04:03:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ce653f3b-9af9-42fe-b6b7-ac24d59ede71 0xc0041baa57 0xc0041baa58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7dghj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7dghj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7dghj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-84,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 04:03:06.674: INFO: Pod "webserver-deployment-84855cf797-q68q8" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-q68q8 webserver-deployment-84855cf797- deployment-1978 /api/v1/namespaces/deployment-1978/pods/webserver-deployment-84855cf797-q68q8 c7f0fe99-fc4b-46d2-a825-97b26a8a648d 699983 0 2020-09-27 04:02:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/ipv4pools:["default-ipv4-ippool"] dce.daocloud.io/parcel.egress.burst:0 dce.daocloud.io/parcel.egress.rate:0 dce.daocloud.io/parcel.ingress.burst:0 dce.daocloud.io/parcel.ingress.rate:0 kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ce653f3b-9af9-42fe-b6b7-ac24d59ede71 0xc0041babc0 0xc0041babc1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7dghj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7dghj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7dghj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:02:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:02:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.6.171.86,PodIP:172.29.108.124,StartTime:2020-09-27 04:02:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-27 04:02:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://b43769ae7faa31207113cfcbb7fa69cd120d18b81849ff1f4e105713a720c1bb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.29.108.124,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 04:03:06.674: INFO: Pod "webserver-deployment-84855cf797-rw8bz" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-rw8bz webserver-deployment-84855cf797- deployment-1978 /api/v1/namespaces/deployment-1978/pods/webserver-deployment-84855cf797-rw8bz e2399172-0121-4f63-b419-12e5ac417324 699986 0 2020-09-27 04:02:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/ipv4pools:["default-ipv4-ippool"] dce.daocloud.io/parcel.egress.burst:0 dce.daocloud.io/parcel.egress.rate:0 dce.daocloud.io/parcel.ingress.burst:0 dce.daocloud.io/parcel.ingress.rate:0 kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ce653f3b-9af9-42fe-b6b7-ac24d59ede71 0xc0041bad67 0xc0041bad68}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7dghj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7dghj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7dghj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:02:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:02:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.6.171.86,PodIP:172.29.108.68,StartTime:2020-09-27 04:02:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-27 04:02:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://cf07f0c3628f6aaa3548a352c7f95572539b8f2c9b93ea869fa7ae6e78efff95,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.29.108.68,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 04:03:06.674: INFO: Pod "webserver-deployment-84855cf797-vgwnd" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-vgwnd webserver-deployment-84855cf797- deployment-1978 /api/v1/namespaces/deployment-1978/pods/webserver-deployment-84855cf797-vgwnd 68aeef0c-c3e2-4c43-aa1b-4d85c19b0036 700133 0 2020-09-27 04:03:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ce653f3b-9af9-42fe-b6b7-ac24d59ede71 0xc0041baf47 0xc0041baf48}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7dghj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7dghj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7dghj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-84,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.6.171.84,PodIP:,StartTime:2020-09-27 04:03:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 04:03:06.675: INFO: Pod "webserver-deployment-84855cf797-vjl94" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-vjl94 webserver-deployment-84855cf797- deployment-1978 /api/v1/namespaces/deployment-1978/pods/webserver-deployment-84855cf797-vjl94 20f67b6b-fe68-43e6-b116-c888df944ae5 699858 0 2020-09-27 04:02:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/ipv4pools:["default-ipv4-ippool"] dce.daocloud.io/parcel.egress.burst:0 dce.daocloud.io/parcel.egress.rate:0 dce.daocloud.io/parcel.ingress.burst:0 dce.daocloud.io/parcel.ingress.rate:0 kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ce653f3b-9af9-42fe-b6b7-ac24d59ede71 0xc0041bb127 0xc0041bb128}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7dghj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7dghj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7dghj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-84,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:02:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:02:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:02:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:02:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.6.171.84,PodIP:172.29.74.251,StartTime:2020-09-27 04:02:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-27 04:02:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://82736a04ff4de2c6ea4316f15ac77662abc52777dd048b848130b91b28c5b632,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.29.74.251,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 04:03:06.675: INFO: Pod "webserver-deployment-84855cf797-wxq7x" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-wxq7x webserver-deployment-84855cf797- deployment-1978 /api/v1/namespaces/deployment-1978/pods/webserver-deployment-84855cf797-wxq7x b910228f-36ff-4650-b251-8ac64f4b6a5f 700104 0 2020-09-27 04:03:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ce653f3b-9af9-42fe-b6b7-ac24d59ede71 0xc0041bb317 0xc0041bb318}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7dghj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7dghj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7dghj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-84,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:03:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:03:06.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1978" for this suite.

â€¢ [SLOW TEST:35.182 seconds]
[sig-apps] Deployment
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":277,"completed":163,"skipped":2875,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:03:07.120: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5393
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-9710
STEP: Creating secret with name secret-test-7bee46e9-d06d-4fe5-aca7-f3290e2d2262
STEP: Creating a pod to test consume secrets
Sep 27 04:03:11.984: INFO: Waiting up to 5m0s for pod "pod-secrets-4fe008b5-9a40-4887-b6b0-15ff6f14cd61" in namespace "secrets-5393" to be "Succeeded or Failed"
Sep 27 04:03:12.033: INFO: Pod "pod-secrets-4fe008b5-9a40-4887-b6b0-15ff6f14cd61": Phase="Pending", Reason="", readiness=false. Elapsed: 49.023882ms
Sep 27 04:03:14.091: INFO: Pod "pod-secrets-4fe008b5-9a40-4887-b6b0-15ff6f14cd61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.106638833s
Sep 27 04:03:16.371: INFO: Pod "pod-secrets-4fe008b5-9a40-4887-b6b0-15ff6f14cd61": Phase="Pending", Reason="", readiness=false. Elapsed: 4.386556241s
Sep 27 04:03:18.986: INFO: Pod "pod-secrets-4fe008b5-9a40-4887-b6b0-15ff6f14cd61": Phase="Pending", Reason="", readiness=false. Elapsed: 7.001866124s
Sep 27 04:03:21.310: INFO: Pod "pod-secrets-4fe008b5-9a40-4887-b6b0-15ff6f14cd61": Phase="Pending", Reason="", readiness=false. Elapsed: 9.325435518s
Sep 27 04:03:23.450: INFO: Pod "pod-secrets-4fe008b5-9a40-4887-b6b0-15ff6f14cd61": Phase="Pending", Reason="", readiness=false. Elapsed: 11.466109954s
Sep 27 04:03:25.880: INFO: Pod "pod-secrets-4fe008b5-9a40-4887-b6b0-15ff6f14cd61": Phase="Pending", Reason="", readiness=false. Elapsed: 13.89565176s
Sep 27 04:03:27.938: INFO: Pod "pod-secrets-4fe008b5-9a40-4887-b6b0-15ff6f14cd61": Phase="Pending", Reason="", readiness=false. Elapsed: 15.953504159s
Sep 27 04:03:30.047: INFO: Pod "pod-secrets-4fe008b5-9a40-4887-b6b0-15ff6f14cd61": Phase="Pending", Reason="", readiness=false. Elapsed: 18.063174987s
Sep 27 04:03:32.907: INFO: Pod "pod-secrets-4fe008b5-9a40-4887-b6b0-15ff6f14cd61": Phase="Pending", Reason="", readiness=false. Elapsed: 20.922608534s
Sep 27 04:03:35.148: INFO: Pod "pod-secrets-4fe008b5-9a40-4887-b6b0-15ff6f14cd61": Phase="Pending", Reason="", readiness=false. Elapsed: 23.163921182s
Sep 27 04:03:37.376: INFO: Pod "pod-secrets-4fe008b5-9a40-4887-b6b0-15ff6f14cd61": Phase="Pending", Reason="", readiness=false. Elapsed: 25.391490203s
Sep 27 04:03:39.434: INFO: Pod "pod-secrets-4fe008b5-9a40-4887-b6b0-15ff6f14cd61": Phase="Pending", Reason="", readiness=false. Elapsed: 27.449753723s
Sep 27 04:03:41.665: INFO: Pod "pod-secrets-4fe008b5-9a40-4887-b6b0-15ff6f14cd61": Phase="Pending", Reason="", readiness=false. Elapsed: 29.680425963s
Sep 27 04:03:43.981: INFO: Pod "pod-secrets-4fe008b5-9a40-4887-b6b0-15ff6f14cd61": Phase="Pending", Reason="", readiness=false. Elapsed: 31.996479511s
Sep 27 04:03:46.413: INFO: Pod "pod-secrets-4fe008b5-9a40-4887-b6b0-15ff6f14cd61": Phase="Pending", Reason="", readiness=false. Elapsed: 34.428638035s
Sep 27 04:03:49.420: INFO: Pod "pod-secrets-4fe008b5-9a40-4887-b6b0-15ff6f14cd61": Phase="Pending", Reason="", readiness=false. Elapsed: 37.436041832s
Sep 27 04:03:51.554: INFO: Pod "pod-secrets-4fe008b5-9a40-4887-b6b0-15ff6f14cd61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 39.570178701s
STEP: Saw pod success
Sep 27 04:03:51.554: INFO: Pod "pod-secrets-4fe008b5-9a40-4887-b6b0-15ff6f14cd61" satisfied condition "Succeeded or Failed"
Sep 27 04:03:51.652: INFO: Trying to get logs from node dce-10-6-171-84 pod pod-secrets-4fe008b5-9a40-4887-b6b0-15ff6f14cd61 container secret-volume-test: <nil>
STEP: delete the pod
Sep 27 04:03:51.788: INFO: Waiting for pod pod-secrets-4fe008b5-9a40-4887-b6b0-15ff6f14cd61 to disappear
Sep 27 04:03:51.870: INFO: Pod pod-secrets-4fe008b5-9a40-4887-b6b0-15ff6f14cd61 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:03:51.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5393" for this suite.
STEP: Destroying namespace "secret-namespace-9710" for this suite.

â€¢ [SLOW TEST:44.877 seconds]
[sig-storage] Secrets
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":277,"completed":164,"skipped":2880,"failed":0}
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:03:51.997: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8480
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating replication controller my-hostname-basic-cda03ff0-cab9-4174-b3a5-007a82fcc3cb
Sep 27 04:03:52.624: INFO: Pod name my-hostname-basic-cda03ff0-cab9-4174-b3a5-007a82fcc3cb: Found 0 pods out of 1
Sep 27 04:03:58.643: INFO: Pod name my-hostname-basic-cda03ff0-cab9-4174-b3a5-007a82fcc3cb: Found 1 pods out of 1
Sep 27 04:03:58.643: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-cda03ff0-cab9-4174-b3a5-007a82fcc3cb" are running
Sep 27 04:04:11.112: INFO: Pod "my-hostname-basic-cda03ff0-cab9-4174-b3a5-007a82fcc3cb-fzlbn" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-27 04:03:52 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-27 04:03:52 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-cda03ff0-cab9-4174-b3a5-007a82fcc3cb]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-27 04:03:52 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-cda03ff0-cab9-4174-b3a5-007a82fcc3cb]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-27 04:03:52 +0000 UTC Reason: Message:}])
Sep 27 04:04:11.112: INFO: Trying to dial the pod
Sep 27 04:04:16.127: INFO: Controller my-hostname-basic-cda03ff0-cab9-4174-b3a5-007a82fcc3cb: Got expected result from replica 1 [my-hostname-basic-cda03ff0-cab9-4174-b3a5-007a82fcc3cb-fzlbn]: "my-hostname-basic-cda03ff0-cab9-4174-b3a5-007a82fcc3cb-fzlbn", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:04:16.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8480" for this suite.

â€¢ [SLOW TEST:24.161 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":277,"completed":165,"skipped":2880,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:04:16.159: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7585
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name secret-emptykey-test-f2e8af17-71a1-4201-bb1d-0f9b1166070e
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:04:16.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7585" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":277,"completed":166,"skipped":2903,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:04:16.443: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7532
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-7532
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 27 04:04:16.682: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 27 04:04:16.778: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 04:04:18.785: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 04:04:21.164: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 04:04:22.963: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 04:04:24.887: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 04:04:26.782: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 04:04:28.886: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 04:04:30.972: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 04:04:32.969: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 04:04:34.925: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 04:04:37.039: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 04:04:38.782: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 04:04:40.782: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 04:04:42.781: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 04:04:44.903: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 04:04:46.787: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep 27 04:04:46.801: INFO: The status of Pod netserver-1 is Running (Ready = true)
Sep 27 04:04:46.815: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Sep 27 04:04:54.927: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.29.74.255 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7532 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 04:04:54.927: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
Sep 27 04:04:56.129: INFO: Found all expected endpoints: [netserver-0]
Sep 27 04:04:56.134: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.29.146.31 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7532 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 04:04:56.134: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
Sep 27 04:04:57.485: INFO: Found all expected endpoints: [netserver-1]
Sep 27 04:04:57.503: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.29.108.75 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7532 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 27 04:04:57.504: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
Sep 27 04:04:59.719: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:04:59.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7532" for this suite.

â€¢ [SLOW TEST:43.293 seconds]
[sig-network] Networking
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":167,"skipped":2915,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:04:59.736: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9669
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Sep 27 04:05:24.505: INFO: Successfully updated pod "labelsupdatedf3a7195-d088-4752-9a60-8f8e7e00b2d4"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:05:26.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9669" for this suite.

â€¢ [SLOW TEST:26.909 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":277,"completed":168,"skipped":2922,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:05:26.646: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8255
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:05:34.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8255" for this suite.

â€¢ [SLOW TEST:7.856 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":277,"completed":169,"skipped":2948,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:05:34.502: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9971
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test substitution in container's args
Sep 27 04:05:35.175: INFO: Waiting up to 5m0s for pod "var-expansion-07ecbff5-7772-4eb1-9b6e-96b40300d0b2" in namespace "var-expansion-9971" to be "Succeeded or Failed"
Sep 27 04:05:35.391: INFO: Pod "var-expansion-07ecbff5-7772-4eb1-9b6e-96b40300d0b2": Phase="Pending", Reason="", readiness=false. Elapsed: 216.037498ms
Sep 27 04:05:37.396: INFO: Pod "var-expansion-07ecbff5-7772-4eb1-9b6e-96b40300d0b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.220638606s
Sep 27 04:05:39.400: INFO: Pod "var-expansion-07ecbff5-7772-4eb1-9b6e-96b40300d0b2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.224915122s
Sep 27 04:05:41.440: INFO: Pod "var-expansion-07ecbff5-7772-4eb1-9b6e-96b40300d0b2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.264574208s
Sep 27 04:05:43.443: INFO: Pod "var-expansion-07ecbff5-7772-4eb1-9b6e-96b40300d0b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.268041296s
STEP: Saw pod success
Sep 27 04:05:43.443: INFO: Pod "var-expansion-07ecbff5-7772-4eb1-9b6e-96b40300d0b2" satisfied condition "Succeeded or Failed"
Sep 27 04:05:43.446: INFO: Trying to get logs from node dce-10-6-171-86 pod var-expansion-07ecbff5-7772-4eb1-9b6e-96b40300d0b2 container dapi-container: <nil>
STEP: delete the pod
Sep 27 04:05:43.473: INFO: Waiting for pod var-expansion-07ecbff5-7772-4eb1-9b6e-96b40300d0b2 to disappear
Sep 27 04:05:43.499: INFO: Pod var-expansion-07ecbff5-7772-4eb1-9b6e-96b40300d0b2 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:05:43.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9971" for this suite.

â€¢ [SLOW TEST:9.005 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":277,"completed":170,"skipped":2977,"failed":0}
S
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:05:43.507: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3826
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap that has name configmap-test-emptyKey-722bdd6d-9274-47d4-90bd-8f5c8d1d944c
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:05:43.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3826" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":277,"completed":171,"skipped":2978,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:05:44.054: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5487
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 04:05:45.100: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 27 04:05:47.111: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776345, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776345, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776345, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776345, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:05:49.361: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776345, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776345, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776345, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776345, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:05:51.175: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776345, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776345, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776345, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776345, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:05:53.216: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776345, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776345, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776345, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776345, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 04:05:56.125: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:05:56.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5487" for this suite.
STEP: Destroying namespace "webhook-5487-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:14.313 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":277,"completed":172,"skipped":2984,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:05:58.367: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2623
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 27 04:05:59.342: INFO: Waiting up to 5m0s for pod "downwardapi-volume-202c3ee0-2682-49f3-9a9a-738e4eb3f725" in namespace "downward-api-2623" to be "Succeeded or Failed"
Sep 27 04:05:59.485: INFO: Pod "downwardapi-volume-202c3ee0-2682-49f3-9a9a-738e4eb3f725": Phase="Pending", Reason="", readiness=false. Elapsed: 143.113158ms
Sep 27 04:06:01.568: INFO: Pod "downwardapi-volume-202c3ee0-2682-49f3-9a9a-738e4eb3f725": Phase="Pending", Reason="", readiness=false. Elapsed: 2.226357489s
Sep 27 04:06:03.587: INFO: Pod "downwardapi-volume-202c3ee0-2682-49f3-9a9a-738e4eb3f725": Phase="Pending", Reason="", readiness=false. Elapsed: 4.245372593s
Sep 27 04:06:05.691: INFO: Pod "downwardapi-volume-202c3ee0-2682-49f3-9a9a-738e4eb3f725": Phase="Pending", Reason="", readiness=false. Elapsed: 6.348784764s
Sep 27 04:06:07.702: INFO: Pod "downwardapi-volume-202c3ee0-2682-49f3-9a9a-738e4eb3f725": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.359773923s
STEP: Saw pod success
Sep 27 04:06:07.702: INFO: Pod "downwardapi-volume-202c3ee0-2682-49f3-9a9a-738e4eb3f725" satisfied condition "Succeeded or Failed"
Sep 27 04:06:07.759: INFO: Trying to get logs from node dce-10-6-171-86 pod downwardapi-volume-202c3ee0-2682-49f3-9a9a-738e4eb3f725 container client-container: <nil>
STEP: delete the pod
Sep 27 04:06:08.027: INFO: Waiting for pod downwardapi-volume-202c3ee0-2682-49f3-9a9a-738e4eb3f725 to disappear
Sep 27 04:06:08.237: INFO: Pod downwardapi-volume-202c3ee0-2682-49f3-9a9a-738e4eb3f725 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:06:08.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2623" for this suite.

â€¢ [SLOW TEST:9.919 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":277,"completed":173,"skipped":2996,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:06:08.286: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3994
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-map-6ffe72da-1f87-4e77-8120-7b77bf7f509f
STEP: Creating a pod to test consume configMaps
Sep 27 04:06:09.524: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0d2307ae-a41d-4abf-a6a4-2b85aa86fa3f" in namespace "projected-3994" to be "Succeeded or Failed"
Sep 27 04:06:09.597: INFO: Pod "pod-projected-configmaps-0d2307ae-a41d-4abf-a6a4-2b85aa86fa3f": Phase="Pending", Reason="", readiness=false. Elapsed: 72.640226ms
Sep 27 04:06:11.602: INFO: Pod "pod-projected-configmaps-0d2307ae-a41d-4abf-a6a4-2b85aa86fa3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.077711346s
Sep 27 04:06:13.846: INFO: Pod "pod-projected-configmaps-0d2307ae-a41d-4abf-a6a4-2b85aa86fa3f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.321452582s
Sep 27 04:06:15.887: INFO: Pod "pod-projected-configmaps-0d2307ae-a41d-4abf-a6a4-2b85aa86fa3f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.362677799s
Sep 27 04:06:18.032: INFO: Pod "pod-projected-configmaps-0d2307ae-a41d-4abf-a6a4-2b85aa86fa3f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.508125042s
Sep 27 04:06:20.039: INFO: Pod "pod-projected-configmaps-0d2307ae-a41d-4abf-a6a4-2b85aa86fa3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.514603939s
STEP: Saw pod success
Sep 27 04:06:20.039: INFO: Pod "pod-projected-configmaps-0d2307ae-a41d-4abf-a6a4-2b85aa86fa3f" satisfied condition "Succeeded or Failed"
Sep 27 04:06:20.044: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-projected-configmaps-0d2307ae-a41d-4abf-a6a4-2b85aa86fa3f container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 27 04:06:20.457: INFO: Waiting for pod pod-projected-configmaps-0d2307ae-a41d-4abf-a6a4-2b85aa86fa3f to disappear
Sep 27 04:06:20.511: INFO: Pod pod-projected-configmaps-0d2307ae-a41d-4abf-a6a4-2b85aa86fa3f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:06:20.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3994" for this suite.

â€¢ [SLOW TEST:12.235 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":277,"completed":174,"skipped":3003,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:06:20.521: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3163
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3163.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3163.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3163.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3163.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3163.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3163.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3163.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3163.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3163.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3163.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 27 04:06:30.245: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:30.252: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:30.257: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:30.265: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:30.282: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:30.289: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:30.294: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:30.299: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:30.314: INFO: Lookups using dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3163.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3163.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local jessie_udp@dns-test-service-2.dns-3163.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3163.svc.cluster.local]

Sep 27 04:06:35.360: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:35.369: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:35.374: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:35.379: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:35.400: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:35.405: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:35.409: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:35.414: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:35.423: INFO: Lookups using dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3163.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3163.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local jessie_udp@dns-test-service-2.dns-3163.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3163.svc.cluster.local]

Sep 27 04:06:40.323: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:40.329: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:40.334: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:40.341: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:40.358: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:40.366: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:40.371: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:40.382: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:40.393: INFO: Lookups using dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3163.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3163.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local jessie_udp@dns-test-service-2.dns-3163.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3163.svc.cluster.local]

Sep 27 04:06:45.741: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:45.761: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:45.810: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:45.815: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:45.909: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:46.103: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:46.132: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:46.138: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:46.150: INFO: Lookups using dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3163.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3163.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local jessie_udp@dns-test-service-2.dns-3163.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3163.svc.cluster.local]

Sep 27 04:06:50.319: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:50.325: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:50.333: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:50.339: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:50.355: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:50.360: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:50.365: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:50.371: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:50.381: INFO: Lookups using dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3163.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3163.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local jessie_udp@dns-test-service-2.dns-3163.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3163.svc.cluster.local]

Sep 27 04:06:55.326: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:55.340: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:55.345: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:55.353: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:55.406: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:55.412: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:55.420: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:55.424: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3163.svc.cluster.local from pod dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc: the server could not find the requested resource (get pods dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc)
Sep 27 04:06:55.444: INFO: Lookups using dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3163.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3163.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3163.svc.cluster.local jessie_udp@dns-test-service-2.dns-3163.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3163.svc.cluster.local]

Sep 27 04:07:00.640: INFO: DNS probes using dns-3163/dns-test-f5d6a70f-b3e0-4cd3-9e4b-6be0391347dc succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:07:01.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3163" for this suite.

â€¢ [SLOW TEST:41.316 seconds]
[sig-network] DNS
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":277,"completed":175,"skipped":3029,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:07:01.837: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6931
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name s-test-opt-del-530b0c2e-980f-4c58-af17-feb593639cd4
STEP: Creating secret with name s-test-opt-upd-d46282e2-a979-4568-866f-835c5431e1c9
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-530b0c2e-980f-4c58-af17-feb593639cd4
STEP: Updating secret s-test-opt-upd-d46282e2-a979-4568-866f-835c5431e1c9
STEP: Creating secret with name s-test-opt-create-2dddb59e-dbf2-4780-9f24-c519a20bcd6c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:08:34.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6931" for this suite.

â€¢ [SLOW TEST:92.446 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":176,"skipped":3055,"failed":0}
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:08:34.283: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-5273
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep 27 04:08:58.676: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 27 04:08:58.681: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 27 04:09:00.682: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 27 04:09:00.687: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 27 04:09:02.682: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 27 04:09:02.708: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 27 04:09:04.682: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 27 04:09:04.686: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 27 04:09:06.682: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 27 04:09:06.771: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 27 04:09:08.682: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 27 04:09:08.686: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:09:08.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5273" for this suite.

â€¢ [SLOW TEST:34.419 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":277,"completed":177,"skipped":3055,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:09:08.703: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2992
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating all guestbook components
Sep 27 04:09:09.365: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-slave
  labels:
    app: agnhost
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: slave
    tier: backend

Sep 27 04:09:09.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 create -f - --namespace=kubectl-2992'
Sep 27 04:09:10.005: INFO: stderr: ""
Sep 27 04:09:10.005: INFO: stdout: "service/agnhost-slave created\n"
Sep 27 04:09:10.005: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-master
  labels:
    app: agnhost
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: master
    tier: backend

Sep 27 04:09:10.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 create -f - --namespace=kubectl-2992'
Sep 27 04:09:10.435: INFO: stderr: ""
Sep 27 04:09:10.435: INFO: stdout: "service/agnhost-master created\n"
Sep 27 04:09:10.435: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Sep 27 04:09:10.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 create -f - --namespace=kubectl-2992'
Sep 27 04:09:10.977: INFO: stderr: ""
Sep 27 04:09:10.977: INFO: stdout: "service/frontend created\n"
Sep 27 04:09:10.978: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Sep 27 04:09:10.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 create -f - --namespace=kubectl-2992'
Sep 27 04:09:11.303: INFO: stderr: ""
Sep 27 04:09:11.303: INFO: stdout: "deployment.apps/frontend created\n"
Sep 27 04:09:11.303: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep 27 04:09:11.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 create -f - --namespace=kubectl-2992'
Sep 27 04:09:12.090: INFO: stderr: ""
Sep 27 04:09:12.090: INFO: stdout: "deployment.apps/agnhost-master created\n"
Sep 27 04:09:12.090: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12
        args: [ "guestbook", "--slaveof", "agnhost-master", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep 27 04:09:12.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 create -f - --namespace=kubectl-2992'
Sep 27 04:09:12.745: INFO: stderr: ""
Sep 27 04:09:12.745: INFO: stdout: "deployment.apps/agnhost-slave created\n"
STEP: validating guestbook app
Sep 27 04:09:12.745: INFO: Waiting for all frontend pods to be Running.
Sep 27 04:09:32.998: INFO: Waiting for frontend to serve content.
Sep 27 04:09:33.027: INFO: Trying to add a new entry to the guestbook.
Sep 27 04:09:33.042: INFO: Verifying that added entry can be retrieved.
Sep 27 04:09:33.051: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Sep 27 04:09:38.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 delete --grace-period=0 --force -f - --namespace=kubectl-2992'
Sep 27 04:09:38.340: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 27 04:09:38.340: INFO: stdout: "service \"agnhost-slave\" force deleted\n"
STEP: using delete to clean up resources
Sep 27 04:09:38.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 delete --grace-period=0 --force -f - --namespace=kubectl-2992'
Sep 27 04:09:38.639: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 27 04:09:38.639: INFO: stdout: "service \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Sep 27 04:09:38.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 delete --grace-period=0 --force -f - --namespace=kubectl-2992'
Sep 27 04:09:39.163: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 27 04:09:39.163: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep 27 04:09:39.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 delete --grace-period=0 --force -f - --namespace=kubectl-2992'
Sep 27 04:09:39.392: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 27 04:09:39.392: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep 27 04:09:39.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 delete --grace-period=0 --force -f - --namespace=kubectl-2992'
Sep 27 04:09:39.605: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 27 04:09:39.605: INFO: stdout: "deployment.apps \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Sep 27 04:09:39.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 delete --grace-period=0 --force -f - --namespace=kubectl-2992'
Sep 27 04:09:39.988: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 27 04:09:39.988: INFO: stdout: "deployment.apps \"agnhost-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:09:39.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2992" for this suite.

â€¢ [SLOW TEST:31.307 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:310
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":277,"completed":178,"skipped":3086,"failed":0}
SSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:09:40.010: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9573
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating secret secrets-9573/secret-test-ec43bf3c-9ef9-4470-ad65-50fc39273a0a
STEP: Creating a pod to test consume secrets
Sep 27 04:09:40.780: INFO: Waiting up to 5m0s for pod "pod-configmaps-bcd9494d-413d-4638-bf2e-5df5b1ba9d40" in namespace "secrets-9573" to be "Succeeded or Failed"
Sep 27 04:09:40.966: INFO: Pod "pod-configmaps-bcd9494d-413d-4638-bf2e-5df5b1ba9d40": Phase="Pending", Reason="", readiness=false. Elapsed: 186.065023ms
Sep 27 04:09:43.067: INFO: Pod "pod-configmaps-bcd9494d-413d-4638-bf2e-5df5b1ba9d40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.287343397s
Sep 27 04:09:45.579: INFO: Pod "pod-configmaps-bcd9494d-413d-4638-bf2e-5df5b1ba9d40": Phase="Pending", Reason="", readiness=false. Elapsed: 4.798951191s
Sep 27 04:09:47.690: INFO: Pod "pod-configmaps-bcd9494d-413d-4638-bf2e-5df5b1ba9d40": Phase="Pending", Reason="", readiness=false. Elapsed: 6.909907637s
Sep 27 04:09:49.912: INFO: Pod "pod-configmaps-bcd9494d-413d-4638-bf2e-5df5b1ba9d40": Phase="Pending", Reason="", readiness=false. Elapsed: 9.131835829s
Sep 27 04:09:52.100: INFO: Pod "pod-configmaps-bcd9494d-413d-4638-bf2e-5df5b1ba9d40": Phase="Pending", Reason="", readiness=false. Elapsed: 11.320419848s
Sep 27 04:09:54.104: INFO: Pod "pod-configmaps-bcd9494d-413d-4638-bf2e-5df5b1ba9d40": Phase="Pending", Reason="", readiness=false. Elapsed: 13.324523584s
Sep 27 04:09:56.219: INFO: Pod "pod-configmaps-bcd9494d-413d-4638-bf2e-5df5b1ba9d40": Phase="Pending", Reason="", readiness=false. Elapsed: 15.439460702s
Sep 27 04:09:58.495: INFO: Pod "pod-configmaps-bcd9494d-413d-4638-bf2e-5df5b1ba9d40": Phase="Pending", Reason="", readiness=false. Elapsed: 17.714918945s
Sep 27 04:10:00.709: INFO: Pod "pod-configmaps-bcd9494d-413d-4638-bf2e-5df5b1ba9d40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 19.929319769s
STEP: Saw pod success
Sep 27 04:10:00.709: INFO: Pod "pod-configmaps-bcd9494d-413d-4638-bf2e-5df5b1ba9d40" satisfied condition "Succeeded or Failed"
Sep 27 04:10:01.000: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-configmaps-bcd9494d-413d-4638-bf2e-5df5b1ba9d40 container env-test: <nil>
STEP: delete the pod
Sep 27 04:10:02.358: INFO: Waiting for pod pod-configmaps-bcd9494d-413d-4638-bf2e-5df5b1ba9d40 to disappear
Sep 27 04:10:02.483: INFO: Pod pod-configmaps-bcd9494d-413d-4638-bf2e-5df5b1ba9d40 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:10:02.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9573" for this suite.

â€¢ [SLOW TEST:22.491 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:35
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":277,"completed":179,"skipped":3089,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:10:02.501: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7714
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name s-test-opt-del-0264e5b9-1658-46e2-9ed3-d7304820d181
STEP: Creating secret with name s-test-opt-upd-3a982572-610e-474a-8e39-f27b9882d12d
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-0264e5b9-1658-46e2-9ed3-d7304820d181
STEP: Updating secret s-test-opt-upd-3a982572-610e-474a-8e39-f27b9882d12d
STEP: Creating secret with name s-test-opt-create-890d7c25-4bea-4464-b015-793c4eea0b53
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:11:38.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7714" for this suite.

â€¢ [SLOW TEST:96.038 seconds]
[sig-storage] Secrets
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":180,"skipped":3091,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:11:38.540: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2962
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl logs
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1288
STEP: creating an pod
Sep 27 04:11:38.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 run logs-generator --image=us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 --namespace=kubectl-2962 -- logs-generator --log-lines-total 100 --run-duration 20s'
Sep 27 04:11:45.788: INFO: stderr: ""
Sep 27 04:11:45.788: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Waiting for log generator to start.
Sep 27 04:11:45.788: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Sep 27 04:11:45.788: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2962" to be "running and ready, or succeeded"
Sep 27 04:11:45.962: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 173.755352ms
Sep 27 04:11:47.966: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.178629929s
Sep 27 04:11:50.015: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.227185834s
Sep 27 04:11:52.042: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.254552107s
Sep 27 04:11:54.080: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 8.29213598s
Sep 27 04:11:56.498: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 10.709916226s
Sep 27 04:11:58.588: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 12.799826705s
Sep 27 04:12:00.591: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 14.803270829s
Sep 27 04:12:00.591: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Sep 27 04:12:00.591: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Sep 27 04:12:00.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 logs logs-generator logs-generator --namespace=kubectl-2962'
Sep 27 04:12:00.731: INFO: stderr: ""
Sep 27 04:12:00.731: INFO: stdout: "I0927 04:11:54.970408       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/hk4 237\nI0927 04:11:55.170650       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/tzt 270\nI0927 04:11:55.370832       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/hsx 312\nI0927 04:11:55.570651       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/52ws 516\nI0927 04:11:55.770683       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/2qhd 558\nI0927 04:11:55.970700       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/rwq 475\nI0927 04:11:56.173087       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/bvj 417\nI0927 04:11:56.370676       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/wbz 507\nI0927 04:11:56.570596       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/dpl 431\nI0927 04:11:56.770578       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/nwpk 385\nI0927 04:11:56.971440       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/g7m9 372\nI0927 04:11:57.170651       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/j77c 396\nI0927 04:11:57.370891       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/ttg 386\nI0927 04:11:57.570616       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/dqk 520\nI0927 04:11:57.770646       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/bbnm 570\nI0927 04:11:57.970641       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/vvvz 326\nI0927 04:11:58.170625       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/vzqq 520\nI0927 04:11:58.370628       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/qt5 438\nI0927 04:11:58.571106       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/99m 531\nI0927 04:11:58.770604       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/25c 377\nI0927 04:11:58.970673       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/k9f 340\nI0927 04:11:59.170621       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/66k 223\nI0927 04:11:59.370912       1 logs_generator.go:76] 22 POST /api/v1/namespaces/ns/pods/xll 468\nI0927 04:11:59.570601       1 logs_generator.go:76] 23 GET /api/v1/namespaces/kube-system/pods/hzg 318\nI0927 04:11:59.770688       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/2hz4 524\nI0927 04:11:59.970601       1 logs_generator.go:76] 25 POST /api/v1/namespaces/ns/pods/pkw7 303\nI0927 04:12:00.170611       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/ns/pods/7lh 447\nI0927 04:12:00.370687       1 logs_generator.go:76] 27 POST /api/v1/namespaces/default/pods/6gfw 484\nI0927 04:12:00.570648       1 logs_generator.go:76] 28 GET /api/v1/namespaces/kube-system/pods/hd9v 334\n"
STEP: limiting log lines
Sep 27 04:12:00.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 logs logs-generator logs-generator --namespace=kubectl-2962 --tail=1'
Sep 27 04:12:00.882: INFO: stderr: ""
Sep 27 04:12:00.882: INFO: stdout: "I0927 04:12:00.770843       1 logs_generator.go:76] 29 GET /api/v1/namespaces/ns/pods/qt7 240\n"
Sep 27 04:12:00.882: INFO: got output "I0927 04:12:00.770843       1 logs_generator.go:76] 29 GET /api/v1/namespaces/ns/pods/qt7 240\n"
STEP: limiting log bytes
Sep 27 04:12:00.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 logs logs-generator logs-generator --namespace=kubectl-2962 --limit-bytes=1'
Sep 27 04:12:01.046: INFO: stderr: ""
Sep 27 04:12:01.046: INFO: stdout: "I"
Sep 27 04:12:01.046: INFO: got output "I"
STEP: exposing timestamps
Sep 27 04:12:01.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 logs logs-generator logs-generator --namespace=kubectl-2962 --tail=1 --timestamps'
Sep 27 04:12:01.201: INFO: stderr: ""
Sep 27 04:12:01.201: INFO: stdout: "2020-09-27T04:12:01.175166835Z I0927 04:12:01.174986       1 logs_generator.go:76] 31 GET /api/v1/namespaces/ns/pods/vvf 369\n"
Sep 27 04:12:01.201: INFO: got output "2020-09-27T04:12:01.175166835Z I0927 04:12:01.174986       1 logs_generator.go:76] 31 GET /api/v1/namespaces/ns/pods/vvf 369\n"
STEP: restricting to a time range
Sep 27 04:12:03.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 logs logs-generator logs-generator --namespace=kubectl-2962 --since=1s'
Sep 27 04:12:03.849: INFO: stderr: ""
Sep 27 04:12:03.849: INFO: stdout: "I0927 04:12:02.970619       1 logs_generator.go:76] 40 POST /api/v1/namespaces/kube-system/pods/k2d6 448\nI0927 04:12:03.170593       1 logs_generator.go:76] 41 POST /api/v1/namespaces/default/pods/qd6 401\nI0927 04:12:03.370619       1 logs_generator.go:76] 42 GET /api/v1/namespaces/ns/pods/mlxj 391\nI0927 04:12:03.570616       1 logs_generator.go:76] 43 PUT /api/v1/namespaces/default/pods/hv5k 337\nI0927 04:12:03.770726       1 logs_generator.go:76] 44 POST /api/v1/namespaces/default/pods/fnzw 236\n"
Sep 27 04:12:03.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 logs logs-generator logs-generator --namespace=kubectl-2962 --since=24h'
Sep 27 04:12:03.997: INFO: stderr: ""
Sep 27 04:12:03.997: INFO: stdout: "I0927 04:11:54.970408       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/hk4 237\nI0927 04:11:55.170650       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/tzt 270\nI0927 04:11:55.370832       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/hsx 312\nI0927 04:11:55.570651       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/52ws 516\nI0927 04:11:55.770683       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/2qhd 558\nI0927 04:11:55.970700       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/rwq 475\nI0927 04:11:56.173087       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/bvj 417\nI0927 04:11:56.370676       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/wbz 507\nI0927 04:11:56.570596       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/dpl 431\nI0927 04:11:56.770578       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/nwpk 385\nI0927 04:11:56.971440       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/g7m9 372\nI0927 04:11:57.170651       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/j77c 396\nI0927 04:11:57.370891       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/ttg 386\nI0927 04:11:57.570616       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/dqk 520\nI0927 04:11:57.770646       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/bbnm 570\nI0927 04:11:57.970641       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/vvvz 326\nI0927 04:11:58.170625       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/vzqq 520\nI0927 04:11:58.370628       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/qt5 438\nI0927 04:11:58.571106       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/99m 531\nI0927 04:11:58.770604       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/25c 377\nI0927 04:11:58.970673       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/k9f 340\nI0927 04:11:59.170621       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/66k 223\nI0927 04:11:59.370912       1 logs_generator.go:76] 22 POST /api/v1/namespaces/ns/pods/xll 468\nI0927 04:11:59.570601       1 logs_generator.go:76] 23 GET /api/v1/namespaces/kube-system/pods/hzg 318\nI0927 04:11:59.770688       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/2hz4 524\nI0927 04:11:59.970601       1 logs_generator.go:76] 25 POST /api/v1/namespaces/ns/pods/pkw7 303\nI0927 04:12:00.170611       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/ns/pods/7lh 447\nI0927 04:12:00.370687       1 logs_generator.go:76] 27 POST /api/v1/namespaces/default/pods/6gfw 484\nI0927 04:12:00.570648       1 logs_generator.go:76] 28 GET /api/v1/namespaces/kube-system/pods/hd9v 334\nI0927 04:12:00.770843       1 logs_generator.go:76] 29 GET /api/v1/namespaces/ns/pods/qt7 240\nI0927 04:12:00.970642       1 logs_generator.go:76] 30 POST /api/v1/namespaces/ns/pods/btbd 368\nI0927 04:12:01.174986       1 logs_generator.go:76] 31 GET /api/v1/namespaces/ns/pods/vvf 369\nI0927 04:12:01.370654       1 logs_generator.go:76] 32 POST /api/v1/namespaces/ns/pods/gfm 477\nI0927 04:12:01.570610       1 logs_generator.go:76] 33 GET /api/v1/namespaces/default/pods/9cbn 443\nI0927 04:12:01.770632       1 logs_generator.go:76] 34 PUT /api/v1/namespaces/default/pods/6ctd 432\nI0927 04:12:01.970690       1 logs_generator.go:76] 35 GET /api/v1/namespaces/kube-system/pods/7rs 356\nI0927 04:12:02.170603       1 logs_generator.go:76] 36 POST /api/v1/namespaces/kube-system/pods/k86j 209\nI0927 04:12:02.370605       1 logs_generator.go:76] 37 POST /api/v1/namespaces/default/pods/dmsh 300\nI0927 04:12:02.570700       1 logs_generator.go:76] 38 GET /api/v1/namespaces/kube-system/pods/6jwm 473\nI0927 04:12:02.770614       1 logs_generator.go:76] 39 PUT /api/v1/namespaces/ns/pods/h729 256\nI0927 04:12:02.970619       1 logs_generator.go:76] 40 POST /api/v1/namespaces/kube-system/pods/k2d6 448\nI0927 04:12:03.170593       1 logs_generator.go:76] 41 POST /api/v1/namespaces/default/pods/qd6 401\nI0927 04:12:03.370619       1 logs_generator.go:76] 42 GET /api/v1/namespaces/ns/pods/mlxj 391\nI0927 04:12:03.570616       1 logs_generator.go:76] 43 PUT /api/v1/namespaces/default/pods/hv5k 337\nI0927 04:12:03.770726       1 logs_generator.go:76] 44 POST /api/v1/namespaces/default/pods/fnzw 236\nI0927 04:12:03.970745       1 logs_generator.go:76] 45 GET /api/v1/namespaces/default/pods/v99 353\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1294
Sep 27 04:12:03.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 delete pod logs-generator --namespace=kubectl-2962'
Sep 27 04:12:11.723: INFO: stderr: ""
Sep 27 04:12:11.723: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:12:11.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2962" for this suite.

â€¢ [SLOW TEST:33.217 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1284
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":277,"completed":181,"skipped":3118,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:12:11.758: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7170
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod liveness-18397ab6-d016-4f87-a2c3-d0ada68b65c0 in namespace container-probe-7170
Sep 27 04:12:20.308: INFO: Started pod liveness-18397ab6-d016-4f87-a2c3-d0ada68b65c0 in namespace container-probe-7170
STEP: checking the pod's current state and verifying that restartCount is present
Sep 27 04:12:20.311: INFO: Initial restart count of pod liveness-18397ab6-d016-4f87-a2c3-d0ada68b65c0 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:16:21.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7170" for this suite.

â€¢ [SLOW TEST:249.816 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":277,"completed":182,"skipped":3127,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:16:21.574: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9705
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 27 04:16:21.906: INFO: Waiting up to 5m0s for pod "downwardapi-volume-74405759-1f6f-47bc-a6e7-5be1f416cd2d" in namespace "projected-9705" to be "Succeeded or Failed"
Sep 27 04:16:21.971: INFO: Pod "downwardapi-volume-74405759-1f6f-47bc-a6e7-5be1f416cd2d": Phase="Pending", Reason="", readiness=false. Elapsed: 65.715787ms
Sep 27 04:16:24.105: INFO: Pod "downwardapi-volume-74405759-1f6f-47bc-a6e7-5be1f416cd2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.199025676s
Sep 27 04:16:26.108: INFO: Pod "downwardapi-volume-74405759-1f6f-47bc-a6e7-5be1f416cd2d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.202296149s
Sep 27 04:16:28.112: INFO: Pod "downwardapi-volume-74405759-1f6f-47bc-a6e7-5be1f416cd2d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.206047956s
Sep 27 04:16:30.352: INFO: Pod "downwardapi-volume-74405759-1f6f-47bc-a6e7-5be1f416cd2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.446391384s
STEP: Saw pod success
Sep 27 04:16:30.352: INFO: Pod "downwardapi-volume-74405759-1f6f-47bc-a6e7-5be1f416cd2d" satisfied condition "Succeeded or Failed"
Sep 27 04:16:30.415: INFO: Trying to get logs from node dce-10-6-171-86 pod downwardapi-volume-74405759-1f6f-47bc-a6e7-5be1f416cd2d container client-container: <nil>
STEP: delete the pod
Sep 27 04:16:30.698: INFO: Waiting for pod downwardapi-volume-74405759-1f6f-47bc-a6e7-5be1f416cd2d to disappear
Sep 27 04:16:31.038: INFO: Pod downwardapi-volume-74405759-1f6f-47bc-a6e7-5be1f416cd2d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:16:31.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9705" for this suite.

â€¢ [SLOW TEST:9.896 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":183,"skipped":3135,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:16:31.470: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4531
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 04:16:34.557: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 27 04:16:36.627: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776994, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776994, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776994, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776994, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:16:38.632: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776994, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776994, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776994, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776994, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:16:40.636: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776994, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776994, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776994, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776994, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:16:42.716: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776994, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776994, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776994, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736776994, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 04:16:45.640: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Sep 27 04:16:45.671: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:16:45.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4531" for this suite.
STEP: Destroying namespace "webhook-4531-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:14.315 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":277,"completed":184,"skipped":3145,"failed":0}
SSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:16:45.785: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-9501
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Sep 27 04:16:46.090: INFO: Pod name pod-release: Found 0 pods out of 1
Sep 27 04:16:51.189: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:16:51.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9501" for this suite.

â€¢ [SLOW TEST:5.881 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":277,"completed":185,"skipped":3148,"failed":0}
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:16:51.666: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6884
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-map-0d2c2d17-2a72-46fb-b1ef-6116442dabad
STEP: Creating a pod to test consume secrets
Sep 27 04:16:52.034: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-871170f3-2aa0-4128-bb03-fab2c996b601" in namespace "projected-6884" to be "Succeeded or Failed"
Sep 27 04:16:52.095: INFO: Pod "pod-projected-secrets-871170f3-2aa0-4128-bb03-fab2c996b601": Phase="Pending", Reason="", readiness=false. Elapsed: 60.601575ms
Sep 27 04:16:54.354: INFO: Pod "pod-projected-secrets-871170f3-2aa0-4128-bb03-fab2c996b601": Phase="Pending", Reason="", readiness=false. Elapsed: 2.320129464s
Sep 27 04:16:56.649: INFO: Pod "pod-projected-secrets-871170f3-2aa0-4128-bb03-fab2c996b601": Phase="Pending", Reason="", readiness=false. Elapsed: 4.614611422s
Sep 27 04:16:58.663: INFO: Pod "pod-projected-secrets-871170f3-2aa0-4128-bb03-fab2c996b601": Phase="Pending", Reason="", readiness=false. Elapsed: 6.628925805s
Sep 27 04:17:00.668: INFO: Pod "pod-projected-secrets-871170f3-2aa0-4128-bb03-fab2c996b601": Phase="Pending", Reason="", readiness=false. Elapsed: 8.633864485s
Sep 27 04:17:02.709: INFO: Pod "pod-projected-secrets-871170f3-2aa0-4128-bb03-fab2c996b601": Phase="Pending", Reason="", readiness=false. Elapsed: 10.674511002s
Sep 27 04:17:04.793: INFO: Pod "pod-projected-secrets-871170f3-2aa0-4128-bb03-fab2c996b601": Phase="Pending", Reason="", readiness=false. Elapsed: 12.758508912s
Sep 27 04:17:06.879: INFO: Pod "pod-projected-secrets-871170f3-2aa0-4128-bb03-fab2c996b601": Phase="Pending", Reason="", readiness=false. Elapsed: 14.844597019s
Sep 27 04:17:08.943: INFO: Pod "pod-projected-secrets-871170f3-2aa0-4128-bb03-fab2c996b601": Phase="Pending", Reason="", readiness=false. Elapsed: 16.908693855s
Sep 27 04:17:10.952: INFO: Pod "pod-projected-secrets-871170f3-2aa0-4128-bb03-fab2c996b601": Phase="Pending", Reason="", readiness=false. Elapsed: 18.917855366s
Sep 27 04:17:12.965: INFO: Pod "pod-projected-secrets-871170f3-2aa0-4128-bb03-fab2c996b601": Phase="Succeeded", Reason="", readiness=false. Elapsed: 20.931035217s
STEP: Saw pod success
Sep 27 04:17:12.965: INFO: Pod "pod-projected-secrets-871170f3-2aa0-4128-bb03-fab2c996b601" satisfied condition "Succeeded or Failed"
Sep 27 04:17:12.969: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-projected-secrets-871170f3-2aa0-4128-bb03-fab2c996b601 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 27 04:17:12.994: INFO: Waiting for pod pod-projected-secrets-871170f3-2aa0-4128-bb03-fab2c996b601 to disappear
Sep 27 04:17:13.014: INFO: Pod pod-projected-secrets-871170f3-2aa0-4128-bb03-fab2c996b601 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:17:13.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6884" for this suite.

â€¢ [SLOW TEST:21.360 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":186,"skipped":3148,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:17:13.026: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5504
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Sep 27 04:17:21.609: INFO: 0 pods remaining
Sep 27 04:17:21.609: INFO: 0 pods has nil DeletionTimestamp
Sep 27 04:17:21.609: INFO: 
STEP: Gathering metrics
Sep 27 04:17:22.603: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:17:22.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0927 04:17:22.603918      23 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-5504" for this suite.

â€¢ [SLOW TEST:9.665 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":277,"completed":187,"skipped":3171,"failed":0}
S
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:17:22.691: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-820
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Sep 27 04:17:23.078: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the sample API server.
Sep 27 04:17:25.493: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Sep 27 04:17:27.830: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:17:29.966: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:17:32.062: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:17:33.840: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:17:35.959: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:17:37.833: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:17:39.850: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:17:41.957: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:17:43.835: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:17:45.926: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777045, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:17:49.888: INFO: Waited 2.040923895s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:17:51.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-820" for this suite.

â€¢ [SLOW TEST:28.596 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":277,"completed":188,"skipped":3172,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:17:51.287: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6322
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Sep 27 04:18:04.595: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:18:04.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0927 04:18:04.595360      23 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-6322" for this suite.

â€¢ [SLOW TEST:13.384 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":277,"completed":189,"skipped":3174,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:18:04.862: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8836
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test env composition
Sep 27 04:18:05.935: INFO: Waiting up to 5m0s for pod "var-expansion-f4d05846-9bb6-4fa0-bc47-8bdee18ebdb4" in namespace "var-expansion-8836" to be "Succeeded or Failed"
Sep 27 04:18:05.939: INFO: Pod "var-expansion-f4d05846-9bb6-4fa0-bc47-8bdee18ebdb4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.233603ms
Sep 27 04:18:08.185: INFO: Pod "var-expansion-f4d05846-9bb6-4fa0-bc47-8bdee18ebdb4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.250326723s
Sep 27 04:18:10.556: INFO: Pod "var-expansion-f4d05846-9bb6-4fa0-bc47-8bdee18ebdb4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.621253351s
Sep 27 04:18:12.670: INFO: Pod "var-expansion-f4d05846-9bb6-4fa0-bc47-8bdee18ebdb4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.7348024s
Sep 27 04:18:14.718: INFO: Pod "var-expansion-f4d05846-9bb6-4fa0-bc47-8bdee18ebdb4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.783199074s
Sep 27 04:18:16.766: INFO: Pod "var-expansion-f4d05846-9bb6-4fa0-bc47-8bdee18ebdb4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.830845991s
Sep 27 04:18:18.811: INFO: Pod "var-expansion-f4d05846-9bb6-4fa0-bc47-8bdee18ebdb4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.876433271s
Sep 27 04:18:20.969: INFO: Pod "var-expansion-f4d05846-9bb6-4fa0-bc47-8bdee18ebdb4": Phase="Pending", Reason="", readiness=false. Elapsed: 15.034089349s
Sep 27 04:18:23.020: INFO: Pod "var-expansion-f4d05846-9bb6-4fa0-bc47-8bdee18ebdb4": Phase="Pending", Reason="", readiness=false. Elapsed: 17.084911432s
Sep 27 04:18:25.093: INFO: Pod "var-expansion-f4d05846-9bb6-4fa0-bc47-8bdee18ebdb4": Phase="Pending", Reason="", readiness=false. Elapsed: 19.158046243s
Sep 27 04:18:27.167: INFO: Pod "var-expansion-f4d05846-9bb6-4fa0-bc47-8bdee18ebdb4": Phase="Pending", Reason="", readiness=false. Elapsed: 21.232110529s
Sep 27 04:18:29.287: INFO: Pod "var-expansion-f4d05846-9bb6-4fa0-bc47-8bdee18ebdb4": Phase="Pending", Reason="", readiness=false. Elapsed: 23.351662702s
Sep 27 04:18:31.368: INFO: Pod "var-expansion-f4d05846-9bb6-4fa0-bc47-8bdee18ebdb4": Phase="Pending", Reason="", readiness=false. Elapsed: 25.433069406s
Sep 27 04:18:33.434: INFO: Pod "var-expansion-f4d05846-9bb6-4fa0-bc47-8bdee18ebdb4": Phase="Pending", Reason="", readiness=false. Elapsed: 27.498997901s
Sep 27 04:18:35.687: INFO: Pod "var-expansion-f4d05846-9bb6-4fa0-bc47-8bdee18ebdb4": Phase="Pending", Reason="", readiness=false. Elapsed: 29.751748648s
Sep 27 04:18:37.721: INFO: Pod "var-expansion-f4d05846-9bb6-4fa0-bc47-8bdee18ebdb4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 31.786417297s
STEP: Saw pod success
Sep 27 04:18:37.722: INFO: Pod "var-expansion-f4d05846-9bb6-4fa0-bc47-8bdee18ebdb4" satisfied condition "Succeeded or Failed"
Sep 27 04:18:37.768: INFO: Trying to get logs from node dce-10-6-171-86 pod var-expansion-f4d05846-9bb6-4fa0-bc47-8bdee18ebdb4 container dapi-container: <nil>
STEP: delete the pod
Sep 27 04:18:37.962: INFO: Waiting for pod var-expansion-f4d05846-9bb6-4fa0-bc47-8bdee18ebdb4 to disappear
Sep 27 04:18:37.981: INFO: Pod var-expansion-f4d05846-9bb6-4fa0-bc47-8bdee18ebdb4 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:18:37.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8836" for this suite.

â€¢ [SLOW TEST:33.282 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":277,"completed":190,"skipped":3193,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:18:38.144: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8544
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 27 04:18:39.350: INFO: Number of nodes with available pods: 0
Sep 27 04:18:39.350: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:18:40.399: INFO: Number of nodes with available pods: 0
Sep 27 04:18:40.399: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:18:41.480: INFO: Number of nodes with available pods: 0
Sep 27 04:18:41.480: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:18:42.380: INFO: Number of nodes with available pods: 0
Sep 27 04:18:42.380: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:18:43.381: INFO: Number of nodes with available pods: 0
Sep 27 04:18:43.381: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:18:44.741: INFO: Number of nodes with available pods: 0
Sep 27 04:18:44.741: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:18:45.943: INFO: Number of nodes with available pods: 0
Sep 27 04:18:45.943: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:18:47.735: INFO: Number of nodes with available pods: 0
Sep 27 04:18:47.735: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:18:48.760: INFO: Number of nodes with available pods: 0
Sep 27 04:18:48.760: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:18:50.084: INFO: Number of nodes with available pods: 0
Sep 27 04:18:50.084: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:18:50.388: INFO: Number of nodes with available pods: 0
Sep 27 04:18:50.388: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:18:51.363: INFO: Number of nodes with available pods: 0
Sep 27 04:18:51.363: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:18:52.363: INFO: Number of nodes with available pods: 1
Sep 27 04:18:52.364: INFO: Node dce-10-6-171-85 is running more than one daemon pod
Sep 27 04:18:53.536: INFO: Number of nodes with available pods: 2
Sep 27 04:18:53.536: INFO: Node dce-10-6-171-85 is running more than one daemon pod
Sep 27 04:18:55.081: INFO: Number of nodes with available pods: 3
Sep 27 04:18:55.081: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Sep 27 04:18:55.252: INFO: Number of nodes with available pods: 2
Sep 27 04:18:55.252: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:18:56.386: INFO: Number of nodes with available pods: 2
Sep 27 04:18:56.386: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:18:57.324: INFO: Number of nodes with available pods: 2
Sep 27 04:18:57.324: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:18:58.446: INFO: Number of nodes with available pods: 2
Sep 27 04:18:58.446: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:18:59.276: INFO: Number of nodes with available pods: 2
Sep 27 04:18:59.276: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:19:00.263: INFO: Number of nodes with available pods: 2
Sep 27 04:19:00.263: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:19:01.268: INFO: Number of nodes with available pods: 2
Sep 27 04:19:01.268: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:19:02.267: INFO: Number of nodes with available pods: 2
Sep 27 04:19:02.267: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:19:03.283: INFO: Number of nodes with available pods: 2
Sep 27 04:19:03.283: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:19:04.264: INFO: Number of nodes with available pods: 2
Sep 27 04:19:04.264: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:19:05.280: INFO: Number of nodes with available pods: 2
Sep 27 04:19:05.280: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:19:06.408: INFO: Number of nodes with available pods: 2
Sep 27 04:19:06.408: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:19:07.545: INFO: Number of nodes with available pods: 2
Sep 27 04:19:07.545: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:19:09.396: INFO: Number of nodes with available pods: 2
Sep 27 04:19:09.396: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:19:10.754: INFO: Number of nodes with available pods: 2
Sep 27 04:19:10.754: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:19:12.640: INFO: Number of nodes with available pods: 2
Sep 27 04:19:12.640: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:19:13.695: INFO: Number of nodes with available pods: 2
Sep 27 04:19:13.695: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:19:14.368: INFO: Number of nodes with available pods: 2
Sep 27 04:19:14.368: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:19:15.443: INFO: Number of nodes with available pods: 2
Sep 27 04:19:15.443: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:19:17.115: INFO: Number of nodes with available pods: 2
Sep 27 04:19:17.115: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:19:17.356: INFO: Number of nodes with available pods: 2
Sep 27 04:19:17.356: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:19:18.416: INFO: Number of nodes with available pods: 2
Sep 27 04:19:18.416: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:19:19.268: INFO: Number of nodes with available pods: 2
Sep 27 04:19:19.268: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:19:22.644: INFO: Number of nodes with available pods: 2
Sep 27 04:19:22.644: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:19:23.315: INFO: Number of nodes with available pods: 3
Sep 27 04:19:23.315: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8544, will wait for the garbage collector to delete the pods
Sep 27 04:19:23.612: INFO: Deleting DaemonSet.extensions daemon-set took: 86.317298ms
Sep 27 04:19:25.812: INFO: Terminating DaemonSet.extensions daemon-set pods took: 2.200241926s
Sep 27 04:19:42.221: INFO: Number of nodes with available pods: 0
Sep 27 04:19:42.221: INFO: Number of running nodes: 0, number of available pods: 0
Sep 27 04:19:42.381: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8544/daemonsets","resourceVersion":"704945"},"items":null}

Sep 27 04:19:42.419: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8544/pods","resourceVersion":"704945"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:19:42.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8544" for this suite.

â€¢ [SLOW TEST:64.597 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":277,"completed":191,"skipped":3204,"failed":0}
SSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:19:42.741: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9084
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 04:19:53.821: INFO: Waiting up to 5m0s for pod "client-envvars-e6c44d64-913a-41c6-be68-d9c038183eb9" in namespace "pods-9084" to be "Succeeded or Failed"
Sep 27 04:19:53.996: INFO: Pod "client-envvars-e6c44d64-913a-41c6-be68-d9c038183eb9": Phase="Pending", Reason="", readiness=false. Elapsed: 174.879045ms
Sep 27 04:19:56.459: INFO: Pod "client-envvars-e6c44d64-913a-41c6-be68-d9c038183eb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.637712426s
Sep 27 04:19:58.465: INFO: Pod "client-envvars-e6c44d64-913a-41c6-be68-d9c038183eb9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.6438518s
Sep 27 04:20:00.474: INFO: Pod "client-envvars-e6c44d64-913a-41c6-be68-d9c038183eb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.653033507s
STEP: Saw pod success
Sep 27 04:20:00.474: INFO: Pod "client-envvars-e6c44d64-913a-41c6-be68-d9c038183eb9" satisfied condition "Succeeded or Failed"
Sep 27 04:20:00.480: INFO: Trying to get logs from node dce-10-6-171-86 pod client-envvars-e6c44d64-913a-41c6-be68-d9c038183eb9 container env3cont: <nil>
STEP: delete the pod
Sep 27 04:20:00.513: INFO: Waiting for pod client-envvars-e6c44d64-913a-41c6-be68-d9c038183eb9 to disappear
Sep 27 04:20:00.539: INFO: Pod client-envvars-e6c44d64-913a-41c6-be68-d9c038183eb9 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:20:00.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9084" for this suite.

â€¢ [SLOW TEST:17.889 seconds]
[k8s.io] Pods
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":277,"completed":192,"skipped":3208,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:20:00.630: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3651
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-2597147e-ba00-4ec5-bbf9-6864b0b6a4a4
STEP: Creating a pod to test consume secrets
Sep 27 04:20:01.068: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-56271e65-0023-4e64-bc23-41382f5e04d4" in namespace "projected-3651" to be "Succeeded or Failed"
Sep 27 04:20:01.084: INFO: Pod "pod-projected-secrets-56271e65-0023-4e64-bc23-41382f5e04d4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.080201ms
Sep 27 04:20:03.091: INFO: Pod "pod-projected-secrets-56271e65-0023-4e64-bc23-41382f5e04d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023106474s
Sep 27 04:20:05.144: INFO: Pod "pod-projected-secrets-56271e65-0023-4e64-bc23-41382f5e04d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.076058782s
Sep 27 04:20:07.414: INFO: Pod "pod-projected-secrets-56271e65-0023-4e64-bc23-41382f5e04d4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.345200032s
Sep 27 04:20:09.553: INFO: Pod "pod-projected-secrets-56271e65-0023-4e64-bc23-41382f5e04d4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.485095225s
Sep 27 04:20:11.974: INFO: Pod "pod-projected-secrets-56271e65-0023-4e64-bc23-41382f5e04d4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.905640412s
Sep 27 04:20:14.028: INFO: Pod "pod-projected-secrets-56271e65-0023-4e64-bc23-41382f5e04d4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.959627935s
Sep 27 04:20:16.274: INFO: Pod "pod-projected-secrets-56271e65-0023-4e64-bc23-41382f5e04d4": Phase="Pending", Reason="", readiness=false. Elapsed: 15.205698755s
Sep 27 04:20:18.278: INFO: Pod "pod-projected-secrets-56271e65-0023-4e64-bc23-41382f5e04d4": Phase="Pending", Reason="", readiness=false. Elapsed: 17.209677927s
Sep 27 04:20:21.535: INFO: Pod "pod-projected-secrets-56271e65-0023-4e64-bc23-41382f5e04d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 20.466877458s
STEP: Saw pod success
Sep 27 04:20:21.535: INFO: Pod "pod-projected-secrets-56271e65-0023-4e64-bc23-41382f5e04d4" satisfied condition "Succeeded or Failed"
Sep 27 04:20:21.950: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-projected-secrets-56271e65-0023-4e64-bc23-41382f5e04d4 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 27 04:20:22.492: INFO: Waiting for pod pod-projected-secrets-56271e65-0023-4e64-bc23-41382f5e04d4 to disappear
Sep 27 04:20:22.705: INFO: Pod pod-projected-secrets-56271e65-0023-4e64-bc23-41382f5e04d4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:20:22.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3651" for this suite.

â€¢ [SLOW TEST:22.909 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":193,"skipped":3220,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:20:23.572: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-990
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test override all
Sep 27 04:20:24.250: INFO: Waiting up to 5m0s for pod "client-containers-8f6a2346-b2e9-4605-932e-8e44137def0a" in namespace "containers-990" to be "Succeeded or Failed"
Sep 27 04:20:24.266: INFO: Pod "client-containers-8f6a2346-b2e9-4605-932e-8e44137def0a": Phase="Pending", Reason="", readiness=false. Elapsed: 15.863958ms
Sep 27 04:20:26.271: INFO: Pod "client-containers-8f6a2346-b2e9-4605-932e-8e44137def0a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02064448s
Sep 27 04:20:28.276: INFO: Pod "client-containers-8f6a2346-b2e9-4605-932e-8e44137def0a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026010093s
Sep 27 04:20:30.279: INFO: Pod "client-containers-8f6a2346-b2e9-4605-932e-8e44137def0a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02910707s
Sep 27 04:20:32.286: INFO: Pod "client-containers-8f6a2346-b2e9-4605-932e-8e44137def0a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.035971326s
STEP: Saw pod success
Sep 27 04:20:32.286: INFO: Pod "client-containers-8f6a2346-b2e9-4605-932e-8e44137def0a" satisfied condition "Succeeded or Failed"
Sep 27 04:20:32.294: INFO: Trying to get logs from node dce-10-6-171-86 pod client-containers-8f6a2346-b2e9-4605-932e-8e44137def0a container test-container: <nil>
STEP: delete the pod
Sep 27 04:20:32.340: INFO: Waiting for pod client-containers-8f6a2346-b2e9-4605-932e-8e44137def0a to disappear
Sep 27 04:20:32.352: INFO: Pod client-containers-8f6a2346-b2e9-4605-932e-8e44137def0a no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:20:32.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-990" for this suite.

â€¢ [SLOW TEST:8.798 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":277,"completed":194,"skipped":3277,"failed":0}
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:20:32.371: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5369
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 04:20:32.715: INFO: Create a RollingUpdate DaemonSet
Sep 27 04:20:32.721: INFO: Check that daemon pods launch on every node of the cluster
Sep 27 04:20:32.821: INFO: Number of nodes with available pods: 0
Sep 27 04:20:32.821: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:20:34.022: INFO: Number of nodes with available pods: 0
Sep 27 04:20:34.022: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:20:34.858: INFO: Number of nodes with available pods: 0
Sep 27 04:20:34.858: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:20:36.854: INFO: Number of nodes with available pods: 0
Sep 27 04:20:36.854: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:20:38.156: INFO: Number of nodes with available pods: 0
Sep 27 04:20:38.156: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:20:39.409: INFO: Number of nodes with available pods: 0
Sep 27 04:20:39.409: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:20:40.066: INFO: Number of nodes with available pods: 0
Sep 27 04:20:40.066: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:20:41.053: INFO: Number of nodes with available pods: 0
Sep 27 04:20:41.053: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:20:41.854: INFO: Number of nodes with available pods: 1
Sep 27 04:20:41.854: INFO: Node dce-10-6-171-85 is running more than one daemon pod
Sep 27 04:20:42.830: INFO: Number of nodes with available pods: 2
Sep 27 04:20:42.830: INFO: Node dce-10-6-171-85 is running more than one daemon pod
Sep 27 04:20:43.862: INFO: Number of nodes with available pods: 3
Sep 27 04:20:43.863: INFO: Number of running nodes: 3, number of available pods: 3
Sep 27 04:20:43.863: INFO: Update the DaemonSet to trigger a rollout
Sep 27 04:20:43.869: INFO: Updating DaemonSet daemon-set
Sep 27 04:20:58.210: INFO: Roll back the DaemonSet before rollout is complete
Sep 27 04:20:58.614: INFO: Updating DaemonSet daemon-set
Sep 27 04:20:58.614: INFO: Make sure DaemonSet rollback is complete
Sep 27 04:20:58.658: INFO: Wrong image for pod: daemon-set-dfzsb. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 27 04:20:58.658: INFO: Pod daemon-set-dfzsb is not available
Sep 27 04:20:59.689: INFO: Wrong image for pod: daemon-set-dfzsb. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 27 04:20:59.689: INFO: Pod daemon-set-dfzsb is not available
Sep 27 04:21:00.679: INFO: Pod daemon-set-f87cc is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5369, will wait for the garbage collector to delete the pods
Sep 27 04:21:00.762: INFO: Deleting DaemonSet.extensions daemon-set took: 4.683377ms
Sep 27 04:21:01.462: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.250806ms
Sep 27 04:21:17.871: INFO: Number of nodes with available pods: 0
Sep 27 04:21:17.871: INFO: Number of running nodes: 0, number of available pods: 0
Sep 27 04:21:17.876: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5369/daemonsets","resourceVersion":"705491"},"items":null}

Sep 27 04:21:17.882: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5369/pods","resourceVersion":"705491"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:21:17.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5369" for this suite.

â€¢ [SLOW TEST:45.546 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":277,"completed":195,"skipped":3282,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:21:17.916: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8047
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:21:31.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8047" for this suite.

â€¢ [SLOW TEST:13.742 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":277,"completed":196,"skipped":3293,"failed":0}
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:21:31.658: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7527
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Sep 27 04:21:32.731: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:21:41.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7527" for this suite.

â€¢ [SLOW TEST:10.110 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":277,"completed":197,"skipped":3294,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:21:41.768: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3715
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 04:21:42.738: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 27 04:21:44.753: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777302, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777302, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777302, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777302, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:21:46.766: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777302, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777302, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777302, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777302, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:21:48.763: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777302, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777302, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777302, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777302, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:21:50.897: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777302, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777302, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777302, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777302, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 04:21:53.921: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:21:54.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3715" for this suite.
STEP: Destroying namespace "webhook-3715-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:13.519 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":277,"completed":198,"skipped":3300,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:21:55.287: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-3320
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Sep 27 04:22:06.387: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:22:06.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3320" for this suite.

â€¢ [SLOW TEST:11.642 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":277,"completed":199,"skipped":3309,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:22:06.929: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4912
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:22:24.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4912" for this suite.

â€¢ [SLOW TEST:17.559 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":277,"completed":200,"skipped":3325,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:22:24.489: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9127
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 27 04:22:24.709: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2a3be56d-49db-4c90-97dd-d6dee5ee6eb1" in namespace "downward-api-9127" to be "Succeeded or Failed"
Sep 27 04:22:24.723: INFO: Pod "downwardapi-volume-2a3be56d-49db-4c90-97dd-d6dee5ee6eb1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.052119ms
Sep 27 04:22:26.727: INFO: Pod "downwardapi-volume-2a3be56d-49db-4c90-97dd-d6dee5ee6eb1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01768228s
Sep 27 04:22:28.793: INFO: Pod "downwardapi-volume-2a3be56d-49db-4c90-97dd-d6dee5ee6eb1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.084411109s
Sep 27 04:22:30.869: INFO: Pod "downwardapi-volume-2a3be56d-49db-4c90-97dd-d6dee5ee6eb1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.160454571s
Sep 27 04:22:33.167: INFO: Pod "downwardapi-volume-2a3be56d-49db-4c90-97dd-d6dee5ee6eb1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.457945752s
Sep 27 04:22:35.248: INFO: Pod "downwardapi-volume-2a3be56d-49db-4c90-97dd-d6dee5ee6eb1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.538740156s
Sep 27 04:22:37.252: INFO: Pod "downwardapi-volume-2a3be56d-49db-4c90-97dd-d6dee5ee6eb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.543168173s
STEP: Saw pod success
Sep 27 04:22:37.252: INFO: Pod "downwardapi-volume-2a3be56d-49db-4c90-97dd-d6dee5ee6eb1" satisfied condition "Succeeded or Failed"
Sep 27 04:22:37.263: INFO: Trying to get logs from node dce-10-6-171-86 pod downwardapi-volume-2a3be56d-49db-4c90-97dd-d6dee5ee6eb1 container client-container: <nil>
STEP: delete the pod
Sep 27 04:22:37.525: INFO: Waiting for pod downwardapi-volume-2a3be56d-49db-4c90-97dd-d6dee5ee6eb1 to disappear
Sep 27 04:22:37.644: INFO: Pod downwardapi-volume-2a3be56d-49db-4c90-97dd-d6dee5ee6eb1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:22:37.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9127" for this suite.

â€¢ [SLOW TEST:13.253 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":277,"completed":201,"skipped":3410,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:22:37.743: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3652
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Sep 27 04:22:38.163: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 27 04:22:38.227: INFO: Waiting for terminating namespaces to be deleted...
Sep 27 04:22:38.232: INFO: 
Logging pods the kubelet thinks is on node dce-10-6-171-84 before test
Sep 27 04:22:38.276: INFO: dce-parcel-agent-85gws from kube-system started at 2020-09-24 09:40:06 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.276: INFO: 	Container dce-parcel-agent ready: true, restart count 4
Sep 27 04:22:38.276: INFO: node-local-dns-hh96g from kube-system started at 2020-09-24 10:25:26 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.276: INFO: 	Container node-cache ready: true, restart count 0
Sep 27 04:22:38.276: INFO: coredns-coredns-7899d777b7-tpxhd from kube-system started at 2020-09-25 02:24:32 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.276: INFO: 	Container coredns-coredns ready: true, restart count 0
Sep 27 04:22:38.276: INFO: uds-host-metrics-dirver-r7msd from storage-system started at 2020-09-25 01:46:24 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.276: INFO: 	Container metrics-collector ready: true, restart count 0
Sep 27 04:22:38.276: INFO: aaa-dao-2048-7897cf64bc-krl9t from default started at 2020-09-25 10:06:04 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.276: INFO: 	Container dao-2048 ready: true, restart count 0
Sep 27 04:22:38.276: INFO: calico-node-7lk8x from kube-system started at 2020-09-24 09:40:07 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.276: INFO: 	Container calico-node ready: true, restart count 1
Sep 27 04:22:38.276: INFO: sonobuoy-systemd-logs-daemon-set-da170140ad3f4ead-glbd8 from sonobuoy started at 2020-09-27 02:39:45 +0000 UTC (2 container statuses recorded)
Sep 27 04:22:38.276: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 27 04:22:38.276: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 04:22:38.276: INFO: release-2048-dao-2048-6ff5c774d9-txzdv from default started at 2020-09-25 10:06:04 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.276: INFO: 	Container dao-2048 ready: true, restart count 0
Sep 27 04:22:38.276: INFO: coredns-coredns-7899d777b7-d9xwj from kube-system started at 2020-09-25 10:06:04 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.276: INFO: 	Container coredns-coredns ready: true, restart count 0
Sep 27 04:22:38.276: INFO: dce-kube-apiserver-proxy-dce-10-6-171-84 from kube-system started at 2020-09-24 09:48:25 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.276: INFO: 	Container dce-kube-apiserver-proxy ready: true, restart count 2
Sep 27 04:22:38.276: INFO: dce-engine-8sng6 from kube-system started at 2020-09-24 09:40:03 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.276: INFO: 	Container dce-engine ready: true, restart count 1
Sep 27 04:22:38.276: INFO: dce-system-dnsservice-74c694858b-c7b2w from dce-system started at 2020-09-25 01:45:36 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.276: INFO: 	Container dce-system-dnsservice ready: true, restart count 0
Sep 27 04:22:38.276: INFO: kube-proxy-6qqrv from kube-system started at 2020-09-24 09:40:05 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.276: INFO: 	Container kube-proxy ready: true, restart count 1
Sep 27 04:22:38.276: INFO: dao-2048-dao-2048-db8cd6864-g8txm from default started at 2020-09-25 10:06:04 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.276: INFO: 	Container dao-2048-dao-2048 ready: true, restart count 0
Sep 27 04:22:38.276: INFO: 
Logging pods the kubelet thinks is on node dce-10-6-171-85 before test
Sep 27 04:22:38.319: INFO: dce-chart-manager-68fbf6bb4c-7288c from kube-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.319: INFO: 	Container chart-manager ready: true, restart count 3
Sep 27 04:22:38.319: INFO: dce-engine-hfc8k from kube-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.319: INFO: 	Container dce-engine ready: true, restart count 3
Sep 27 04:22:38.319: INFO: dce-core-keepalived-59dd799677-m42wb from kube-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.319: INFO: 	Container dce-core-keepalived ready: true, restart count 3
Sep 27 04:22:38.319: INFO: uds-storage-server-5f7b5f6fdc-q7kmm from storage-system started at 2020-09-25 02:32:02 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.319: INFO: 	Container main ready: false, restart count 2
Sep 27 04:22:38.319: INFO: dce-kube-controller-manager-dce-10-6-171-85 from kube-system started at 2020-09-24 10:36:44 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.319: INFO: 	Container dce-kube-controller-manager ready: true, restart count 4
Sep 27 04:22:38.319: INFO: calico-node-pcdmf from kube-system started at 2020-09-24 09:15:05 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.319: INFO: 	Container calico-node ready: true, restart count 3
Sep 27 04:22:38.319: INFO: dce-system-uds-58c4c68dbf-bbmdk from dce-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.319: INFO: 	Container dce-system-uds ready: false, restart count 5
Sep 27 04:22:38.319: INFO: dce-kube-apiserver-dce-10-6-171-85 from kube-system started at 2020-09-24 10:36:42 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.319: INFO: 	Container dce-kube-apiserver ready: true, restart count 0
Sep 27 04:22:38.319: INFO: dce-system-uds-58c4c68dbf-g8fb6 from dce-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.319: INFO: 	Container dce-system-uds ready: true, restart count 3
Sep 27 04:22:38.319: INFO: uds-storage-server-5f7b5f6fdc-r46qt from storage-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.319: INFO: 	Container main ready: true, restart count 3
Sep 27 04:22:38.319: INFO: node-local-dns-9lnth from kube-system started at 2020-09-24 10:25:26 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.319: INFO: 	Container node-cache ready: true, restart count 1
Sep 27 04:22:38.319: INFO: dce-etcd-dce-10-6-171-85 from kube-system started at 2020-09-24 10:36:41 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.319: INFO: 	Container dce-etcd ready: true, restart count 3
Sep 27 04:22:38.319: INFO: dce-kube-apiserver-proxy-dce-10-6-171-85 from kube-system started at 2020-09-24 10:36:43 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.319: INFO: 	Container dce-kube-apiserver-proxy ready: true, restart count 3
Sep 27 04:22:38.319: INFO: dce-parcel-server-rwgb8 from kube-system started at 2020-09-24 09:12:07 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.319: INFO: 	Container dce-parcel-server ready: true, restart count 5
Sep 27 04:22:38.319: INFO: dce-prometheus-5dc44cf4dd-jjsjl from kube-system started at 2020-09-24 09:15:28 +0000 UTC (2 container statuses recorded)
Sep 27 04:22:38.319: INFO: 	Container dce-metrics-server ready: true, restart count 3
Sep 27 04:22:38.319: INFO: 	Container dce-prometheus ready: true, restart count 3
Sep 27 04:22:38.319: INFO: sonobuoy-systemd-logs-daemon-set-da170140ad3f4ead-2mrkt from sonobuoy started at 2020-09-27 02:39:45 +0000 UTC (2 container statuses recorded)
Sep 27 04:22:38.319: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 27 04:22:38.319: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 04:22:38.319: INFO: metrics-server-6768fdf9db-56ks5 from kube-system started at 2020-09-24 09:15:28 +0000 UTC (2 container statuses recorded)
Sep 27 04:22:38.319: INFO: 	Container metrics-server ready: true, restart count 3
Sep 27 04:22:38.319: INFO: 	Container metrics-server-nanny ready: true, restart count 3
Sep 27 04:22:38.319: INFO: dce-registry-6cc79fc8c-fpp82 from kube-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.319: INFO: 	Container dce-registry ready: true, restart count 3
Sep 27 04:22:38.319: INFO: calico-kube-controllers-7449c6cbb8-bq8n5 from kube-system started at 2020-09-24 09:15:06 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.319: INFO: 	Container calico-kube-controllers ready: true, restart count 5
Sep 27 04:22:38.319: INFO: uds-host-metrics-dirver-884nn from storage-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.319: INFO: 	Container metrics-collector ready: true, restart count 3
Sep 27 04:22:38.319: INFO: dce-kube-scheduler-dce-10-6-171-85 from kube-system started at 2020-09-24 10:36:45 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.319: INFO: 	Container dce-kube-scheduler ready: true, restart count 4
Sep 27 04:22:38.319: INFO: kube-proxy-mbrkq from kube-system started at 2020-09-24 09:12:07 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.319: INFO: 	Container kube-proxy ready: true, restart count 3
Sep 27 04:22:38.319: INFO: dce-parcel-agent-ndwd9 from kube-system started at 2020-09-24 09:12:07 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.319: INFO: 	Container dce-parcel-agent ready: true, restart count 5
Sep 27 04:22:38.319: INFO: 
Logging pods the kubelet thinks is on node dce-10-6-171-86 before test
Sep 27 04:22:38.330: INFO: sonobuoy-systemd-logs-daemon-set-da170140ad3f4ead-fbqzn from sonobuoy started at 2020-09-27 02:39:45 +0000 UTC (2 container statuses recorded)
Sep 27 04:22:38.330: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 27 04:22:38.330: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 04:22:38.330: INFO: node-local-dns-sdlq5 from kube-system started at 2020-09-27 03:44:17 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.330: INFO: 	Container node-cache ready: true, restart count 0
Sep 27 04:22:38.330: INFO: dce-engine-dvcxl from kube-system started at 2020-09-24 09:35:13 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.330: INFO: 	Container dce-engine ready: true, restart count 1
Sep 27 04:22:38.330: INFO: sonobuoy-e2e-job-f515d88659da49be from sonobuoy started at 2020-09-27 02:39:45 +0000 UTC (2 container statuses recorded)
Sep 27 04:22:38.330: INFO: 	Container e2e ready: true, restart count 0
Sep 27 04:22:38.330: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 04:22:38.330: INFO: kube-proxy-wjt5k from kube-system started at 2020-09-24 09:35:13 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.330: INFO: 	Container kube-proxy ready: true, restart count 1
Sep 27 04:22:38.330: INFO: dce-parcel-agent-fczgp from kube-system started at 2020-09-27 03:43:40 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.330: INFO: 	Container dce-parcel-agent ready: true, restart count 0
Sep 27 04:22:38.330: INFO: calico-node-hsm9f from kube-system started at 2020-09-24 09:35:13 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.330: INFO: 	Container calico-node ready: true, restart count 1
Sep 27 04:22:38.330: INFO: dce-kube-apiserver-proxy-dce-10-6-171-86 from kube-system started at 2020-09-24 09:47:03 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.330: INFO: 	Container dce-kube-apiserver-proxy ready: true, restart count 1
Sep 27 04:22:38.330: INFO: sonobuoy from sonobuoy started at 2020-09-27 02:39:34 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.330: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 27 04:22:38.330: INFO: uds-host-metrics-dirver-48m2w from storage-system started at 2020-09-25 01:46:33 +0000 UTC (1 container statuses recorded)
Sep 27 04:22:38.330: INFO: 	Container metrics-collector ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: verifying the node has the label node dce-10-6-171-84
STEP: verifying the node has the label node dce-10-6-171-85
STEP: verifying the node has the label node dce-10-6-171-86
Sep 27 04:22:38.567: INFO: Pod dce-system-dnsservice-74c694858b-c7b2w requesting resource cpu=300m on Node dce-10-6-171-84
Sep 27 04:22:38.567: INFO: Pod dce-system-uds-58c4c68dbf-bbmdk requesting resource cpu=300m on Node dce-10-6-171-85
Sep 27 04:22:38.567: INFO: Pod dce-system-uds-58c4c68dbf-g8fb6 requesting resource cpu=300m on Node dce-10-6-171-85
Sep 27 04:22:38.567: INFO: Pod aaa-dao-2048-7897cf64bc-krl9t requesting resource cpu=50m on Node dce-10-6-171-84
Sep 27 04:22:38.567: INFO: Pod dao-2048-dao-2048-db8cd6864-g8txm requesting resource cpu=64m on Node dce-10-6-171-84
Sep 27 04:22:38.567: INFO: Pod release-2048-dao-2048-6ff5c774d9-txzdv requesting resource cpu=50m on Node dce-10-6-171-84
Sep 27 04:22:38.567: INFO: Pod calico-kube-controllers-7449c6cbb8-bq8n5 requesting resource cpu=100m on Node dce-10-6-171-85
Sep 27 04:22:38.567: INFO: Pod calico-node-7lk8x requesting resource cpu=200m on Node dce-10-6-171-84
Sep 27 04:22:38.567: INFO: Pod calico-node-hsm9f requesting resource cpu=200m on Node dce-10-6-171-86
Sep 27 04:22:38.567: INFO: Pod calico-node-pcdmf requesting resource cpu=200m on Node dce-10-6-171-85
Sep 27 04:22:38.567: INFO: Pod coredns-coredns-7899d777b7-d9xwj requesting resource cpu=250m on Node dce-10-6-171-84
Sep 27 04:22:38.567: INFO: Pod coredns-coredns-7899d777b7-tpxhd requesting resource cpu=250m on Node dce-10-6-171-84
Sep 27 04:22:38.567: INFO: Pod dce-chart-manager-68fbf6bb4c-7288c requesting resource cpu=250m on Node dce-10-6-171-85
Sep 27 04:22:38.567: INFO: Pod dce-core-keepalived-59dd799677-m42wb requesting resource cpu=100m on Node dce-10-6-171-85
Sep 27 04:22:38.567: INFO: Pod dce-engine-8sng6 requesting resource cpu=100m on Node dce-10-6-171-84
Sep 27 04:22:38.567: INFO: Pod dce-engine-dvcxl requesting resource cpu=100m on Node dce-10-6-171-86
Sep 27 04:22:38.567: INFO: Pod dce-engine-hfc8k requesting resource cpu=100m on Node dce-10-6-171-85
Sep 27 04:22:38.567: INFO: Pod dce-etcd-dce-10-6-171-85 requesting resource cpu=400m on Node dce-10-6-171-85
Sep 27 04:22:38.567: INFO: Pod dce-kube-apiserver-dce-10-6-171-85 requesting resource cpu=400m on Node dce-10-6-171-85
Sep 27 04:22:38.567: INFO: Pod dce-kube-apiserver-proxy-dce-10-6-171-84 requesting resource cpu=100m on Node dce-10-6-171-84
Sep 27 04:22:38.567: INFO: Pod dce-kube-apiserver-proxy-dce-10-6-171-85 requesting resource cpu=100m on Node dce-10-6-171-85
Sep 27 04:22:38.567: INFO: Pod dce-kube-apiserver-proxy-dce-10-6-171-86 requesting resource cpu=100m on Node dce-10-6-171-86
Sep 27 04:22:38.567: INFO: Pod dce-kube-controller-manager-dce-10-6-171-85 requesting resource cpu=200m on Node dce-10-6-171-85
Sep 27 04:22:38.567: INFO: Pod dce-kube-scheduler-dce-10-6-171-85 requesting resource cpu=200m on Node dce-10-6-171-85
Sep 27 04:22:38.567: INFO: Pod dce-parcel-agent-85gws requesting resource cpu=250m on Node dce-10-6-171-84
Sep 27 04:22:38.567: INFO: Pod dce-parcel-agent-fczgp requesting resource cpu=250m on Node dce-10-6-171-86
Sep 27 04:22:38.567: INFO: Pod dce-parcel-agent-ndwd9 requesting resource cpu=250m on Node dce-10-6-171-85
Sep 27 04:22:38.567: INFO: Pod dce-parcel-server-rwgb8 requesting resource cpu=200m on Node dce-10-6-171-85
Sep 27 04:22:38.567: INFO: Pod dce-prometheus-5dc44cf4dd-jjsjl requesting resource cpu=125m on Node dce-10-6-171-85
Sep 27 04:22:38.567: INFO: Pod dce-registry-6cc79fc8c-fpp82 requesting resource cpu=300m on Node dce-10-6-171-85
Sep 27 04:22:38.567: INFO: Pod kube-proxy-6qqrv requesting resource cpu=100m on Node dce-10-6-171-84
Sep 27 04:22:38.567: INFO: Pod kube-proxy-mbrkq requesting resource cpu=100m on Node dce-10-6-171-85
Sep 27 04:22:38.567: INFO: Pod kube-proxy-wjt5k requesting resource cpu=100m on Node dce-10-6-171-86
Sep 27 04:22:38.567: INFO: Pod metrics-server-6768fdf9db-56ks5 requesting resource cpu=79m on Node dce-10-6-171-85
Sep 27 04:22:38.567: INFO: Pod node-local-dns-9lnth requesting resource cpu=250m on Node dce-10-6-171-85
Sep 27 04:22:38.567: INFO: Pod node-local-dns-hh96g requesting resource cpu=250m on Node dce-10-6-171-84
Sep 27 04:22:38.567: INFO: Pod node-local-dns-sdlq5 requesting resource cpu=250m on Node dce-10-6-171-86
Sep 27 04:22:38.567: INFO: Pod sonobuoy requesting resource cpu=0m on Node dce-10-6-171-86
Sep 27 04:22:38.567: INFO: Pod sonobuoy-e2e-job-f515d88659da49be requesting resource cpu=0m on Node dce-10-6-171-86
Sep 27 04:22:38.567: INFO: Pod sonobuoy-systemd-logs-daemon-set-da170140ad3f4ead-2mrkt requesting resource cpu=0m on Node dce-10-6-171-85
Sep 27 04:22:38.567: INFO: Pod sonobuoy-systemd-logs-daemon-set-da170140ad3f4ead-fbqzn requesting resource cpu=0m on Node dce-10-6-171-86
Sep 27 04:22:38.567: INFO: Pod sonobuoy-systemd-logs-daemon-set-da170140ad3f4ead-glbd8 requesting resource cpu=0m on Node dce-10-6-171-84
Sep 27 04:22:38.567: INFO: Pod uds-host-metrics-dirver-48m2w requesting resource cpu=50m on Node dce-10-6-171-86
Sep 27 04:22:38.567: INFO: Pod uds-host-metrics-dirver-884nn requesting resource cpu=50m on Node dce-10-6-171-85
Sep 27 04:22:38.567: INFO: Pod uds-host-metrics-dirver-r7msd requesting resource cpu=50m on Node dce-10-6-171-84
Sep 27 04:22:38.567: INFO: Pod uds-storage-server-5f7b5f6fdc-q7kmm requesting resource cpu=100m on Node dce-10-6-171-85
Sep 27 04:22:38.567: INFO: Pod uds-storage-server-5f7b5f6fdc-r46qt requesting resource cpu=100m on Node dce-10-6-171-85
STEP: Starting Pods to consume most of the cluster CPU.
Sep 27 04:22:38.567: INFO: Creating a pod which consumes cpu=3840m on Node dce-10-6-171-84
Sep 27 04:22:38.578: INFO: Creating a pod which consumes cpu=2307m on Node dce-10-6-171-85
Sep 27 04:22:38.593: INFO: Creating a pod which consumes cpu=4515m on Node dce-10-6-171-86
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-13898033-d615-4c8e-8934-65eee247b411.1638893c11e355e9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3652/filler-pod-13898033-d615-4c8e-8934-65eee247b411 to dce-10-6-171-85]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-13898033-d615-4c8e-8934-65eee247b411.1638893df88206fe], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-13898033-d615-4c8e-8934-65eee247b411.1638893e0b32496e], Reason = [Created], Message = [Created container filler-pod-13898033-d615-4c8e-8934-65eee247b411]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-13898033-d615-4c8e-8934-65eee247b411.1638893e239a2e39], Reason = [Started], Message = [Started container filler-pod-13898033-d615-4c8e-8934-65eee247b411]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6f0ffff7-0429-4d61-890b-001aea740cfa.1638893c138b0293], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3652/filler-pod-6f0ffff7-0429-4d61-890b-001aea740cfa to dce-10-6-171-86]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6f0ffff7-0429-4d61-890b-001aea740cfa.1638893e209ae37a], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6f0ffff7-0429-4d61-890b-001aea740cfa.1638893e38092bf7], Reason = [Created], Message = [Created container filler-pod-6f0ffff7-0429-4d61-890b-001aea740cfa]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6f0ffff7-0429-4d61-890b-001aea740cfa.1638893e58f48f22], Reason = [Started], Message = [Started container filler-pod-6f0ffff7-0429-4d61-890b-001aea740cfa]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d34684d4-cd58-4969-91fe-d9af44d3474c.1638893c102693a0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3652/filler-pod-d34684d4-cd58-4969-91fe-d9af44d3474c to dce-10-6-171-84]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d34684d4-cd58-4969-91fe-d9af44d3474c.1638893d3556a16b], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d34684d4-cd58-4969-91fe-d9af44d3474c.1638893d6424ae5d], Reason = [Created], Message = [Created container filler-pod-d34684d4-cd58-4969-91fe-d9af44d3474c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d34684d4-cd58-4969-91fe-d9af44d3474c.1638893d81b59f5c], Reason = [Started], Message = [Started container filler-pod-d34684d4-cd58-4969-91fe-d9af44d3474c]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1638893e78a3f35f], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node dce-10-6-171-85
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node dce-10-6-171-86
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node dce-10-6-171-84
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:22:50.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3652" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

â€¢ [SLOW TEST:12.524 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":277,"completed":202,"skipped":3436,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:22:50.267: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2649
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep 27 04:22:50.519: INFO: Waiting up to 5m0s for pod "pod-fa49b86d-ae7b-4b5b-bf90-1af904048aad" in namespace "emptydir-2649" to be "Succeeded or Failed"
Sep 27 04:22:50.547: INFO: Pod "pod-fa49b86d-ae7b-4b5b-bf90-1af904048aad": Phase="Pending", Reason="", readiness=false. Elapsed: 27.480239ms
Sep 27 04:22:53.020: INFO: Pod "pod-fa49b86d-ae7b-4b5b-bf90-1af904048aad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.501045946s
Sep 27 04:22:55.267: INFO: Pod "pod-fa49b86d-ae7b-4b5b-bf90-1af904048aad": Phase="Pending", Reason="", readiness=false. Elapsed: 4.747708817s
Sep 27 04:22:57.335: INFO: Pod "pod-fa49b86d-ae7b-4b5b-bf90-1af904048aad": Phase="Pending", Reason="", readiness=false. Elapsed: 6.815611955s
Sep 27 04:22:59.355: INFO: Pod "pod-fa49b86d-ae7b-4b5b-bf90-1af904048aad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.836085413s
STEP: Saw pod success
Sep 27 04:22:59.355: INFO: Pod "pod-fa49b86d-ae7b-4b5b-bf90-1af904048aad" satisfied condition "Succeeded or Failed"
Sep 27 04:22:59.358: INFO: Trying to get logs from node dce-10-6-171-84 pod pod-fa49b86d-ae7b-4b5b-bf90-1af904048aad container test-container: <nil>
STEP: delete the pod
Sep 27 04:22:59.426: INFO: Waiting for pod pod-fa49b86d-ae7b-4b5b-bf90-1af904048aad to disappear
Sep 27 04:22:59.464: INFO: Pod pod-fa49b86d-ae7b-4b5b-bf90-1af904048aad no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:22:59.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2649" for this suite.

â€¢ [SLOW TEST:9.209 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":203,"skipped":3451,"failed":0}
SSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:22:59.476: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3915
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Sep 27 04:22:59.868: INFO: Waiting up to 5m0s for pod "downward-api-1c8fe992-f6d8-4d9b-b8ca-14c43f8ca0e6" in namespace "downward-api-3915" to be "Succeeded or Failed"
Sep 27 04:22:59.872: INFO: Pod "downward-api-1c8fe992-f6d8-4d9b-b8ca-14c43f8ca0e6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.002626ms
Sep 27 04:23:01.876: INFO: Pod "downward-api-1c8fe992-f6d8-4d9b-b8ca-14c43f8ca0e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00829388s
Sep 27 04:23:03.881: INFO: Pod "downward-api-1c8fe992-f6d8-4d9b-b8ca-14c43f8ca0e6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013173522s
Sep 27 04:23:05.998: INFO: Pod "downward-api-1c8fe992-f6d8-4d9b-b8ca-14c43f8ca0e6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.130175307s
Sep 27 04:23:08.036: INFO: Pod "downward-api-1c8fe992-f6d8-4d9b-b8ca-14c43f8ca0e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.167962282s
STEP: Saw pod success
Sep 27 04:23:08.036: INFO: Pod "downward-api-1c8fe992-f6d8-4d9b-b8ca-14c43f8ca0e6" satisfied condition "Succeeded or Failed"
Sep 27 04:23:08.100: INFO: Trying to get logs from node dce-10-6-171-86 pod downward-api-1c8fe992-f6d8-4d9b-b8ca-14c43f8ca0e6 container dapi-container: <nil>
STEP: delete the pod
Sep 27 04:23:08.558: INFO: Waiting for pod downward-api-1c8fe992-f6d8-4d9b-b8ca-14c43f8ca0e6 to disappear
Sep 27 04:23:08.568: INFO: Pod downward-api-1c8fe992-f6d8-4d9b-b8ca-14c43f8ca0e6 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:23:08.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3915" for this suite.

â€¢ [SLOW TEST:9.519 seconds]
[sig-node] Downward API
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":277,"completed":204,"skipped":3454,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:23:08.996: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8298
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:23:27.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8298" for this suite.

â€¢ [SLOW TEST:18.825 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":277,"completed":205,"skipped":3456,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:23:27.821: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-7346
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Sep 27 04:23:28.014: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 27 04:24:28.064: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 04:24:28.069: INFO: Starting informer...
STEP: Starting pods...
Sep 27 04:24:28.324: INFO: Pod1 is running on dce-10-6-171-86. Tainting Node
Sep 27 04:24:38.585: INFO: Pod2 is running on dce-10-6-171-86. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Sep 27 04:24:57.504: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Sep 27 04:25:17.485: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:25:17.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-7346" for this suite.

â€¢ [SLOW TEST:110.017 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":277,"completed":206,"skipped":3484,"failed":0}
SS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:25:17.838: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6232
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 27 04:25:32.314: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:25:32.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6232" for this suite.

â€¢ [SLOW TEST:14.631 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  blackbox test
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
    on terminated container
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:133
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":277,"completed":207,"skipped":3486,"failed":0}
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:25:32.470: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9419
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-downwardapi-xb5g
STEP: Creating a pod to test atomic-volume-subpath
Sep 27 04:25:34.217: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-xb5g" in namespace "subpath-9419" to be "Succeeded or Failed"
Sep 27 04:25:34.288: INFO: Pod "pod-subpath-test-downwardapi-xb5g": Phase="Pending", Reason="", readiness=false. Elapsed: 70.739655ms
Sep 27 04:25:36.295: INFO: Pod "pod-subpath-test-downwardapi-xb5g": Phase="Pending", Reason="", readiness=false. Elapsed: 2.077591812s
Sep 27 04:25:38.298: INFO: Pod "pod-subpath-test-downwardapi-xb5g": Phase="Pending", Reason="", readiness=false. Elapsed: 4.081447327s
Sep 27 04:25:40.332: INFO: Pod "pod-subpath-test-downwardapi-xb5g": Phase="Pending", Reason="", readiness=false. Elapsed: 6.115216058s
Sep 27 04:25:42.339: INFO: Pod "pod-subpath-test-downwardapi-xb5g": Phase="Running", Reason="", readiness=true. Elapsed: 8.121730913s
Sep 27 04:25:44.801: INFO: Pod "pod-subpath-test-downwardapi-xb5g": Phase="Running", Reason="", readiness=true. Elapsed: 10.583803802s
Sep 27 04:25:46.805: INFO: Pod "pod-subpath-test-downwardapi-xb5g": Phase="Running", Reason="", readiness=true. Elapsed: 12.587597069s
Sep 27 04:25:48.810: INFO: Pod "pod-subpath-test-downwardapi-xb5g": Phase="Running", Reason="", readiness=true. Elapsed: 14.593212402s
Sep 27 04:25:50.926: INFO: Pod "pod-subpath-test-downwardapi-xb5g": Phase="Running", Reason="", readiness=true. Elapsed: 16.708886512s
Sep 27 04:25:53.143: INFO: Pod "pod-subpath-test-downwardapi-xb5g": Phase="Running", Reason="", readiness=true. Elapsed: 18.925860126s
Sep 27 04:25:55.282: INFO: Pod "pod-subpath-test-downwardapi-xb5g": Phase="Running", Reason="", readiness=true. Elapsed: 21.064482966s
Sep 27 04:25:57.335: INFO: Pod "pod-subpath-test-downwardapi-xb5g": Phase="Running", Reason="", readiness=true. Elapsed: 23.118135743s
Sep 27 04:25:59.528: INFO: Pod "pod-subpath-test-downwardapi-xb5g": Phase="Running", Reason="", readiness=true. Elapsed: 25.31137267s
Sep 27 04:26:01.546: INFO: Pod "pod-subpath-test-downwardapi-xb5g": Phase="Running", Reason="", readiness=true. Elapsed: 27.329077815s
Sep 27 04:26:03.556: INFO: Pod "pod-subpath-test-downwardapi-xb5g": Phase="Succeeded", Reason="", readiness=false. Elapsed: 29.339460488s
STEP: Saw pod success
Sep 27 04:26:03.557: INFO: Pod "pod-subpath-test-downwardapi-xb5g" satisfied condition "Succeeded or Failed"
Sep 27 04:26:03.562: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-subpath-test-downwardapi-xb5g container test-container-subpath-downwardapi-xb5g: <nil>
STEP: delete the pod
Sep 27 04:26:03.610: INFO: Waiting for pod pod-subpath-test-downwardapi-xb5g to disappear
Sep 27 04:26:03.632: INFO: Pod pod-subpath-test-downwardapi-xb5g no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-xb5g
Sep 27 04:26:03.632: INFO: Deleting pod "pod-subpath-test-downwardapi-xb5g" in namespace "subpath-9419"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:26:03.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9419" for this suite.

â€¢ [SLOW TEST:31.186 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":277,"completed":208,"skipped":3492,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:26:03.656: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2790
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 04:26:04.060: INFO: Pod name rollover-pod: Found 0 pods out of 1
Sep 27 04:26:09.550: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 27 04:26:19.891: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Sep 27 04:26:21.930: INFO: Creating deployment "test-rollover-deployment"
Sep 27 04:26:23.020: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Sep 27 04:26:25.127: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Sep 27 04:26:25.134: INFO: Ensure that both replica sets have 1 created replica
Sep 27 04:26:25.142: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Sep 27 04:26:25.170: INFO: Updating deployment test-rollover-deployment
Sep 27 04:26:25.170: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Sep 27 04:26:27.301: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Sep 27 04:26:27.793: INFO: Make sure deployment "test-rollover-deployment" is complete
Sep 27 04:26:27.958: INFO: all replica sets need to contain the pod-template-hash label
Sep 27 04:26:27.958: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777583, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777583, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777585, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777582, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:26:30.451: INFO: all replica sets need to contain the pod-template-hash label
Sep 27 04:26:30.451: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777583, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777583, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777585, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777582, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:26:32.813: INFO: all replica sets need to contain the pod-template-hash label
Sep 27 04:26:32.813: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777583, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777583, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777585, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777582, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:26:34.020: INFO: all replica sets need to contain the pod-template-hash label
Sep 27 04:26:34.020: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777583, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777583, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777585, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777582, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:26:36.380: INFO: all replica sets need to contain the pod-template-hash label
Sep 27 04:26:36.385: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777583, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777583, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777585, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777582, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:26:37.971: INFO: all replica sets need to contain the pod-template-hash label
Sep 27 04:26:37.972: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777583, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777583, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777585, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777582, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:26:40.304: INFO: all replica sets need to contain the pod-template-hash label
Sep 27 04:26:40.304: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777583, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777583, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777599, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777582, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:26:42.107: INFO: all replica sets need to contain the pod-template-hash label
Sep 27 04:26:42.107: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777583, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777583, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777599, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777582, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:26:44.268: INFO: all replica sets need to contain the pod-template-hash label
Sep 27 04:26:44.268: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777583, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777583, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777599, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777582, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:26:45.969: INFO: all replica sets need to contain the pod-template-hash label
Sep 27 04:26:45.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777583, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777583, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777599, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777582, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:26:47.969: INFO: all replica sets need to contain the pod-template-hash label
Sep 27 04:26:47.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777583, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777583, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777599, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777582, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:26:49.968: INFO: 
Sep 27 04:26:49.968: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Sep 27 04:26:49.978: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-2790 /apis/apps/v1/namespaces/deployment-2790/deployments/test-rollover-deployment 0829e333-6355-4c68-8e7c-c4b694d90383 707216 2 2020-09-27 04:26:21 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00452b188 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-09-27 04:26:23 +0000 UTC,LastTransitionTime:2020-09-27 04:26:23 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-84f7f6f64b" has successfully progressed.,LastUpdateTime:2020-09-27 04:26:49 +0000 UTC,LastTransitionTime:2020-09-27 04:26:22 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 27 04:26:49.982: INFO: New ReplicaSet "test-rollover-deployment-84f7f6f64b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-84f7f6f64b  deployment-2790 /apis/apps/v1/namespaces/deployment-2790/replicasets/test-rollover-deployment-84f7f6f64b 1c06c2bf-58c4-41a5-9ef0-41d62492f41e 707204 2 2020-09-27 04:26:25 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:84f7f6f64b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 0829e333-6355-4c68-8e7c-c4b694d90383 0xc00452b787 0xc00452b788}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 84f7f6f64b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:84f7f6f64b] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00452b7f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 27 04:26:49.982: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Sep 27 04:26:49.982: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2790 /apis/apps/v1/namespaces/deployment-2790/replicasets/test-rollover-controller a2401300-8afa-41c0-838a-7191e9fc2196 707213 2 2020-09-27 04:26:04 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 0829e333-6355-4c68-8e7c-c4b694d90383 0xc00452b5d7 0xc00452b5d8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00452b638 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 27 04:26:49.983: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-5686c4cfd5  deployment-2790 /apis/apps/v1/namespaces/deployment-2790/replicasets/test-rollover-deployment-5686c4cfd5 772ad60b-2c80-4e08-ad76-48e9023b57aa 707129 2 2020-09-27 04:26:22 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5686c4cfd5] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 0829e333-6355-4c68-8e7c-c4b694d90383 0xc00452b6a7 0xc00452b6a8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5686c4cfd5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5686c4cfd5] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00452b718 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 27 04:26:49.986: INFO: Pod "test-rollover-deployment-84f7f6f64b-4kl5h" is available:
&Pod{ObjectMeta:{test-rollover-deployment-84f7f6f64b-4kl5h test-rollover-deployment-84f7f6f64b- deployment-2790 /api/v1/namespaces/deployment-2790/pods/test-rollover-deployment-84f7f6f64b-4kl5h d8b48cf1-1ed1-4cef-9b59-8d6105269122 707177 0 2020-09-27 04:26:25 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:84f7f6f64b] map[cni.projectcalico.org/ipv4pools:["default-ipv4-ippool"] dce.daocloud.io/parcel.egress.burst:0 dce.daocloud.io/parcel.egress.rate:0 dce.daocloud.io/parcel.ingress.burst:0 dce.daocloud.io/parcel.ingress.rate:0 kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet test-rollover-deployment-84f7f6f64b 1c06c2bf-58c4-41a5-9ef0-41d62492f41e 0xc0025ff317 0xc0025ff318}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-94rmz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-94rmz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-94rmz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:26:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:26:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:26:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:26:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.6.171.86,PodIP:172.29.108.71,StartTime:2020-09-27 04:26:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-27 04:26:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,ImageID:docker-pullable://us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c,ContainerID:docker://450aae1db6f25f867d894e1e474967d09992213fb4058290fdb3092137e395d5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.29.108.71,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:26:49.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2790" for this suite.

â€¢ [SLOW TEST:46.340 seconds]
[sig-apps] Deployment
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":277,"completed":209,"skipped":3519,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:26:49.997: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5000
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating service multi-endpoint-test in namespace services-5000
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5000 to expose endpoints map[]
Sep 27 04:26:50.456: INFO: successfully validated that service multi-endpoint-test in namespace services-5000 exposes endpoints map[] (37.629268ms elapsed)
STEP: Creating pod pod1 in namespace services-5000
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5000 to expose endpoints map[pod1:[100]]
Sep 27 04:26:56.149: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]] (5.686053654s elapsed, will retry)
Sep 27 04:27:03.380: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]] (12.916868103s elapsed, will retry)
Sep 27 04:27:07.541: INFO: successfully validated that service multi-endpoint-test in namespace services-5000 exposes endpoints map[pod1:[100]] (17.077898311s elapsed)
STEP: Creating pod pod2 in namespace services-5000
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5000 to expose endpoints map[pod1:[100] pod2:[101]]
Sep 27 04:27:11.710: INFO: Unexpected endpoints: found map[9c30ebc7-149d-474f-9c9c-40a118bad920:[100]], expected map[pod1:[100] pod2:[101]] (4.149116403s elapsed, will retry)
Sep 27 04:27:13.733: INFO: successfully validated that service multi-endpoint-test in namespace services-5000 exposes endpoints map[pod1:[100] pod2:[101]] (6.171610173s elapsed)
STEP: Deleting pod pod1 in namespace services-5000
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5000 to expose endpoints map[pod2:[101]]
Sep 27 04:27:13.913: INFO: successfully validated that service multi-endpoint-test in namespace services-5000 exposes endpoints map[pod2:[101]] (174.618662ms elapsed)
STEP: Deleting pod pod2 in namespace services-5000
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5000 to expose endpoints map[]
Sep 27 04:27:14.527: INFO: successfully validated that service multi-endpoint-test in namespace services-5000 exposes endpoints map[] (107.26123ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:27:14.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5000" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

â€¢ [SLOW TEST:24.896 seconds]
[sig-network] Services
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":277,"completed":210,"skipped":3527,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:27:14.893: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8502
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Sep 27 04:27:28.323: INFO: Successfully updated pod "labelsupdate44d87250-b885-41e7-afac-8d34c1dc132f"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:27:31.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8502" for this suite.

â€¢ [SLOW TEST:16.584 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":277,"completed":211,"skipped":3546,"failed":0}
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:27:31.478: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4831
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4831.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4831.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4831.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4831.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 27 04:27:43.293: INFO: DNS probes using dns-test-bbed81bf-6334-469f-b5ae-c7a27f6d1ce0 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4831.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4831.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4831.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4831.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 27 04:27:54.195: INFO: File wheezy_udp@dns-test-service-3.dns-4831.svc.cluster.local from pod  dns-4831/dns-test-7f0f8d6a-67a1-4efa-8adc-122ae076fe80 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 27 04:27:54.281: INFO: File jessie_udp@dns-test-service-3.dns-4831.svc.cluster.local from pod  dns-4831/dns-test-7f0f8d6a-67a1-4efa-8adc-122ae076fe80 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 27 04:27:54.281: INFO: Lookups using dns-4831/dns-test-7f0f8d6a-67a1-4efa-8adc-122ae076fe80 failed for: [wheezy_udp@dns-test-service-3.dns-4831.svc.cluster.local jessie_udp@dns-test-service-3.dns-4831.svc.cluster.local]

Sep 27 04:27:59.286: INFO: File wheezy_udp@dns-test-service-3.dns-4831.svc.cluster.local from pod  dns-4831/dns-test-7f0f8d6a-67a1-4efa-8adc-122ae076fe80 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 27 04:27:59.290: INFO: File jessie_udp@dns-test-service-3.dns-4831.svc.cluster.local from pod  dns-4831/dns-test-7f0f8d6a-67a1-4efa-8adc-122ae076fe80 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 27 04:27:59.290: INFO: Lookups using dns-4831/dns-test-7f0f8d6a-67a1-4efa-8adc-122ae076fe80 failed for: [wheezy_udp@dns-test-service-3.dns-4831.svc.cluster.local jessie_udp@dns-test-service-3.dns-4831.svc.cluster.local]

Sep 27 04:28:04.301: INFO: File wheezy_udp@dns-test-service-3.dns-4831.svc.cluster.local from pod  dns-4831/dns-test-7f0f8d6a-67a1-4efa-8adc-122ae076fe80 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 27 04:28:04.306: INFO: File jessie_udp@dns-test-service-3.dns-4831.svc.cluster.local from pod  dns-4831/dns-test-7f0f8d6a-67a1-4efa-8adc-122ae076fe80 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 27 04:28:04.306: INFO: Lookups using dns-4831/dns-test-7f0f8d6a-67a1-4efa-8adc-122ae076fe80 failed for: [wheezy_udp@dns-test-service-3.dns-4831.svc.cluster.local jessie_udp@dns-test-service-3.dns-4831.svc.cluster.local]

Sep 27 04:28:09.311: INFO: DNS probes using dns-test-7f0f8d6a-67a1-4efa-8adc-122ae076fe80 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4831.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4831.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4831.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4831.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 27 04:28:21.891: INFO: DNS probes using dns-test-c1786589-829d-47b4-b603-d80f7daaee9f succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:28:22.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4831" for this suite.

â€¢ [SLOW TEST:50.825 seconds]
[sig-network] DNS
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":277,"completed":212,"skipped":3553,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:28:22.304: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9296
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Sep 27 04:28:22.699: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 27 04:28:22.710: INFO: Waiting for terminating namespaces to be deleted...
Sep 27 04:28:22.712: INFO: 
Logging pods the kubelet thinks is on node dce-10-6-171-84 before test
Sep 27 04:28:22.746: INFO: coredns-coredns-7899d777b7-d9xwj from kube-system started at 2020-09-25 10:06:04 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.746: INFO: 	Container coredns-coredns ready: true, restart count 0
Sep 27 04:28:22.746: INFO: dce-kube-apiserver-proxy-dce-10-6-171-84 from kube-system started at 2020-09-24 09:48:25 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.746: INFO: 	Container dce-kube-apiserver-proxy ready: true, restart count 2
Sep 27 04:28:22.746: INFO: dce-engine-8sng6 from kube-system started at 2020-09-24 09:40:03 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.746: INFO: 	Container dce-engine ready: true, restart count 1
Sep 27 04:28:22.746: INFO: release-2048-dao-2048-6ff5c774d9-txzdv from default started at 2020-09-25 10:06:04 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.746: INFO: 	Container dao-2048 ready: true, restart count 0
Sep 27 04:28:22.746: INFO: dce-system-dnsservice-74c694858b-c7b2w from dce-system started at 2020-09-25 01:45:36 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.746: INFO: 	Container dce-system-dnsservice ready: true, restart count 0
Sep 27 04:28:22.746: INFO: kube-proxy-6qqrv from kube-system started at 2020-09-24 09:40:05 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.746: INFO: 	Container kube-proxy ready: true, restart count 1
Sep 27 04:28:22.746: INFO: dao-2048-dao-2048-db8cd6864-g8txm from default started at 2020-09-25 10:06:04 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.746: INFO: 	Container dao-2048-dao-2048 ready: true, restart count 0
Sep 27 04:28:22.746: INFO: dce-parcel-agent-85gws from kube-system started at 2020-09-24 09:40:06 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.746: INFO: 	Container dce-parcel-agent ready: true, restart count 4
Sep 27 04:28:22.746: INFO: coredns-coredns-7899d777b7-tpxhd from kube-system started at 2020-09-25 02:24:32 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.746: INFO: 	Container coredns-coredns ready: true, restart count 0
Sep 27 04:28:22.746: INFO: uds-host-metrics-dirver-r7msd from storage-system started at 2020-09-25 01:46:24 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.746: INFO: 	Container metrics-collector ready: true, restart count 0
Sep 27 04:28:22.746: INFO: node-local-dns-hh96g from kube-system started at 2020-09-24 10:25:26 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.746: INFO: 	Container node-cache ready: true, restart count 0
Sep 27 04:28:22.746: INFO: calico-node-7lk8x from kube-system started at 2020-09-24 09:40:07 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.746: INFO: 	Container calico-node ready: true, restart count 1
Sep 27 04:28:22.746: INFO: sonobuoy-systemd-logs-daemon-set-da170140ad3f4ead-glbd8 from sonobuoy started at 2020-09-27 02:39:45 +0000 UTC (2 container statuses recorded)
Sep 27 04:28:22.746: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 27 04:28:22.746: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 04:28:22.746: INFO: aaa-dao-2048-7897cf64bc-krl9t from default started at 2020-09-25 10:06:04 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.746: INFO: 	Container dao-2048 ready: true, restart count 0
Sep 27 04:28:22.746: INFO: 
Logging pods the kubelet thinks is on node dce-10-6-171-85 before test
Sep 27 04:28:22.783: INFO: uds-host-metrics-dirver-884nn from storage-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.783: INFO: 	Container metrics-collector ready: true, restart count 3
Sep 27 04:28:22.783: INFO: dce-kube-scheduler-dce-10-6-171-85 from kube-system started at 2020-09-24 10:36:45 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.783: INFO: 	Container dce-kube-scheduler ready: true, restart count 4
Sep 27 04:28:22.783: INFO: kube-proxy-mbrkq from kube-system started at 2020-09-24 09:12:07 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.783: INFO: 	Container kube-proxy ready: true, restart count 3
Sep 27 04:28:22.783: INFO: dce-parcel-agent-ndwd9 from kube-system started at 2020-09-24 09:12:07 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.783: INFO: 	Container dce-parcel-agent ready: true, restart count 5
Sep 27 04:28:22.783: INFO: calico-kube-controllers-7449c6cbb8-bq8n5 from kube-system started at 2020-09-24 09:15:06 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.783: INFO: 	Container calico-kube-controllers ready: true, restart count 5
Sep 27 04:28:22.783: INFO: dce-engine-hfc8k from kube-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.783: INFO: 	Container dce-engine ready: true, restart count 3
Sep 27 04:28:22.783: INFO: dce-core-keepalived-59dd799677-m42wb from kube-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.783: INFO: 	Container dce-core-keepalived ready: true, restart count 3
Sep 27 04:28:22.783: INFO: uds-storage-server-5f7b5f6fdc-q7kmm from storage-system started at 2020-09-25 02:32:02 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.783: INFO: 	Container main ready: false, restart count 2
Sep 27 04:28:22.783: INFO: dce-kube-controller-manager-dce-10-6-171-85 from kube-system started at 2020-09-24 10:36:44 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.783: INFO: 	Container dce-kube-controller-manager ready: true, restart count 4
Sep 27 04:28:22.783: INFO: calico-node-pcdmf from kube-system started at 2020-09-24 09:15:05 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.783: INFO: 	Container calico-node ready: true, restart count 3
Sep 27 04:28:22.783: INFO: dce-system-uds-58c4c68dbf-bbmdk from dce-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.783: INFO: 	Container dce-system-uds ready: false, restart count 5
Sep 27 04:28:22.783: INFO: dce-chart-manager-68fbf6bb4c-7288c from kube-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.783: INFO: 	Container chart-manager ready: true, restart count 3
Sep 27 04:28:22.783: INFO: dce-kube-apiserver-dce-10-6-171-85 from kube-system started at 2020-09-24 10:36:42 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.783: INFO: 	Container dce-kube-apiserver ready: true, restart count 0
Sep 27 04:28:22.783: INFO: uds-storage-server-5f7b5f6fdc-r46qt from storage-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.783: INFO: 	Container main ready: true, restart count 3
Sep 27 04:28:22.783: INFO: node-local-dns-9lnth from kube-system started at 2020-09-24 10:25:26 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.783: INFO: 	Container node-cache ready: true, restart count 1
Sep 27 04:28:22.783: INFO: dce-etcd-dce-10-6-171-85 from kube-system started at 2020-09-24 10:36:41 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.783: INFO: 	Container dce-etcd ready: true, restart count 3
Sep 27 04:28:22.783: INFO: dce-kube-apiserver-proxy-dce-10-6-171-85 from kube-system started at 2020-09-24 10:36:43 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.783: INFO: 	Container dce-kube-apiserver-proxy ready: true, restart count 3
Sep 27 04:28:22.783: INFO: dce-parcel-server-rwgb8 from kube-system started at 2020-09-24 09:12:07 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.783: INFO: 	Container dce-parcel-server ready: true, restart count 5
Sep 27 04:28:22.783: INFO: dce-system-uds-58c4c68dbf-g8fb6 from dce-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.783: INFO: 	Container dce-system-uds ready: true, restart count 3
Sep 27 04:28:22.783: INFO: sonobuoy-systemd-logs-daemon-set-da170140ad3f4ead-2mrkt from sonobuoy started at 2020-09-27 02:39:45 +0000 UTC (2 container statuses recorded)
Sep 27 04:28:22.783: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 27 04:28:22.783: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 04:28:22.783: INFO: metrics-server-6768fdf9db-56ks5 from kube-system started at 2020-09-24 09:15:28 +0000 UTC (2 container statuses recorded)
Sep 27 04:28:22.783: INFO: 	Container metrics-server ready: true, restart count 3
Sep 27 04:28:22.783: INFO: 	Container metrics-server-nanny ready: true, restart count 3
Sep 27 04:28:22.783: INFO: dce-registry-6cc79fc8c-fpp82 from kube-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.783: INFO: 	Container dce-registry ready: true, restart count 3
Sep 27 04:28:22.783: INFO: dce-prometheus-5dc44cf4dd-jjsjl from kube-system started at 2020-09-24 09:15:28 +0000 UTC (2 container statuses recorded)
Sep 27 04:28:22.783: INFO: 	Container dce-metrics-server ready: true, restart count 3
Sep 27 04:28:22.783: INFO: 	Container dce-prometheus ready: true, restart count 3
Sep 27 04:28:22.783: INFO: 
Logging pods the kubelet thinks is on node dce-10-6-171-86 before test
Sep 27 04:28:22.796: INFO: node-local-dns-pj747 from kube-system started at 2020-09-27 04:25:17 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.796: INFO: 	Container node-cache ready: true, restart count 0
Sep 27 04:28:22.796: INFO: dce-kube-apiserver-proxy-dce-10-6-171-86 from kube-system started at 2020-09-24 09:47:03 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.796: INFO: 	Container dce-kube-apiserver-proxy ready: true, restart count 1
Sep 27 04:28:22.796: INFO: sonobuoy from sonobuoy started at 2020-09-27 02:39:34 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.796: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 27 04:28:22.796: INFO: dce-parcel-agent-p94cd from kube-system started at 2020-09-27 04:25:17 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.796: INFO: 	Container dce-parcel-agent ready: true, restart count 0
Sep 27 04:28:22.796: INFO: uds-host-metrics-dirver-48m2w from storage-system started at 2020-09-25 01:46:33 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.796: INFO: 	Container metrics-collector ready: true, restart count 0
Sep 27 04:28:22.796: INFO: sonobuoy-systemd-logs-daemon-set-da170140ad3f4ead-fbqzn from sonobuoy started at 2020-09-27 02:39:45 +0000 UTC (2 container statuses recorded)
Sep 27 04:28:22.796: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 27 04:28:22.796: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 04:28:22.796: INFO: sonobuoy-e2e-job-f515d88659da49be from sonobuoy started at 2020-09-27 02:39:45 +0000 UTC (2 container statuses recorded)
Sep 27 04:28:22.796: INFO: 	Container e2e ready: true, restart count 0
Sep 27 04:28:22.796: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 04:28:22.796: INFO: kube-proxy-wjt5k from kube-system started at 2020-09-24 09:35:13 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.796: INFO: 	Container kube-proxy ready: true, restart count 1
Sep 27 04:28:22.796: INFO: dce-engine-dvcxl from kube-system started at 2020-09-24 09:35:13 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.796: INFO: 	Container dce-engine ready: true, restart count 1
Sep 27 04:28:22.796: INFO: calico-node-hsm9f from kube-system started at 2020-09-24 09:35:13 +0000 UTC (1 container statuses recorded)
Sep 27 04:28:22.796: INFO: 	Container calico-node ready: true, restart count 1
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-786f38f6-6aea-4f17-b2c7-10a8fa4acba2 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-786f38f6-6aea-4f17-b2c7-10a8fa4acba2 off the node dce-10-6-171-86
STEP: verifying the node doesn't have the label kubernetes.io/e2e-786f38f6-6aea-4f17-b2c7-10a8fa4acba2
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:28:43.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9296" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

â€¢ [SLOW TEST:20.778 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":277,"completed":213,"skipped":3600,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:28:43.082: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2820
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 04:28:46.211: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777725, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777725, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777725, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777725, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:28:48.694: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777725, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777725, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777725, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777725, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:28:50.245: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777725, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777725, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777725, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777725, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:28:52.234: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777725, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777725, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777725, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777725, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:28:54.665: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777725, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777725, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777725, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736777725, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 04:28:57.471: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:28:59.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2820" for this suite.
STEP: Destroying namespace "webhook-2820-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:16.691 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":277,"completed":214,"skipped":3603,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:28:59.774: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4665
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 27 04:29:12.326: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:29:12.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4665" for this suite.

â€¢ [SLOW TEST:12.589 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  blackbox test
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
    on terminated container
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:133
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":277,"completed":215,"skipped":3632,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:29:12.364: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-4752
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 04:29:12.592: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-f34b3bba-9d6c-4bb8-b0db-f2393ef2b71a" in namespace "security-context-test-4752" to be "Succeeded or Failed"
Sep 27 04:29:12.608: INFO: Pod "busybox-readonly-false-f34b3bba-9d6c-4bb8-b0db-f2393ef2b71a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.260188ms
Sep 27 04:29:14.725: INFO: Pod "busybox-readonly-false-f34b3bba-9d6c-4bb8-b0db-f2393ef2b71a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.132896875s
Sep 27 04:29:16.734: INFO: Pod "busybox-readonly-false-f34b3bba-9d6c-4bb8-b0db-f2393ef2b71a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.142206713s
Sep 27 04:29:18.740: INFO: Pod "busybox-readonly-false-f34b3bba-9d6c-4bb8-b0db-f2393ef2b71a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.148078014s
Sep 27 04:29:18.740: INFO: Pod "busybox-readonly-false-f34b3bba-9d6c-4bb8-b0db-f2393ef2b71a" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:29:18.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4752" for this suite.

â€¢ [SLOW TEST:6.388 seconds]
[k8s.io] Security Context
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  When creating a pod with readOnlyRootFilesystem
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:166
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":277,"completed":216,"skipped":3656,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:29:18.752: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3963
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating pod
Sep 27 04:29:37.791: INFO: Pod pod-hostip-0da1983a-2b9d-4b87-baac-af392c824fe9 has hostIP: 10.6.171.86
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:29:37.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3963" for this suite.

â€¢ [SLOW TEST:19.047 seconds]
[k8s.io] Pods
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":277,"completed":217,"skipped":3664,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:29:37.799: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6254
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 04:29:38.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 version'
Sep 27 04:29:38.131: INFO: stderr: ""
Sep 27 04:29:38.131: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"18\", GitVersion:\"v1.18.6\", GitCommit:\"dff82dc0de47299ab66c83c626e08b245ab19037\", GitTreeState:\"clean\", BuildDate:\"2020-07-15T16:58:53Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"18\", GitVersion:\"v1.18.6\", GitCommit:\"cae93a63867\", GitTreeState:\"clean\", BuildDate:\"2020-08-11T08:41:18Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:29:38.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6254" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":277,"completed":218,"skipped":3674,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:29:38.140: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-1218
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 04:29:38.332: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:29:39.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1218" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":277,"completed":219,"skipped":3686,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:29:39.594: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-3739
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Sep 27 04:29:40.754: INFO: Pod name wrapped-volume-race-9fc1f410-7b74-43e3-b160-2ce299e5118b: Found 0 pods out of 5
Sep 27 04:29:45.959: INFO: Pod name wrapped-volume-race-9fc1f410-7b74-43e3-b160-2ce299e5118b: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-9fc1f410-7b74-43e3-b160-2ce299e5118b in namespace emptydir-wrapper-3739, will wait for the garbage collector to delete the pods
Sep 27 04:30:14.360: INFO: Deleting ReplicationController wrapped-volume-race-9fc1f410-7b74-43e3-b160-2ce299e5118b took: 7.75871ms
Sep 27 04:30:15.061: INFO: Terminating ReplicationController wrapped-volume-race-9fc1f410-7b74-43e3-b160-2ce299e5118b pods took: 700.338368ms
STEP: Creating RC which spawns configmap-volume pods
Sep 27 04:30:42.396: INFO: Pod name wrapped-volume-race-0df4d2a2-f6c9-44bb-b09f-e23ba2394967: Found 0 pods out of 5
Sep 27 04:30:47.404: INFO: Pod name wrapped-volume-race-0df4d2a2-f6c9-44bb-b09f-e23ba2394967: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-0df4d2a2-f6c9-44bb-b09f-e23ba2394967 in namespace emptydir-wrapper-3739, will wait for the garbage collector to delete the pods
Sep 27 04:31:15.517: INFO: Deleting ReplicationController wrapped-volume-race-0df4d2a2-f6c9-44bb-b09f-e23ba2394967 took: 9.359592ms
Sep 27 04:31:16.417: INFO: Terminating ReplicationController wrapped-volume-race-0df4d2a2-f6c9-44bb-b09f-e23ba2394967 pods took: 900.303783ms
STEP: Creating RC which spawns configmap-volume pods
Sep 27 04:31:27.948: INFO: Pod name wrapped-volume-race-558f1da9-150f-483d-99b0-765b6807ac31: Found 0 pods out of 5
Sep 27 04:31:32.983: INFO: Pod name wrapped-volume-race-558f1da9-150f-483d-99b0-765b6807ac31: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-558f1da9-150f-483d-99b0-765b6807ac31 in namespace emptydir-wrapper-3739, will wait for the garbage collector to delete the pods
Sep 27 04:31:53.082: INFO: Deleting ReplicationController wrapped-volume-race-558f1da9-150f-483d-99b0-765b6807ac31 took: 9.180075ms
Sep 27 04:31:53.683: INFO: Terminating ReplicationController wrapped-volume-race-558f1da9-150f-483d-99b0-765b6807ac31 pods took: 600.280071ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:32:18.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3739" for this suite.

â€¢ [SLOW TEST:159.390 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":277,"completed":220,"skipped":3709,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:32:18.985: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-4819
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Sep 27 04:32:27.865: INFO: Successfully updated pod "adopt-release-tb5cl"
STEP: Checking that the Job readopts the Pod
Sep 27 04:32:27.865: INFO: Waiting up to 15m0s for pod "adopt-release-tb5cl" in namespace "job-4819" to be "adopted"
Sep 27 04:32:27.884: INFO: Pod "adopt-release-tb5cl": Phase="Running", Reason="", readiness=true. Elapsed: 18.634465ms
Sep 27 04:32:30.248: INFO: Pod "adopt-release-tb5cl": Phase="Running", Reason="", readiness=true. Elapsed: 2.382507656s
Sep 27 04:32:30.248: INFO: Pod "adopt-release-tb5cl" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Sep 27 04:32:32.516: INFO: Successfully updated pod "adopt-release-tb5cl"
STEP: Checking that the Job releases the Pod
Sep 27 04:32:32.516: INFO: Waiting up to 15m0s for pod "adopt-release-tb5cl" in namespace "job-4819" to be "released"
Sep 27 04:32:32.906: INFO: Pod "adopt-release-tb5cl": Phase="Running", Reason="", readiness=true. Elapsed: 389.278967ms
Sep 27 04:32:32.906: INFO: Pod "adopt-release-tb5cl" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:32:32.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4819" for this suite.

â€¢ [SLOW TEST:14.408 seconds]
[sig-apps] Job
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":277,"completed":221,"skipped":3756,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:32:33.394: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-558
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Update Demo
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:271
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a replication controller
Sep 27 04:32:34.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 create -f - --namespace=kubectl-558'
Sep 27 04:32:38.682: INFO: stderr: ""
Sep 27 04:32:38.682: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 27 04:32:38.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-558'
Sep 27 04:32:38.948: INFO: stderr: ""
Sep 27 04:32:38.948: INFO: stdout: "update-demo-nautilus-2qqs8 update-demo-nautilus-ddz6m "
Sep 27 04:32:38.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods update-demo-nautilus-2qqs8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-558'
Sep 27 04:32:39.079: INFO: stderr: ""
Sep 27 04:32:39.079: INFO: stdout: ""
Sep 27 04:32:39.079: INFO: update-demo-nautilus-2qqs8 is created but not running
Sep 27 04:32:44.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-558'
Sep 27 04:32:45.868: INFO: stderr: ""
Sep 27 04:32:45.868: INFO: stdout: "update-demo-nautilus-2qqs8 update-demo-nautilus-ddz6m "
Sep 27 04:32:45.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods update-demo-nautilus-2qqs8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-558'
Sep 27 04:32:47.303: INFO: stderr: ""
Sep 27 04:32:47.303: INFO: stdout: ""
Sep 27 04:32:47.303: INFO: update-demo-nautilus-2qqs8 is created but not running
Sep 27 04:32:52.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-558'
Sep 27 04:32:52.392: INFO: stderr: ""
Sep 27 04:32:52.392: INFO: stdout: "update-demo-nautilus-2qqs8 update-demo-nautilus-ddz6m "
Sep 27 04:32:52.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods update-demo-nautilus-2qqs8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-558'
Sep 27 04:32:52.491: INFO: stderr: ""
Sep 27 04:32:52.491: INFO: stdout: ""
Sep 27 04:32:52.491: INFO: update-demo-nautilus-2qqs8 is created but not running
Sep 27 04:32:57.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-558'
Sep 27 04:32:58.200: INFO: stderr: ""
Sep 27 04:32:58.200: INFO: stdout: "update-demo-nautilus-2qqs8 update-demo-nautilus-ddz6m "
Sep 27 04:32:58.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods update-demo-nautilus-2qqs8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-558'
Sep 27 04:32:58.975: INFO: stderr: ""
Sep 27 04:32:58.975: INFO: stdout: "true"
Sep 27 04:32:58.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods update-demo-nautilus-2qqs8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-558'
Sep 27 04:32:59.130: INFO: stderr: ""
Sep 27 04:32:59.130: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 27 04:32:59.130: INFO: validating pod update-demo-nautilus-2qqs8
Sep 27 04:32:59.236: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 27 04:32:59.237: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 27 04:32:59.237: INFO: update-demo-nautilus-2qqs8 is verified up and running
Sep 27 04:32:59.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods update-demo-nautilus-ddz6m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-558'
Sep 27 04:32:59.336: INFO: stderr: ""
Sep 27 04:32:59.336: INFO: stdout: "true"
Sep 27 04:32:59.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods update-demo-nautilus-ddz6m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-558'
Sep 27 04:32:59.739: INFO: stderr: ""
Sep 27 04:32:59.739: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 27 04:32:59.739: INFO: validating pod update-demo-nautilus-ddz6m
Sep 27 04:32:59.745: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 27 04:32:59.745: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 27 04:32:59.745: INFO: update-demo-nautilus-ddz6m is verified up and running
STEP: scaling down the replication controller
Sep 27 04:32:59.748: INFO: scanned /root for discovery docs: <nil>
Sep 27 04:32:59.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-558'
Sep 27 04:33:01.536: INFO: stderr: ""
Sep 27 04:33:01.536: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 27 04:33:01.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-558'
Sep 27 04:33:01.672: INFO: stderr: ""
Sep 27 04:33:01.672: INFO: stdout: "update-demo-nautilus-2qqs8 update-demo-nautilus-ddz6m "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep 27 04:33:06.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-558'
Sep 27 04:33:06.816: INFO: stderr: ""
Sep 27 04:33:06.816: INFO: stdout: "update-demo-nautilus-2qqs8 update-demo-nautilus-ddz6m "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep 27 04:33:11.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-558'
Sep 27 04:33:11.942: INFO: stderr: ""
Sep 27 04:33:11.942: INFO: stdout: "update-demo-nautilus-2qqs8 "
Sep 27 04:33:11.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods update-demo-nautilus-2qqs8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-558'
Sep 27 04:33:12.072: INFO: stderr: ""
Sep 27 04:33:12.072: INFO: stdout: "true"
Sep 27 04:33:12.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods update-demo-nautilus-2qqs8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-558'
Sep 27 04:33:12.197: INFO: stderr: ""
Sep 27 04:33:12.197: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 27 04:33:12.198: INFO: validating pod update-demo-nautilus-2qqs8
Sep 27 04:33:12.219: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 27 04:33:12.220: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 27 04:33:12.220: INFO: update-demo-nautilus-2qqs8 is verified up and running
STEP: scaling up the replication controller
Sep 27 04:33:12.222: INFO: scanned /root for discovery docs: <nil>
Sep 27 04:33:12.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-558'
Sep 27 04:33:13.394: INFO: stderr: ""
Sep 27 04:33:13.394: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 27 04:33:13.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-558'
Sep 27 04:33:13.571: INFO: stderr: ""
Sep 27 04:33:13.571: INFO: stdout: "update-demo-nautilus-2qqs8 update-demo-nautilus-r8wwh "
Sep 27 04:33:13.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods update-demo-nautilus-2qqs8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-558'
Sep 27 04:33:13.691: INFO: stderr: ""
Sep 27 04:33:13.691: INFO: stdout: "true"
Sep 27 04:33:13.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods update-demo-nautilus-2qqs8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-558'
Sep 27 04:33:13.822: INFO: stderr: ""
Sep 27 04:33:13.822: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 27 04:33:13.822: INFO: validating pod update-demo-nautilus-2qqs8
Sep 27 04:33:13.827: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 27 04:33:13.827: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 27 04:33:13.827: INFO: update-demo-nautilus-2qqs8 is verified up and running
Sep 27 04:33:13.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods update-demo-nautilus-r8wwh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-558'
Sep 27 04:33:13.968: INFO: stderr: ""
Sep 27 04:33:13.968: INFO: stdout: ""
Sep 27 04:33:13.968: INFO: update-demo-nautilus-r8wwh is created but not running
Sep 27 04:33:18.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-558'
Sep 27 04:33:19.413: INFO: stderr: ""
Sep 27 04:33:19.413: INFO: stdout: "update-demo-nautilus-2qqs8 update-demo-nautilus-r8wwh "
Sep 27 04:33:19.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods update-demo-nautilus-2qqs8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-558'
Sep 27 04:33:19.556: INFO: stderr: ""
Sep 27 04:33:19.556: INFO: stdout: "true"
Sep 27 04:33:19.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods update-demo-nautilus-2qqs8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-558'
Sep 27 04:33:19.914: INFO: stderr: ""
Sep 27 04:33:19.914: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 27 04:33:19.914: INFO: validating pod update-demo-nautilus-2qqs8
Sep 27 04:33:19.955: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 27 04:33:19.955: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 27 04:33:19.955: INFO: update-demo-nautilus-2qqs8 is verified up and running
Sep 27 04:33:19.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods update-demo-nautilus-r8wwh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-558'
Sep 27 04:33:20.510: INFO: stderr: ""
Sep 27 04:33:20.510: INFO: stdout: ""
Sep 27 04:33:20.510: INFO: update-demo-nautilus-r8wwh is created but not running
Sep 27 04:33:25.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-558'
Sep 27 04:33:25.757: INFO: stderr: ""
Sep 27 04:33:25.757: INFO: stdout: "update-demo-nautilus-2qqs8 update-demo-nautilus-r8wwh "
Sep 27 04:33:25.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods update-demo-nautilus-2qqs8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-558'
Sep 27 04:33:25.876: INFO: stderr: ""
Sep 27 04:33:25.876: INFO: stdout: "true"
Sep 27 04:33:25.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods update-demo-nautilus-2qqs8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-558'
Sep 27 04:33:25.976: INFO: stderr: ""
Sep 27 04:33:25.976: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 27 04:33:25.976: INFO: validating pod update-demo-nautilus-2qqs8
Sep 27 04:33:25.980: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 27 04:33:25.980: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 27 04:33:25.980: INFO: update-demo-nautilus-2qqs8 is verified up and running
Sep 27 04:33:25.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods update-demo-nautilus-r8wwh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-558'
Sep 27 04:33:26.070: INFO: stderr: ""
Sep 27 04:33:26.070: INFO: stdout: "true"
Sep 27 04:33:26.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods update-demo-nautilus-r8wwh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-558'
Sep 27 04:33:26.166: INFO: stderr: ""
Sep 27 04:33:26.166: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 27 04:33:26.166: INFO: validating pod update-demo-nautilus-r8wwh
Sep 27 04:33:26.171: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 27 04:33:26.171: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 27 04:33:26.171: INFO: update-demo-nautilus-r8wwh is verified up and running
STEP: using delete to clean up resources
Sep 27 04:33:26.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 delete --grace-period=0 --force -f - --namespace=kubectl-558'
Sep 27 04:33:26.279: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 27 04:33:26.279: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep 27 04:33:26.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-558'
Sep 27 04:33:26.387: INFO: stderr: "No resources found in kubectl-558 namespace.\n"
Sep 27 04:33:26.387: INFO: stdout: ""
Sep 27 04:33:26.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods -l name=update-demo --namespace=kubectl-558 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 27 04:33:26.486: INFO: stderr: ""
Sep 27 04:33:26.486: INFO: stdout: "update-demo-nautilus-2qqs8\nupdate-demo-nautilus-r8wwh\n"
Sep 27 04:33:26.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-558'
Sep 27 04:33:27.129: INFO: stderr: "No resources found in kubectl-558 namespace.\n"
Sep 27 04:33:27.129: INFO: stdout: ""
Sep 27 04:33:27.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 get pods -l name=update-demo --namespace=kubectl-558 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 27 04:33:27.254: INFO: stderr: ""
Sep 27 04:33:27.254: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:33:27.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-558" for this suite.

â€¢ [SLOW TEST:53.870 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:269
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":277,"completed":222,"skipped":3808,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:33:27.264: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5893
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 27 04:33:28.115: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d7a10c91-b5de-4eff-a8b6-16cf9fc266a4" in namespace "downward-api-5893" to be "Succeeded or Failed"
Sep 27 04:33:28.399: INFO: Pod "downwardapi-volume-d7a10c91-b5de-4eff-a8b6-16cf9fc266a4": Phase="Pending", Reason="", readiness=false. Elapsed: 283.949126ms
Sep 27 04:33:30.593: INFO: Pod "downwardapi-volume-d7a10c91-b5de-4eff-a8b6-16cf9fc266a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.477974191s
Sep 27 04:33:33.084: INFO: Pod "downwardapi-volume-d7a10c91-b5de-4eff-a8b6-16cf9fc266a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.968513486s
Sep 27 04:33:35.201: INFO: Pod "downwardapi-volume-d7a10c91-b5de-4eff-a8b6-16cf9fc266a4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.085745098s
Sep 27 04:33:37.286: INFO: Pod "downwardapi-volume-d7a10c91-b5de-4eff-a8b6-16cf9fc266a4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.170739178s
Sep 27 04:33:39.358: INFO: Pod "downwardapi-volume-d7a10c91-b5de-4eff-a8b6-16cf9fc266a4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.242631039s
Sep 27 04:33:41.362: INFO: Pod "downwardapi-volume-d7a10c91-b5de-4eff-a8b6-16cf9fc266a4": Phase="Pending", Reason="", readiness=false. Elapsed: 13.247151287s
Sep 27 04:33:43.472: INFO: Pod "downwardapi-volume-d7a10c91-b5de-4eff-a8b6-16cf9fc266a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 15.35682754s
STEP: Saw pod success
Sep 27 04:33:43.472: INFO: Pod "downwardapi-volume-d7a10c91-b5de-4eff-a8b6-16cf9fc266a4" satisfied condition "Succeeded or Failed"
Sep 27 04:33:43.505: INFO: Trying to get logs from node dce-10-6-171-86 pod downwardapi-volume-d7a10c91-b5de-4eff-a8b6-16cf9fc266a4 container client-container: <nil>
STEP: delete the pod
Sep 27 04:33:43.944: INFO: Waiting for pod downwardapi-volume-d7a10c91-b5de-4eff-a8b6-16cf9fc266a4 to disappear
Sep 27 04:33:44.091: INFO: Pod downwardapi-volume-d7a10c91-b5de-4eff-a8b6-16cf9fc266a4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:33:44.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5893" for this suite.

â€¢ [SLOW TEST:17.088 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":277,"completed":223,"skipped":3811,"failed":0}
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:33:44.352: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1687
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test substitution in container's command
Sep 27 04:33:44.840: INFO: Waiting up to 5m0s for pod "var-expansion-6445fbba-7055-411b-aaae-22f29a055499" in namespace "var-expansion-1687" to be "Succeeded or Failed"
Sep 27 04:33:44.875: INFO: Pod "var-expansion-6445fbba-7055-411b-aaae-22f29a055499": Phase="Pending", Reason="", readiness=false. Elapsed: 35.197231ms
Sep 27 04:33:46.941: INFO: Pod "var-expansion-6445fbba-7055-411b-aaae-22f29a055499": Phase="Pending", Reason="", readiness=false. Elapsed: 2.101368824s
Sep 27 04:33:49.030: INFO: Pod "var-expansion-6445fbba-7055-411b-aaae-22f29a055499": Phase="Pending", Reason="", readiness=false. Elapsed: 4.189988126s
Sep 27 04:33:51.034: INFO: Pod "var-expansion-6445fbba-7055-411b-aaae-22f29a055499": Phase="Pending", Reason="", readiness=false. Elapsed: 6.194559381s
Sep 27 04:33:53.045: INFO: Pod "var-expansion-6445fbba-7055-411b-aaae-22f29a055499": Phase="Pending", Reason="", readiness=false. Elapsed: 8.205557835s
Sep 27 04:33:55.081: INFO: Pod "var-expansion-6445fbba-7055-411b-aaae-22f29a055499": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.241150612s
STEP: Saw pod success
Sep 27 04:33:55.081: INFO: Pod "var-expansion-6445fbba-7055-411b-aaae-22f29a055499" satisfied condition "Succeeded or Failed"
Sep 27 04:33:55.085: INFO: Trying to get logs from node dce-10-6-171-86 pod var-expansion-6445fbba-7055-411b-aaae-22f29a055499 container dapi-container: <nil>
STEP: delete the pod
Sep 27 04:33:55.197: INFO: Waiting for pod var-expansion-6445fbba-7055-411b-aaae-22f29a055499 to disappear
Sep 27 04:33:55.203: INFO: Pod var-expansion-6445fbba-7055-411b-aaae-22f29a055499 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:33:55.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1687" for this suite.

â€¢ [SLOW TEST:10.864 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":277,"completed":224,"skipped":3816,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:33:55.216: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-7454
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Sep 27 04:34:03.826: INFO: &Pod{ObjectMeta:{send-events-93bcdec2-05ea-4da4-b377-0ddb6ae393ce  events-7454 /api/v1/namespaces/events-7454/pods/send-events-93bcdec2-05ea-4da4-b377-0ddb6ae393ce c9bfcc5b-8bfc-4542-92c1-d1a5fdbdc7a4 710304 0 2020-09-27 04:33:55 +0000 UTC <nil> <nil> map[name:foo time:771943338] map[cni.projectcalico.org/ipv4pools:["default-ipv4-ippool"] dce.daocloud.io/parcel.egress.burst:0 dce.daocloud.io/parcel.egress.rate:0 dce.daocloud.io/parcel.ingress.burst:0 dce.daocloud.io/parcel.ingress.rate:0 kubernetes.io/psp:dce-psp-allow-all] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-9xmxr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-9xmxr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-9xmxr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:33:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:34:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:34:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:33:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.6.171.86,PodIP:172.29.108.114,StartTime:2020-09-27 04:33:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-27 04:34:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,ImageID:docker-pullable://us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c,ContainerID:docker://71b461201db1611dc74ea4633d29a3e567376a24f3b8a02526bd3c34e92f9398,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.29.108.114,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Sep 27 04:34:05.833: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Sep 27 04:34:07.839: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:34:07.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7454" for this suite.

â€¢ [SLOW TEST:12.702 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":277,"completed":225,"skipped":3826,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:34:07.919: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8037
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-fe5eddb9-187c-4d32-aec4-9fa2c4859a3d
STEP: Creating a pod to test consume secrets
Sep 27 04:34:08.191: INFO: Waiting up to 5m0s for pod "pod-secrets-395fe146-484c-4fb8-a6a2-ebddb6595e5e" in namespace "secrets-8037" to be "Succeeded or Failed"
Sep 27 04:34:08.201: INFO: Pod "pod-secrets-395fe146-484c-4fb8-a6a2-ebddb6595e5e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.563791ms
Sep 27 04:34:10.209: INFO: Pod "pod-secrets-395fe146-484c-4fb8-a6a2-ebddb6595e5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017664111s
Sep 27 04:34:12.216: INFO: Pod "pod-secrets-395fe146-484c-4fb8-a6a2-ebddb6595e5e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024717612s
Sep 27 04:34:14.284: INFO: Pod "pod-secrets-395fe146-484c-4fb8-a6a2-ebddb6595e5e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.092810087s
Sep 27 04:34:16.288: INFO: Pod "pod-secrets-395fe146-484c-4fb8-a6a2-ebddb6595e5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.096755152s
STEP: Saw pod success
Sep 27 04:34:16.288: INFO: Pod "pod-secrets-395fe146-484c-4fb8-a6a2-ebddb6595e5e" satisfied condition "Succeeded or Failed"
Sep 27 04:34:16.291: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-secrets-395fe146-484c-4fb8-a6a2-ebddb6595e5e container secret-volume-test: <nil>
STEP: delete the pod
Sep 27 04:34:16.339: INFO: Waiting for pod pod-secrets-395fe146-484c-4fb8-a6a2-ebddb6595e5e to disappear
Sep 27 04:34:16.345: INFO: Pod pod-secrets-395fe146-484c-4fb8-a6a2-ebddb6595e5e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:34:16.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8037" for this suite.

â€¢ [SLOW TEST:8.442 seconds]
[sig-storage] Secrets
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":226,"skipped":3839,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:34:16.362: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-230
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:34:26.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-230" for this suite.

â€¢ [SLOW TEST:10.395 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:137
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":227,"skipped":3877,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] version v1
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:34:26.757: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-3952
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 04:34:27.206: INFO: (0) /api/v1/nodes/dce-10-6-171-85/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 27.361016ms)
Sep 27 04:34:27.213: INFO: (1) /api/v1/nodes/dce-10-6-171-85/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 5.959233ms)
Sep 27 04:34:27.217: INFO: (2) /api/v1/nodes/dce-10-6-171-85/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 4.317076ms)
Sep 27 04:34:27.221: INFO: (3) /api/v1/nodes/dce-10-6-171-85/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 4.063108ms)
Sep 27 04:34:27.225: INFO: (4) /api/v1/nodes/dce-10-6-171-85/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 3.510668ms)
Sep 27 04:34:27.228: INFO: (5) /api/v1/nodes/dce-10-6-171-85/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 3.465604ms)
Sep 27 04:34:27.232: INFO: (6) /api/v1/nodes/dce-10-6-171-85/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 3.440071ms)
Sep 27 04:34:27.236: INFO: (7) /api/v1/nodes/dce-10-6-171-85/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 4.261103ms)
Sep 27 04:34:27.241: INFO: (8) /api/v1/nodes/dce-10-6-171-85/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 4.764314ms)
Sep 27 04:34:27.247: INFO: (9) /api/v1/nodes/dce-10-6-171-85/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 5.948229ms)
Sep 27 04:34:27.257: INFO: (10) /api/v1/nodes/dce-10-6-171-85/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 9.901909ms)
Sep 27 04:34:27.262: INFO: (11) /api/v1/nodes/dce-10-6-171-85/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 5.333655ms)
Sep 27 04:34:27.267: INFO: (12) /api/v1/nodes/dce-10-6-171-85/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 4.75955ms)
Sep 27 04:34:27.272: INFO: (13) /api/v1/nodes/dce-10-6-171-85/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 4.947717ms)
Sep 27 04:34:27.277: INFO: (14) /api/v1/nodes/dce-10-6-171-85/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 5.04137ms)
Sep 27 04:34:27.289: INFO: (15) /api/v1/nodes/dce-10-6-171-85/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 12.16916ms)
Sep 27 04:34:27.293: INFO: (16) /api/v1/nodes/dce-10-6-171-85/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 4.254675ms)
Sep 27 04:34:27.297: INFO: (17) /api/v1/nodes/dce-10-6-171-85/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 3.954821ms)
Sep 27 04:34:27.307: INFO: (18) /api/v1/nodes/dce-10-6-171-85/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 8.999707ms)
Sep 27 04:34:27.312: INFO: (19) /api/v1/nodes/dce-10-6-171-85/proxy/logs/: <pre>
<a href="Xorg.9.log">Xorg.9.log</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audi... (200; 5.68531ms)
[AfterEach] version v1
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:34:27.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3952" for this suite.
â€¢{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]","total":277,"completed":228,"skipped":3885,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:34:27.325: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8236
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 04:34:27.885: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Sep 27 04:34:27.915: INFO: Number of nodes with available pods: 0
Sep 27 04:34:27.915: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Sep 27 04:34:28.041: INFO: Number of nodes with available pods: 0
Sep 27 04:34:28.041: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:34:29.051: INFO: Number of nodes with available pods: 0
Sep 27 04:34:29.051: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:34:30.056: INFO: Number of nodes with available pods: 0
Sep 27 04:34:30.056: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:34:31.332: INFO: Number of nodes with available pods: 0
Sep 27 04:34:31.332: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:34:32.968: INFO: Number of nodes with available pods: 0
Sep 27 04:34:32.968: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:34:33.336: INFO: Number of nodes with available pods: 0
Sep 27 04:34:33.336: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:34:34.365: INFO: Number of nodes with available pods: 0
Sep 27 04:34:34.365: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:34:35.071: INFO: Number of nodes with available pods: 0
Sep 27 04:34:35.071: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:34:36.045: INFO: Number of nodes with available pods: 0
Sep 27 04:34:36.046: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:34:37.045: INFO: Number of nodes with available pods: 0
Sep 27 04:34:37.045: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:34:38.045: INFO: Number of nodes with available pods: 1
Sep 27 04:34:38.045: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Sep 27 04:34:38.079: INFO: Number of nodes with available pods: 1
Sep 27 04:34:38.079: INFO: Number of running nodes: 0, number of available pods: 1
Sep 27 04:34:39.119: INFO: Number of nodes with available pods: 0
Sep 27 04:34:39.120: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Sep 27 04:34:39.131: INFO: Number of nodes with available pods: 0
Sep 27 04:34:39.131: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:34:40.208: INFO: Number of nodes with available pods: 0
Sep 27 04:34:40.208: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:34:41.190: INFO: Number of nodes with available pods: 0
Sep 27 04:34:41.190: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:34:42.566: INFO: Number of nodes with available pods: 0
Sep 27 04:34:42.566: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:34:43.221: INFO: Number of nodes with available pods: 0
Sep 27 04:34:43.222: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:34:44.327: INFO: Number of nodes with available pods: 0
Sep 27 04:34:44.327: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:34:45.165: INFO: Number of nodes with available pods: 0
Sep 27 04:34:45.165: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:34:46.137: INFO: Number of nodes with available pods: 0
Sep 27 04:34:46.137: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:34:47.212: INFO: Number of nodes with available pods: 0
Sep 27 04:34:47.212: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:34:48.347: INFO: Number of nodes with available pods: 0
Sep 27 04:34:48.347: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:34:49.135: INFO: Number of nodes with available pods: 0
Sep 27 04:34:49.135: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:34:50.137: INFO: Number of nodes with available pods: 0
Sep 27 04:34:50.137: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:34:51.136: INFO: Number of nodes with available pods: 0
Sep 27 04:34:51.136: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:34:52.134: INFO: Number of nodes with available pods: 0
Sep 27 04:34:52.134: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:34:53.134: INFO: Number of nodes with available pods: 0
Sep 27 04:34:53.134: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:34:54.184: INFO: Number of nodes with available pods: 0
Sep 27 04:34:54.184: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:34:55.135: INFO: Number of nodes with available pods: 0
Sep 27 04:34:55.135: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:34:56.156: INFO: Number of nodes with available pods: 0
Sep 27 04:34:56.156: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:34:57.141: INFO: Number of nodes with available pods: 0
Sep 27 04:34:57.141: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:34:58.209: INFO: Number of nodes with available pods: 0
Sep 27 04:34:58.209: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:34:59.136: INFO: Number of nodes with available pods: 0
Sep 27 04:34:59.136: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:35:00.135: INFO: Number of nodes with available pods: 0
Sep 27 04:35:00.135: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:35:01.137: INFO: Number of nodes with available pods: 0
Sep 27 04:35:01.137: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:35:02.450: INFO: Number of nodes with available pods: 0
Sep 27 04:35:02.450: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:35:03.134: INFO: Number of nodes with available pods: 0
Sep 27 04:35:03.134: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:35:04.135: INFO: Number of nodes with available pods: 0
Sep 27 04:35:04.135: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:35:05.265: INFO: Number of nodes with available pods: 0
Sep 27 04:35:05.265: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:35:06.818: INFO: Number of nodes with available pods: 0
Sep 27 04:35:06.818: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:35:07.306: INFO: Number of nodes with available pods: 0
Sep 27 04:35:07.306: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:35:08.429: INFO: Number of nodes with available pods: 0
Sep 27 04:35:08.429: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:35:09.387: INFO: Number of nodes with available pods: 0
Sep 27 04:35:09.387: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:35:10.203: INFO: Number of nodes with available pods: 1
Sep 27 04:35:10.203: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8236, will wait for the garbage collector to delete the pods
Sep 27 04:35:10.276: INFO: Deleting DaemonSet.extensions daemon-set took: 5.746872ms
Sep 27 04:35:10.876: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.327752ms
Sep 27 04:35:18.080: INFO: Number of nodes with available pods: 0
Sep 27 04:35:18.080: INFO: Number of running nodes: 0, number of available pods: 0
Sep 27 04:35:18.085: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8236/daemonsets","resourceVersion":"710700"},"items":null}

Sep 27 04:35:18.088: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8236/pods","resourceVersion":"710700"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:35:18.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8236" for this suite.

â€¢ [SLOW TEST:50.835 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":277,"completed":229,"skipped":3926,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:35:18.160: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9964
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating service nodeport-test with type=NodePort in namespace services-9964
STEP: creating replication controller nodeport-test in namespace services-9964
I0927 04:35:18.650654      23 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-9964, replica count: 2
I0927 04:35:21.702218      23 runners.go:190] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 04:35:24.702640      23 runners.go:190] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 04:35:27.702952      23 runners.go:190] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 27 04:35:30.703: INFO: Creating new exec pod
I0927 04:35:30.703254      23 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 27 04:35:40.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=services-9964 execpodcwcq4 -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Sep 27 04:35:40.370: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Sep 27 04:35:40.370: INFO: stdout: ""
Sep 27 04:35:40.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=services-9964 execpodcwcq4 -- /bin/sh -x -c nc -zv -t -w 2 172.31.95.110 80'
Sep 27 04:35:40.678: INFO: stderr: "+ nc -zv -t -w 2 172.31.95.110 80\nConnection to 172.31.95.110 80 port [tcp/http] succeeded!\n"
Sep 27 04:35:40.678: INFO: stdout: ""
Sep 27 04:35:40.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=services-9964 execpodcwcq4 -- /bin/sh -x -c nc -zv -t -w 2 10.6.171.84 30007'
Sep 27 04:35:40.973: INFO: stderr: "+ nc -zv -t -w 2 10.6.171.84 30007\nConnection to 10.6.171.84 30007 port [tcp/30007] succeeded!\n"
Sep 27 04:35:40.973: INFO: stdout: ""
Sep 27 04:35:40.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=services-9964 execpodcwcq4 -- /bin/sh -x -c nc -zv -t -w 2 10.6.171.85 30007'
Sep 27 04:35:41.267: INFO: stderr: "+ nc -zv -t -w 2 10.6.171.85 30007\nConnection to 10.6.171.85 30007 port [tcp/30007] succeeded!\n"
Sep 27 04:35:41.267: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:35:41.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9964" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

â€¢ [SLOW TEST:23.117 seconds]
[sig-network] Services
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":277,"completed":230,"skipped":3927,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:35:41.277: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3826
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 27 04:35:41.778: INFO: Number of nodes with available pods: 0
Sep 27 04:35:41.778: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:35:42.865: INFO: Number of nodes with available pods: 0
Sep 27 04:35:42.865: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:35:43.788: INFO: Number of nodes with available pods: 0
Sep 27 04:35:43.788: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:35:44.788: INFO: Number of nodes with available pods: 0
Sep 27 04:35:44.788: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:35:45.799: INFO: Number of nodes with available pods: 0
Sep 27 04:35:45.799: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:35:46.847: INFO: Number of nodes with available pods: 0
Sep 27 04:35:46.847: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:35:48.377: INFO: Number of nodes with available pods: 0
Sep 27 04:35:48.377: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:35:49.646: INFO: Number of nodes with available pods: 0
Sep 27 04:35:49.646: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:35:50.228: INFO: Number of nodes with available pods: 0
Sep 27 04:35:50.228: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:35:51.375: INFO: Number of nodes with available pods: 0
Sep 27 04:35:51.375: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:35:52.532: INFO: Number of nodes with available pods: 0
Sep 27 04:35:52.532: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:35:53.389: INFO: Number of nodes with available pods: 0
Sep 27 04:35:53.389: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:35:54.204: INFO: Number of nodes with available pods: 1
Sep 27 04:35:54.204: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:35:54.938: INFO: Number of nodes with available pods: 1
Sep 27 04:35:54.938: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:35:56.120: INFO: Number of nodes with available pods: 1
Sep 27 04:35:56.120: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:35:57.394: INFO: Number of nodes with available pods: 2
Sep 27 04:35:57.394: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:35:58.206: INFO: Number of nodes with available pods: 2
Sep 27 04:35:58.206: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:35:59.503: INFO: Number of nodes with available pods: 2
Sep 27 04:35:59.503: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:36:00.287: INFO: Number of nodes with available pods: 2
Sep 27 04:36:00.287: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:36:01.018: INFO: Number of nodes with available pods: 2
Sep 27 04:36:01.018: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:36:02.136: INFO: Number of nodes with available pods: 2
Sep 27 04:36:02.136: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:36:03.807: INFO: Number of nodes with available pods: 2
Sep 27 04:36:03.807: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:36:04.954: INFO: Number of nodes with available pods: 3
Sep 27 04:36:04.954: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Sep 27 04:36:05.680: INFO: Number of nodes with available pods: 2
Sep 27 04:36:05.680: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:36:06.737: INFO: Number of nodes with available pods: 2
Sep 27 04:36:06.737: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:36:08.244: INFO: Number of nodes with available pods: 2
Sep 27 04:36:08.244: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:36:09.296: INFO: Number of nodes with available pods: 2
Sep 27 04:36:09.296: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:36:10.176: INFO: Number of nodes with available pods: 2
Sep 27 04:36:10.176: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:36:12.115: INFO: Number of nodes with available pods: 2
Sep 27 04:36:12.115: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:36:12.705: INFO: Number of nodes with available pods: 2
Sep 27 04:36:12.705: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:36:13.873: INFO: Number of nodes with available pods: 2
Sep 27 04:36:13.873: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:36:14.808: INFO: Number of nodes with available pods: 2
Sep 27 04:36:14.808: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:36:15.869: INFO: Number of nodes with available pods: 2
Sep 27 04:36:15.869: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:36:16.877: INFO: Number of nodes with available pods: 2
Sep 27 04:36:16.877: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:36:17.764: INFO: Number of nodes with available pods: 2
Sep 27 04:36:17.764: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:36:19.147: INFO: Number of nodes with available pods: 2
Sep 27 04:36:19.147: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:36:19.693: INFO: Number of nodes with available pods: 2
Sep 27 04:36:19.694: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:36:20.890: INFO: Number of nodes with available pods: 2
Sep 27 04:36:20.890: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:36:21.925: INFO: Number of nodes with available pods: 2
Sep 27 04:36:21.925: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:36:23.049: INFO: Number of nodes with available pods: 2
Sep 27 04:36:23.050: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:36:23.777: INFO: Number of nodes with available pods: 2
Sep 27 04:36:23.777: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:36:24.847: INFO: Number of nodes with available pods: 2
Sep 27 04:36:24.850: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:36:25.757: INFO: Number of nodes with available pods: 2
Sep 27 04:36:25.757: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:36:26.739: INFO: Number of nodes with available pods: 3
Sep 27 04:36:26.739: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3826, will wait for the garbage collector to delete the pods
Sep 27 04:36:26.854: INFO: Deleting DaemonSet.extensions daemon-set took: 8.710515ms
Sep 27 04:36:27.591: INFO: Terminating DaemonSet.extensions daemon-set pods took: 737.278055ms
Sep 27 04:36:48.115: INFO: Number of nodes with available pods: 0
Sep 27 04:36:48.115: INFO: Number of running nodes: 0, number of available pods: 0
Sep 27 04:36:48.118: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3826/daemonsets","resourceVersion":"711176"},"items":null}

Sep 27 04:36:48.123: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3826/pods","resourceVersion":"711176"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:36:48.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3826" for this suite.

â€¢ [SLOW TEST:66.889 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":277,"completed":231,"skipped":3937,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:36:48.166: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-878
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-map-68d619f3-de52-4d2b-be9f-25b6bafd5da4
STEP: Creating a pod to test consume configMaps
Sep 27 04:36:49.219: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cb257f79-a011-4508-83d5-c5b96505fd17" in namespace "projected-878" to be "Succeeded or Failed"
Sep 27 04:36:49.301: INFO: Pod "pod-projected-configmaps-cb257f79-a011-4508-83d5-c5b96505fd17": Phase="Pending", Reason="", readiness=false. Elapsed: 82.35827ms
Sep 27 04:36:51.402: INFO: Pod "pod-projected-configmaps-cb257f79-a011-4508-83d5-c5b96505fd17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.183169481s
Sep 27 04:36:53.582: INFO: Pod "pod-projected-configmaps-cb257f79-a011-4508-83d5-c5b96505fd17": Phase="Pending", Reason="", readiness=false. Elapsed: 4.363474963s
Sep 27 04:36:55.596: INFO: Pod "pod-projected-configmaps-cb257f79-a011-4508-83d5-c5b96505fd17": Phase="Pending", Reason="", readiness=false. Elapsed: 6.377359258s
Sep 27 04:36:57.813: INFO: Pod "pod-projected-configmaps-cb257f79-a011-4508-83d5-c5b96505fd17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.594108312s
STEP: Saw pod success
Sep 27 04:36:57.813: INFO: Pod "pod-projected-configmaps-cb257f79-a011-4508-83d5-c5b96505fd17" satisfied condition "Succeeded or Failed"
Sep 27 04:36:58.230: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-projected-configmaps-cb257f79-a011-4508-83d5-c5b96505fd17 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 27 04:36:58.869: INFO: Waiting for pod pod-projected-configmaps-cb257f79-a011-4508-83d5-c5b96505fd17 to disappear
Sep 27 04:36:58.917: INFO: Pod pod-projected-configmaps-cb257f79-a011-4508-83d5-c5b96505fd17 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:36:58.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-878" for this suite.

â€¢ [SLOW TEST:10.765 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":277,"completed":232,"skipped":3942,"failed":0}
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:36:58.931: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6851
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 27 04:36:59.656: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a5dacbc7-e373-4cac-910a-32eb861c84ce" in namespace "projected-6851" to be "Succeeded or Failed"
Sep 27 04:36:59.692: INFO: Pod "downwardapi-volume-a5dacbc7-e373-4cac-910a-32eb861c84ce": Phase="Pending", Reason="", readiness=false. Elapsed: 35.977647ms
Sep 27 04:37:01.705: INFO: Pod "downwardapi-volume-a5dacbc7-e373-4cac-910a-32eb861c84ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0490181s
Sep 27 04:37:03.760: INFO: Pod "downwardapi-volume-a5dacbc7-e373-4cac-910a-32eb861c84ce": Phase="Pending", Reason="", readiness=false. Elapsed: 4.104268725s
Sep 27 04:37:05.930: INFO: Pod "downwardapi-volume-a5dacbc7-e373-4cac-910a-32eb861c84ce": Phase="Pending", Reason="", readiness=false. Elapsed: 6.274045916s
Sep 27 04:37:08.076: INFO: Pod "downwardapi-volume-a5dacbc7-e373-4cac-910a-32eb861c84ce": Phase="Pending", Reason="", readiness=false. Elapsed: 8.419739207s
Sep 27 04:37:10.518: INFO: Pod "downwardapi-volume-a5dacbc7-e373-4cac-910a-32eb861c84ce": Phase="Pending", Reason="", readiness=false. Elapsed: 10.861811785s
Sep 27 04:37:12.628: INFO: Pod "downwardapi-volume-a5dacbc7-e373-4cac-910a-32eb861c84ce": Phase="Pending", Reason="", readiness=false. Elapsed: 12.972525796s
Sep 27 04:37:14.634: INFO: Pod "downwardapi-volume-a5dacbc7-e373-4cac-910a-32eb861c84ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.978236418s
STEP: Saw pod success
Sep 27 04:37:14.634: INFO: Pod "downwardapi-volume-a5dacbc7-e373-4cac-910a-32eb861c84ce" satisfied condition "Succeeded or Failed"
Sep 27 04:37:14.637: INFO: Trying to get logs from node dce-10-6-171-86 pod downwardapi-volume-a5dacbc7-e373-4cac-910a-32eb861c84ce container client-container: <nil>
STEP: delete the pod
Sep 27 04:37:14.656: INFO: Waiting for pod downwardapi-volume-a5dacbc7-e373-4cac-910a-32eb861c84ce to disappear
Sep 27 04:37:14.695: INFO: Pod downwardapi-volume-a5dacbc7-e373-4cac-910a-32eb861c84ce no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:37:14.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6851" for this suite.

â€¢ [SLOW TEST:15.774 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":277,"completed":233,"skipped":3942,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:37:14.705: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3883
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3883.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3883.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3883.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3883.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3883.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3883.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3883.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3883.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3883.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3883.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3883.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 1.252.31.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.31.252.1_udp@PTR;check="$$(dig +tcp +noall +answer +search 1.252.31.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.31.252.1_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3883.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3883.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3883.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3883.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3883.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3883.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3883.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3883.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3883.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3883.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3883.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 1.252.31.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.31.252.1_udp@PTR;check="$$(dig +tcp +noall +answer +search 1.252.31.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.31.252.1_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 27 04:37:27.807: INFO: Unable to read wheezy_udp@dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:28.022: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:28.028: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:28.033: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:28.064: INFO: Unable to read jessie_udp@dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:28.068: INFO: Unable to read jessie_tcp@dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:28.073: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:28.079: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:28.179: INFO: Lookups using dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35 failed for: [wheezy_udp@dns-test-service.dns-3883.svc.cluster.local wheezy_tcp@dns-test-service.dns-3883.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local jessie_udp@dns-test-service.dns-3883.svc.cluster.local jessie_tcp@dns-test-service.dns-3883.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local]

Sep 27 04:37:33.335: INFO: Unable to read wheezy_udp@dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:33.472: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:33.625: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:33.629: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:34.243: INFO: Unable to read jessie_udp@dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:34.643: INFO: Unable to read jessie_tcp@dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:34.647: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:34.650: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:34.667: INFO: Lookups using dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35 failed for: [wheezy_udp@dns-test-service.dns-3883.svc.cluster.local wheezy_tcp@dns-test-service.dns-3883.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local jessie_udp@dns-test-service.dns-3883.svc.cluster.local jessie_tcp@dns-test-service.dns-3883.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local]

Sep 27 04:37:38.247: INFO: Unable to read wheezy_udp@dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:38.252: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:38.257: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:38.267: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:38.333: INFO: Unable to read jessie_udp@dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:38.348: INFO: Unable to read jessie_tcp@dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:38.363: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:38.375: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:38.409: INFO: Lookups using dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35 failed for: [wheezy_udp@dns-test-service.dns-3883.svc.cluster.local wheezy_tcp@dns-test-service.dns-3883.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local jessie_udp@dns-test-service.dns-3883.svc.cluster.local jessie_tcp@dns-test-service.dns-3883.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local]

Sep 27 04:37:43.191: INFO: Unable to read wheezy_udp@dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:43.199: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:43.204: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:43.209: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:43.247: INFO: Unable to read jessie_udp@dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:43.250: INFO: Unable to read jessie_tcp@dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:43.257: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:43.267: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:43.303: INFO: Lookups using dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35 failed for: [wheezy_udp@dns-test-service.dns-3883.svc.cluster.local wheezy_tcp@dns-test-service.dns-3883.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local jessie_udp@dns-test-service.dns-3883.svc.cluster.local jessie_tcp@dns-test-service.dns-3883.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local]

Sep 27 04:37:48.185: INFO: Unable to read wheezy_udp@dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:48.193: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:48.201: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:48.208: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:48.271: INFO: Unable to read jessie_udp@dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:48.275: INFO: Unable to read jessie_tcp@dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:48.286: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:48.292: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:48.355: INFO: Lookups using dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35 failed for: [wheezy_udp@dns-test-service.dns-3883.svc.cluster.local wheezy_tcp@dns-test-service.dns-3883.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local jessie_udp@dns-test-service.dns-3883.svc.cluster.local jessie_tcp@dns-test-service.dns-3883.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local]

Sep 27 04:37:53.184: INFO: Unable to read wheezy_udp@dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:53.189: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:53.195: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:53.200: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:53.234: INFO: Unable to read jessie_udp@dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:53.239: INFO: Unable to read jessie_tcp@dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:53.243: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:53.248: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local from pod dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35: the server could not find the requested resource (get pods dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35)
Sep 27 04:37:53.295: INFO: Lookups using dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35 failed for: [wheezy_udp@dns-test-service.dns-3883.svc.cluster.local wheezy_tcp@dns-test-service.dns-3883.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local jessie_udp@dns-test-service.dns-3883.svc.cluster.local jessie_tcp@dns-test-service.dns-3883.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3883.svc.cluster.local]

Sep 27 04:37:58.712: INFO: DNS probes using dns-3883/dns-test-8ab29da1-a9cf-4fd7-87d1-344962d6fc35 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:37:59.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3883" for this suite.

â€¢ [SLOW TEST:44.569 seconds]
[sig-network] DNS
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":277,"completed":234,"skipped":3943,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:37:59.274: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2513
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 04:38:00.563: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 27 04:38:02.995: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736778280, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736778280, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736778280, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736778280, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:38:05.081: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736778280, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736778280, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736778280, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736778280, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:38:07.361: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736778280, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736778280, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736778280, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736778280, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:38:09.162: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736778280, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736778280, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736778280, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736778280, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:38:11.036: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736778280, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736778280, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736778280, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736778280, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:38:13.314: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736778280, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736778280, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736778280, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736778280, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:38:15.448: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736778280, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736778280, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736778280, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736778280, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 04:38:18.231: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
Sep 27 04:38:19.480: INFO: Waiting for webhook configuration to be ready...
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:38:20.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2513" for this suite.
STEP: Destroying namespace "webhook-2513-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:23.497 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":277,"completed":235,"skipped":3958,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:38:22.772: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9352
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-projected-all-test-volume-ecb3ad96-f62b-4513-b348-15af67d44db1
STEP: Creating secret with name secret-projected-all-test-volume-9f1b1f28-3b39-4dbe-af2c-f87dca213c73
STEP: Creating a pod to test Check all projections for projected volume plugin
Sep 27 04:38:24.136: INFO: Waiting up to 5m0s for pod "projected-volume-28893bf4-4d7c-48a8-b821-41e083d12393" in namespace "projected-9352" to be "Succeeded or Failed"
Sep 27 04:38:24.317: INFO: Pod "projected-volume-28893bf4-4d7c-48a8-b821-41e083d12393": Phase="Pending", Reason="", readiness=false. Elapsed: 181.552373ms
Sep 27 04:38:26.464: INFO: Pod "projected-volume-28893bf4-4d7c-48a8-b821-41e083d12393": Phase="Pending", Reason="", readiness=false. Elapsed: 2.328507617s
Sep 27 04:38:28.519: INFO: Pod "projected-volume-28893bf4-4d7c-48a8-b821-41e083d12393": Phase="Pending", Reason="", readiness=false. Elapsed: 4.382962959s
Sep 27 04:38:30.617: INFO: Pod "projected-volume-28893bf4-4d7c-48a8-b821-41e083d12393": Phase="Pending", Reason="", readiness=false. Elapsed: 6.480947722s
Sep 27 04:38:32.721: INFO: Pod "projected-volume-28893bf4-4d7c-48a8-b821-41e083d12393": Phase="Pending", Reason="", readiness=false. Elapsed: 8.585510412s
Sep 27 04:38:34.755: INFO: Pod "projected-volume-28893bf4-4d7c-48a8-b821-41e083d12393": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.619255532s
STEP: Saw pod success
Sep 27 04:38:34.755: INFO: Pod "projected-volume-28893bf4-4d7c-48a8-b821-41e083d12393" satisfied condition "Succeeded or Failed"
Sep 27 04:38:34.822: INFO: Trying to get logs from node dce-10-6-171-86 pod projected-volume-28893bf4-4d7c-48a8-b821-41e083d12393 container projected-all-volume-test: <nil>
STEP: delete the pod
Sep 27 04:38:34.931: INFO: Waiting for pod projected-volume-28893bf4-4d7c-48a8-b821-41e083d12393 to disappear
Sep 27 04:38:35.211: INFO: Pod projected-volume-28893bf4-4d7c-48a8-b821-41e083d12393 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:38:35.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9352" for this suite.

â€¢ [SLOW TEST:12.515 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":277,"completed":236,"skipped":3994,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:38:35.288: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1912
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1418
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 27 04:38:35.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-1912'
Sep 27 04:38:36.327: INFO: stderr: ""
Sep 27 04:38:36.327: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1423
Sep 27 04:38:36.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 delete pods e2e-test-httpd-pod --namespace=kubectl-1912'
Sep 27 04:38:41.953: INFO: stderr: ""
Sep 27 04:38:41.953: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:38:41.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1912" for this suite.

â€¢ [SLOW TEST:6.888 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":277,"completed":237,"skipped":4049,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:38:42.177: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5410
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Sep 27 04:38:43.312: INFO: Waiting up to 5m0s for pod "downward-api-a414a007-87a5-494b-8ae8-619d2200e05a" in namespace "downward-api-5410" to be "Succeeded or Failed"
Sep 27 04:38:43.496: INFO: Pod "downward-api-a414a007-87a5-494b-8ae8-619d2200e05a": Phase="Pending", Reason="", readiness=false. Elapsed: 183.869389ms
Sep 27 04:38:45.625: INFO: Pod "downward-api-a414a007-87a5-494b-8ae8-619d2200e05a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.312817239s
Sep 27 04:38:47.690: INFO: Pod "downward-api-a414a007-87a5-494b-8ae8-619d2200e05a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.377969193s
Sep 27 04:38:49.713: INFO: Pod "downward-api-a414a007-87a5-494b-8ae8-619d2200e05a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.400966172s
Sep 27 04:38:51.726: INFO: Pod "downward-api-a414a007-87a5-494b-8ae8-619d2200e05a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.4143129s
Sep 27 04:38:53.730: INFO: Pod "downward-api-a414a007-87a5-494b-8ae8-619d2200e05a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.418530407s
Sep 27 04:38:55.752: INFO: Pod "downward-api-a414a007-87a5-494b-8ae8-619d2200e05a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.440481491s
Sep 27 04:38:57.760: INFO: Pod "downward-api-a414a007-87a5-494b-8ae8-619d2200e05a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.447745484s
Sep 27 04:38:59.763: INFO: Pod "downward-api-a414a007-87a5-494b-8ae8-619d2200e05a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.451274428s
STEP: Saw pod success
Sep 27 04:38:59.763: INFO: Pod "downward-api-a414a007-87a5-494b-8ae8-619d2200e05a" satisfied condition "Succeeded or Failed"
Sep 27 04:38:59.767: INFO: Trying to get logs from node dce-10-6-171-86 pod downward-api-a414a007-87a5-494b-8ae8-619d2200e05a container dapi-container: <nil>
STEP: delete the pod
Sep 27 04:39:00.164: INFO: Waiting for pod downward-api-a414a007-87a5-494b-8ae8-619d2200e05a to disappear
Sep 27 04:39:00.199: INFO: Pod downward-api-a414a007-87a5-494b-8ae8-619d2200e05a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:39:00.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5410" for this suite.

â€¢ [SLOW TEST:18.037 seconds]
[sig-node] Downward API
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":277,"completed":238,"skipped":4064,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:39:00.214: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-3873
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Sep 27 04:39:00.607: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:39:15.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3873" for this suite.

â€¢ [SLOW TEST:15.378 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":277,"completed":239,"skipped":4092,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:39:15.593: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7882
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Sep 27 04:39:16.020: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
Sep 27 04:39:22.482: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:39:44.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7882" for this suite.

â€¢ [SLOW TEST:28.749 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":277,"completed":240,"skipped":4120,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:39:44.342: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1903
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 27 04:39:51.902: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:39:52.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1903" for this suite.

â€¢ [SLOW TEST:7.792 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  blackbox test
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
    on terminated container
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:133
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":277,"completed":241,"skipped":4149,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:39:52.135: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4386
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-map-48cca599-5974-4831-b786-9a9d7178360d
STEP: Creating a pod to test consume configMaps
Sep 27 04:39:53.010: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1d3725e9-3a5d-4f37-acaa-a4a9af46cb3b" in namespace "projected-4386" to be "Succeeded or Failed"
Sep 27 04:39:53.015: INFO: Pod "pod-projected-configmaps-1d3725e9-3a5d-4f37-acaa-a4a9af46cb3b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.426651ms
Sep 27 04:39:55.306: INFO: Pod "pod-projected-configmaps-1d3725e9-3a5d-4f37-acaa-a4a9af46cb3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.296055137s
Sep 27 04:39:57.442: INFO: Pod "pod-projected-configmaps-1d3725e9-3a5d-4f37-acaa-a4a9af46cb3b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.431542012s
Sep 27 04:39:59.627: INFO: Pod "pod-projected-configmaps-1d3725e9-3a5d-4f37-acaa-a4a9af46cb3b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.616465237s
Sep 27 04:40:01.682: INFO: Pod "pod-projected-configmaps-1d3725e9-3a5d-4f37-acaa-a4a9af46cb3b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.672072291s
Sep 27 04:40:03.819: INFO: Pod "pod-projected-configmaps-1d3725e9-3a5d-4f37-acaa-a4a9af46cb3b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.808336791s
Sep 27 04:40:05.988: INFO: Pod "pod-projected-configmaps-1d3725e9-3a5d-4f37-acaa-a4a9af46cb3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.977483895s
STEP: Saw pod success
Sep 27 04:40:05.988: INFO: Pod "pod-projected-configmaps-1d3725e9-3a5d-4f37-acaa-a4a9af46cb3b" satisfied condition "Succeeded or Failed"
Sep 27 04:40:05.995: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-projected-configmaps-1d3725e9-3a5d-4f37-acaa-a4a9af46cb3b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 27 04:40:06.135: INFO: Waiting for pod pod-projected-configmaps-1d3725e9-3a5d-4f37-acaa-a4a9af46cb3b to disappear
Sep 27 04:40:06.232: INFO: Pod pod-projected-configmaps-1d3725e9-3a5d-4f37-acaa-a4a9af46cb3b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:40:06.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4386" for this suite.

â€¢ [SLOW TEST:14.121 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":242,"skipped":4171,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:40:06.256: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4031
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:40:52.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4031" for this suite.

â€¢ [SLOW TEST:46.501 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  blackbox test
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
    when starting a container that exits
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":277,"completed":243,"skipped":4189,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:40:52.758: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-338
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Sep 27 04:40:53.063: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 27 04:40:53.126: INFO: Waiting for terminating namespaces to be deleted...
Sep 27 04:40:53.130: INFO: 
Logging pods the kubelet thinks is on node dce-10-6-171-84 before test
Sep 27 04:40:53.187: INFO: coredns-coredns-7899d777b7-d9xwj from kube-system started at 2020-09-25 10:06:04 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.187: INFO: 	Container coredns-coredns ready: true, restart count 0
Sep 27 04:40:53.187: INFO: dce-kube-apiserver-proxy-dce-10-6-171-84 from kube-system started at 2020-09-24 09:48:25 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.187: INFO: 	Container dce-kube-apiserver-proxy ready: true, restart count 2
Sep 27 04:40:53.187: INFO: dce-engine-8sng6 from kube-system started at 2020-09-24 09:40:03 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.187: INFO: 	Container dce-engine ready: true, restart count 1
Sep 27 04:40:53.187: INFO: release-2048-dao-2048-6ff5c774d9-txzdv from default started at 2020-09-25 10:06:04 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.187: INFO: 	Container dao-2048 ready: true, restart count 0
Sep 27 04:40:53.187: INFO: dce-system-dnsservice-74c694858b-c7b2w from dce-system started at 2020-09-25 01:45:36 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.187: INFO: 	Container dce-system-dnsservice ready: true, restart count 0
Sep 27 04:40:53.187: INFO: kube-proxy-6qqrv from kube-system started at 2020-09-24 09:40:05 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.187: INFO: 	Container kube-proxy ready: true, restart count 1
Sep 27 04:40:53.187: INFO: dao-2048-dao-2048-db8cd6864-g8txm from default started at 2020-09-25 10:06:04 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.187: INFO: 	Container dao-2048-dao-2048 ready: true, restart count 0
Sep 27 04:40:53.187: INFO: dce-parcel-agent-85gws from kube-system started at 2020-09-24 09:40:06 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.187: INFO: 	Container dce-parcel-agent ready: true, restart count 4
Sep 27 04:40:53.187: INFO: coredns-coredns-7899d777b7-tpxhd from kube-system started at 2020-09-25 02:24:32 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.187: INFO: 	Container coredns-coredns ready: true, restart count 0
Sep 27 04:40:53.187: INFO: uds-host-metrics-dirver-r7msd from storage-system started at 2020-09-25 01:46:24 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.187: INFO: 	Container metrics-collector ready: true, restart count 0
Sep 27 04:40:53.187: INFO: node-local-dns-hh96g from kube-system started at 2020-09-24 10:25:26 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.187: INFO: 	Container node-cache ready: true, restart count 0
Sep 27 04:40:53.187: INFO: calico-node-7lk8x from kube-system started at 2020-09-24 09:40:07 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.187: INFO: 	Container calico-node ready: true, restart count 1
Sep 27 04:40:53.187: INFO: sonobuoy-systemd-logs-daemon-set-da170140ad3f4ead-glbd8 from sonobuoy started at 2020-09-27 02:39:45 +0000 UTC (2 container statuses recorded)
Sep 27 04:40:53.187: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Sep 27 04:40:53.188: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 04:40:53.188: INFO: aaa-dao-2048-7897cf64bc-krl9t from default started at 2020-09-25 10:06:04 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.188: INFO: 	Container dao-2048 ready: true, restart count 0
Sep 27 04:40:53.188: INFO: 
Logging pods the kubelet thinks is on node dce-10-6-171-85 before test
Sep 27 04:40:53.231: INFO: dce-prometheus-5dc44cf4dd-jjsjl from kube-system started at 2020-09-24 09:15:28 +0000 UTC (2 container statuses recorded)
Sep 27 04:40:53.231: INFO: 	Container dce-metrics-server ready: true, restart count 3
Sep 27 04:40:53.231: INFO: 	Container dce-prometheus ready: true, restart count 3
Sep 27 04:40:53.231: INFO: kube-proxy-mbrkq from kube-system started at 2020-09-24 09:12:07 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.231: INFO: 	Container kube-proxy ready: true, restart count 3
Sep 27 04:40:53.231: INFO: dce-parcel-agent-ndwd9 from kube-system started at 2020-09-24 09:12:07 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.231: INFO: 	Container dce-parcel-agent ready: true, restart count 5
Sep 27 04:40:53.231: INFO: calico-kube-controllers-7449c6cbb8-bq8n5 from kube-system started at 2020-09-24 09:15:06 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.231: INFO: 	Container calico-kube-controllers ready: true, restart count 5
Sep 27 04:40:53.231: INFO: dce-chart-manager-68fbf6bb4c-7288c from kube-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.231: INFO: 	Container chart-manager ready: true, restart count 3
Sep 27 04:40:53.231: INFO: dce-engine-hfc8k from kube-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.231: INFO: 	Container dce-engine ready: true, restart count 3
Sep 27 04:40:53.231: INFO: dce-core-keepalived-59dd799677-m42wb from kube-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.231: INFO: 	Container dce-core-keepalived ready: true, restart count 3
Sep 27 04:40:53.231: INFO: uds-storage-server-5f7b5f6fdc-q7kmm from storage-system started at 2020-09-25 02:32:02 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.231: INFO: 	Container main ready: false, restart count 2
Sep 27 04:40:53.231: INFO: dce-etcd-dce-10-6-171-85 from kube-system started at 2020-09-24 10:36:41 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.231: INFO: 	Container dce-etcd ready: true, restart count 3
Sep 27 04:40:53.231: INFO: dce-kube-apiserver-proxy-dce-10-6-171-85 from kube-system started at 2020-09-24 10:36:43 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.231: INFO: 	Container dce-kube-apiserver-proxy ready: true, restart count 3
Sep 27 04:40:53.231: INFO: uds-storage-server-5f7b5f6fdc-r46qt from storage-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.231: INFO: 	Container main ready: true, restart count 3
Sep 27 04:40:53.231: INFO: sonobuoy-systemd-logs-daemon-set-da170140ad3f4ead-2mrkt from sonobuoy started at 2020-09-27 02:39:45 +0000 UTC (2 container statuses recorded)
Sep 27 04:40:53.231: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Sep 27 04:40:53.231: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 04:40:53.231: INFO: metrics-server-6768fdf9db-56ks5 from kube-system started at 2020-09-24 09:15:28 +0000 UTC (2 container statuses recorded)
Sep 27 04:40:53.231: INFO: 	Container metrics-server ready: true, restart count 3
Sep 27 04:40:53.231: INFO: 	Container metrics-server-nanny ready: true, restart count 3
Sep 27 04:40:53.231: INFO: dce-registry-6cc79fc8c-fpp82 from kube-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.231: INFO: 	Container dce-registry ready: true, restart count 3
Sep 27 04:40:53.231: INFO: dce-kube-scheduler-dce-10-6-171-85 from kube-system started at 2020-09-24 10:36:45 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.231: INFO: 	Container dce-kube-scheduler ready: true, restart count 4
Sep 27 04:40:53.231: INFO: uds-host-metrics-dirver-884nn from storage-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.231: INFO: 	Container metrics-collector ready: true, restart count 3
Sep 27 04:40:53.231: INFO: dce-kube-controller-manager-dce-10-6-171-85 from kube-system started at 2020-09-24 10:36:44 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.231: INFO: 	Container dce-kube-controller-manager ready: true, restart count 4
Sep 27 04:40:53.231: INFO: calico-node-pcdmf from kube-system started at 2020-09-24 09:15:05 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.231: INFO: 	Container calico-node ready: true, restart count 3
Sep 27 04:40:53.231: INFO: dce-system-uds-58c4c68dbf-bbmdk from dce-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.231: INFO: 	Container dce-system-uds ready: false, restart count 5
Sep 27 04:40:53.231: INFO: dce-kube-apiserver-dce-10-6-171-85 from kube-system started at 2020-09-24 10:36:42 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.231: INFO: 	Container dce-kube-apiserver ready: true, restart count 0
Sep 27 04:40:53.231: INFO: dce-parcel-server-rwgb8 from kube-system started at 2020-09-24 09:12:07 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.231: INFO: 	Container dce-parcel-server ready: true, restart count 5
Sep 27 04:40:53.231: INFO: dce-system-uds-58c4c68dbf-g8fb6 from dce-system started at 2020-09-24 09:15:28 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.231: INFO: 	Container dce-system-uds ready: true, restart count 3
Sep 27 04:40:53.231: INFO: node-local-dns-9lnth from kube-system started at 2020-09-24 10:25:26 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.231: INFO: 	Container node-cache ready: true, restart count 1
Sep 27 04:40:53.231: INFO: 
Logging pods the kubelet thinks is on node dce-10-6-171-86 before test
Sep 27 04:40:53.246: INFO: dce-kube-apiserver-proxy-dce-10-6-171-86 from kube-system started at 2020-09-24 09:47:03 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.246: INFO: 	Container dce-kube-apiserver-proxy ready: true, restart count 1
Sep 27 04:40:53.246: INFO: sonobuoy from sonobuoy started at 2020-09-27 02:39:34 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.246: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 27 04:40:53.246: INFO: dce-parcel-agent-p94cd from kube-system started at 2020-09-27 04:25:17 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.246: INFO: 	Container dce-parcel-agent ready: true, restart count 0
Sep 27 04:40:53.246: INFO: uds-host-metrics-dirver-48m2w from storage-system started at 2020-09-25 01:46:33 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.246: INFO: 	Container metrics-collector ready: true, restart count 0
Sep 27 04:40:53.246: INFO: sonobuoy-systemd-logs-daemon-set-da170140ad3f4ead-fbqzn from sonobuoy started at 2020-09-27 02:39:45 +0000 UTC (2 container statuses recorded)
Sep 27 04:40:53.246: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Sep 27 04:40:53.246: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 04:40:53.246: INFO: kube-proxy-wjt5k from kube-system started at 2020-09-24 09:35:13 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.246: INFO: 	Container kube-proxy ready: true, restart count 1
Sep 27 04:40:53.246: INFO: dce-engine-dvcxl from kube-system started at 2020-09-24 09:35:13 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.246: INFO: 	Container dce-engine ready: true, restart count 1
Sep 27 04:40:53.246: INFO: sonobuoy-e2e-job-f515d88659da49be from sonobuoy started at 2020-09-27 02:39:45 +0000 UTC (2 container statuses recorded)
Sep 27 04:40:53.246: INFO: 	Container e2e ready: true, restart count 0
Sep 27 04:40:53.246: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 04:40:53.246: INFO: calico-node-hsm9f from kube-system started at 2020-09-24 09:35:13 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.246: INFO: 	Container calico-node ready: true, restart count 1
Sep 27 04:40:53.246: INFO: node-local-dns-pj747 from kube-system started at 2020-09-27 04:25:17 +0000 UTC (1 container statuses recorded)
Sep 27 04:40:53.246: INFO: 	Container node-cache ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-5980559a-08d7-4250-b2d2-c23f6b6eca4a 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-5980559a-08d7-4250-b2d2-c23f6b6eca4a off the node dce-10-6-171-86
STEP: verifying the node doesn't have the label kubernetes.io/e2e-5980559a-08d7-4250-b2d2-c23f6b6eca4a
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:41:43.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-338" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

â€¢ [SLOW TEST:51.348 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":277,"completed":244,"skipped":4207,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:41:44.107: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9341
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:41:50.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9341" for this suite.

â€¢ [SLOW TEST:6.408 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when scheduling a busybox command in a pod
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:41
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":277,"completed":245,"skipped":4233,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:41:50.515: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-6883
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 04:41:50.836: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Creating first CR 
Sep 27 04:41:51.605: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-09-27T04:41:51Z generation:1 name:name1 resourceVersion:712788 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:f98e60e0-c800-4f9c-ad78-e56f842781c7] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Sep 27 04:42:01.611: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-09-27T04:42:01Z generation:1 name:name2 resourceVersion:712840 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:1a256f30-e003-496a-87fd-675b29ab39a7] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Sep 27 04:42:11.616: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-09-27T04:41:51Z generation:2 name:name1 resourceVersion:712870 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:f98e60e0-c800-4f9c-ad78-e56f842781c7] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Sep 27 04:42:21.622: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-09-27T04:42:01Z generation:2 name:name2 resourceVersion:712901 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:1a256f30-e003-496a-87fd-675b29ab39a7] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Sep 27 04:42:31.628: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-09-27T04:41:51Z generation:2 name:name1 resourceVersion:712931 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:f98e60e0-c800-4f9c-ad78-e56f842781c7] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Sep 27 04:42:41.640: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-09-27T04:42:01Z generation:2 name:name2 resourceVersion:712966 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:1a256f30-e003-496a-87fd-675b29ab39a7] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:42:52.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-6883" for this suite.

â€¢ [SLOW TEST:62.077 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":277,"completed":246,"skipped":4246,"failed":0}
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:42:52.592: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1647
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: validating cluster-info
Sep 27 04:42:53.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 cluster-info'
Sep 27 04:42:58.227: INFO: stderr: ""
Sep 27 04:42:58.227: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://172.31.0.1:443\x1b[0m\n\x1b[0;32mcoredns\x1b[0m is running at \x1b[0;33mhttps://172.31.0.1:443/api/v1/namespaces/kube-system/services/coredns-coredns:dns/proxy\x1b[0m\n\x1b[0;32mcoredns-coredns-second\x1b[0m is running at \x1b[0;33mhttps://172.31.0.1:443/api/v1/namespaces/kube-system/services/coredns-coredns-second:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:42:58.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1647" for this suite.

â€¢ [SLOW TEST:5.648 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:952
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":277,"completed":247,"skipped":4246,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:42:58.241: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2641
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod test-webserver-ea3d79c8-f11b-4a9c-9239-3b02532a630b in namespace container-probe-2641
Sep 27 04:43:10.583: INFO: Started pod test-webserver-ea3d79c8-f11b-4a9c-9239-3b02532a630b in namespace container-probe-2641
STEP: checking the pod's current state and verifying that restartCount is present
Sep 27 04:43:10.590: INFO: Initial restart count of pod test-webserver-ea3d79c8-f11b-4a9c-9239-3b02532a630b is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:47:12.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2641" for this suite.

â€¢ [SLOW TEST:254.348 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":277,"completed":248,"skipped":4267,"failed":0}
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:47:12.589: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4800
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-4800
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating statefulset ss in namespace statefulset-4800
Sep 27 04:47:13.041: INFO: Found 0 stateful pods, waiting for 1
Sep 27 04:47:23.049: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Sep 27 04:47:33.046: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Sep 27 04:47:33.065: INFO: Deleting all statefulset in ns statefulset-4800
Sep 27 04:47:33.071: INFO: Scaling statefulset ss to 0
Sep 27 04:48:03.142: INFO: Waiting for statefulset status.replicas updated to 0
Sep 27 04:48:03.145: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:48:03.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4800" for this suite.

â€¢ [SLOW TEST:50.631 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":277,"completed":249,"skipped":4272,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:48:03.220: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7765
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Sep 27 04:48:03.534: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7765 /api/v1/namespaces/watch-7765/configmaps/e2e-watch-test-configmap-a 9ce03922-ffc5-4d84-a1a2-aef0cf1dff4d 714006 0 2020-09-27 04:48:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 04:48:03.535: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7765 /api/v1/namespaces/watch-7765/configmaps/e2e-watch-test-configmap-a 9ce03922-ffc5-4d84-a1a2-aef0cf1dff4d 714006 0 2020-09-27 04:48:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Sep 27 04:48:13.557: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7765 /api/v1/namespaces/watch-7765/configmaps/e2e-watch-test-configmap-a 9ce03922-ffc5-4d84-a1a2-aef0cf1dff4d 714060 0 2020-09-27 04:48:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 04:48:13.557: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7765 /api/v1/namespaces/watch-7765/configmaps/e2e-watch-test-configmap-a 9ce03922-ffc5-4d84-a1a2-aef0cf1dff4d 714060 0 2020-09-27 04:48:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Sep 27 04:48:23.564: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7765 /api/v1/namespaces/watch-7765/configmaps/e2e-watch-test-configmap-a 9ce03922-ffc5-4d84-a1a2-aef0cf1dff4d 714085 0 2020-09-27 04:48:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 04:48:23.564: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7765 /api/v1/namespaces/watch-7765/configmaps/e2e-watch-test-configmap-a 9ce03922-ffc5-4d84-a1a2-aef0cf1dff4d 714085 0 2020-09-27 04:48:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Sep 27 04:48:33.572: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7765 /api/v1/namespaces/watch-7765/configmaps/e2e-watch-test-configmap-a 9ce03922-ffc5-4d84-a1a2-aef0cf1dff4d 714114 0 2020-09-27 04:48:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 04:48:33.572: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7765 /api/v1/namespaces/watch-7765/configmaps/e2e-watch-test-configmap-a 9ce03922-ffc5-4d84-a1a2-aef0cf1dff4d 714114 0 2020-09-27 04:48:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Sep 27 04:48:43.609: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7765 /api/v1/namespaces/watch-7765/configmaps/e2e-watch-test-configmap-b ab638da1-e9af-4d86-8c57-c42f66236689 714141 0 2020-09-27 04:48:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 04:48:43.609: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7765 /api/v1/namespaces/watch-7765/configmaps/e2e-watch-test-configmap-b ab638da1-e9af-4d86-8c57-c42f66236689 714141 0 2020-09-27 04:48:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Sep 27 04:48:53.614: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7765 /api/v1/namespaces/watch-7765/configmaps/e2e-watch-test-configmap-b ab638da1-e9af-4d86-8c57-c42f66236689 714172 0 2020-09-27 04:48:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 04:48:53.614: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7765 /api/v1/namespaces/watch-7765/configmaps/e2e-watch-test-configmap-b ab638da1-e9af-4d86-8c57-c42f66236689 714172 0 2020-09-27 04:48:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:49:03.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7765" for this suite.

â€¢ [SLOW TEST:60.404 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":277,"completed":250,"skipped":4291,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:49:03.624: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2604
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Starting the proxy
Sep 27 04:49:03.819: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-085554482 proxy --unix-socket=/tmp/kubectl-proxy-unix803685817/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:49:03.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2604" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":277,"completed":251,"skipped":4310,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:49:03.903: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7597
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7597.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7597.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 27 04:49:16.273: INFO: DNS probes using dns-7597/dns-test-23d70d3f-a8a8-458b-90c6-281b24bc9f6c succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:49:16.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7597" for this suite.

â€¢ [SLOW TEST:12.664 seconds]
[sig-network] DNS
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":277,"completed":252,"skipped":4361,"failed":0}
SSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:49:16.567: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-8445
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 04:49:16.891: INFO: Creating ReplicaSet my-hostname-basic-80b5f418-a87e-4742-91b4-5f3929c539b8
Sep 27 04:49:16.901: INFO: Pod name my-hostname-basic-80b5f418-a87e-4742-91b4-5f3929c539b8: Found 0 pods out of 1
Sep 27 04:49:21.982: INFO: Pod name my-hostname-basic-80b5f418-a87e-4742-91b4-5f3929c539b8: Found 1 pods out of 1
Sep 27 04:49:21.982: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-80b5f418-a87e-4742-91b4-5f3929c539b8" is running
Sep 27 04:49:28.027: INFO: Pod "my-hostname-basic-80b5f418-a87e-4742-91b4-5f3929c539b8-82nhc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-27 04:49:16 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-27 04:49:16 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-80b5f418-a87e-4742-91b4-5f3929c539b8]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-27 04:49:16 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-80b5f418-a87e-4742-91b4-5f3929c539b8]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-27 04:49:16 +0000 UTC Reason: Message:}])
Sep 27 04:49:28.027: INFO: Trying to dial the pod
Sep 27 04:49:34.369: INFO: Controller my-hostname-basic-80b5f418-a87e-4742-91b4-5f3929c539b8: Got expected result from replica 1 [my-hostname-basic-80b5f418-a87e-4742-91b4-5f3929c539b8-82nhc]: "my-hostname-basic-80b5f418-a87e-4742-91b4-5f3929c539b8-82nhc", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:49:34.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8445" for this suite.

â€¢ [SLOW TEST:17.911 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":277,"completed":253,"skipped":4364,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:49:34.479: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3172
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: starting the proxy server
Sep 27 04:49:35.836: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-085554482 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:49:35.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3172" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":277,"completed":254,"skipped":4392,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:49:35.946: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8411
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8411
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8411
STEP: creating replication controller externalsvc in namespace services-8411
I0927 04:49:37.171390      23 runners.go:190] Created replication controller with name: externalsvc, namespace: services-8411, replica count: 2
I0927 04:49:40.358448      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 04:49:43.425381      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 04:49:46.425629      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 04:49:49.425911      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 04:49:52.426266      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Sep 27 04:49:52.459: INFO: Creating new exec pod
Sep 27 04:50:02.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=services-8411 execpodgsm6s -- /bin/sh -x -c nslookup clusterip-service'
Sep 27 04:50:02.830: INFO: stderr: "+ nslookup clusterip-service\n"
Sep 27 04:50:02.830: INFO: stdout: "Server:\t\t172.31.0.10\nAddress:\t172.31.0.10#53\n\nclusterip-service.services-8411.svc.cluster.local\tcanonical name = externalsvc.services-8411.svc.cluster.local.\nName:\texternalsvc.services-8411.svc.cluster.local\nAddress: 172.31.216.170\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8411, will wait for the garbage collector to delete the pods
Sep 27 04:50:02.890: INFO: Deleting ReplicationController externalsvc took: 6.563977ms
Sep 27 04:50:03.900: INFO: Terminating ReplicationController externalsvc pods took: 1.009880867s
Sep 27 04:50:17.771: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:50:17.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8411" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

â€¢ [SLOW TEST:41.915 seconds]
[sig-network] Services
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":277,"completed":255,"skipped":4401,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:50:17.862: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9767
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 04:50:18.804: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Sep 27 04:50:23.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 --namespace=crd-publish-openapi-9767 create -f -'
Sep 27 04:50:27.188: INFO: stderr: ""
Sep 27 04:50:27.188: INFO: stdout: "e2e-test-crd-publish-openapi-7485-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep 27 04:50:27.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 --namespace=crd-publish-openapi-9767 delete e2e-test-crd-publish-openapi-7485-crds test-foo'
Sep 27 04:50:27.471: INFO: stderr: ""
Sep 27 04:50:27.471: INFO: stdout: "e2e-test-crd-publish-openapi-7485-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Sep 27 04:50:27.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 --namespace=crd-publish-openapi-9767 apply -f -'
Sep 27 04:50:27.805: INFO: stderr: ""
Sep 27 04:50:27.805: INFO: stdout: "e2e-test-crd-publish-openapi-7485-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep 27 04:50:27.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 --namespace=crd-publish-openapi-9767 delete e2e-test-crd-publish-openapi-7485-crds test-foo'
Sep 27 04:50:27.991: INFO: stderr: ""
Sep 27 04:50:27.991: INFO: stdout: "e2e-test-crd-publish-openapi-7485-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Sep 27 04:50:27.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 --namespace=crd-publish-openapi-9767 create -f -'
Sep 27 04:50:28.650: INFO: rc: 1
Sep 27 04:50:28.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 --namespace=crd-publish-openapi-9767 apply -f -'
Sep 27 04:50:28.956: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Sep 27 04:50:28.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 --namespace=crd-publish-openapi-9767 create -f -'
Sep 27 04:50:29.327: INFO: rc: 1
Sep 27 04:50:29.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 --namespace=crd-publish-openapi-9767 apply -f -'
Sep 27 04:50:29.591: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Sep 27 04:50:29.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 explain e2e-test-crd-publish-openapi-7485-crds'
Sep 27 04:50:31.000: INFO: stderr: ""
Sep 27 04:50:31.000: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7485-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Sep 27 04:50:31.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 explain e2e-test-crd-publish-openapi-7485-crds.metadata'
Sep 27 04:50:32.662: INFO: stderr: ""
Sep 27 04:50:32.662: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7485-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Sep 27 04:50:32.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 explain e2e-test-crd-publish-openapi-7485-crds.spec'
Sep 27 04:50:35.247: INFO: stderr: ""
Sep 27 04:50:35.247: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7485-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Sep 27 04:50:35.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 explain e2e-test-crd-publish-openapi-7485-crds.spec.bars'
Sep 27 04:50:36.675: INFO: stderr: ""
Sep 27 04:50:36.675: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7485-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Sep 27 04:50:36.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 explain e2e-test-crd-publish-openapi-7485-crds.spec.bars2'
Sep 27 04:50:38.778: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:50:43.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9767" for this suite.

â€¢ [SLOW TEST:25.444 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":277,"completed":256,"skipped":4401,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:50:43.307: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-227
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service externalname-service with the type=ExternalName in namespace services-227
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-227
I0927 04:50:45.895390      23 runners.go:190] Created replication controller with name: externalname-service, namespace: services-227, replica count: 2
I0927 04:50:48.946154      23 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 04:50:51.946458      23 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 27 04:50:54.947: INFO: Creating new exec pod
I0927 04:50:54.947526      23 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 27 04:51:03.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=services-227 execpodw8c6g -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Sep 27 04:51:04.554: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep 27 04:51:04.554: INFO: stdout: ""
Sep 27 04:51:04.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=services-227 execpodw8c6g -- /bin/sh -x -c nc -zv -t -w 2 172.31.47.208 80'
Sep 27 04:51:04.882: INFO: stderr: "+ nc -zv -t -w 2 172.31.47.208 80\nConnection to 172.31.47.208 80 port [tcp/http] succeeded!\n"
Sep 27 04:51:04.882: INFO: stdout: ""
Sep 27 04:51:04.882: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:51:04.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-227" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

â€¢ [SLOW TEST:21.688 seconds]
[sig-network] Services
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":277,"completed":257,"skipped":4421,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:51:04.995: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2883
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-c7d7cfab-88eb-4c23-be4e-6213890b3182
STEP: Creating a pod to test consume configMaps
Sep 27 04:51:05.349: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-22f5923b-6366-450c-a9c3-2089200d836b" in namespace "projected-2883" to be "Succeeded or Failed"
Sep 27 04:51:05.487: INFO: Pod "pod-projected-configmaps-22f5923b-6366-450c-a9c3-2089200d836b": Phase="Pending", Reason="", readiness=false. Elapsed: 137.779967ms
Sep 27 04:51:07.498: INFO: Pod "pod-projected-configmaps-22f5923b-6366-450c-a9c3-2089200d836b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.149311794s
Sep 27 04:51:09.512: INFO: Pod "pod-projected-configmaps-22f5923b-6366-450c-a9c3-2089200d836b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.16290326s
Sep 27 04:51:11.516: INFO: Pod "pod-projected-configmaps-22f5923b-6366-450c-a9c3-2089200d836b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.16732563s
STEP: Saw pod success
Sep 27 04:51:11.516: INFO: Pod "pod-projected-configmaps-22f5923b-6366-450c-a9c3-2089200d836b" satisfied condition "Succeeded or Failed"
Sep 27 04:51:11.522: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-projected-configmaps-22f5923b-6366-450c-a9c3-2089200d836b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 27 04:51:11.689: INFO: Waiting for pod pod-projected-configmaps-22f5923b-6366-450c-a9c3-2089200d836b to disappear
Sep 27 04:51:11.693: INFO: Pod pod-projected-configmaps-22f5923b-6366-450c-a9c3-2089200d836b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:51:11.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2883" for this suite.

â€¢ [SLOW TEST:6.711 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":258,"skipped":4454,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:51:11.707: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6745
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep 27 04:51:12.050: INFO: Waiting up to 5m0s for pod "pod-70af8658-63d0-471e-af29-c1d707df728e" in namespace "emptydir-6745" to be "Succeeded or Failed"
Sep 27 04:51:12.108: INFO: Pod "pod-70af8658-63d0-471e-af29-c1d707df728e": Phase="Pending", Reason="", readiness=false. Elapsed: 57.862359ms
Sep 27 04:51:14.114: INFO: Pod "pod-70af8658-63d0-471e-af29-c1d707df728e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063729951s
Sep 27 04:51:16.125: INFO: Pod "pod-70af8658-63d0-471e-af29-c1d707df728e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.074881405s
Sep 27 04:51:18.324: INFO: Pod "pod-70af8658-63d0-471e-af29-c1d707df728e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.273439574s
Sep 27 04:51:20.521: INFO: Pod "pod-70af8658-63d0-471e-af29-c1d707df728e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.470968227s
Sep 27 04:51:22.526: INFO: Pod "pod-70af8658-63d0-471e-af29-c1d707df728e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.475411477s
Sep 27 04:51:24.532: INFO: Pod "pod-70af8658-63d0-471e-af29-c1d707df728e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.482242178s
Sep 27 04:51:26.536: INFO: Pod "pod-70af8658-63d0-471e-af29-c1d707df728e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.485753433s
STEP: Saw pod success
Sep 27 04:51:26.536: INFO: Pod "pod-70af8658-63d0-471e-af29-c1d707df728e" satisfied condition "Succeeded or Failed"
Sep 27 04:51:26.539: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-70af8658-63d0-471e-af29-c1d707df728e container test-container: <nil>
STEP: delete the pod
Sep 27 04:51:26.568: INFO: Waiting for pod pod-70af8658-63d0-471e-af29-c1d707df728e to disappear
Sep 27 04:51:26.575: INFO: Pod pod-70af8658-63d0-471e-af29-c1d707df728e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:51:26.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6745" for this suite.

â€¢ [SLOW TEST:14.877 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":259,"skipped":4491,"failed":0}
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:51:26.584: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6253
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 04:51:26.881: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Sep 27 04:51:26.917: INFO: Number of nodes with available pods: 0
Sep 27 04:51:26.917: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:51:27.977: INFO: Number of nodes with available pods: 0
Sep 27 04:51:27.978: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:51:28.928: INFO: Number of nodes with available pods: 0
Sep 27 04:51:28.928: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:51:30.248: INFO: Number of nodes with available pods: 0
Sep 27 04:51:30.248: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:51:31.080: INFO: Number of nodes with available pods: 0
Sep 27 04:51:31.080: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:51:32.238: INFO: Number of nodes with available pods: 0
Sep 27 04:51:32.238: INFO: Node dce-10-6-171-84 is running more than one daemon pod
Sep 27 04:51:34.248: INFO: Number of nodes with available pods: 1
Sep 27 04:51:34.248: INFO: Node dce-10-6-171-85 is running more than one daemon pod
Sep 27 04:51:35.132: INFO: Number of nodes with available pods: 1
Sep 27 04:51:35.132: INFO: Node dce-10-6-171-85 is running more than one daemon pod
Sep 27 04:51:36.665: INFO: Number of nodes with available pods: 1
Sep 27 04:51:36.665: INFO: Node dce-10-6-171-85 is running more than one daemon pod
Sep 27 04:51:36.940: INFO: Number of nodes with available pods: 1
Sep 27 04:51:36.940: INFO: Node dce-10-6-171-85 is running more than one daemon pod
Sep 27 04:51:37.928: INFO: Number of nodes with available pods: 1
Sep 27 04:51:37.928: INFO: Node dce-10-6-171-85 is running more than one daemon pod
Sep 27 04:51:39.166: INFO: Number of nodes with available pods: 2
Sep 27 04:51:39.166: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:51:39.927: INFO: Number of nodes with available pods: 3
Sep 27 04:51:39.927: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Sep 27 04:51:40.250: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:40.250: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:40.250: INFO: Wrong image for pod: daemon-set-gbx2v. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:41.335: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:41.335: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:41.335: INFO: Wrong image for pod: daemon-set-gbx2v. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:42.374: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:42.374: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:42.374: INFO: Wrong image for pod: daemon-set-gbx2v. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:43.337: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:43.337: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:43.337: INFO: Wrong image for pod: daemon-set-gbx2v. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:44.470: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:44.471: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:44.471: INFO: Wrong image for pod: daemon-set-gbx2v. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:46.025: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:46.025: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:46.025: INFO: Wrong image for pod: daemon-set-gbx2v. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:46.905: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:46.906: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:46.906: INFO: Wrong image for pod: daemon-set-gbx2v. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:47.418: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:47.418: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:47.418: INFO: Wrong image for pod: daemon-set-gbx2v. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:48.336: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:48.336: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:48.336: INFO: Wrong image for pod: daemon-set-gbx2v. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:49.336: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:49.336: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:49.336: INFO: Wrong image for pod: daemon-set-gbx2v. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:50.335: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:50.335: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:50.335: INFO: Wrong image for pod: daemon-set-gbx2v. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:51.347: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:51.347: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:51.347: INFO: Wrong image for pod: daemon-set-gbx2v. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:51.347: INFO: Pod daemon-set-gbx2v is not available
Sep 27 04:51:52.395: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:52.396: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:52.396: INFO: Wrong image for pod: daemon-set-gbx2v. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:52.396: INFO: Pod daemon-set-gbx2v is not available
Sep 27 04:51:53.355: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:53.355: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:53.355: INFO: Pod daemon-set-vplsc is not available
Sep 27 04:51:54.340: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:54.340: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:54.340: INFO: Pod daemon-set-vplsc is not available
Sep 27 04:51:55.334: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:55.334: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:55.334: INFO: Pod daemon-set-vplsc is not available
Sep 27 04:51:56.714: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:56.791: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:56.791: INFO: Pod daemon-set-vplsc is not available
Sep 27 04:51:57.627: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:57.627: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:57.627: INFO: Pod daemon-set-vplsc is not available
Sep 27 04:51:58.430: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:58.430: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:58.430: INFO: Pod daemon-set-vplsc is not available
Sep 27 04:51:59.335: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:59.335: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:51:59.335: INFO: Pod daemon-set-vplsc is not available
Sep 27 04:52:00.337: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:00.338: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:00.338: INFO: Pod daemon-set-vplsc is not available
Sep 27 04:52:01.335: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:01.335: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:02.337: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:02.337: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:03.445: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:03.445: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:04.511: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:04.511: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:05.671: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:05.671: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:06.591: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:06.591: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:07.676: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:07.676: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:08.338: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:08.338: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:09.591: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:09.591: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:10.385: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:10.385: INFO: Pod daemon-set-7m9ft is not available
Sep 27 04:52:10.385: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:11.344: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:11.344: INFO: Pod daemon-set-7m9ft is not available
Sep 27 04:52:11.344: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:12.336: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:12.336: INFO: Pod daemon-set-7m9ft is not available
Sep 27 04:52:12.336: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:13.336: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:13.336: INFO: Pod daemon-set-7m9ft is not available
Sep 27 04:52:13.336: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:14.335: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:14.335: INFO: Pod daemon-set-7m9ft is not available
Sep 27 04:52:14.335: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:15.336: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:15.336: INFO: Pod daemon-set-7m9ft is not available
Sep 27 04:52:15.336: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:16.336: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:16.336: INFO: Pod daemon-set-7m9ft is not available
Sep 27 04:52:16.336: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:17.336: INFO: Wrong image for pod: daemon-set-7m9ft. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:17.336: INFO: Pod daemon-set-7m9ft is not available
Sep 27 04:52:17.336: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:18.353: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:18.353: INFO: Pod daemon-set-mlhld is not available
Sep 27 04:52:19.386: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:19.386: INFO: Pod daemon-set-mlhld is not available
Sep 27 04:52:20.362: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:20.362: INFO: Pod daemon-set-mlhld is not available
Sep 27 04:52:21.335: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:21.335: INFO: Pod daemon-set-mlhld is not available
Sep 27 04:52:22.338: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:22.338: INFO: Pod daemon-set-mlhld is not available
Sep 27 04:52:23.438: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:23.438: INFO: Pod daemon-set-mlhld is not available
Sep 27 04:52:24.434: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:24.434: INFO: Pod daemon-set-mlhld is not available
Sep 27 04:52:25.524: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:25.524: INFO: Pod daemon-set-mlhld is not available
Sep 27 04:52:26.359: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:26.359: INFO: Pod daemon-set-mlhld is not available
Sep 27 04:52:27.360: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:27.360: INFO: Pod daemon-set-mlhld is not available
Sep 27 04:52:28.393: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:28.393: INFO: Pod daemon-set-mlhld is not available
Sep 27 04:52:29.336: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:29.336: INFO: Pod daemon-set-mlhld is not available
Sep 27 04:52:30.367: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:31.560: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:32.445: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:33.656: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:35.344: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:36.459: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:37.452: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:38.336: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:38.336: INFO: Pod daemon-set-bqknh is not available
Sep 27 04:52:39.423: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:39.423: INFO: Pod daemon-set-bqknh is not available
Sep 27 04:52:40.576: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:40.576: INFO: Pod daemon-set-bqknh is not available
Sep 27 04:52:41.357: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:41.357: INFO: Pod daemon-set-bqknh is not available
Sep 27 04:52:42.335: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:42.335: INFO: Pod daemon-set-bqknh is not available
Sep 27 04:52:43.339: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:43.340: INFO: Pod daemon-set-bqknh is not available
Sep 27 04:52:44.608: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:44.608: INFO: Pod daemon-set-bqknh is not available
Sep 27 04:52:45.489: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:45.489: INFO: Pod daemon-set-bqknh is not available
Sep 27 04:52:46.446: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:46.446: INFO: Pod daemon-set-bqknh is not available
Sep 27 04:52:47.337: INFO: Wrong image for pod: daemon-set-bqknh. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 27 04:52:47.337: INFO: Pod daemon-set-bqknh is not available
Sep 27 04:52:48.341: INFO: Pod daemon-set-cg8ms is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Sep 27 04:52:48.360: INFO: Number of nodes with available pods: 2
Sep 27 04:52:48.360: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:52:49.504: INFO: Number of nodes with available pods: 2
Sep 27 04:52:49.504: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:52:50.639: INFO: Number of nodes with available pods: 2
Sep 27 04:52:50.639: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:52:51.710: INFO: Number of nodes with available pods: 2
Sep 27 04:52:51.710: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:52:52.378: INFO: Number of nodes with available pods: 2
Sep 27 04:52:52.378: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:52:53.493: INFO: Number of nodes with available pods: 2
Sep 27 04:52:53.493: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:52:54.369: INFO: Number of nodes with available pods: 2
Sep 27 04:52:54.369: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:52:55.484: INFO: Number of nodes with available pods: 2
Sep 27 04:52:55.484: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:52:56.369: INFO: Number of nodes with available pods: 2
Sep 27 04:52:56.369: INFO: Node dce-10-6-171-86 is running more than one daemon pod
Sep 27 04:52:57.632: INFO: Number of nodes with available pods: 3
Sep 27 04:52:57.632: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6253, will wait for the garbage collector to delete the pods
Sep 27 04:52:57.971: INFO: Deleting DaemonSet.extensions daemon-set took: 269.708001ms
Sep 27 04:52:58.671: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.240444ms
Sep 27 04:53:12.177: INFO: Number of nodes with available pods: 0
Sep 27 04:53:12.177: INFO: Number of running nodes: 0, number of available pods: 0
Sep 27 04:53:12.180: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6253/daemonsets","resourceVersion":"715515"},"items":null}

Sep 27 04:53:12.184: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6253/pods","resourceVersion":"715515"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:53:12.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6253" for this suite.

â€¢ [SLOW TEST:105.627 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":277,"completed":260,"skipped":4491,"failed":0}
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:53:12.211: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7725
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7725 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7725;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7725 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7725;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7725.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7725.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7725.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7725.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7725.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7725.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7725.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7725.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7725.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7725.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7725.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7725.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7725.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 12.114.31.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.31.114.12_udp@PTR;check="$$(dig +tcp +noall +answer +search 12.114.31.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.31.114.12_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7725 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7725;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7725 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7725;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7725.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7725.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7725.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7725.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7725.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7725.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7725.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7725.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7725.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7725.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7725.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7725.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7725.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 12.114.31.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.31.114.12_udp@PTR;check="$$(dig +tcp +noall +answer +search 12.114.31.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.31.114.12_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 27 04:53:27.828: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:27.834: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:27.838: INFO: Unable to read wheezy_udp@dns-test-service.dns-7725 from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:27.842: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7725 from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:27.860: INFO: Unable to read wheezy_udp@dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:27.868: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:27.873: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:27.878: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:27.913: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:27.917: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:27.921: INFO: Unable to read jessie_udp@dns-test-service.dns-7725 from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:27.925: INFO: Unable to read jessie_tcp@dns-test-service.dns-7725 from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:27.930: INFO: Unable to read jessie_udp@dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:27.935: INFO: Unable to read jessie_tcp@dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:27.941: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:27.946: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:27.971: INFO: Lookups using dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7725 wheezy_tcp@dns-test-service.dns-7725 wheezy_udp@dns-test-service.dns-7725.svc wheezy_tcp@dns-test-service.dns-7725.svc wheezy_udp@_http._tcp.dns-test-service.dns-7725.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7725.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7725 jessie_tcp@dns-test-service.dns-7725 jessie_udp@dns-test-service.dns-7725.svc jessie_tcp@dns-test-service.dns-7725.svc jessie_udp@_http._tcp.dns-test-service.dns-7725.svc jessie_tcp@_http._tcp.dns-test-service.dns-7725.svc]

Sep 27 04:53:33.146: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:33.494: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:34.066: INFO: Unable to read wheezy_udp@dns-test-service.dns-7725 from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:34.442: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7725 from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:34.994: INFO: Unable to read wheezy_udp@dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:35.167: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:35.255: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:35.417: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:35.924: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:35.928: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:35.931: INFO: Unable to read jessie_udp@dns-test-service.dns-7725 from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:35.936: INFO: Unable to read jessie_tcp@dns-test-service.dns-7725 from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:35.940: INFO: Unable to read jessie_udp@dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:35.944: INFO: Unable to read jessie_tcp@dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:35.948: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:35.951: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:35.978: INFO: Lookups using dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7725 wheezy_tcp@dns-test-service.dns-7725 wheezy_udp@dns-test-service.dns-7725.svc wheezy_tcp@dns-test-service.dns-7725.svc wheezy_udp@_http._tcp.dns-test-service.dns-7725.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7725.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7725 jessie_tcp@dns-test-service.dns-7725 jessie_udp@dns-test-service.dns-7725.svc jessie_tcp@dns-test-service.dns-7725.svc jessie_udp@_http._tcp.dns-test-service.dns-7725.svc jessie_tcp@_http._tcp.dns-test-service.dns-7725.svc]

Sep 27 04:53:38.303: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:38.308: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:38.314: INFO: Unable to read wheezy_udp@dns-test-service.dns-7725 from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:38.318: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7725 from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:38.323: INFO: Unable to read wheezy_udp@dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:38.327: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:38.331: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:38.336: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:38.365: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:38.369: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:38.373: INFO: Unable to read jessie_udp@dns-test-service.dns-7725 from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:38.377: INFO: Unable to read jessie_tcp@dns-test-service.dns-7725 from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:38.381: INFO: Unable to read jessie_udp@dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:38.386: INFO: Unable to read jessie_tcp@dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:38.389: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:38.392: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:38.418: INFO: Lookups using dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7725 wheezy_tcp@dns-test-service.dns-7725 wheezy_udp@dns-test-service.dns-7725.svc wheezy_tcp@dns-test-service.dns-7725.svc wheezy_udp@_http._tcp.dns-test-service.dns-7725.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7725.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7725 jessie_tcp@dns-test-service.dns-7725 jessie_udp@dns-test-service.dns-7725.svc jessie_tcp@dns-test-service.dns-7725.svc jessie_udp@_http._tcp.dns-test-service.dns-7725.svc jessie_tcp@_http._tcp.dns-test-service.dns-7725.svc]

Sep 27 04:53:42.977: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:42.984: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:42.999: INFO: Unable to read wheezy_udp@dns-test-service.dns-7725 from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:43.011: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7725 from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:43.081: INFO: Unable to read wheezy_udp@dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:43.275: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:43.281: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:43.287: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:43.341: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:43.347: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:43.352: INFO: Unable to read jessie_udp@dns-test-service.dns-7725 from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:43.357: INFO: Unable to read jessie_tcp@dns-test-service.dns-7725 from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:43.362: INFO: Unable to read jessie_udp@dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:43.366: INFO: Unable to read jessie_tcp@dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:43.370: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:43.374: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:43.397: INFO: Lookups using dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7725 wheezy_tcp@dns-test-service.dns-7725 wheezy_udp@dns-test-service.dns-7725.svc wheezy_tcp@dns-test-service.dns-7725.svc wheezy_udp@_http._tcp.dns-test-service.dns-7725.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7725.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7725 jessie_tcp@dns-test-service.dns-7725 jessie_udp@dns-test-service.dns-7725.svc jessie_tcp@dns-test-service.dns-7725.svc jessie_udp@_http._tcp.dns-test-service.dns-7725.svc jessie_tcp@_http._tcp.dns-test-service.dns-7725.svc]

Sep 27 04:53:47.977: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:47.983: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:47.989: INFO: Unable to read wheezy_udp@dns-test-service.dns-7725 from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:47.996: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7725 from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:48.004: INFO: Unable to read wheezy_udp@dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:48.014: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:48.019: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:48.025: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:48.076: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:48.083: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:48.092: INFO: Unable to read jessie_udp@dns-test-service.dns-7725 from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:48.100: INFO: Unable to read jessie_tcp@dns-test-service.dns-7725 from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:48.109: INFO: Unable to read jessie_udp@dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:48.117: INFO: Unable to read jessie_tcp@dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:48.124: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:48.128: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:48.180: INFO: Lookups using dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7725 wheezy_tcp@dns-test-service.dns-7725 wheezy_udp@dns-test-service.dns-7725.svc wheezy_tcp@dns-test-service.dns-7725.svc wheezy_udp@_http._tcp.dns-test-service.dns-7725.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7725.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7725 jessie_tcp@dns-test-service.dns-7725 jessie_udp@dns-test-service.dns-7725.svc jessie_tcp@dns-test-service.dns-7725.svc jessie_udp@_http._tcp.dns-test-service.dns-7725.svc jessie_tcp@_http._tcp.dns-test-service.dns-7725.svc]

Sep 27 04:53:52.975: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:52.979: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:52.982: INFO: Unable to read wheezy_udp@dns-test-service.dns-7725 from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:52.985: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7725 from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:52.989: INFO: Unable to read wheezy_udp@dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:52.993: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:52.997: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:53.003: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:53.031: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:53.034: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:53.038: INFO: Unable to read jessie_udp@dns-test-service.dns-7725 from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:53.043: INFO: Unable to read jessie_tcp@dns-test-service.dns-7725 from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:53.048: INFO: Unable to read jessie_udp@dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:53.055: INFO: Unable to read jessie_tcp@dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:53.059: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:53.063: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:53.340: INFO: Lookups using dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7725 wheezy_tcp@dns-test-service.dns-7725 wheezy_udp@dns-test-service.dns-7725.svc wheezy_tcp@dns-test-service.dns-7725.svc wheezy_udp@_http._tcp.dns-test-service.dns-7725.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7725.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7725 jessie_tcp@dns-test-service.dns-7725 jessie_udp@dns-test-service.dns-7725.svc jessie_tcp@dns-test-service.dns-7725.svc jessie_udp@_http._tcp.dns-test-service.dns-7725.svc jessie_tcp@_http._tcp.dns-test-service.dns-7725.svc]

Sep 27 04:53:58.080: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:58.858: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7725.svc from pod dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb: the server could not find the requested resource (get pods dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb)
Sep 27 04:53:59.857: INFO: Lookups using dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb failed for: [wheezy_udp@dns-test-service wheezy_udp@_http._tcp.dns-test-service.dns-7725.svc]

Sep 27 04:54:03.106: INFO: DNS probes using dns-7725/dns-test-555e6b33-0244-418e-b1a5-2ff7eb6ed6bb succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:54:04.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7725" for this suite.

â€¢ [SLOW TEST:52.718 seconds]
[sig-network] DNS
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":277,"completed":261,"skipped":4491,"failed":0}
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:54:04.929: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6260
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-0b21fa64-6443-4efe-9c15-23cd68e9a549
STEP: Creating a pod to test consume configMaps
Sep 27 04:54:05.457: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-52917267-c16b-4001-a74a-da8052309012" in namespace "projected-6260" to be "Succeeded or Failed"
Sep 27 04:54:05.681: INFO: Pod "pod-projected-configmaps-52917267-c16b-4001-a74a-da8052309012": Phase="Pending", Reason="", readiness=false. Elapsed: 223.464576ms
Sep 27 04:54:08.006: INFO: Pod "pod-projected-configmaps-52917267-c16b-4001-a74a-da8052309012": Phase="Pending", Reason="", readiness=false. Elapsed: 2.54834326s
Sep 27 04:54:10.293: INFO: Pod "pod-projected-configmaps-52917267-c16b-4001-a74a-da8052309012": Phase="Pending", Reason="", readiness=false. Elapsed: 4.835124371s
Sep 27 04:54:12.376: INFO: Pod "pod-projected-configmaps-52917267-c16b-4001-a74a-da8052309012": Phase="Pending", Reason="", readiness=false. Elapsed: 6.918857015s
Sep 27 04:54:14.380: INFO: Pod "pod-projected-configmaps-52917267-c16b-4001-a74a-da8052309012": Phase="Pending", Reason="", readiness=false. Elapsed: 8.922313485s
Sep 27 04:54:16.409: INFO: Pod "pod-projected-configmaps-52917267-c16b-4001-a74a-da8052309012": Phase="Pending", Reason="", readiness=false. Elapsed: 10.951476863s
Sep 27 04:54:18.449: INFO: Pod "pod-projected-configmaps-52917267-c16b-4001-a74a-da8052309012": Phase="Pending", Reason="", readiness=false. Elapsed: 12.991678046s
Sep 27 04:54:20.453: INFO: Pod "pod-projected-configmaps-52917267-c16b-4001-a74a-da8052309012": Phase="Pending", Reason="", readiness=false. Elapsed: 14.995706378s
Sep 27 04:54:22.457: INFO: Pod "pod-projected-configmaps-52917267-c16b-4001-a74a-da8052309012": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.999477224s
STEP: Saw pod success
Sep 27 04:54:22.457: INFO: Pod "pod-projected-configmaps-52917267-c16b-4001-a74a-da8052309012" satisfied condition "Succeeded or Failed"
Sep 27 04:54:22.462: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-projected-configmaps-52917267-c16b-4001-a74a-da8052309012 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 27 04:54:22.708: INFO: Waiting for pod pod-projected-configmaps-52917267-c16b-4001-a74a-da8052309012 to disappear
Sep 27 04:54:22.814: INFO: Pod pod-projected-configmaps-52917267-c16b-4001-a74a-da8052309012 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:54:22.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6260" for this suite.

â€¢ [SLOW TEST:17.900 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":277,"completed":262,"skipped":4491,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:54:22.829: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7145
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7145
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-7145
I0927 04:54:23.790811      23 runners.go:190] Created replication controller with name: externalname-service, namespace: services-7145, replica count: 2
I0927 04:54:26.852252      23 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 04:54:29.852840      23 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 04:54:32.853153      23 runners.go:190] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 27 04:54:35.855: INFO: Creating new exec pod
I0927 04:54:35.855199      23 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 27 04:54:50.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=services-7145 execpod79xpd -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Sep 27 04:54:51.236: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep 27 04:54:51.236: INFO: stdout: ""
Sep 27 04:54:51.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=services-7145 execpod79xpd -- /bin/sh -x -c nc -zv -t -w 2 172.31.96.224 80'
Sep 27 04:54:51.587: INFO: stderr: "+ nc -zv -t -w 2 172.31.96.224 80\nConnection to 172.31.96.224 80 port [tcp/http] succeeded!\n"
Sep 27 04:54:51.587: INFO: stdout: ""
Sep 27 04:54:51.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=services-7145 execpod79xpd -- /bin/sh -x -c nc -zv -t -w 2 10.6.171.86 30270'
Sep 27 04:54:51.928: INFO: stderr: "+ nc -zv -t -w 2 10.6.171.86 30270\nConnection to 10.6.171.86 30270 port [tcp/30270] succeeded!\n"
Sep 27 04:54:51.928: INFO: stdout: ""
Sep 27 04:54:51.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 exec --namespace=services-7145 execpod79xpd -- /bin/sh -x -c nc -zv -t -w 2 10.6.171.84 30270'
Sep 27 04:54:52.268: INFO: stderr: "+ nc -zv -t -w 2 10.6.171.84 30270\nConnection to 10.6.171.84 30270 port [tcp/30270] succeeded!\n"
Sep 27 04:54:52.268: INFO: stdout: ""
Sep 27 04:54:52.268: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:54:52.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7145" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

â€¢ [SLOW TEST:29.528 seconds]
[sig-network] Services
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":277,"completed":263,"skipped":4528,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:54:52.357: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5521
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5521.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5521.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5521.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5521.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5521.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5521.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 27 04:55:16.655: INFO: DNS probes using dns-5521/dns-test-c8d09b8d-88a2-4f9b-b8aa-d4eca6b7140a succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:55:16.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5521" for this suite.

â€¢ [SLOW TEST:24.387 seconds]
[sig-network] DNS
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":277,"completed":264,"skipped":4557,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:55:16.745: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2038
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name cm-test-opt-del-6911a0e3-bfa0-48af-a0d5-e6a87e5852d9
STEP: Creating configMap with name cm-test-opt-upd-29d590ec-80fc-4a25-ad60-949693be5071
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-6911a0e3-bfa0-48af-a0d5-e6a87e5852d9
STEP: Updating configmap cm-test-opt-upd-29d590ec-80fc-4a25-ad60-949693be5071
STEP: Creating configMap with name cm-test-opt-create-b51c3583-c423-4c04-84a8-9a9a75939427
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:55:34.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2038" for this suite.

â€¢ [SLOW TEST:17.461 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":265,"skipped":4563,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:55:34.206: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6163
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6163.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6163.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6163.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6163.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6163.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6163.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 27 04:55:50.979: INFO: DNS probes using dns-6163/dns-test-97821d7b-d33b-42e6-916a-7ed3423e8835 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:55:51.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6163" for this suite.

â€¢ [SLOW TEST:17.035 seconds]
[sig-network] DNS
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":277,"completed":266,"skipped":4583,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:55:51.241: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7306
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 27 04:55:51.973: INFO: Waiting up to 5m0s for pod "downwardapi-volume-012f1598-71de-4a0c-b6a3-7d9f096fa6bd" in namespace "downward-api-7306" to be "Succeeded or Failed"
Sep 27 04:55:51.977: INFO: Pod "downwardapi-volume-012f1598-71de-4a0c-b6a3-7d9f096fa6bd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.290581ms
Sep 27 04:55:54.055: INFO: Pod "downwardapi-volume-012f1598-71de-4a0c-b6a3-7d9f096fa6bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.081919588s
Sep 27 04:55:56.171: INFO: Pod "downwardapi-volume-012f1598-71de-4a0c-b6a3-7d9f096fa6bd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.198192912s
Sep 27 04:55:58.364: INFO: Pod "downwardapi-volume-012f1598-71de-4a0c-b6a3-7d9f096fa6bd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.39150152s
Sep 27 04:56:00.463: INFO: Pod "downwardapi-volume-012f1598-71de-4a0c-b6a3-7d9f096fa6bd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.490729025s
Sep 27 04:56:02.704: INFO: Pod "downwardapi-volume-012f1598-71de-4a0c-b6a3-7d9f096fa6bd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.731134456s
Sep 27 04:56:04.710: INFO: Pod "downwardapi-volume-012f1598-71de-4a0c-b6a3-7d9f096fa6bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.737612888s
STEP: Saw pod success
Sep 27 04:56:04.710: INFO: Pod "downwardapi-volume-012f1598-71de-4a0c-b6a3-7d9f096fa6bd" satisfied condition "Succeeded or Failed"
Sep 27 04:56:04.715: INFO: Trying to get logs from node dce-10-6-171-86 pod downwardapi-volume-012f1598-71de-4a0c-b6a3-7d9f096fa6bd container client-container: <nil>
STEP: delete the pod
Sep 27 04:56:04.768: INFO: Waiting for pod downwardapi-volume-012f1598-71de-4a0c-b6a3-7d9f096fa6bd to disappear
Sep 27 04:56:04.776: INFO: Pod downwardapi-volume-012f1598-71de-4a0c-b6a3-7d9f096fa6bd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:56:04.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7306" for this suite.

â€¢ [SLOW TEST:13.552 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":267,"skipped":4597,"failed":0}
SS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:56:04.794: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-2423
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:56:55.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2423" for this suite.

â€¢ [SLOW TEST:50.633 seconds]
[sig-apps] Job
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":277,"completed":268,"skipped":4599,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:56:55.427: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5570
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 27 04:56:56.077: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c5019fc6-b7e5-4789-a444-1ae68773cf73" in namespace "downward-api-5570" to be "Succeeded or Failed"
Sep 27 04:56:56.084: INFO: Pod "downwardapi-volume-c5019fc6-b7e5-4789-a444-1ae68773cf73": Phase="Pending", Reason="", readiness=false. Elapsed: 7.107932ms
Sep 27 04:56:58.856: INFO: Pod "downwardapi-volume-c5019fc6-b7e5-4789-a444-1ae68773cf73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.779353232s
Sep 27 04:57:00.898: INFO: Pod "downwardapi-volume-c5019fc6-b7e5-4789-a444-1ae68773cf73": Phase="Pending", Reason="", readiness=false. Elapsed: 4.821439896s
Sep 27 04:57:02.918: INFO: Pod "downwardapi-volume-c5019fc6-b7e5-4789-a444-1ae68773cf73": Phase="Pending", Reason="", readiness=false. Elapsed: 6.841005289s
Sep 27 04:57:04.958: INFO: Pod "downwardapi-volume-c5019fc6-b7e5-4789-a444-1ae68773cf73": Phase="Pending", Reason="", readiness=false. Elapsed: 8.881043422s
Sep 27 04:57:07.282: INFO: Pod "downwardapi-volume-c5019fc6-b7e5-4789-a444-1ae68773cf73": Phase="Pending", Reason="", readiness=false. Elapsed: 11.205420814s
Sep 27 04:57:09.378: INFO: Pod "downwardapi-volume-c5019fc6-b7e5-4789-a444-1ae68773cf73": Phase="Pending", Reason="", readiness=false. Elapsed: 13.301095525s
Sep 27 04:57:15.461: INFO: Pod "downwardapi-volume-c5019fc6-b7e5-4789-a444-1ae68773cf73": Phase="Pending", Reason="", readiness=false. Elapsed: 19.383860846s
Sep 27 04:57:17.521: INFO: Pod "downwardapi-volume-c5019fc6-b7e5-4789-a444-1ae68773cf73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 21.444461219s
STEP: Saw pod success
Sep 27 04:57:17.521: INFO: Pod "downwardapi-volume-c5019fc6-b7e5-4789-a444-1ae68773cf73" satisfied condition "Succeeded or Failed"
Sep 27 04:57:17.525: INFO: Trying to get logs from node dce-10-6-171-86 pod downwardapi-volume-c5019fc6-b7e5-4789-a444-1ae68773cf73 container client-container: <nil>
STEP: delete the pod
Sep 27 04:57:17.571: INFO: Waiting for pod downwardapi-volume-c5019fc6-b7e5-4789-a444-1ae68773cf73 to disappear
Sep 27 04:57:17.576: INFO: Pod downwardapi-volume-c5019fc6-b7e5-4789-a444-1ae68773cf73 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:57:17.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5570" for this suite.

â€¢ [SLOW TEST:22.163 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":277,"completed":269,"skipped":4617,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:57:17.591: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6901
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 04:57:18.056: INFO: Creating deployment "test-recreate-deployment"
Sep 27 04:57:18.064: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Sep 27 04:57:18.144: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Sep 27 04:57:20.286: INFO: Waiting deployment "test-recreate-deployment" to complete
Sep 27 04:57:20.343: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736779438, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736779438, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736779438, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736779438, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-74d98b5f7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:57:22.534: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736779438, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736779438, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736779438, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736779438, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-74d98b5f7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:57:24.393: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736779438, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736779438, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736779438, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736779438, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-74d98b5f7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:57:26.347: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736779438, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736779438, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736779438, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736779438, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-74d98b5f7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:57:28.347: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736779438, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736779438, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736779438, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736779438, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-74d98b5f7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:57:30.448: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736779438, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736779438, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736779438, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736779438, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-74d98b5f7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:57:32.347: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736779438, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736779438, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736779438, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736779438, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-74d98b5f7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:57:34.761: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736779438, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736779438, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736779438, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736779438, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-74d98b5f7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 04:57:36.417: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Sep 27 04:57:36.459: INFO: Updating deployment test-recreate-deployment
Sep 27 04:57:36.459: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Sep 27 04:57:37.120: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-6901 /apis/apps/v1/namespaces/deployment-6901/deployments/test-recreate-deployment a96443cf-5d43-476a-b445-d57c6b0de4c9 716947 2 2020-09-27 04:57:18 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00429f488 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-09-27 04:57:36 +0000 UTC,LastTransitionTime:2020-09-27 04:57:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-d5667d9c7" is progressing.,LastUpdateTime:2020-09-27 04:57:36 +0000 UTC,LastTransitionTime:2020-09-27 04:57:18 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Sep 27 04:57:37.127: INFO: New ReplicaSet "test-recreate-deployment-d5667d9c7" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-d5667d9c7  deployment-6901 /apis/apps/v1/namespaces/deployment-6901/replicasets/test-recreate-deployment-d5667d9c7 bc3cb5e4-a49d-47f8-aeb5-1c6b9fe9a6ef 716946 1 2020-09-27 04:57:36 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:d5667d9c7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment a96443cf-5d43-476a-b445-d57c6b0de4c9 0xc00429f990 0xc00429f991}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: d5667d9c7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:d5667d9c7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00429f9f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 27 04:57:37.127: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Sep 27 04:57:37.127: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-74d98b5f7c  deployment-6901 /apis/apps/v1/namespaces/deployment-6901/replicasets/test-recreate-deployment-74d98b5f7c 246dd3ed-e9e2-4db5-8973-99a328139514 716934 2 2020-09-27 04:57:18 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:74d98b5f7c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment a96443cf-5d43-476a-b445-d57c6b0de4c9 0xc00429f8b7 0xc00429f8b8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 74d98b5f7c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:74d98b5f7c] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00429f928 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 27 04:57:37.134: INFO: Pod "test-recreate-deployment-d5667d9c7-7j5hb" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-d5667d9c7-7j5hb test-recreate-deployment-d5667d9c7- deployment-6901 /api/v1/namespaces/deployment-6901/pods/test-recreate-deployment-d5667d9c7-7j5hb 9f748dda-44a1-4343-9702-8420596c3e80 716945 0 2020-09-27 04:57:36 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:d5667d9c7] map[kubernetes.io/psp:dce-psp-allow-all] [{apps/v1 ReplicaSet test-recreate-deployment-d5667d9c7 bc3cb5e4-a49d-47f8-aeb5-1c6b9fe9a6ef 0xc00429fef0 0xc00429fef1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f7n65,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f7n65,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f7n65,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dce-10-6-171-86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:57:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:57:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:57:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-27 04:57:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.6.171.86,PodIP:,StartTime:2020-09-27 04:57:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:57:37.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6901" for this suite.

â€¢ [SLOW TEST:19.564 seconds]
[sig-apps] Deployment
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":277,"completed":270,"skipped":4629,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:57:37.155: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3246
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: validating api versions
Sep 27 04:57:37.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-085554482 api-versions'
Sep 27 04:57:37.806: INFO: stderr: ""
Sep 27 04:57:37.806: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbatch/v2alpha1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncustom.metrics.k8s.io/v1beta1\ndce.daocloud.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nuds.daocloud.io/v1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:57:37.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3246" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":277,"completed":271,"skipped":4633,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:57:37.817: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-258
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-2f6ca5c6-835e-4e51-9f0e-0c55290dbcc3
STEP: Creating a pod to test consume configMaps
Sep 27 04:57:38.375: INFO: Waiting up to 5m0s for pod "pod-configmaps-51bc7630-6930-4147-bc15-0fa27924a930" in namespace "configmap-258" to be "Succeeded or Failed"
Sep 27 04:57:38.483: INFO: Pod "pod-configmaps-51bc7630-6930-4147-bc15-0fa27924a930": Phase="Pending", Reason="", readiness=false. Elapsed: 107.851542ms
Sep 27 04:57:40.567: INFO: Pod "pod-configmaps-51bc7630-6930-4147-bc15-0fa27924a930": Phase="Pending", Reason="", readiness=false. Elapsed: 2.191748627s
Sep 27 04:57:42.723: INFO: Pod "pod-configmaps-51bc7630-6930-4147-bc15-0fa27924a930": Phase="Pending", Reason="", readiness=false. Elapsed: 4.347117062s
Sep 27 04:57:44.957: INFO: Pod "pod-configmaps-51bc7630-6930-4147-bc15-0fa27924a930": Phase="Pending", Reason="", readiness=false. Elapsed: 6.581452168s
Sep 27 04:57:47.007: INFO: Pod "pod-configmaps-51bc7630-6930-4147-bc15-0fa27924a930": Phase="Pending", Reason="", readiness=false. Elapsed: 8.631244461s
Sep 27 04:57:49.189: INFO: Pod "pod-configmaps-51bc7630-6930-4147-bc15-0fa27924a930": Phase="Pending", Reason="", readiness=false. Elapsed: 10.81336472s
Sep 27 04:57:51.380: INFO: Pod "pod-configmaps-51bc7630-6930-4147-bc15-0fa27924a930": Phase="Pending", Reason="", readiness=false. Elapsed: 13.004573313s
Sep 27 04:57:53.390: INFO: Pod "pod-configmaps-51bc7630-6930-4147-bc15-0fa27924a930": Phase="Pending", Reason="", readiness=false. Elapsed: 15.014466726s
Sep 27 04:57:55.434: INFO: Pod "pod-configmaps-51bc7630-6930-4147-bc15-0fa27924a930": Phase="Succeeded", Reason="", readiness=false. Elapsed: 17.058335857s
STEP: Saw pod success
Sep 27 04:57:55.434: INFO: Pod "pod-configmaps-51bc7630-6930-4147-bc15-0fa27924a930" satisfied condition "Succeeded or Failed"
Sep 27 04:57:55.437: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-configmaps-51bc7630-6930-4147-bc15-0fa27924a930 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 27 04:57:55.540: INFO: Waiting for pod pod-configmaps-51bc7630-6930-4147-bc15-0fa27924a930 to disappear
Sep 27 04:57:55.568: INFO: Pod pod-configmaps-51bc7630-6930-4147-bc15-0fa27924a930 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:57:55.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-258" for this suite.

â€¢ [SLOW TEST:18.008 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":277,"completed":272,"skipped":4638,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:57:55.873: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-9252
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5132
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3140
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:58:05.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9252" for this suite.
STEP: Destroying namespace "nsdeletetest-5132" for this suite.
Sep 27 04:58:05.489: INFO: Namespace nsdeletetest-5132 was already deleted
STEP: Destroying namespace "nsdeletetest-3140" for this suite.

â€¢ [SLOW TEST:9.911 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":277,"completed":273,"skipped":4673,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:58:05.784: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2504
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:58:17.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2504" for this suite.

â€¢ [SLOW TEST:11.986 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":277,"completed":274,"skipped":4682,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:58:17.770: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4854
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-map-4044055e-f9cb-4fd5-a126-59434d228322
STEP: Creating a pod to test consume configMaps
Sep 27 04:58:18.262: INFO: Waiting up to 5m0s for pod "pod-configmaps-556827c1-5674-491c-8147-74a6bc3476a9" in namespace "configmap-4854" to be "Succeeded or Failed"
Sep 27 04:58:18.312: INFO: Pod "pod-configmaps-556827c1-5674-491c-8147-74a6bc3476a9": Phase="Pending", Reason="", readiness=false. Elapsed: 50.334326ms
Sep 27 04:58:20.316: INFO: Pod "pod-configmaps-556827c1-5674-491c-8147-74a6bc3476a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054255517s
Sep 27 04:58:22.381: INFO: Pod "pod-configmaps-556827c1-5674-491c-8147-74a6bc3476a9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.119669536s
Sep 27 04:58:24.385: INFO: Pod "pod-configmaps-556827c1-5674-491c-8147-74a6bc3476a9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.123783746s
Sep 27 04:58:26.390: INFO: Pod "pod-configmaps-556827c1-5674-491c-8147-74a6bc3476a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.127991069s
STEP: Saw pod success
Sep 27 04:58:26.390: INFO: Pod "pod-configmaps-556827c1-5674-491c-8147-74a6bc3476a9" satisfied condition "Succeeded or Failed"
Sep 27 04:58:26.392: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-configmaps-556827c1-5674-491c-8147-74a6bc3476a9 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 27 04:58:26.700: INFO: Waiting for pod pod-configmaps-556827c1-5674-491c-8147-74a6bc3476a9 to disappear
Sep 27 04:58:26.704: INFO: Pod pod-configmaps-556827c1-5674-491c-8147-74a6bc3476a9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:58:26.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4854" for this suite.

â€¢ [SLOW TEST:8.945 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":275,"skipped":4689,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:58:26.715: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6620
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-3c3a9089-df20-4cdd-954b-0d9fdd0e9df9
STEP: Creating a pod to test consume secrets
Sep 27 04:58:27.006: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f5344872-32e3-4707-9599-13c4b24bdb56" in namespace "projected-6620" to be "Succeeded or Failed"
Sep 27 04:58:27.030: INFO: Pod "pod-projected-secrets-f5344872-32e3-4707-9599-13c4b24bdb56": Phase="Pending", Reason="", readiness=false. Elapsed: 24.644045ms
Sep 27 04:58:29.040: INFO: Pod "pod-projected-secrets-f5344872-32e3-4707-9599-13c4b24bdb56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034671879s
Sep 27 04:58:31.061: INFO: Pod "pod-projected-secrets-f5344872-32e3-4707-9599-13c4b24bdb56": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054833714s
Sep 27 04:58:33.286: INFO: Pod "pod-projected-secrets-f5344872-32e3-4707-9599-13c4b24bdb56": Phase="Pending", Reason="", readiness=false. Elapsed: 6.280056307s
Sep 27 04:58:35.432: INFO: Pod "pod-projected-secrets-f5344872-32e3-4707-9599-13c4b24bdb56": Phase="Pending", Reason="", readiness=false. Elapsed: 8.42608102s
Sep 27 04:58:37.497: INFO: Pod "pod-projected-secrets-f5344872-32e3-4707-9599-13c4b24bdb56": Phase="Pending", Reason="", readiness=false. Elapsed: 10.490848837s
Sep 27 04:58:39.506: INFO: Pod "pod-projected-secrets-f5344872-32e3-4707-9599-13c4b24bdb56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.499788328s
STEP: Saw pod success
Sep 27 04:58:39.506: INFO: Pod "pod-projected-secrets-f5344872-32e3-4707-9599-13c4b24bdb56" satisfied condition "Succeeded or Failed"
Sep 27 04:58:39.512: INFO: Trying to get logs from node dce-10-6-171-86 pod pod-projected-secrets-f5344872-32e3-4707-9599-13c4b24bdb56 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 27 04:58:39.942: INFO: Waiting for pod pod-projected-secrets-f5344872-32e3-4707-9599-13c4b24bdb56 to disappear
Sep 27 04:58:39.965: INFO: Pod pod-projected-secrets-f5344872-32e3-4707-9599-13c4b24bdb56 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:58:39.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6620" for this suite.

â€¢ [SLOW TEST:13.266 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":277,"completed":276,"skipped":4703,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 27 04:58:39.981: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7302
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 27 04:58:40.423: INFO: >>> kubeConfig: /tmp/kubeconfig-085554482
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 27 04:58:52.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7302" for this suite.

â€¢ [SLOW TEST:12.691 seconds]
[k8s.io] Pods
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":277,"completed":277,"skipped":4712,"failed":0}
SSSSep 27 04:58:52.672: INFO: Running AfterSuite actions on all nodes
Sep 27 04:58:52.672: INFO: Running AfterSuite actions on node 1
Sep 27 04:58:52.672: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":277,"completed":277,"skipped":4715,"failed":0}

Ran 277 of 4992 Specs in 8334.145 seconds
SUCCESS! -- 277 Passed | 0 Failed | 0 Pending | 4715 Skipped
PASS

Ginkgo ran 1 suite in 2h18m56.836737676s
Test Suite Passed
