I0916 23:07:16.264428      23 test_context.go:410] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-480724876
I0916 23:07:16.264462      23 test_context.go:423] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0916 23:07:16.264749      23 e2e.go:124] Starting e2e run "91443dd6-2583-4790-8e10-b1b7123afebe" on Ginkgo node 1
{"msg":"Test Suite starting","total":277,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1600297634 - Will randomize all specs
Will run 277 of 4992 specs

Sep 16 23:07:16.278: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
E0916 23:07:16.279709      23 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post http://localhost:8099/progress: dial tcp 127.0.0.1:8099: connect: connection refused
Sep 16 23:07:16.281: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Sep 16 23:07:16.296: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Sep 16 23:07:16.335: INFO: 31 / 31 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Sep 16 23:07:16.335: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Sep 16 23:07:16.335: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Sep 16 23:07:16.344: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Sep 16 23:07:16.344: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-multus-ds-amd64' (0 seconds elapsed)
Sep 16 23:07:16.344: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Sep 16 23:07:16.344: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-sriov-device-plugin-amd64' (0 seconds elapsed)
Sep 16 23:07:16.344: INFO: e2e test version: v1.18.6
Sep 16 23:07:16.345: INFO: kube-apiserver version: v1.18.6
Sep 16 23:07:16.345: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
Sep 16 23:07:16.349: INFO: Cluster IP family: ipv4
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:07:16.349: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename crd-publish-openapi
Sep 16 23:07:16.393: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 16 23:07:16.393: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 16 23:07:21.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 --namespace=crd-publish-openapi-3067 create -f -'
Sep 16 23:07:21.634: INFO: stderr: ""
Sep 16 23:07:21.634: INFO: stdout: "e2e-test-crd-publish-openapi-3176-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep 16 23:07:21.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 --namespace=crd-publish-openapi-3067 delete e2e-test-crd-publish-openapi-3176-crds test-cr'
Sep 16 23:07:21.765: INFO: stderr: ""
Sep 16 23:07:21.765: INFO: stdout: "e2e-test-crd-publish-openapi-3176-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Sep 16 23:07:21.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 --namespace=crd-publish-openapi-3067 apply -f -'
Sep 16 23:07:22.120: INFO: stderr: ""
Sep 16 23:07:22.120: INFO: stdout: "e2e-test-crd-publish-openapi-3176-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep 16 23:07:22.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 --namespace=crd-publish-openapi-3067 delete e2e-test-crd-publish-openapi-3176-crds test-cr'
Sep 16 23:07:22.218: INFO: stderr: ""
Sep 16 23:07:22.219: INFO: stdout: "e2e-test-crd-publish-openapi-3176-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Sep 16 23:07:22.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 explain e2e-test-crd-publish-openapi-3176-crds'
Sep 16 23:07:22.534: INFO: stderr: ""
Sep 16 23:07:22.534: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3176-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:07:27.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3067" for this suite.

• [SLOW TEST:11.070 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":277,"completed":1,"skipped":3,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:07:27.421: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:07:38.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8453" for this suite.

• [SLOW TEST:11.084 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":277,"completed":2,"skipped":100,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:07:38.505: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-25929b46-13cb-401d-afed-7b4b083cc5a8
STEP: Creating a pod to test consume secrets
Sep 16 23:07:38.571: INFO: Waiting up to 5m0s for pod "pod-secrets-5f602222-0446-4973-aa31-27a80c2ed316" in namespace "secrets-7269" to be "Succeeded or Failed"
Sep 16 23:07:38.573: INFO: Pod "pod-secrets-5f602222-0446-4973-aa31-27a80c2ed316": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055151ms
Sep 16 23:07:40.577: INFO: Pod "pod-secrets-5f602222-0446-4973-aa31-27a80c2ed316": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00577424s
Sep 16 23:07:42.580: INFO: Pod "pod-secrets-5f602222-0446-4973-aa31-27a80c2ed316": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009287637s
STEP: Saw pod success
Sep 16 23:07:42.580: INFO: Pod "pod-secrets-5f602222-0446-4973-aa31-27a80c2ed316" satisfied condition "Succeeded or Failed"
Sep 16 23:07:42.582: INFO: Trying to get logs from node eqx03-flash06 pod pod-secrets-5f602222-0446-4973-aa31-27a80c2ed316 container secret-volume-test: <nil>
STEP: delete the pod
Sep 16 23:07:42.621: INFO: Waiting for pod pod-secrets-5f602222-0446-4973-aa31-27a80c2ed316 to disappear
Sep 16 23:07:42.623: INFO: Pod pod-secrets-5f602222-0446-4973-aa31-27a80c2ed316 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:07:42.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7269" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":277,"completed":3,"skipped":118,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:07:42.632: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 16 23:07:43.172: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 16 23:07:45.181: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894463, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894463, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894463, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894463, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 16 23:07:47.185: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894463, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894463, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894463, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894463, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 16 23:07:49.185: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894463, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894463, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894463, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894463, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 16 23:07:51.185: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894463, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894463, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894463, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894463, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 16 23:07:53.185: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894463, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894463, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894463, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894463, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 16 23:07:56.194: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 16 23:07:56.197: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8358-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:07:57.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3662" for this suite.
STEP: Destroying namespace "webhook-3662-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:14.708 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":277,"completed":4,"skipped":130,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:07:57.341: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 16 23:07:58.855: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 16 23:08:00.864: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894478, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894478, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894478, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894478, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 16 23:08:03.881: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:08:03.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5576" for this suite.
STEP: Destroying namespace "webhook-5576-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.636 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":277,"completed":5,"skipped":192,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:08:03.978: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 16 23:08:04.044: INFO: Waiting up to 5m0s for pod "busybox-user-65534-64fbc2b5-cb03-4f66-850f-62a8ae488f38" in namespace "security-context-test-6490" to be "Succeeded or Failed"
Sep 16 23:08:04.047: INFO: Pod "busybox-user-65534-64fbc2b5-cb03-4f66-850f-62a8ae488f38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.403512ms
Sep 16 23:08:06.050: INFO: Pod "busybox-user-65534-64fbc2b5-cb03-4f66-850f-62a8ae488f38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005370607s
Sep 16 23:08:08.054: INFO: Pod "busybox-user-65534-64fbc2b5-cb03-4f66-850f-62a8ae488f38": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009326105s
Sep 16 23:08:10.058: INFO: Pod "busybox-user-65534-64fbc2b5-cb03-4f66-850f-62a8ae488f38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013150335s
Sep 16 23:08:10.058: INFO: Pod "busybox-user-65534-64fbc2b5-cb03-4f66-850f-62a8ae488f38" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:08:10.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6490" for this suite.

• [SLOW TEST:6.089 seconds]
[k8s.io] Security Context
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  When creating a container with runAsUser
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:45
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":6,"skipped":209,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:08:10.067: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 16 23:08:10.120: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-0ae5f1cb-36ff-44de-8695-a4a5294719b2" in namespace "security-context-test-7756" to be "Succeeded or Failed"
Sep 16 23:08:10.123: INFO: Pod "busybox-privileged-false-0ae5f1cb-36ff-44de-8695-a4a5294719b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.233583ms
Sep 16 23:08:12.126: INFO: Pod "busybox-privileged-false-0ae5f1cb-36ff-44de-8695-a4a5294719b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005714671s
Sep 16 23:08:12.126: INFO: Pod "busybox-privileged-false-0ae5f1cb-36ff-44de-8695-a4a5294719b2" satisfied condition "Succeeded or Failed"
Sep 16 23:08:12.168: INFO: Got logs for pod "busybox-privileged-false-0ae5f1cb-36ff-44de-8695-a4a5294719b2": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:08:12.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7756" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":7,"skipped":283,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:08:12.178: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl replace
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 16 23:08:12.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-4584'
Sep 16 23:08:12.342: INFO: stderr: ""
Sep 16 23:08:12.342: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Sep 16 23:08:22.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pod e2e-test-httpd-pod --namespace=kubectl-4584 -o json'
Sep 16 23:08:22.481: INFO: stderr: ""
Sep 16 23:08:22.481: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"172.21.0.39/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.21.0.39/32\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"calico\\\",\\n    \\\"ips\\\": [\\n        \\\"172.21.0.39\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"calico\\\",\\n    \\\"ips\\\": [\\n        \\\"172.21.0.39\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\"\n        },\n        \"creationTimestamp\": \"2020-09-16T23:08:12Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-09-16T23:08:12Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \".\": {},\n                            \"f:cni.projectcalico.org/podIP\": {},\n                            \"f:cni.projectcalico.org/podIPs\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-09-16T23:08:13Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \"f:k8s.v1.cni.cncf.io/network-status\": {},\n                            \"f:k8s.v1.cni.cncf.io/networks-status\": {}\n                        }\n                    }\n                },\n                \"manager\": \"multus\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-09-16T23:08:14Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"172.21.0.39\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-09-16T23:08:22Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4584\",\n        \"resourceVersion\": \"4219491\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-4584/pods/e2e-test-httpd-pod\",\n        \"uid\": \"8b642cbd-c5ef-4d86-a358-a568533a5645\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-zb622\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"eqx03-flash06\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-zb622\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-zb622\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-09-16T23:08:12Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-09-16T23:08:22Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-09-16T23:08:22Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-09-16T23:08:12Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"robin://6479670d6485f9377363db6a48e13a59387c4fe27b3712a95dd737e0ed3d6bb7\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-09-16T23:08:21Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.9.140.106\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.21.0.39\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.21.0.39\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-09-16T23:08:12Z\"\n    }\n}\n"
STEP: replace the image in the pod
Sep 16 23:08:22.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 replace -f - --namespace=kubectl-4584'
Sep 16 23:08:22.845: INFO: stderr: ""
Sep 16 23:08:22.845: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
Sep 16 23:08:22.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 delete pods e2e-test-httpd-pod --namespace=kubectl-4584'
Sep 16 23:08:27.402: INFO: stderr: ""
Sep 16 23:08:27.402: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:08:27.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4584" for this suite.

• [SLOW TEST:15.236 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1450
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":277,"completed":8,"skipped":298,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:08:27.414: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2669.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2669.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2669.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2669.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2669.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2669.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 16 23:08:41.516: INFO: Unable to read wheezy_udp@PodARecord from pod dns-2669/dns-test-bd8a2c0b-5a19-4fae-9404-e14a22983253: the server could not find the requested resource (get pods dns-test-bd8a2c0b-5a19-4fae-9404-e14a22983253)
Sep 16 23:08:41.518: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-2669/dns-test-bd8a2c0b-5a19-4fae-9404-e14a22983253: the server could not find the requested resource (get pods dns-test-bd8a2c0b-5a19-4fae-9404-e14a22983253)
Sep 16 23:08:41.531: INFO: Lookups using dns-2669/dns-test-bd8a2c0b-5a19-4fae-9404-e14a22983253 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord]

Sep 16 23:08:46.555: INFO: DNS probes using dns-2669/dns-test-bd8a2c0b-5a19-4fae-9404-e14a22983253 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:08:46.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2669" for this suite.

• [SLOW TEST:19.174 seconds]
[sig-network] DNS
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":277,"completed":9,"skipped":330,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:08:46.588: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 16 23:08:46.639: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Sep 16 23:08:51.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 --namespace=crd-publish-openapi-18 create -f -'
Sep 16 23:08:51.866: INFO: stderr: ""
Sep 16 23:08:51.866: INFO: stdout: "e2e-test-crd-publish-openapi-1786-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep 16 23:08:51.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 --namespace=crd-publish-openapi-18 delete e2e-test-crd-publish-openapi-1786-crds test-foo'
Sep 16 23:08:52.010: INFO: stderr: ""
Sep 16 23:08:52.011: INFO: stdout: "e2e-test-crd-publish-openapi-1786-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Sep 16 23:08:52.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 --namespace=crd-publish-openapi-18 apply -f -'
Sep 16 23:08:52.345: INFO: stderr: ""
Sep 16 23:08:52.345: INFO: stdout: "e2e-test-crd-publish-openapi-1786-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep 16 23:08:52.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 --namespace=crd-publish-openapi-18 delete e2e-test-crd-publish-openapi-1786-crds test-foo'
Sep 16 23:08:52.447: INFO: stderr: ""
Sep 16 23:08:52.447: INFO: stdout: "e2e-test-crd-publish-openapi-1786-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Sep 16 23:08:52.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 --namespace=crd-publish-openapi-18 create -f -'
Sep 16 23:08:52.659: INFO: rc: 1
Sep 16 23:08:52.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 --namespace=crd-publish-openapi-18 apply -f -'
Sep 16 23:08:52.874: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Sep 16 23:08:52.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 --namespace=crd-publish-openapi-18 create -f -'
Sep 16 23:08:53.090: INFO: rc: 1
Sep 16 23:08:53.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 --namespace=crd-publish-openapi-18 apply -f -'
Sep 16 23:08:53.405: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Sep 16 23:08:53.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 explain e2e-test-crd-publish-openapi-1786-crds'
Sep 16 23:08:53.718: INFO: stderr: ""
Sep 16 23:08:53.718: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1786-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Sep 16 23:08:53.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 explain e2e-test-crd-publish-openapi-1786-crds.metadata'
Sep 16 23:08:54.052: INFO: stderr: ""
Sep 16 23:08:54.052: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1786-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Sep 16 23:08:54.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 explain e2e-test-crd-publish-openapi-1786-crds.spec'
Sep 16 23:08:54.353: INFO: stderr: ""
Sep 16 23:08:54.353: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1786-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Sep 16 23:08:54.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 explain e2e-test-crd-publish-openapi-1786-crds.spec.bars'
Sep 16 23:08:54.688: INFO: stderr: ""
Sep 16 23:08:54.688: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1786-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Sep 16 23:08:54.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 explain e2e-test-crd-publish-openapi-1786-crds.spec.bars2'
Sep 16 23:08:55.021: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:09:00.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-18" for this suite.

• [SLOW TEST:13.830 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":277,"completed":10,"skipped":333,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:09:00.419: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod liveness-b1a76a90-60b9-4fdd-bfce-1e3a89b039b1 in namespace container-probe-6870
Sep 16 23:09:04.487: INFO: Started pod liveness-b1a76a90-60b9-4fdd-bfce-1e3a89b039b1 in namespace container-probe-6870
STEP: checking the pod's current state and verifying that restartCount is present
Sep 16 23:09:04.490: INFO: Initial restart count of pod liveness-b1a76a90-60b9-4fdd-bfce-1e3a89b039b1 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:13:04.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6870" for this suite.

• [SLOW TEST:244.515 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":277,"completed":11,"skipped":335,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:13:04.934: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:13:16.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4417" for this suite.

• [SLOW TEST:11.113 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":277,"completed":12,"skipped":343,"failed":0}
SS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:13:16.047: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 16 23:13:16.102: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Sep 16 23:13:18.136: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:13:19.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4740" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":277,"completed":13,"skipped":345,"failed":0}
SSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:13:19.155: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod liveness-26397049-d34d-4f13-8796-a9dee9494d2d in namespace container-probe-9175
Sep 16 23:13:23.216: INFO: Started pod liveness-26397049-d34d-4f13-8796-a9dee9494d2d in namespace container-probe-9175
STEP: checking the pod's current state and verifying that restartCount is present
Sep 16 23:13:23.219: INFO: Initial restart count of pod liveness-26397049-d34d-4f13-8796-a9dee9494d2d is 0
Sep 16 23:13:45.258: INFO: Restart count of pod container-probe-9175/liveness-26397049-d34d-4f13-8796-a9dee9494d2d is now 1 (22.039035733s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:13:45.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9175" for this suite.

• [SLOW TEST:26.120 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":277,"completed":14,"skipped":351,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:13:45.276: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0916 23:13:55.392043      23 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep 16 23:13:55.392: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:13:55.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8170" for this suite.

• [SLOW TEST:10.123 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":277,"completed":15,"skipped":396,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:13:55.399: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 16 23:13:55.814: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 16 23:13:57.822: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894835, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894835, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894835, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894835, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 16 23:14:00.836: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:14:00.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6512" for this suite.
STEP: Destroying namespace "webhook-6512-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.572 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":277,"completed":16,"skipped":400,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:14:00.971: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep 16 23:14:01.062: INFO: Waiting up to 5m0s for pod "pod-450c83c4-f4bb-4e09-82be-8d423a88979a" in namespace "emptydir-4157" to be "Succeeded or Failed"
Sep 16 23:14:01.064: INFO: Pod "pod-450c83c4-f4bb-4e09-82be-8d423a88979a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.918426ms
Sep 16 23:14:03.070: INFO: Pod "pod-450c83c4-f4bb-4e09-82be-8d423a88979a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007217517s
Sep 16 23:14:05.073: INFO: Pod "pod-450c83c4-f4bb-4e09-82be-8d423a88979a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0108896s
Sep 16 23:14:07.077: INFO: Pod "pod-450c83c4-f4bb-4e09-82be-8d423a88979a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01436346s
Sep 16 23:14:09.080: INFO: Pod "pod-450c83c4-f4bb-4e09-82be-8d423a88979a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.017902639s
Sep 16 23:14:11.084: INFO: Pod "pod-450c83c4-f4bb-4e09-82be-8d423a88979a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.021391699s
STEP: Saw pod success
Sep 16 23:14:11.084: INFO: Pod "pod-450c83c4-f4bb-4e09-82be-8d423a88979a" satisfied condition "Succeeded or Failed"
Sep 16 23:14:11.086: INFO: Trying to get logs from node eqx03-flash07 pod pod-450c83c4-f4bb-4e09-82be-8d423a88979a container test-container: <nil>
STEP: delete the pod
Sep 16 23:14:11.137: INFO: Waiting for pod pod-450c83c4-f4bb-4e09-82be-8d423a88979a to disappear
Sep 16 23:14:11.139: INFO: Pod pod-450c83c4-f4bb-4e09-82be-8d423a88979a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:14:11.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4157" for this suite.

• [SLOW TEST:10.181 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":17,"skipped":403,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:14:11.153: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:14:15.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7356" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":277,"completed":18,"skipped":427,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:14:15.266: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:14:27.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-348" for this suite.

• [SLOW TEST:12.070 seconds]
[sig-apps] Job
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":277,"completed":19,"skipped":439,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:14:27.337: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test override arguments
Sep 16 23:14:27.454: INFO: Waiting up to 5m0s for pod "client-containers-6e090c1f-c6d2-4d4e-903c-f6da6a8b2129" in namespace "containers-7616" to be "Succeeded or Failed"
Sep 16 23:14:27.457: INFO: Pod "client-containers-6e090c1f-c6d2-4d4e-903c-f6da6a8b2129": Phase="Pending", Reason="", readiness=false. Elapsed: 2.306111ms
Sep 16 23:14:29.460: INFO: Pod "client-containers-6e090c1f-c6d2-4d4e-903c-f6da6a8b2129": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006011114s
Sep 16 23:14:31.464: INFO: Pod "client-containers-6e090c1f-c6d2-4d4e-903c-f6da6a8b2129": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00980014s
Sep 16 23:14:33.467: INFO: Pod "client-containers-6e090c1f-c6d2-4d4e-903c-f6da6a8b2129": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013013162s
Sep 16 23:14:35.618: INFO: Pod "client-containers-6e090c1f-c6d2-4d4e-903c-f6da6a8b2129": Phase="Pending", Reason="", readiness=false. Elapsed: 8.163739655s
Sep 16 23:14:37.622: INFO: Pod "client-containers-6e090c1f-c6d2-4d4e-903c-f6da6a8b2129": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.167829401s
STEP: Saw pod success
Sep 16 23:14:37.622: INFO: Pod "client-containers-6e090c1f-c6d2-4d4e-903c-f6da6a8b2129" satisfied condition "Succeeded or Failed"
Sep 16 23:14:37.625: INFO: Trying to get logs from node eqx03-flash07 pod client-containers-6e090c1f-c6d2-4d4e-903c-f6da6a8b2129 container test-container: <nil>
STEP: delete the pod
Sep 16 23:14:37.734: INFO: Waiting for pod client-containers-6e090c1f-c6d2-4d4e-903c-f6da6a8b2129 to disappear
Sep 16 23:14:37.737: INFO: Pod client-containers-6e090c1f-c6d2-4d4e-903c-f6da6a8b2129 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:14:37.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7616" for this suite.

• [SLOW TEST:10.408 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":277,"completed":20,"skipped":446,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:14:37.745: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 16 23:14:37.819: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f1e71596-a1c5-4084-bc5f-553655e9fd63" in namespace "downward-api-7681" to be "Succeeded or Failed"
Sep 16 23:14:37.821: INFO: Pod "downwardapi-volume-f1e71596-a1c5-4084-bc5f-553655e9fd63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.139825ms
Sep 16 23:14:39.825: INFO: Pod "downwardapi-volume-f1e71596-a1c5-4084-bc5f-553655e9fd63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005920258s
Sep 16 23:14:41.829: INFO: Pod "downwardapi-volume-f1e71596-a1c5-4084-bc5f-553655e9fd63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009991791s
STEP: Saw pod success
Sep 16 23:14:41.829: INFO: Pod "downwardapi-volume-f1e71596-a1c5-4084-bc5f-553655e9fd63" satisfied condition "Succeeded or Failed"
Sep 16 23:14:41.832: INFO: Trying to get logs from node eqx03-flash06 pod downwardapi-volume-f1e71596-a1c5-4084-bc5f-553655e9fd63 container client-container: <nil>
STEP: delete the pod
Sep 16 23:14:41.857: INFO: Waiting for pod downwardapi-volume-f1e71596-a1c5-4084-bc5f-553655e9fd63 to disappear
Sep 16 23:14:41.859: INFO: Pod downwardapi-volume-f1e71596-a1c5-4084-bc5f-553655e9fd63 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:14:41.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7681" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":277,"completed":21,"skipped":447,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:14:41.868: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 16 23:14:41.936: INFO: Waiting up to 5m0s for pod "downwardapi-volume-85155df8-5f8e-44f6-ab61-5b33427b41d8" in namespace "projected-9572" to be "Succeeded or Failed"
Sep 16 23:14:41.939: INFO: Pod "downwardapi-volume-85155df8-5f8e-44f6-ab61-5b33427b41d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.952259ms
Sep 16 23:14:43.942: INFO: Pod "downwardapi-volume-85155df8-5f8e-44f6-ab61-5b33427b41d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006364281s
Sep 16 23:14:45.946: INFO: Pod "downwardapi-volume-85155df8-5f8e-44f6-ab61-5b33427b41d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010313631s
STEP: Saw pod success
Sep 16 23:14:45.947: INFO: Pod "downwardapi-volume-85155df8-5f8e-44f6-ab61-5b33427b41d8" satisfied condition "Succeeded or Failed"
Sep 16 23:14:45.949: INFO: Trying to get logs from node eqx03-flash06 pod downwardapi-volume-85155df8-5f8e-44f6-ab61-5b33427b41d8 container client-container: <nil>
STEP: delete the pod
Sep 16 23:14:45.978: INFO: Waiting for pod downwardapi-volume-85155df8-5f8e-44f6-ab61-5b33427b41d8 to disappear
Sep 16 23:14:45.980: INFO: Pod downwardapi-volume-85155df8-5f8e-44f6-ab61-5b33427b41d8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:14:45.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9572" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":22,"skipped":480,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:14:45.991: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Sep 16 23:14:46.045: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 16 23:14:46.060: INFO: Waiting for terminating namespaces to be deleted...
Sep 16 23:14:46.064: INFO: 
Logging pods the kubelet thinks is on node eqx03-flash06 before test
Sep 16 23:14:46.084: INFO: kube-scheduler-eqx03-flash06 from kube-system started at 2020-09-04 20:35:41 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.084: INFO: 	Container kube-scheduler ready: true, restart count 0
Sep 16 23:14:46.084: INFO: csi-nodeplugin-robin-t4vbj from robinio started at 2020-09-09 19:56:04 +0000 UTC (2 container statuses recorded)
Sep 16 23:14:46.084: INFO: 	Container driver-registrar ready: true, restart count 0
Sep 16 23:14:46.084: INFO: 	Container robin ready: true, restart count 0
Sep 16 23:14:46.084: INFO: csi-snapshotter-robin-0 from robinio started at 2020-09-09 19:56:45 +0000 UTC (2 container statuses recorded)
Sep 16 23:14:46.084: INFO: 	Container csi-snapshotter ready: true, restart count 0
Sep 16 23:14:46.084: INFO: 	Container robin ready: true, restart count 0
Sep 16 23:14:46.084: INFO: kube-multus-ds-amd64-fp5v6 from kube-system started at 2020-09-04 20:35:41 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.084: INFO: 	Container kube-multus ready: true, restart count 0
Sep 16 23:14:46.084: INFO: kube-proxy-mchgt from kube-system started at 2020-09-04 20:35:41 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.084: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 16 23:14:46.084: INFO: neo-neo4j-core-1 from madhura started at 2020-09-08 16:38:25 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.084: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 16 23:14:46.084: INFO: neo-neo4j-core-0 from ripul started at 2020-09-09 05:07:44 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.084: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 16 23:14:46.084: INFO: csi-attacher-robin-5d956884cf-ztfp8 from robinio started at 2020-09-09 19:56:03 +0000 UTC (2 container statuses recorded)
Sep 16 23:14:46.084: INFO: 	Container csi-attacher ready: true, restart count 0
Sep 16 23:14:46.084: INFO: 	Container robin ready: true, restart count 0
Sep 16 23:14:46.084: INFO: sonobuoy-e2e-job-362cab5d438b4e90 from sonobuoy started at 2020-09-16 23:06:47 +0000 UTC (2 container statuses recorded)
Sep 16 23:14:46.084: INFO: 	Container e2e ready: true, restart count 0
Sep 16 23:14:46.084: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 16 23:14:46.084: INFO: sonobuoy-systemd-logs-daemon-set-71797a053a4d44ec-pp8g4 from sonobuoy started at 2020-09-16 23:06:47 +0000 UTC (2 container statuses recorded)
Sep 16 23:14:46.084: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 16 23:14:46.084: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 16 23:14:46.084: INFO: kube-sriov-device-plugin-amd64-jshrq from kube-system started at 2020-09-04 20:36:01 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.084: INFO: 	Container kube-sriovdp ready: true, restart count 0
Sep 16 23:14:46.084: INFO: neo-neo4j-core-2 from madhura started at 2020-09-08 16:38:25 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.084: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 16 23:14:46.084: INFO: my-csi-robin-app from default started at 2020-09-09 04:40:50 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.084: INFO: 	Container my-frontend ready: true, restart count 0
Sep 16 23:14:46.084: INFO: neo-neo4j-core-1 from ripul started at 2020-09-09 05:07:45 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.084: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 16 23:14:46.084: INFO: neo-neo4j-core-1 from default started at 2020-09-09 05:50:29 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.084: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 16 23:14:46.084: INFO: etcd-eqx03-flash06 from kube-system started at 2020-09-04 20:35:41 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.084: INFO: 	Container etcd ready: true, restart count 0
Sep 16 23:14:46.084: INFO: neo-neo4j-core-2 from default started at 2020-09-09 05:50:30 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.084: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 16 23:14:46.084: INFO: calico-node-zqtk2 from kube-system started at 2020-09-09 19:40:56 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.084: INFO: 	Container calico-node ready: true, restart count 0
Sep 16 23:14:46.084: INFO: neo-neo4j-replica-0 from default started at 2020-09-09 05:50:31 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.084: INFO: 	Container neo4j ready: true, restart count 0
Sep 16 23:14:46.084: INFO: neo-neo4j-core-0 from default started at 2020-09-09 05:50:30 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.084: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 16 23:14:46.084: INFO: robin-master-mkdgv from robinio started at 2020-09-09 19:48:01 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.084: INFO: 	Container robinrcm ready: true, restart count 0
Sep 16 23:14:46.084: INFO: neo-neo4j-replica-0 from ripul started at 2020-09-09 05:07:45 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.084: INFO: 	Container neo4j ready: true, restart count 0
Sep 16 23:14:46.084: INFO: neo-neo4j-core-2 from ripul started at 2020-09-09 05:07:45 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.084: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 16 23:14:46.084: INFO: kube-apiserver-eqx03-flash06 from kube-system started at 2020-09-04 20:36:01 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.084: INFO: 	Container kube-apiserver ready: true, restart count 0
Sep 16 23:14:46.084: INFO: neo-neo4j-replica-0 from madhura started at 2020-09-08 16:38:24 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.084: INFO: 	Container neo4j ready: true, restart count 0
Sep 16 23:14:46.084: INFO: neo-neo4j-core-0 from madhura started at 2020-09-08 16:38:25 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.084: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 16 23:14:46.084: INFO: csi-provisioner-robin-6646bdd46b-hfqj5 from robinio started at 2020-09-09 19:56:03 +0000 UTC (2 container statuses recorded)
Sep 16 23:14:46.084: INFO: 	Container csi-provisioner ready: true, restart count 0
Sep 16 23:14:46.084: INFO: 	Container robin ready: true, restart count 0
Sep 16 23:14:46.084: INFO: kube-controller-manager-eqx03-flash06 from kube-system started at 2020-09-04 20:36:01 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.084: INFO: 	Container kube-controller-manager ready: true, restart count 1
Sep 16 23:14:46.084: INFO: 
Logging pods the kubelet thinks is on node eqx03-flash07 before test
Sep 16 23:14:46.106: INFO: snapshot-controller-0 from robinio started at 2020-09-09 19:56:15 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.106: INFO: 	Container snapshot-controller ready: true, restart count 0
Sep 16 23:14:46.106: INFO: kube-controller-manager-eqx03-flash07 from kube-system started at 2020-09-04 20:24:11 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.106: INFO: 	Container kube-controller-manager ready: true, restart count 1
Sep 16 23:14:46.106: INFO: hari-elasticsearch-client-c59586c8-k84fl from spk started at 2020-09-05 06:37:23 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.106: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:14:46.106: INFO: hari-logstash-0 from spk started at 2020-09-05 06:37:24 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.106: INFO: 	Container logstash ready: true, restart count 0
Sep 16 23:14:46.106: INFO: test1-kibana-84f6c46bf6-mndsz from t001-u000003 started at 2020-09-05 06:42:07 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.106: INFO: 	Container kibana ready: true, restart count 0
Sep 16 23:14:46.106: INFO: test1-logstash-0 from t001-u000003 started at 2020-09-05 06:42:09 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.106: INFO: 	Container logstash ready: true, restart count 0
Sep 16 23:14:46.106: INFO: csi-nodeplugin-robin-bcq5b from robinio started at 2020-09-09 19:56:04 +0000 UTC (2 container statuses recorded)
Sep 16 23:14:46.106: INFO: 	Container driver-registrar ready: true, restart count 0
Sep 16 23:14:46.106: INFO: 	Container robin ready: true, restart count 0
Sep 16 23:14:46.106: INFO: elkname-elasticsearch-data-0 from t001-u000003 started at 2020-09-05 06:08:19 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.106: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:14:46.106: INFO: kube-scheduler-eqx03-flash07 from kube-system started at 2020-09-04 20:24:11 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.106: INFO: 	Container kube-scheduler ready: true, restart count 0
Sep 16 23:14:46.106: INFO: cert-manager-cainjector-5ffff9dd7c-7fv5l from cert-manager started at 2020-09-04 20:38:07 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.106: INFO: 	Container cert-manager ready: true, restart count 0
Sep 16 23:14:46.106: INFO: mg-runner-gitlab-runner-97dd45f4-8z4zg from gitlab-runner started at 2020-09-10 19:25:18 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.106: INFO: 	Container mg-runner-gitlab-runner ready: false, restart count 1144
Sep 16 23:14:46.106: INFO: hari-elasticsearch-data-0 from spk started at 2020-09-05 06:37:25 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.106: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:14:46.106: INFO: test1-elasticsearch-client-df46b4cd8-2knzt from t001-u000003 started at 2020-09-05 06:42:07 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.106: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:14:46.106: INFO: calico-node-8t2kf from kube-system started at 2020-09-09 19:41:23 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.106: INFO: 	Container calico-node ready: true, restart count 0
Sep 16 23:14:46.106: INFO: elkname-kibana-56c986874d-nx4t6 from t001-u000003 started at 2020-09-05 06:08:19 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.106: INFO: 	Container kibana ready: true, restart count 0
Sep 16 23:14:46.106: INFO: test1-elasticsearch-data-0 from t001-u000003 started at 2020-09-05 06:42:08 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.106: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:14:46.106: INFO: sonobuoy from sonobuoy started at 2020-09-16 23:06:40 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.106: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 16 23:14:46.106: INFO: kube-proxy-9zgnq from kube-system started at 2020-09-04 20:24:11 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.106: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 16 23:14:46.106: INFO: etcd-eqx03-flash07 from kube-system started at 2020-09-04 20:24:37 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.106: INFO: 	Container etcd ready: true, restart count 0
Sep 16 23:14:46.106: INFO: kube-sriov-device-plugin-amd64-rxs7g from kube-system started at 2020-09-04 20:25:01 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.106: INFO: 	Container kube-sriovdp ready: true, restart count 0
Sep 16 23:14:46.106: INFO: cert-manager-578cd6d964-sq59x from cert-manager started at 2020-09-04 20:38:07 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.106: INFO: 	Container cert-manager ready: true, restart count 0
Sep 16 23:14:46.106: INFO: elkname-logstash-0 from t001-u000003 started at 2020-09-05 06:08:19 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.106: INFO: 	Container logstash ready: true, restart count 0
Sep 16 23:14:46.106: INFO: hari-kibana-b6f7d64-j96s2 from spk started at 2020-09-05 06:37:23 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.106: INFO: 	Container kibana ready: true, restart count 0
Sep 16 23:14:46.106: INFO: csi-resizer-robin-7d566f9df9-sq477 from robinio started at 2020-09-09 19:56:05 +0000 UTC (2 container statuses recorded)
Sep 16 23:14:46.106: INFO: 	Container csi-resizer ready: true, restart count 0
Sep 16 23:14:46.106: INFO: 	Container robin ready: true, restart count 0
Sep 16 23:14:46.106: INFO: sonobuoy-systemd-logs-daemon-set-71797a053a4d44ec-4zr6m from sonobuoy started at 2020-09-16 23:06:47 +0000 UTC (2 container statuses recorded)
Sep 16 23:14:46.106: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 16 23:14:46.106: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 16 23:14:46.106: INFO: robin-master-w2m7f from robinio started at 2020-09-09 19:47:17 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.106: INFO: 	Container robinrcm ready: true, restart count 0
Sep 16 23:14:46.106: INFO: cert-manager-webhook-556b9d7dfd-swlll from cert-manager started at 2020-09-04 20:38:07 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.106: INFO: 	Container cert-manager ready: true, restart count 0
Sep 16 23:14:46.106: INFO: elkname-elasticsearch-client-6768458985-6sgxz from t001-u000003 started at 2020-09-05 06:08:19 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.106: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:14:46.106: INFO: elkname-elasticsearch-master-0 from t001-u000003 started at 2020-09-05 06:08:19 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.106: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:14:46.106: INFO: kube-multus-ds-amd64-lf8vr from kube-system started at 2020-09-04 20:24:11 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.106: INFO: 	Container kube-multus ready: true, restart count 0
Sep 16 23:14:46.106: INFO: kube-apiserver-eqx03-flash07 from kube-system started at 2020-09-04 20:24:11 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.106: INFO: 	Container kube-apiserver ready: true, restart count 0
Sep 16 23:14:46.106: INFO: coredns-6878cb8f64-7rgtp from kube-system started at 2020-09-04 20:25:00 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.106: INFO: 	Container coredns ready: true, restart count 0
Sep 16 23:14:46.106: INFO: 
Logging pods the kubelet thinks is on node eqx04-flash04 before test
Sep 16 23:14:46.128: INFO: kube-sriov-device-plugin-amd64-xlvxl from kube-system started at 2020-09-04 20:12:30 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.128: INFO: 	Container kube-sriovdp ready: true, restart count 0
Sep 16 23:14:46.128: INFO: test1-elasticsearch-data-1 from t001-u000003 started at 2020-09-05 06:44:40 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.128: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:14:46.128: INFO: calico-node-wpq99 from kube-system started at 2020-09-09 19:41:33 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.128: INFO: 	Container calico-node ready: true, restart count 0
Sep 16 23:14:46.128: INFO: kube-scheduler-eqx04-flash04 from kube-system started at 2020-09-04 20:12:22 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.128: INFO: 	Container kube-scheduler ready: true, restart count 2
Sep 16 23:14:46.128: INFO: calico-kube-controllers-6c49f88586-jzdjm from kube-system started at 2020-09-04 20:12:28 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.128: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep 16 23:14:46.128: INFO: sonobuoy-systemd-logs-daemon-set-71797a053a4d44ec-dx2j7 from sonobuoy started at 2020-09-16 23:06:47 +0000 UTC (2 container statuses recorded)
Sep 16 23:14:46.128: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 16 23:14:46.128: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 16 23:14:46.128: INFO: kube-apiserver-eqx04-flash04 from kube-system started at 2020-09-04 20:12:22 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.128: INFO: 	Container kube-apiserver ready: true, restart count 0
Sep 16 23:14:46.128: INFO: robin-master-hkj7r from robinio started at 2020-09-09 19:48:36 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.129: INFO: 	Container robinrcm ready: true, restart count 0
Sep 16 23:14:46.129: INFO: kube-controller-manager-eqx04-flash04 from kube-system started at 2020-09-04 20:12:22 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.129: INFO: 	Container kube-controller-manager ready: true, restart count 1
Sep 16 23:14:46.129: INFO: kube-multus-ds-amd64-mzx44 from kube-system started at 2020-09-04 20:12:27 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.129: INFO: 	Container kube-multus ready: true, restart count 0
Sep 16 23:14:46.129: INFO: coredns-6878cb8f64-n75hx from kube-system started at 2020-09-04 20:12:55 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.129: INFO: 	Container coredns ready: true, restart count 0
Sep 16 23:14:46.129: INFO: elkname-elasticsearch-master-2 from t001-u000003 started at 2020-09-05 06:09:50 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.129: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:14:46.129: INFO: hari-elasticsearch-master-0 from spk started at 2020-09-05 06:37:24 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.129: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:14:46.129: INFO: csi-nodeplugin-robin-6xzkh from robinio started at 2020-09-09 19:56:04 +0000 UTC (2 container statuses recorded)
Sep 16 23:14:46.129: INFO: 	Container driver-registrar ready: true, restart count 0
Sep 16 23:14:46.129: INFO: 	Container robin ready: true, restart count 0
Sep 16 23:14:46.129: INFO: kube-proxy-2mmk7 from kube-system started at 2020-09-04 20:12:22 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.129: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 16 23:14:46.129: INFO: etcd-eqx04-flash04 from kube-system started at 2020-09-04 20:12:22 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.129: INFO: 	Container etcd ready: true, restart count 0
Sep 16 23:14:46.129: INFO: 
Logging pods the kubelet thinks is on node eqx04-flash06 before test
Sep 16 23:14:46.154: INFO: elkname-elasticsearch-master-1 from t001-u000003 started at 2020-09-05 06:09:25 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.154: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:14:46.154: INFO: test1-elasticsearch-master-1 from t001-u000003 started at 2020-09-05 06:43:08 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.154: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:14:46.154: INFO: test1-elasticsearch-master-2 from t001-u000003 started at 2020-09-05 06:43:44 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.154: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:14:46.154: INFO: calico-node-x8tqn from kube-system started at 2020-09-09 19:41:10 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.154: INFO: 	Container calico-node ready: true, restart count 0
Sep 16 23:14:46.154: INFO: kube-multus-ds-amd64-j9jmp from kube-system started at 2020-09-04 20:34:34 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.154: INFO: 	Container kube-multus ready: true, restart count 0
Sep 16 23:14:46.154: INFO: elkname-elasticsearch-data-1 from t001-u000003 started at 2020-09-05 06:10:51 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.154: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:14:46.154: INFO: test1-elasticsearch-master-0 from t001-u000003 started at 2020-09-05 06:42:08 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.154: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:14:46.154: INFO: gitlab-runner-gitlab-runner-69dbb57dd4-sptqz from gitlab-runner started at 2020-09-10 19:12:45 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.154: INFO: 	Container gitlab-runner-gitlab-runner ready: false, restart count 0
Sep 16 23:14:46.154: INFO: hari-elasticsearch-client-c59586c8-xvm2k from spk started at 2020-09-05 06:37:23 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.154: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:14:46.154: INFO: hari-elasticsearch-master-1 from spk started at 2020-09-05 06:38:21 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.154: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:14:46.154: INFO: elkname-elasticsearch-client-6768458985-j2fnt from t001-u000003 started at 2020-09-05 06:08:19 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.154: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:14:46.154: INFO: hari-elasticsearch-master-2 from spk started at 2020-09-05 06:38:48 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.154: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:14:46.154: INFO: test1-elasticsearch-client-df46b4cd8-52bhb from t001-u000003 started at 2020-09-05 06:42:07 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.154: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:14:46.154: INFO: csi-nodeplugin-robin-wk7pp from robinio started at 2020-09-09 19:56:04 +0000 UTC (2 container statuses recorded)
Sep 16 23:14:46.154: INFO: 	Container driver-registrar ready: true, restart count 0
Sep 16 23:14:46.154: INFO: 	Container robin ready: true, restart count 0
Sep 16 23:14:46.154: INFO: sonobuoy-systemd-logs-daemon-set-71797a053a4d44ec-w59x2 from sonobuoy started at 2020-09-16 23:06:47 +0000 UTC (2 container statuses recorded)
Sep 16 23:14:46.154: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 16 23:14:46.154: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 16 23:14:46.154: INFO: kube-proxy-n5ckt from kube-system started at 2020-09-04 20:34:34 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.154: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 16 23:14:46.154: INFO: robin-worker-6zwcm from robinio started at 2020-09-09 19:53:16 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.154: INFO: 	Container robinrcm ready: true, restart count 0
Sep 16 23:14:46.154: INFO: kube-sriov-device-plugin-amd64-wwdl2 from kube-system started at 2020-09-04 20:34:39 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.154: INFO: 	Container kube-sriovdp ready: true, restart count 0
Sep 16 23:14:46.154: INFO: hari-elasticsearch-data-1 from spk started at 2020-09-05 06:40:02 +0000 UTC (1 container statuses recorded)
Sep 16 23:14:46.154: INFO: 	Container elasticsearch ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-01b0a452-d68a-4a71-b680-532376c701e1 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-01b0a452-d68a-4a71-b680-532376c701e1 off the node eqx03-flash06
STEP: verifying the node doesn't have the label kubernetes.io/e2e-01b0a452-d68a-4a71-b680-532376c701e1
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:15:00.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5337" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

• [SLOW TEST:14.301 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":277,"completed":23,"skipped":493,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:15:00.293: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl label
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1206
STEP: creating the pod
Sep 16 23:15:00.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 create -f - --namespace=kubectl-617'
Sep 16 23:15:00.686: INFO: stderr: ""
Sep 16 23:15:00.686: INFO: stdout: "pod/pause created\n"
Sep 16 23:15:00.686: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Sep 16 23:15:00.686: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-617" to be "running and ready"
Sep 16 23:15:00.694: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 7.839087ms
Sep 16 23:15:02.697: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011322573s
Sep 16 23:15:04.701: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.015107584s
Sep 16 23:15:04.701: INFO: Pod "pause" satisfied condition "running and ready"
Sep 16 23:15:04.701: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: adding the label testing-label with value testing-label-value to a pod
Sep 16 23:15:04.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 label pods pause testing-label=testing-label-value --namespace=kubectl-617'
Sep 16 23:15:04.819: INFO: stderr: ""
Sep 16 23:15:04.819: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Sep 16 23:15:04.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pod pause -L testing-label --namespace=kubectl-617'
Sep 16 23:15:04.918: INFO: stderr: ""
Sep 16 23:15:04.918: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Sep 16 23:15:04.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 label pods pause testing-label- --namespace=kubectl-617'
Sep 16 23:15:05.032: INFO: stderr: ""
Sep 16 23:15:05.032: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Sep 16 23:15:05.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pod pause -L testing-label --namespace=kubectl-617'
Sep 16 23:15:05.137: INFO: stderr: ""
Sep 16 23:15:05.137: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1213
STEP: using delete to clean up resources
Sep 16 23:15:05.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 delete --grace-period=0 --force -f - --namespace=kubectl-617'
Sep 16 23:15:05.251: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 16 23:15:05.251: INFO: stdout: "pod \"pause\" force deleted\n"
Sep 16 23:15:05.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get rc,svc -l name=pause --no-headers --namespace=kubectl-617'
Sep 16 23:15:05.364: INFO: stderr: "No resources found in kubectl-617 namespace.\n"
Sep 16 23:15:05.364: INFO: stdout: ""
Sep 16 23:15:05.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pods -l name=pause --namespace=kubectl-617 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 16 23:15:05.475: INFO: stderr: ""
Sep 16 23:15:05.475: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:15:05.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-617" for this suite.

• [SLOW TEST:5.193 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1203
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":277,"completed":24,"skipped":507,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] version v1
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:15:05.486: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 16 23:15:05.558: INFO: (0) /api/v1/nodes/eqx04-flash04:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 16.681083ms)
Sep 16 23:15:05.561: INFO: (1) /api/v1/nodes/eqx04-flash04:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.258197ms)
Sep 16 23:15:05.565: INFO: (2) /api/v1/nodes/eqx04-flash04:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.649851ms)
Sep 16 23:15:05.569: INFO: (3) /api/v1/nodes/eqx04-flash04:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.32704ms)
Sep 16 23:15:05.572: INFO: (4) /api/v1/nodes/eqx04-flash04:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.285561ms)
Sep 16 23:15:05.575: INFO: (5) /api/v1/nodes/eqx04-flash04:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.284368ms)
Sep 16 23:15:05.578: INFO: (6) /api/v1/nodes/eqx04-flash04:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.227993ms)
Sep 16 23:15:05.582: INFO: (7) /api/v1/nodes/eqx04-flash04:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.331322ms)
Sep 16 23:15:05.585: INFO: (8) /api/v1/nodes/eqx04-flash04:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.130752ms)
Sep 16 23:15:05.591: INFO: (9) /api/v1/nodes/eqx04-flash04:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.604993ms)
Sep 16 23:15:05.594: INFO: (10) /api/v1/nodes/eqx04-flash04:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.316649ms)
Sep 16 23:15:05.597: INFO: (11) /api/v1/nodes/eqx04-flash04:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.031782ms)
Sep 16 23:15:05.600: INFO: (12) /api/v1/nodes/eqx04-flash04:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.171519ms)
Sep 16 23:15:05.604: INFO: (13) /api/v1/nodes/eqx04-flash04:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.413352ms)
Sep 16 23:15:05.607: INFO: (14) /api/v1/nodes/eqx04-flash04:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.303219ms)
Sep 16 23:15:05.610: INFO: (15) /api/v1/nodes/eqx04-flash04:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.231872ms)
Sep 16 23:15:05.614: INFO: (16) /api/v1/nodes/eqx04-flash04:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.266727ms)
Sep 16 23:15:05.620: INFO: (17) /api/v1/nodes/eqx04-flash04:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.698255ms)
Sep 16 23:15:05.623: INFO: (18) /api/v1/nodes/eqx04-flash04:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.160722ms)
Sep 16 23:15:05.627: INFO: (19) /api/v1/nodes/eqx04-flash04:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.08308ms)
[AfterEach] version v1
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:15:05.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6784" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]","total":277,"completed":25,"skipped":527,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:15:05.636: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 16 23:15:05.716: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Sep 16 23:15:05.734: INFO: Number of nodes with available pods: 0
Sep 16 23:15:05.734: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Sep 16 23:15:05.758: INFO: Number of nodes with available pods: 0
Sep 16 23:15:05.758: INFO: Node eqx03-flash07 is running more than one daemon pod
Sep 16 23:15:06.762: INFO: Number of nodes with available pods: 0
Sep 16 23:15:06.762: INFO: Node eqx03-flash07 is running more than one daemon pod
Sep 16 23:15:07.762: INFO: Number of nodes with available pods: 0
Sep 16 23:15:07.762: INFO: Node eqx03-flash07 is running more than one daemon pod
Sep 16 23:15:08.762: INFO: Number of nodes with available pods: 0
Sep 16 23:15:08.762: INFO: Node eqx03-flash07 is running more than one daemon pod
Sep 16 23:15:09.762: INFO: Number of nodes with available pods: 0
Sep 16 23:15:09.762: INFO: Node eqx03-flash07 is running more than one daemon pod
Sep 16 23:15:10.762: INFO: Number of nodes with available pods: 0
Sep 16 23:15:10.762: INFO: Node eqx03-flash07 is running more than one daemon pod
Sep 16 23:15:11.762: INFO: Number of nodes with available pods: 0
Sep 16 23:15:11.762: INFO: Node eqx03-flash07 is running more than one daemon pod
Sep 16 23:15:12.762: INFO: Number of nodes with available pods: 0
Sep 16 23:15:12.762: INFO: Node eqx03-flash07 is running more than one daemon pod
Sep 16 23:15:13.762: INFO: Number of nodes with available pods: 0
Sep 16 23:15:13.762: INFO: Node eqx03-flash07 is running more than one daemon pod
Sep 16 23:15:14.762: INFO: Number of nodes with available pods: 1
Sep 16 23:15:14.762: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Sep 16 23:15:14.776: INFO: Number of nodes with available pods: 1
Sep 16 23:15:14.776: INFO: Number of running nodes: 0, number of available pods: 1
Sep 16 23:15:15.779: INFO: Number of nodes with available pods: 0
Sep 16 23:15:15.779: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Sep 16 23:15:15.789: INFO: Number of nodes with available pods: 0
Sep 16 23:15:15.789: INFO: Node eqx03-flash07 is running more than one daemon pod
Sep 16 23:15:16.793: INFO: Number of nodes with available pods: 0
Sep 16 23:15:16.793: INFO: Node eqx03-flash07 is running more than one daemon pod
Sep 16 23:15:17.793: INFO: Number of nodes with available pods: 0
Sep 16 23:15:17.793: INFO: Node eqx03-flash07 is running more than one daemon pod
Sep 16 23:15:18.794: INFO: Number of nodes with available pods: 0
Sep 16 23:15:18.794: INFO: Node eqx03-flash07 is running more than one daemon pod
Sep 16 23:15:19.792: INFO: Number of nodes with available pods: 0
Sep 16 23:15:19.792: INFO: Node eqx03-flash07 is running more than one daemon pod
Sep 16 23:15:20.793: INFO: Number of nodes with available pods: 0
Sep 16 23:15:20.793: INFO: Node eqx03-flash07 is running more than one daemon pod
Sep 16 23:15:21.793: INFO: Number of nodes with available pods: 0
Sep 16 23:15:21.793: INFO: Node eqx03-flash07 is running more than one daemon pod
Sep 16 23:15:22.793: INFO: Number of nodes with available pods: 1
Sep 16 23:15:22.793: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3001, will wait for the garbage collector to delete the pods
Sep 16 23:15:22.857: INFO: Deleting DaemonSet.extensions daemon-set took: 7.071577ms
Sep 16 23:15:23.757: INFO: Terminating DaemonSet.extensions daemon-set pods took: 900.247393ms
Sep 16 23:15:27.961: INFO: Number of nodes with available pods: 0
Sep 16 23:15:27.961: INFO: Number of running nodes: 0, number of available pods: 0
Sep 16 23:15:27.965: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3001/daemonsets","resourceVersion":"4222231"},"items":null}

Sep 16 23:15:27.967: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3001/pods","resourceVersion":"4222231"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:15:27.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3001" for this suite.

• [SLOW TEST:22.363 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":277,"completed":26,"skipped":528,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:15:28.000: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 16 23:15:28.062: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Sep 16 23:15:28.071: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep 16 23:15:33.075: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 16 23:15:33.075: INFO: Creating deployment "test-rolling-update-deployment"
Sep 16 23:15:33.084: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Sep 16 23:15:33.090: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Sep 16 23:15:35.096: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Sep 16 23:15:35.098: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894933, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894933, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894933, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894933, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-59d5cb45c7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 16 23:15:37.102: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894933, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894933, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894933, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894933, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-59d5cb45c7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 16 23:15:39.102: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Sep 16 23:15:39.110: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7312 /apis/apps/v1/namespaces/deployment-7312/deployments/test-rolling-update-deployment 70e2734a-df0e-49c9-ba6e-410a264521cb 4222344 1 2020-09-16 23:15:33 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2020-09-16 23:15:33 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-09-16 23:15:37 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00310d258 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-09-16 23:15:33 +0000 UTC,LastTransitionTime:2020-09-16 23:15:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-59d5cb45c7" has successfully progressed.,LastUpdateTime:2020-09-16 23:15:37 +0000 UTC,LastTransitionTime:2020-09-16 23:15:33 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 16 23:15:39.113: INFO: New ReplicaSet "test-rolling-update-deployment-59d5cb45c7" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-59d5cb45c7  deployment-7312 /apis/apps/v1/namespaces/deployment-7312/replicasets/test-rolling-update-deployment-59d5cb45c7 82f57479-bb1b-4380-80ae-0df3b484e828 4222333 1 2020-09-16 23:15:33 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:59d5cb45c7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 70e2734a-df0e-49c9-ba6e-410a264521cb 0xc00310d7d7 0xc00310d7d8}] []  [{kube-controller-manager Update apps/v1 2020-09-16 23:15:37 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 48 101 50 55 51 52 97 45 100 102 48 101 45 52 57 99 57 45 98 97 54 101 45 52 49 48 97 50 54 52 53 50 49 99 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 59d5cb45c7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:59d5cb45c7] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00310d868 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 16 23:15:39.113: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Sep 16 23:15:39.113: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7312 /apis/apps/v1/namespaces/deployment-7312/replicasets/test-rolling-update-controller 4314a5f8-e551-4eac-8d44-cbec163e49cc 4222342 2 2020-09-16 23:15:28 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 70e2734a-df0e-49c9-ba6e-410a264521cb 0xc00310d6c7 0xc00310d6c8}] []  [{e2e.test Update apps/v1 2020-09-16 23:15:28 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-09-16 23:15:37 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 48 101 50 55 51 52 97 45 100 102 48 101 45 52 57 99 57 45 98 97 54 101 45 52 49 48 97 50 54 52 53 50 49 99 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00310d768 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 16 23:15:39.116: INFO: Pod "test-rolling-update-deployment-59d5cb45c7-k8qtl" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-59d5cb45c7-k8qtl test-rolling-update-deployment-59d5cb45c7- deployment-7312 /api/v1/namespaces/deployment-7312/pods/test-rolling-update-deployment-59d5cb45c7-k8qtl a9a1939c-b870-45fe-bfcc-11e32fbce19a 4222332 0 2020-09-16 23:15:33 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:59d5cb45c7] map[cni.projectcalico.org/podIP:172.21.9.69/32 cni.projectcalico.org/podIPs:172.21.9.69/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "calico",
    "ips": [
        "172.21.9.69"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "calico",
    "ips": [
        "172.21.9.69"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-rolling-update-deployment-59d5cb45c7 82f57479-bb1b-4380-80ae-0df3b484e828 0xc00310dd77 0xc00310dd78}] []  [{kube-controller-manager Update v1 2020-09-16 23:15:33 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 56 50 102 53 55 52 55 57 45 98 98 49 98 45 52 51 56 48 45 56 48 97 101 45 48 100 102 51 98 52 56 52 101 56 50 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-09-16 23:15:34 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {multus Update v1 2020-09-16 23:15:34 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 107 56 115 46 118 49 46 99 110 105 46 99 110 99 102 46 105 111 47 110 101 116 119 111 114 107 45 115 116 97 116 117 115 34 58 123 125 44 34 102 58 107 56 115 46 118 49 46 99 110 105 46 99 110 99 102 46 105 111 47 110 101 116 119 111 114 107 115 45 115 116 97 116 117 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-09-16 23:15:37 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 50 49 46 57 46 54 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qlz9m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qlz9m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qlz9m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash07,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-16 23:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-16 23:15:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-16 23:15:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-16 23:15:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.140.107,PodIP:172.21.9.69,StartTime:2020-09-16 23:15:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-16 23:15:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,ImageID:docker-pullable://us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c,ContainerID:robin://6673cc6011c6ed56d446f5db9af85300cf231dbef7a16c7260c147798bead9ef,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.9.69,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:15:39.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7312" for this suite.

• [SLOW TEST:11.125 seconds]
[sig-apps] Deployment
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":277,"completed":27,"skipped":531,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:15:39.125: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name cm-test-opt-del-ada048df-2db6-4a9c-a28e-840229236ce5
STEP: Creating configMap with name cm-test-opt-upd-15275033-d89e-43be-ab51-dfad84bf6e64
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-ada048df-2db6-4a9c-a28e-840229236ce5
STEP: Updating configmap cm-test-opt-upd-15275033-d89e-43be-ab51-dfad84bf6e64
STEP: Creating configMap with name cm-test-opt-create-81e11a7b-50dd-4d14-b9bb-5cf720894b24
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:15:47.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8470" for this suite.

• [SLOW TEST:8.296 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":28,"skipped":583,"failed":0}
SS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:15:47.421: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:157
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:15:47.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1088" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":277,"completed":29,"skipped":585,"failed":0}

------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:15:47.505: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 16 23:15:47.553: INFO: Creating deployment "test-recreate-deployment"
Sep 16 23:15:47.563: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Sep 16 23:15:47.567: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Sep 16 23:15:49.573: INFO: Waiting deployment "test-recreate-deployment" to complete
Sep 16 23:15:49.576: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894947, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894947, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894947, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735894947, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-74d98b5f7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 16 23:15:51.579: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Sep 16 23:15:51.589: INFO: Updating deployment test-recreate-deployment
Sep 16 23:15:51.589: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Sep 16 23:15:51.717: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-2893 /apis/apps/v1/namespaces/deployment-2893/deployments/test-recreate-deployment 4949006e-d2ae-49e1-b43d-0ed6904d3fc6 4222510 2 2020-09-16 23:15:47 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-09-16 23:15:51 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-09-16 23:15:51 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 110 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003717958 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-09-16 23:15:51 +0000 UTC,LastTransitionTime:2020-09-16 23:15:51 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-d5667d9c7" is progressing.,LastUpdateTime:2020-09-16 23:15:51 +0000 UTC,LastTransitionTime:2020-09-16 23:15:47 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Sep 16 23:15:51.720: INFO: New ReplicaSet "test-recreate-deployment-d5667d9c7" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-d5667d9c7  deployment-2893 /apis/apps/v1/namespaces/deployment-2893/replicasets/test-recreate-deployment-d5667d9c7 16a40b16-54b7-4375-92a2-62c217b0c32a 4222508 1 2020-09-16 23:15:51 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:d5667d9c7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 4949006e-d2ae-49e1-b43d-0ed6904d3fc6 0xc003717e70 0xc003717e71}] []  [{kube-controller-manager Update apps/v1 2020-09-16 23:15:51 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 52 57 52 57 48 48 54 101 45 100 50 97 101 45 52 57 101 49 45 98 52 51 100 45 48 101 100 54 57 48 52 100 51 102 99 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: d5667d9c7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:d5667d9c7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003717ee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 16 23:15:51.720: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Sep 16 23:15:51.720: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-74d98b5f7c  deployment-2893 /apis/apps/v1/namespaces/deployment-2893/replicasets/test-recreate-deployment-74d98b5f7c 8fcde8a9-e05b-469f-afe7-53345d46baeb 4222501 2 2020-09-16 23:15:47 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:74d98b5f7c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 4949006e-d2ae-49e1-b43d-0ed6904d3fc6 0xc003717d77 0xc003717d78}] []  [{kube-controller-manager Update apps/v1 2020-09-16 23:15:51 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 52 57 52 57 48 48 54 101 45 100 50 97 101 45 52 57 101 49 45 98 52 51 100 45 48 101 100 54 57 48 52 100 51 102 99 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 74d98b5f7c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:74d98b5f7c] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003717e08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 16 23:15:51.730: INFO: Pod "test-recreate-deployment-d5667d9c7-kmm5l" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-d5667d9c7-kmm5l test-recreate-deployment-d5667d9c7- deployment-2893 /api/v1/namespaces/deployment-2893/pods/test-recreate-deployment-d5667d9c7-kmm5l 52b5d351-3542-4cfe-a452-95a3041d94a9 4222511 0 2020-09-16 23:15:51 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:d5667d9c7] map[] [{apps/v1 ReplicaSet test-recreate-deployment-d5667d9c7 16a40b16-54b7-4375-92a2-62c217b0c32a 0xc0037c43d0 0xc0037c43d1}] []  [{kube-controller-manager Update v1 2020-09-16 23:15:51 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 49 54 97 52 48 98 49 54 45 53 52 98 55 45 52 51 55 53 45 57 50 97 50 45 54 50 99 50 49 55 98 48 99 51 50 97 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bnj66,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bnj66,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bnj66,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash07,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-16 23:15:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:15:51.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2893" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":277,"completed":30,"skipped":585,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:15:51.739: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-map-1a309be2-809a-4586-b8d8-3843aeadb090
STEP: Creating a pod to test consume configMaps
Sep 16 23:15:51.800: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d60fb343-630f-4a2c-be0c-5cc467afeea2" in namespace "projected-2509" to be "Succeeded or Failed"
Sep 16 23:15:51.803: INFO: Pod "pod-projected-configmaps-d60fb343-630f-4a2c-be0c-5cc467afeea2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.67906ms
Sep 16 23:15:53.807: INFO: Pod "pod-projected-configmaps-d60fb343-630f-4a2c-be0c-5cc467afeea2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00636876s
Sep 16 23:15:55.810: INFO: Pod "pod-projected-configmaps-d60fb343-630f-4a2c-be0c-5cc467afeea2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009654598s
STEP: Saw pod success
Sep 16 23:15:55.810: INFO: Pod "pod-projected-configmaps-d60fb343-630f-4a2c-be0c-5cc467afeea2" satisfied condition "Succeeded or Failed"
Sep 16 23:15:55.813: INFO: Trying to get logs from node eqx03-flash06 pod pod-projected-configmaps-d60fb343-630f-4a2c-be0c-5cc467afeea2 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 16 23:15:55.844: INFO: Waiting for pod pod-projected-configmaps-d60fb343-630f-4a2c-be0c-5cc467afeea2 to disappear
Sep 16 23:15:55.847: INFO: Pod pod-projected-configmaps-d60fb343-630f-4a2c-be0c-5cc467afeea2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:15:55.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2509" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":277,"completed":31,"skipped":598,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:15:55.856: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Sep 16 23:15:55.921: INFO: Waiting up to 5m0s for pod "downward-api-4852420f-c961-4924-9b1e-e88af1e04935" in namespace "downward-api-1757" to be "Succeeded or Failed"
Sep 16 23:15:55.923: INFO: Pod "downward-api-4852420f-c961-4924-9b1e-e88af1e04935": Phase="Pending", Reason="", readiness=false. Elapsed: 2.188708ms
Sep 16 23:15:57.927: INFO: Pod "downward-api-4852420f-c961-4924-9b1e-e88af1e04935": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005944916s
Sep 16 23:15:59.932: INFO: Pod "downward-api-4852420f-c961-4924-9b1e-e88af1e04935": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010757079s
STEP: Saw pod success
Sep 16 23:15:59.932: INFO: Pod "downward-api-4852420f-c961-4924-9b1e-e88af1e04935" satisfied condition "Succeeded or Failed"
Sep 16 23:15:59.935: INFO: Trying to get logs from node eqx03-flash06 pod downward-api-4852420f-c961-4924-9b1e-e88af1e04935 container dapi-container: <nil>
STEP: delete the pod
Sep 16 23:15:59.965: INFO: Waiting for pod downward-api-4852420f-c961-4924-9b1e-e88af1e04935 to disappear
Sep 16 23:15:59.967: INFO: Pod downward-api-4852420f-c961-4924-9b1e-e88af1e04935 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:15:59.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1757" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":277,"completed":32,"skipped":623,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:15:59.978: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Sep 16 23:16:04.581: INFO: Successfully updated pod "annotationupdatedeb5e55c-c3ee-47c6-9901-24f95a83a347"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:16:08.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3075" for this suite.

• [SLOW TEST:8.667 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":277,"completed":33,"skipped":633,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:16:08.645: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Sep 16 23:16:13.302: INFO: Successfully updated pod "annotationupdate7d0c0686-4637-4f54-87e3-e0063f19bab4"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:16:17.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1115" for this suite.

• [SLOW TEST:8.762 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":277,"completed":34,"skipped":634,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:16:17.407: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 16 23:16:17.482: INFO: The status of Pod test-webserver-3dd528f8-7159-4711-a4c0-11be584d0b6b is Pending, waiting for it to be Running (with Ready = true)
Sep 16 23:16:19.486: INFO: The status of Pod test-webserver-3dd528f8-7159-4711-a4c0-11be584d0b6b is Pending, waiting for it to be Running (with Ready = true)
Sep 16 23:16:21.486: INFO: The status of Pod test-webserver-3dd528f8-7159-4711-a4c0-11be584d0b6b is Running (Ready = false)
Sep 16 23:16:23.486: INFO: The status of Pod test-webserver-3dd528f8-7159-4711-a4c0-11be584d0b6b is Running (Ready = false)
Sep 16 23:16:25.486: INFO: The status of Pod test-webserver-3dd528f8-7159-4711-a4c0-11be584d0b6b is Running (Ready = false)
Sep 16 23:16:27.486: INFO: The status of Pod test-webserver-3dd528f8-7159-4711-a4c0-11be584d0b6b is Running (Ready = false)
Sep 16 23:16:29.486: INFO: The status of Pod test-webserver-3dd528f8-7159-4711-a4c0-11be584d0b6b is Running (Ready = false)
Sep 16 23:16:31.485: INFO: The status of Pod test-webserver-3dd528f8-7159-4711-a4c0-11be584d0b6b is Running (Ready = false)
Sep 16 23:16:33.486: INFO: The status of Pod test-webserver-3dd528f8-7159-4711-a4c0-11be584d0b6b is Running (Ready = false)
Sep 16 23:16:35.485: INFO: The status of Pod test-webserver-3dd528f8-7159-4711-a4c0-11be584d0b6b is Running (Ready = false)
Sep 16 23:16:37.486: INFO: The status of Pod test-webserver-3dd528f8-7159-4711-a4c0-11be584d0b6b is Running (Ready = true)
Sep 16 23:16:37.488: INFO: Container started at 2020-09-16 23:16:19 +0000 UTC, pod became ready at 2020-09-16 23:16:36 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:16:37.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-94" for this suite.

• [SLOW TEST:20.092 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":277,"completed":35,"skipped":682,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:16:37.499: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Sep 16 23:16:43.568: INFO: 0 pods remaining
Sep 16 23:16:43.568: INFO: 0 pods has nil DeletionTimestamp
Sep 16 23:16:43.568: INFO: 
STEP: Gathering metrics
W0916 23:16:44.577615      23 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep 16 23:16:44.577: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:16:44.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-471" for this suite.

• [SLOW TEST:7.086 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":277,"completed":36,"skipped":693,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:16:44.586: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 16 23:16:44.673: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e04ee449-d0d2-4871-982e-23160da2485a" in namespace "projected-943" to be "Succeeded or Failed"
Sep 16 23:16:44.676: INFO: Pod "downwardapi-volume-e04ee449-d0d2-4871-982e-23160da2485a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.680044ms
Sep 16 23:16:46.679: INFO: Pod "downwardapi-volume-e04ee449-d0d2-4871-982e-23160da2485a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006006094s
Sep 16 23:16:48.684: INFO: Pod "downwardapi-volume-e04ee449-d0d2-4871-982e-23160da2485a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010260727s
STEP: Saw pod success
Sep 16 23:16:48.684: INFO: Pod "downwardapi-volume-e04ee449-d0d2-4871-982e-23160da2485a" satisfied condition "Succeeded or Failed"
Sep 16 23:16:48.686: INFO: Trying to get logs from node eqx03-flash06 pod downwardapi-volume-e04ee449-d0d2-4871-982e-23160da2485a container client-container: <nil>
STEP: delete the pod
Sep 16 23:16:48.712: INFO: Waiting for pod downwardapi-volume-e04ee449-d0d2-4871-982e-23160da2485a to disappear
Sep 16 23:16:48.714: INFO: Pod downwardapi-volume-e04ee449-d0d2-4871-982e-23160da2485a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:16:48.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-943" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":37,"skipped":752,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:16:48.724: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6875
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-6875
I0916 23:16:48.799949      23 runners.go:190] Created replication controller with name: externalname-service, namespace: services-6875, replica count: 2
I0916 23:16:51.850370      23 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0916 23:16:54.850603      23 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 16 23:16:54.850: INFO: Creating new exec pod
Sep 16 23:16:59.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=services-6875 execpodd6pjq -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Sep 16 23:17:00.339: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep 16 23:17:00.339: INFO: stdout: ""
Sep 16 23:17:00.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=services-6875 execpodd6pjq -- /bin/sh -x -c nc -zv -t -w 2 172.19.183.248 80'
Sep 16 23:17:01.056: INFO: stderr: "+ nc -zv -t -w 2 172.19.183.248 80\nConnection to 172.19.183.248 80 port [tcp/http] succeeded!\n"
Sep 16 23:17:01.056: INFO: stdout: ""
Sep 16 23:17:01.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=services-6875 execpodd6pjq -- /bin/sh -x -c nc -zv -t -w 2 10.9.140.107 32399'
Sep 16 23:17:01.677: INFO: stderr: "+ nc -zv -t -w 2 10.9.140.107 32399\nConnection to 10.9.140.107 32399 port [tcp/32399] succeeded!\n"
Sep 16 23:17:01.677: INFO: stdout: ""
Sep 16 23:17:01.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=services-6875 execpodd6pjq -- /bin/sh -x -c nc -zv -t -w 2 10.9.40.104 32399'
Sep 16 23:17:02.377: INFO: stderr: "+ nc -zv -t -w 2 10.9.40.104 32399\nConnection to 10.9.40.104 32399 port [tcp/32399] succeeded!\n"
Sep 16 23:17:02.377: INFO: stdout: ""
Sep 16 23:17:02.377: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:17:02.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6875" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:13.686 seconds]
[sig-network] Services
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":277,"completed":38,"skipped":753,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:17:02.410: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl logs
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1288
STEP: creating an pod
Sep 16 23:17:02.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 run logs-generator --image=us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 --namespace=kubectl-9157 -- logs-generator --log-lines-total 100 --run-duration 20s'
Sep 16 23:17:02.598: INFO: stderr: ""
Sep 16 23:17:02.598: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Waiting for log generator to start.
Sep 16 23:17:02.598: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Sep 16 23:17:02.598: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9157" to be "running and ready, or succeeded"
Sep 16 23:17:02.601: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.469176ms
Sep 16 23:17:04.604: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005688751s
Sep 16 23:17:06.608: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.009808077s
Sep 16 23:17:06.608: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Sep 16 23:17:06.608: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Sep 16 23:17:06.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 logs logs-generator logs-generator --namespace=kubectl-9157'
Sep 16 23:17:06.785: INFO: stderr: ""
Sep 16 23:17:06.785: INFO: stdout: "I0916 23:17:05.088497       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/lgs 506\nI0916 23:17:05.288656       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/9r2l 488\nI0916 23:17:05.488689       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/trn2 478\nI0916 23:17:05.688625       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/mmh9 311\nI0916 23:17:05.888758       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/58r7 430\nI0916 23:17:06.088658       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/fvj 518\nI0916 23:17:06.288676       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/5r2q 572\nI0916 23:17:06.489009       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/llh 505\nI0916 23:17:06.688684       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/7sn 416\n"
STEP: limiting log lines
Sep 16 23:17:06.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 logs logs-generator logs-generator --namespace=kubectl-9157 --tail=1'
Sep 16 23:17:06.954: INFO: stderr: ""
Sep 16 23:17:06.954: INFO: stdout: "I0916 23:17:06.888663       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/rn2m 309\n"
Sep 16 23:17:06.954: INFO: got output "I0916 23:17:06.888663       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/rn2m 309\n"
STEP: limiting log bytes
Sep 16 23:17:06.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 logs logs-generator logs-generator --namespace=kubectl-9157 --limit-bytes=1'
Sep 16 23:17:07.060: INFO: stderr: ""
Sep 16 23:17:07.060: INFO: stdout: "I"
Sep 16 23:17:07.060: INFO: got output "I"
STEP: exposing timestamps
Sep 16 23:17:07.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 logs logs-generator logs-generator --namespace=kubectl-9157 --tail=1 --timestamps'
Sep 16 23:17:07.173: INFO: stderr: ""
Sep 16 23:17:07.173: INFO: stdout: "2020-09-16T23:17:07.088715527Z I0916 23:17:07.088636       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/bdj 501\n"
Sep 16 23:17:07.173: INFO: got output "2020-09-16T23:17:07.088715527Z I0916 23:17:07.088636       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/bdj 501\n"
STEP: restricting to a time range
Sep 16 23:17:09.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 logs logs-generator logs-generator --namespace=kubectl-9157 --since=1s'
Sep 16 23:17:09.789: INFO: stderr: ""
Sep 16 23:17:09.789: INFO: stdout: "I0916 23:17:08.888667       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/zkrf 431\nI0916 23:17:09.088711       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/nhlx 478\nI0916 23:17:09.288641       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/fcj 229\nI0916 23:17:09.488689       1 logs_generator.go:76] 22 POST /api/v1/namespaces/default/pods/jp5t 472\nI0916 23:17:09.688656       1 logs_generator.go:76] 23 POST /api/v1/namespaces/ns/pods/xk29 233\n"
Sep 16 23:17:09.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 logs logs-generator logs-generator --namespace=kubectl-9157 --since=24h'
Sep 16 23:17:09.915: INFO: stderr: ""
Sep 16 23:17:09.916: INFO: stdout: "I0916 23:17:05.088497       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/lgs 506\nI0916 23:17:05.288656       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/9r2l 488\nI0916 23:17:05.488689       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/trn2 478\nI0916 23:17:05.688625       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/mmh9 311\nI0916 23:17:05.888758       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/58r7 430\nI0916 23:17:06.088658       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/fvj 518\nI0916 23:17:06.288676       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/5r2q 572\nI0916 23:17:06.489009       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/llh 505\nI0916 23:17:06.688684       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/7sn 416\nI0916 23:17:06.888663       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/rn2m 309\nI0916 23:17:07.088636       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/bdj 501\nI0916 23:17:07.288713       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/wj4s 545\nI0916 23:17:07.488608       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/tcsz 504\nI0916 23:17:07.688679       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/tns 396\nI0916 23:17:07.888693       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/6dwp 252\nI0916 23:17:08.088671       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/9298 560\nI0916 23:17:08.288639       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/bg4k 584\nI0916 23:17:08.488757       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/jqbj 489\nI0916 23:17:08.688649       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/256q 207\nI0916 23:17:08.888667       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/zkrf 431\nI0916 23:17:09.088711       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/nhlx 478\nI0916 23:17:09.288641       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/fcj 229\nI0916 23:17:09.488689       1 logs_generator.go:76] 22 POST /api/v1/namespaces/default/pods/jp5t 472\nI0916 23:17:09.688656       1 logs_generator.go:76] 23 POST /api/v1/namespaces/ns/pods/xk29 233\nI0916 23:17:09.888690       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/ml25 441\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1294
Sep 16 23:17:09.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 delete pod logs-generator --namespace=kubectl-9157'
Sep 16 23:17:16.713: INFO: stderr: ""
Sep 16 23:17:16.713: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:17:16.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9157" for this suite.

• [SLOW TEST:14.313 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1284
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":277,"completed":39,"skipped":754,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:17:16.724: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Sep 16 23:17:26.794: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
W0916 23:17:26.794372      23 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep 16 23:17:26.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4481" for this suite.

• [SLOW TEST:10.079 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":277,"completed":40,"skipped":769,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:17:26.802: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8685
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-8685
I0916 23:17:26.877014      23 runners.go:190] Created replication controller with name: externalname-service, namespace: services-8685, replica count: 2
Sep 16 23:17:29.927: INFO: Creating new exec pod
I0916 23:17:29.927411      23 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 16 23:17:34.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=services-8685 execpod9qrbp -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Sep 16 23:17:35.812: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep 16 23:17:35.812: INFO: stdout: ""
Sep 16 23:17:35.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=services-8685 execpod9qrbp -- /bin/sh -x -c nc -zv -t -w 2 172.19.80.139 80'
Sep 16 23:17:36.507: INFO: stderr: "+ nc -zv -t -w 2 172.19.80.139 80\nConnection to 172.19.80.139 80 port [tcp/http] succeeded!\n"
Sep 16 23:17:36.508: INFO: stdout: ""
Sep 16 23:17:36.508: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:17:36.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8685" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:9.738 seconds]
[sig-network] Services
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":277,"completed":41,"skipped":777,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:17:36.541: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Sep 16 23:17:36.602: INFO: Waiting up to 5m0s for pod "downward-api-182fef0a-7100-4aaa-aa28-41ae080c02ef" in namespace "downward-api-398" to be "Succeeded or Failed"
Sep 16 23:17:36.604: INFO: Pod "downward-api-182fef0a-7100-4aaa-aa28-41ae080c02ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.157108ms
Sep 16 23:17:38.608: INFO: Pod "downward-api-182fef0a-7100-4aaa-aa28-41ae080c02ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005736359s
Sep 16 23:17:40.611: INFO: Pod "downward-api-182fef0a-7100-4aaa-aa28-41ae080c02ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009123164s
STEP: Saw pod success
Sep 16 23:17:40.611: INFO: Pod "downward-api-182fef0a-7100-4aaa-aa28-41ae080c02ef" satisfied condition "Succeeded or Failed"
Sep 16 23:17:40.613: INFO: Trying to get logs from node eqx03-flash06 pod downward-api-182fef0a-7100-4aaa-aa28-41ae080c02ef container dapi-container: <nil>
STEP: delete the pod
Sep 16 23:17:40.643: INFO: Waiting for pod downward-api-182fef0a-7100-4aaa-aa28-41ae080c02ef to disappear
Sep 16 23:17:40.646: INFO: Pod downward-api-182fef0a-7100-4aaa-aa28-41ae080c02ef no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:17:40.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-398" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":277,"completed":42,"skipped":800,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:17:40.657: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-9ee9b51d-efac-4a76-ab4b-c92488173581
STEP: Creating a pod to test consume secrets
Sep 16 23:17:40.723: INFO: Waiting up to 5m0s for pod "pod-secrets-bdffe091-ca4e-4b87-927b-4d550c9162b7" in namespace "secrets-183" to be "Succeeded or Failed"
Sep 16 23:17:40.725: INFO: Pod "pod-secrets-bdffe091-ca4e-4b87-927b-4d550c9162b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050973ms
Sep 16 23:17:42.729: INFO: Pod "pod-secrets-bdffe091-ca4e-4b87-927b-4d550c9162b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006106216s
Sep 16 23:17:44.733: INFO: Pod "pod-secrets-bdffe091-ca4e-4b87-927b-4d550c9162b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009753234s
STEP: Saw pod success
Sep 16 23:17:44.733: INFO: Pod "pod-secrets-bdffe091-ca4e-4b87-927b-4d550c9162b7" satisfied condition "Succeeded or Failed"
Sep 16 23:17:44.735: INFO: Trying to get logs from node eqx03-flash06 pod pod-secrets-bdffe091-ca4e-4b87-927b-4d550c9162b7 container secret-volume-test: <nil>
STEP: delete the pod
Sep 16 23:17:44.795: INFO: Waiting for pod pod-secrets-bdffe091-ca4e-4b87-927b-4d550c9162b7 to disappear
Sep 16 23:17:44.797: INFO: Pod pod-secrets-bdffe091-ca4e-4b87-927b-4d550c9162b7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:17:44.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-183" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":43,"skipped":808,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:17:44.808: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-7836
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 16 23:17:44.900: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 16 23:17:44.973: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 16 23:17:46.977: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 16 23:17:48.976: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 16 23:17:50.977: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 16 23:17:52.977: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 16 23:17:54.976: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 16 23:17:56.977: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 16 23:17:58.976: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 16 23:18:00.977: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 16 23:18:02.976: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep 16 23:18:02.982: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep 16 23:18:04.985: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep 16 23:18:06.985: INFO: The status of Pod netserver-1 is Running (Ready = true)
Sep 16 23:18:06.990: INFO: The status of Pod netserver-2 is Running (Ready = false)
Sep 16 23:18:08.993: INFO: The status of Pod netserver-2 is Running (Ready = true)
Sep 16 23:18:08.997: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
Sep 16 23:18:15.021: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.12.158:8080/dial?request=hostname&protocol=http&host=172.21.8.149&port=8080&tries=1'] Namespace:pod-network-test-7836 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 16 23:18:15.021: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
Sep 16 23:18:15.895: INFO: Waiting for responses: map[]
Sep 16 23:18:15.899: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.12.158:8080/dial?request=hostname&protocol=http&host=172.21.12.56&port=8080&tries=1'] Namespace:pod-network-test-7836 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 16 23:18:15.899: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
Sep 16 23:18:16.552: INFO: Waiting for responses: map[]
Sep 16 23:18:16.556: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.12.158:8080/dial?request=hostname&protocol=http&host=172.21.9.69&port=8080&tries=1'] Namespace:pod-network-test-7836 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 16 23:18:16.556: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
Sep 16 23:18:17.032: INFO: Waiting for responses: map[]
Sep 16 23:18:17.035: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.12.158:8080/dial?request=hostname&protocol=http&host=172.21.0.39&port=8080&tries=1'] Namespace:pod-network-test-7836 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 16 23:18:17.035: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
Sep 16 23:18:17.417: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:18:17.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7836" for this suite.

• [SLOW TEST:32.628 seconds]
[sig-network] Networking
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":277,"completed":44,"skipped":816,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:18:17.436: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: set up a multi version CRD
Sep 16 23:18:17.490: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:18:47.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4961" for this suite.

• [SLOW TEST:29.942 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":277,"completed":45,"skipped":818,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:18:47.379: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-4674, will wait for the garbage collector to delete the pods
Sep 16 23:18:51.514: INFO: Deleting Job.batch foo took: 11.209753ms
Sep 16 23:18:52.414: INFO: Terminating Job.batch foo pods took: 900.267567ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:19:36.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4674" for this suite.

• [SLOW TEST:49.460 seconds]
[sig-apps] Job
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":277,"completed":46,"skipped":843,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:19:36.840: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Sep 16 23:19:36.899: INFO: Waiting up to 5m0s for pod "downward-api-c75927fd-b945-4f1a-847a-f7d2171631d3" in namespace "downward-api-6983" to be "Succeeded or Failed"
Sep 16 23:19:36.901: INFO: Pod "downward-api-c75927fd-b945-4f1a-847a-f7d2171631d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.436236ms
Sep 16 23:19:38.912: INFO: Pod "downward-api-c75927fd-b945-4f1a-847a-f7d2171631d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012953037s
Sep 16 23:19:40.915: INFO: Pod "downward-api-c75927fd-b945-4f1a-847a-f7d2171631d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016258339s
STEP: Saw pod success
Sep 16 23:19:40.915: INFO: Pod "downward-api-c75927fd-b945-4f1a-847a-f7d2171631d3" satisfied condition "Succeeded or Failed"
Sep 16 23:19:40.917: INFO: Trying to get logs from node eqx03-flash07 pod downward-api-c75927fd-b945-4f1a-847a-f7d2171631d3 container dapi-container: <nil>
STEP: delete the pod
Sep 16 23:19:40.963: INFO: Waiting for pod downward-api-c75927fd-b945-4f1a-847a-f7d2171631d3 to disappear
Sep 16 23:19:40.965: INFO: Pod downward-api-c75927fd-b945-4f1a-847a-f7d2171631d3 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:19:40.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6983" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":277,"completed":47,"skipped":857,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:19:40.975: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Sep 16 23:19:41.668: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Sep 16 23:19:43.677: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735895181, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735895181, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735895181, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735895181, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-65c6cd5fdf\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 16 23:19:46.710: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 16 23:19:46.714: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:19:47.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9324" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.957 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":277,"completed":48,"skipped":873,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:19:47.932: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 16 23:19:48.473: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 16 23:19:50.481: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735895188, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735895188, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735895188, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735895188, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 16 23:19:53.500: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:19:53.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8813" for this suite.
STEP: Destroying namespace "webhook-8813-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.986 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":277,"completed":49,"skipped":888,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:19:53.918: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-map-1f6d45a4-6478-45a8-bf95-bacda8fca12e
STEP: Creating a pod to test consume secrets
Sep 16 23:19:54.013: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4581821f-58b8-4d4d-b2c3-fd935d00f83d" in namespace "projected-5250" to be "Succeeded or Failed"
Sep 16 23:19:54.015: INFO: Pod "pod-projected-secrets-4581821f-58b8-4d4d-b2c3-fd935d00f83d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.100882ms
Sep 16 23:19:56.018: INFO: Pod "pod-projected-secrets-4581821f-58b8-4d4d-b2c3-fd935d00f83d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004924306s
Sep 16 23:19:58.021: INFO: Pod "pod-projected-secrets-4581821f-58b8-4d4d-b2c3-fd935d00f83d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008128295s
STEP: Saw pod success
Sep 16 23:19:58.021: INFO: Pod "pod-projected-secrets-4581821f-58b8-4d4d-b2c3-fd935d00f83d" satisfied condition "Succeeded or Failed"
Sep 16 23:19:58.023: INFO: Trying to get logs from node eqx03-flash06 pod pod-projected-secrets-4581821f-58b8-4d4d-b2c3-fd935d00f83d container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 16 23:19:58.084: INFO: Waiting for pod pod-projected-secrets-4581821f-58b8-4d4d-b2c3-fd935d00f83d to disappear
Sep 16 23:19:58.086: INFO: Pod pod-projected-secrets-4581821f-58b8-4d4d-b2c3-fd935d00f83d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:19:58.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5250" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":50,"skipped":928,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:19:58.104: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep 16 23:19:58.161: INFO: Waiting up to 5m0s for pod "pod-514507f6-b918-4aa0-8d1a-b8a9496faa97" in namespace "emptydir-962" to be "Succeeded or Failed"
Sep 16 23:19:58.183: INFO: Pod "pod-514507f6-b918-4aa0-8d1a-b8a9496faa97": Phase="Pending", Reason="", readiness=false. Elapsed: 22.414133ms
Sep 16 23:20:00.186: INFO: Pod "pod-514507f6-b918-4aa0-8d1a-b8a9496faa97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02561534s
Sep 16 23:20:02.190: INFO: Pod "pod-514507f6-b918-4aa0-8d1a-b8a9496faa97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028982194s
STEP: Saw pod success
Sep 16 23:20:02.190: INFO: Pod "pod-514507f6-b918-4aa0-8d1a-b8a9496faa97" satisfied condition "Succeeded or Failed"
Sep 16 23:20:02.192: INFO: Trying to get logs from node eqx03-flash06 pod pod-514507f6-b918-4aa0-8d1a-b8a9496faa97 container test-container: <nil>
STEP: delete the pod
Sep 16 23:20:02.249: INFO: Waiting for pod pod-514507f6-b918-4aa0-8d1a-b8a9496faa97 to disappear
Sep 16 23:20:02.251: INFO: Pod pod-514507f6-b918-4aa0-8d1a-b8a9496faa97 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:20:02.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-962" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":51,"skipped":951,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:20:02.276: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir volume type on node default medium
Sep 16 23:20:02.339: INFO: Waiting up to 5m0s for pod "pod-5c807f4b-9a97-44c2-ae08-bd40207174f7" in namespace "emptydir-5132" to be "Succeeded or Failed"
Sep 16 23:20:02.341: INFO: Pod "pod-5c807f4b-9a97-44c2-ae08-bd40207174f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.204785ms
Sep 16 23:20:04.344: INFO: Pod "pod-5c807f4b-9a97-44c2-ae08-bd40207174f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004926757s
Sep 16 23:20:06.352: INFO: Pod "pod-5c807f4b-9a97-44c2-ae08-bd40207174f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012954935s
STEP: Saw pod success
Sep 16 23:20:06.352: INFO: Pod "pod-5c807f4b-9a97-44c2-ae08-bd40207174f7" satisfied condition "Succeeded or Failed"
Sep 16 23:20:06.354: INFO: Trying to get logs from node eqx03-flash06 pod pod-5c807f4b-9a97-44c2-ae08-bd40207174f7 container test-container: <nil>
STEP: delete the pod
Sep 16 23:20:06.386: INFO: Waiting for pod pod-5c807f4b-9a97-44c2-ae08-bd40207174f7 to disappear
Sep 16 23:20:06.388: INFO: Pod pod-5c807f4b-9a97-44c2-ae08-bd40207174f7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:20:06.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5132" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":52,"skipped":968,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:20:06.399: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:20:06.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7633" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":277,"completed":53,"skipped":976,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:20:06.512: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-903e58f7-eb0a-4497-ba6c-3b887e4cd560
STEP: Creating a pod to test consume secrets
Sep 16 23:20:06.596: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cded243c-c680-4ce1-bb19-ceb22f315c98" in namespace "projected-2510" to be "Succeeded or Failed"
Sep 16 23:20:06.598: INFO: Pod "pod-projected-secrets-cded243c-c680-4ce1-bb19-ceb22f315c98": Phase="Pending", Reason="", readiness=false. Elapsed: 2.277443ms
Sep 16 23:20:08.602: INFO: Pod "pod-projected-secrets-cded243c-c680-4ce1-bb19-ceb22f315c98": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005912409s
Sep 16 23:20:10.605: INFO: Pod "pod-projected-secrets-cded243c-c680-4ce1-bb19-ceb22f315c98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008743566s
STEP: Saw pod success
Sep 16 23:20:10.605: INFO: Pod "pod-projected-secrets-cded243c-c680-4ce1-bb19-ceb22f315c98" satisfied condition "Succeeded or Failed"
Sep 16 23:20:10.607: INFO: Trying to get logs from node eqx03-flash07 pod pod-projected-secrets-cded243c-c680-4ce1-bb19-ceb22f315c98 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 16 23:20:10.640: INFO: Waiting for pod pod-projected-secrets-cded243c-c680-4ce1-bb19-ceb22f315c98 to disappear
Sep 16 23:20:10.642: INFO: Pod pod-projected-secrets-cded243c-c680-4ce1-bb19-ceb22f315c98 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:20:10.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2510" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":54,"skipped":1000,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:20:10.676: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-68348c83-3e87-42e1-a4da-8f4a4d29b357
STEP: Creating a pod to test consume configMaps
Sep 16 23:20:10.761: INFO: Waiting up to 5m0s for pod "pod-configmaps-cd9d28b2-04d7-4a35-bb92-b6d2fada86f2" in namespace "configmap-1320" to be "Succeeded or Failed"
Sep 16 23:20:10.780: INFO: Pod "pod-configmaps-cd9d28b2-04d7-4a35-bb92-b6d2fada86f2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.831507ms
Sep 16 23:20:12.784: INFO: Pod "pod-configmaps-cd9d28b2-04d7-4a35-bb92-b6d2fada86f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022174377s
Sep 16 23:20:14.787: INFO: Pod "pod-configmaps-cd9d28b2-04d7-4a35-bb92-b6d2fada86f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025178676s
STEP: Saw pod success
Sep 16 23:20:14.787: INFO: Pod "pod-configmaps-cd9d28b2-04d7-4a35-bb92-b6d2fada86f2" satisfied condition "Succeeded or Failed"
Sep 16 23:20:14.789: INFO: Trying to get logs from node eqx03-flash06 pod pod-configmaps-cd9d28b2-04d7-4a35-bb92-b6d2fada86f2 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 16 23:20:14.873: INFO: Waiting for pod pod-configmaps-cd9d28b2-04d7-4a35-bb92-b6d2fada86f2 to disappear
Sep 16 23:20:14.875: INFO: Pod pod-configmaps-cd9d28b2-04d7-4a35-bb92-b6d2fada86f2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:20:14.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1320" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":277,"completed":55,"skipped":1053,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:20:14.887: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:20:42.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9635" for this suite.

• [SLOW TEST:27.341 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  blackbox test
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
    when starting a container that exits
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":277,"completed":56,"skipped":1072,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:20:42.228: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-upd-ed635bdc-1332-457a-a96f-c738b9673107
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-ed635bdc-1332-457a-a96f-c738b9673107
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:22:09.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6482" for this suite.

• [SLOW TEST:87.393 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":57,"skipped":1076,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:22:09.621: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Sep 16 23:22:09.666: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
Sep 16 23:22:15.304: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:22:35.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9698" for this suite.

• [SLOW TEST:25.739 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":277,"completed":58,"skipped":1114,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:22:35.361: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 16 23:22:36.483: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 16 23:22:38.491: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735895356, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735895356, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735895356, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735895356, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 16 23:22:41.525: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:22:41.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7468" for this suite.
STEP: Destroying namespace "webhook-7468-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.378 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":277,"completed":59,"skipped":1115,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:22:41.740: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with configMap that has name projected-configmap-test-upd-860295e2-7634-412f-8694-1b47ad67f289
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-860295e2-7634-412f-8694-1b47ad67f289
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:23:59.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3713" for this suite.

• [SLOW TEST:77.329 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":60,"skipped":1136,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:23:59.070: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:23:59.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6030" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":277,"completed":61,"skipped":1151,"failed":0}
SSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:23:59.136: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 16 23:24:03.260: INFO: Waiting up to 5m0s for pod "client-envvars-e677da9d-2ccf-468c-ba6f-880c32332186" in namespace "pods-1502" to be "Succeeded or Failed"
Sep 16 23:24:03.263: INFO: Pod "client-envvars-e677da9d-2ccf-468c-ba6f-880c32332186": Phase="Pending", Reason="", readiness=false. Elapsed: 3.06182ms
Sep 16 23:24:05.266: INFO: Pod "client-envvars-e677da9d-2ccf-468c-ba6f-880c32332186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006136298s
Sep 16 23:24:07.270: INFO: Pod "client-envvars-e677da9d-2ccf-468c-ba6f-880c32332186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009641761s
STEP: Saw pod success
Sep 16 23:24:07.270: INFO: Pod "client-envvars-e677da9d-2ccf-468c-ba6f-880c32332186" satisfied condition "Succeeded or Failed"
Sep 16 23:24:07.272: INFO: Trying to get logs from node eqx03-flash06 pod client-envvars-e677da9d-2ccf-468c-ba6f-880c32332186 container env3cont: <nil>
STEP: delete the pod
Sep 16 23:24:07.314: INFO: Waiting for pod client-envvars-e677da9d-2ccf-468c-ba6f-880c32332186 to disappear
Sep 16 23:24:07.323: INFO: Pod client-envvars-e677da9d-2ccf-468c-ba6f-880c32332186 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:24:07.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1502" for this suite.

• [SLOW TEST:8.201 seconds]
[k8s.io] Pods
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":277,"completed":62,"skipped":1156,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:24:07.337: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 16 23:24:07.484: INFO: Waiting up to 5m0s for pod "downwardapi-volume-58148988-d36a-4f9a-a9c8-26960c964163" in namespace "downward-api-3675" to be "Succeeded or Failed"
Sep 16 23:24:07.486: INFO: Pod "downwardapi-volume-58148988-d36a-4f9a-a9c8-26960c964163": Phase="Pending", Reason="", readiness=false. Elapsed: 2.304232ms
Sep 16 23:24:09.489: INFO: Pod "downwardapi-volume-58148988-d36a-4f9a-a9c8-26960c964163": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005028061s
Sep 16 23:24:11.500: INFO: Pod "downwardapi-volume-58148988-d36a-4f9a-a9c8-26960c964163": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01650656s
STEP: Saw pod success
Sep 16 23:24:11.500: INFO: Pod "downwardapi-volume-58148988-d36a-4f9a-a9c8-26960c964163" satisfied condition "Succeeded or Failed"
Sep 16 23:24:11.503: INFO: Trying to get logs from node eqx03-flash07 pod downwardapi-volume-58148988-d36a-4f9a-a9c8-26960c964163 container client-container: <nil>
STEP: delete the pod
Sep 16 23:24:11.543: INFO: Waiting for pod downwardapi-volume-58148988-d36a-4f9a-a9c8-26960c964163 to disappear
Sep 16 23:24:11.545: INFO: Pod downwardapi-volume-58148988-d36a-4f9a-a9c8-26960c964163 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:24:11.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3675" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":277,"completed":63,"skipped":1161,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:24:11.558: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Starting the proxy
Sep 16 23:24:11.609: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-480724876 proxy --unix-socket=/tmp/kubectl-proxy-unix863865235/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:24:11.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5499" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":277,"completed":64,"skipped":1179,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:24:11.697: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8514.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8514.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8514.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8514.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8514.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8514.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 16 23:24:15.795: INFO: Unable to read wheezy_udp@PodARecord from pod dns-8514/dns-test-29a412fa-e228-4af3-9214-ea5351b9e4f7: the server could not find the requested resource (get pods dns-test-29a412fa-e228-4af3-9214-ea5351b9e4f7)
Sep 16 23:24:15.797: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-8514/dns-test-29a412fa-e228-4af3-9214-ea5351b9e4f7: the server could not find the requested resource (get pods dns-test-29a412fa-e228-4af3-9214-ea5351b9e4f7)
Sep 16 23:24:15.804: INFO: Unable to read jessie_udp@PodARecord from pod dns-8514/dns-test-29a412fa-e228-4af3-9214-ea5351b9e4f7: the server could not find the requested resource (get pods dns-test-29a412fa-e228-4af3-9214-ea5351b9e4f7)
Sep 16 23:24:15.806: INFO: Unable to read jessie_tcp@PodARecord from pod dns-8514/dns-test-29a412fa-e228-4af3-9214-ea5351b9e4f7: the server could not find the requested resource (get pods dns-test-29a412fa-e228-4af3-9214-ea5351b9e4f7)
Sep 16 23:24:15.806: INFO: Lookups using dns-8514/dns-test-29a412fa-e228-4af3-9214-ea5351b9e4f7 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep 16 23:24:20.828: INFO: DNS probes using dns-8514/dns-test-29a412fa-e228-4af3-9214-ea5351b9e4f7 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:24:20.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8514" for this suite.

• [SLOW TEST:9.184 seconds]
[sig-network] DNS
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":277,"completed":65,"skipped":1181,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:24:20.881: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 16 23:24:22.321: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 16 23:24:24.329: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735895462, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735895462, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735895462, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735895462, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 16 23:24:27.356: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 16 23:24:27.359: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:24:28.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4023" for this suite.
STEP: Destroying namespace "webhook-4023-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.691 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":277,"completed":66,"skipped":1197,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:24:28.573: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: set up a multi version CRD
Sep 16 23:24:28.626: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:24:57.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5621" for this suite.

• [SLOW TEST:28.691 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":277,"completed":67,"skipped":1202,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:24:57.264: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 16 23:24:57.317: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-81a6e02b-31cd-40b3-a68b-276092d924d4" in namespace "security-context-test-4420" to be "Succeeded or Failed"
Sep 16 23:24:57.319: INFO: Pod "busybox-readonly-false-81a6e02b-31cd-40b3-a68b-276092d924d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002517ms
Sep 16 23:24:59.321: INFO: Pod "busybox-readonly-false-81a6e02b-31cd-40b3-a68b-276092d924d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004796225s
Sep 16 23:25:01.325: INFO: Pod "busybox-readonly-false-81a6e02b-31cd-40b3-a68b-276092d924d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008350257s
Sep 16 23:25:01.325: INFO: Pod "busybox-readonly-false-81a6e02b-31cd-40b3-a68b-276092d924d4" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:25:01.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4420" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":277,"completed":68,"skipped":1211,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:25:01.337: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 16 23:25:01.407: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c2f7da1c-2e7b-45ab-b422-f115bc109a03" in namespace "projected-5293" to be "Succeeded or Failed"
Sep 16 23:25:01.409: INFO: Pod "downwardapi-volume-c2f7da1c-2e7b-45ab-b422-f115bc109a03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.29191ms
Sep 16 23:25:03.415: INFO: Pod "downwardapi-volume-c2f7da1c-2e7b-45ab-b422-f115bc109a03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00831911s
Sep 16 23:25:05.419: INFO: Pod "downwardapi-volume-c2f7da1c-2e7b-45ab-b422-f115bc109a03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012084487s
STEP: Saw pod success
Sep 16 23:25:05.419: INFO: Pod "downwardapi-volume-c2f7da1c-2e7b-45ab-b422-f115bc109a03" satisfied condition "Succeeded or Failed"
Sep 16 23:25:05.422: INFO: Trying to get logs from node eqx03-flash06 pod downwardapi-volume-c2f7da1c-2e7b-45ab-b422-f115bc109a03 container client-container: <nil>
STEP: delete the pod
Sep 16 23:25:05.460: INFO: Waiting for pod downwardapi-volume-c2f7da1c-2e7b-45ab-b422-f115bc109a03 to disappear
Sep 16 23:25:05.462: INFO: Pod downwardapi-volume-c2f7da1c-2e7b-45ab-b422-f115bc109a03 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:25:05.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5293" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":277,"completed":69,"skipped":1237,"failed":0}
SSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:25:05.480: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod busybox-6811c770-44be-4815-9d51-c23920099a52 in namespace container-probe-9290
Sep 16 23:25:07.566: INFO: Started pod busybox-6811c770-44be-4815-9d51-c23920099a52 in namespace container-probe-9290
STEP: checking the pod's current state and verifying that restartCount is present
Sep 16 23:25:07.568: INFO: Initial restart count of pod busybox-6811c770-44be-4815-9d51-c23920099a52 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:29:07.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9290" for this suite.

• [SLOW TEST:242.528 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":277,"completed":70,"skipped":1243,"failed":0}
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:29:08.009: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:29:12.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9137" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":71,"skipped":1243,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:29:12.122: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 16 23:29:12.227: INFO: Number of nodes with available pods: 0
Sep 16 23:29:12.227: INFO: Node eqx03-flash06 is running more than one daemon pod
Sep 16 23:29:13.234: INFO: Number of nodes with available pods: 0
Sep 16 23:29:13.234: INFO: Node eqx03-flash06 is running more than one daemon pod
Sep 16 23:29:14.242: INFO: Number of nodes with available pods: 0
Sep 16 23:29:14.242: INFO: Node eqx03-flash06 is running more than one daemon pod
Sep 16 23:29:15.235: INFO: Number of nodes with available pods: 2
Sep 16 23:29:15.235: INFO: Node eqx04-flash04 is running more than one daemon pod
Sep 16 23:29:16.235: INFO: Number of nodes with available pods: 2
Sep 16 23:29:16.235: INFO: Node eqx04-flash04 is running more than one daemon pod
Sep 16 23:29:17.249: INFO: Number of nodes with available pods: 2
Sep 16 23:29:17.249: INFO: Node eqx04-flash04 is running more than one daemon pod
Sep 16 23:29:18.264: INFO: Number of nodes with available pods: 2
Sep 16 23:29:18.264: INFO: Node eqx04-flash04 is running more than one daemon pod
Sep 16 23:29:19.294: INFO: Number of nodes with available pods: 2
Sep 16 23:29:19.294: INFO: Node eqx04-flash04 is running more than one daemon pod
Sep 16 23:29:20.234: INFO: Number of nodes with available pods: 4
Sep 16 23:29:20.234: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Sep 16 23:29:20.281: INFO: Number of nodes with available pods: 3
Sep 16 23:29:20.281: INFO: Node eqx04-flash04 is running more than one daemon pod
Sep 16 23:29:21.288: INFO: Number of nodes with available pods: 3
Sep 16 23:29:21.288: INFO: Node eqx04-flash04 is running more than one daemon pod
Sep 16 23:29:22.289: INFO: Number of nodes with available pods: 3
Sep 16 23:29:22.289: INFO: Node eqx04-flash04 is running more than one daemon pod
Sep 16 23:29:23.289: INFO: Number of nodes with available pods: 4
Sep 16 23:29:23.289: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-745, will wait for the garbage collector to delete the pods
Sep 16 23:29:23.356: INFO: Deleting DaemonSet.extensions daemon-set took: 9.568777ms
Sep 16 23:29:24.256: INFO: Terminating DaemonSet.extensions daemon-set pods took: 900.168048ms
Sep 16 23:29:27.759: INFO: Number of nodes with available pods: 0
Sep 16 23:29:27.759: INFO: Number of running nodes: 0, number of available pods: 0
Sep 16 23:29:27.761: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-745/daemonsets","resourceVersion":"4227816"},"items":null}

Sep 16 23:29:27.763: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-745/pods","resourceVersion":"4227816"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:29:27.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-745" for this suite.

• [SLOW TEST:15.665 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":277,"completed":72,"skipped":1289,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:29:27.787: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:171
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating server pod server in namespace prestop-8918
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-8918
STEP: Deleting pre-stop pod
Sep 16 23:29:40.895: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:29:40.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-8918" for this suite.

• [SLOW TEST:13.136 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":277,"completed":73,"skipped":1299,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:29:40.923: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 16 23:29:41.651: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 16 23:29:43.661: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735895781, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735895781, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735895781, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735895781, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 16 23:29:46.688: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 16 23:29:46.691: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8836-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:29:47.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1861" for this suite.
STEP: Destroying namespace "webhook-1861-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.988 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":277,"completed":74,"skipped":1300,"failed":0}
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] version v1
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:29:47.912: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-9bk4n in namespace proxy-7726
I0916 23:29:48.008544      23 runners.go:190] Created replication controller with name: proxy-service-9bk4n, namespace: proxy-7726, replica count: 1
I0916 23:29:49.058948      23 runners.go:190] proxy-service-9bk4n Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0916 23:29:50.059155      23 runners.go:190] proxy-service-9bk4n Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0916 23:29:51.059360      23 runners.go:190] proxy-service-9bk4n Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0916 23:29:52.059602      23 runners.go:190] proxy-service-9bk4n Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0916 23:29:53.059778      23 runners.go:190] proxy-service-9bk4n Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 16 23:29:53.062: INFO: setup took 5.095241698s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Sep 16 23:29:53.066: INFO: (0) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/rewriteme">test</a> (200; 3.881361ms)
Sep 16 23:29:53.066: INFO: (0) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 4.139391ms)
Sep 16 23:29:53.068: INFO: (0) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname1/proxy/: foo (200; 5.368175ms)
Sep 16 23:29:53.068: INFO: (0) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 5.811701ms)
Sep 16 23:29:53.068: INFO: (0) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 5.683061ms)
Sep 16 23:29:53.068: INFO: (0) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 5.819127ms)
Sep 16 23:29:53.069: INFO: (0) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname1/proxy/: foo (200; 6.429283ms)
Sep 16 23:29:53.069: INFO: (0) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">... (200; 6.95446ms)
Sep 16 23:29:53.069: INFO: (0) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname2/proxy/: bar (200; 7.142835ms)
Sep 16 23:29:53.069: INFO: (0) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname2/proxy/: bar (200; 6.994694ms)
Sep 16 23:29:53.069: INFO: (0) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">test<... (200; 7.007377ms)
Sep 16 23:29:53.074: INFO: (0) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:460/proxy/: tls baz (200; 11.851219ms)
Sep 16 23:29:53.074: INFO: (0) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:462/proxy/: tls qux (200; 12.005816ms)
Sep 16 23:29:53.074: INFO: (0) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname2/proxy/: tls qux (200; 11.858409ms)
Sep 16 23:29:53.074: INFO: (0) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname1/proxy/: tls baz (200; 12.089791ms)
Sep 16 23:29:53.074: INFO: (0) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/tlsrewritem... (200; 11.935785ms)
Sep 16 23:29:53.078: INFO: (1) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 3.753841ms)
Sep 16 23:29:53.078: INFO: (1) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">... (200; 3.84919ms)
Sep 16 23:29:53.078: INFO: (1) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:460/proxy/: tls baz (200; 4.050882ms)
Sep 16 23:29:53.079: INFO: (1) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/tlsrewritem... (200; 4.140631ms)
Sep 16 23:29:53.079: INFO: (1) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:462/proxy/: tls qux (200; 4.2199ms)
Sep 16 23:29:53.079: INFO: (1) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">test<... (200; 4.270325ms)
Sep 16 23:29:53.079: INFO: (1) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 4.255809ms)
Sep 16 23:29:53.079: INFO: (1) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 4.534008ms)
Sep 16 23:29:53.079: INFO: (1) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/rewriteme">test</a> (200; 4.566054ms)
Sep 16 23:29:53.079: INFO: (1) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 4.506295ms)
Sep 16 23:29:53.079: INFO: (1) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname2/proxy/: tls qux (200; 4.931034ms)
Sep 16 23:29:53.080: INFO: (1) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname1/proxy/: foo (200; 5.624846ms)
Sep 16 23:29:53.080: INFO: (1) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname1/proxy/: tls baz (200; 5.595156ms)
Sep 16 23:29:53.080: INFO: (1) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname2/proxy/: bar (200; 5.6384ms)
Sep 16 23:29:53.080: INFO: (1) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname1/proxy/: foo (200; 5.750848ms)
Sep 16 23:29:53.080: INFO: (1) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname2/proxy/: bar (200; 5.741062ms)
Sep 16 23:29:53.082: INFO: (2) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">... (200; 2.276353ms)
Sep 16 23:29:53.086: INFO: (2) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 5.350894ms)
Sep 16 23:29:53.087: INFO: (2) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:462/proxy/: tls qux (200; 6.510522ms)
Sep 16 23:29:53.087: INFO: (2) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 6.557107ms)
Sep 16 23:29:53.087: INFO: (2) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 7.007835ms)
Sep 16 23:29:53.087: INFO: (2) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/rewriteme">test</a> (200; 7.021106ms)
Sep 16 23:29:53.087: INFO: (2) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/tlsrewritem... (200; 6.951188ms)
Sep 16 23:29:53.087: INFO: (2) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:460/proxy/: tls baz (200; 7.009681ms)
Sep 16 23:29:53.087: INFO: (2) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 7.052588ms)
Sep 16 23:29:53.088: INFO: (2) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname2/proxy/: bar (200; 8.272408ms)
Sep 16 23:29:53.089: INFO: (2) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">test<... (200; 8.242354ms)
Sep 16 23:29:53.089: INFO: (2) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname1/proxy/: tls baz (200; 8.288287ms)
Sep 16 23:29:53.089: INFO: (2) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname2/proxy/: tls qux (200; 8.316527ms)
Sep 16 23:29:53.089: INFO: (2) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname1/proxy/: foo (200; 8.35295ms)
Sep 16 23:29:53.089: INFO: (2) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname2/proxy/: bar (200; 8.845949ms)
Sep 16 23:29:53.089: INFO: (2) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname1/proxy/: foo (200; 8.847166ms)
Sep 16 23:29:53.092: INFO: (3) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 2.986845ms)
Sep 16 23:29:53.095: INFO: (3) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:460/proxy/: tls baz (200; 5.346875ms)
Sep 16 23:29:53.095: INFO: (3) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 5.584225ms)
Sep 16 23:29:53.095: INFO: (3) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">test<... (200; 5.567671ms)
Sep 16 23:29:53.095: INFO: (3) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:462/proxy/: tls qux (200; 6.093183ms)
Sep 16 23:29:53.095: INFO: (3) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 6.1235ms)
Sep 16 23:29:53.095: INFO: (3) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/rewriteme">test</a> (200; 6.091511ms)
Sep 16 23:29:53.095: INFO: (3) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 6.100789ms)
Sep 16 23:29:53.095: INFO: (3) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">... (200; 6.103723ms)
Sep 16 23:29:53.095: INFO: (3) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/tlsrewritem... (200; 6.20026ms)
Sep 16 23:29:53.096: INFO: (3) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname1/proxy/: tls baz (200; 6.508706ms)
Sep 16 23:29:53.097: INFO: (3) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname2/proxy/: bar (200; 7.6918ms)
Sep 16 23:29:53.097: INFO: (3) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname1/proxy/: foo (200; 7.797843ms)
Sep 16 23:29:53.097: INFO: (3) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname2/proxy/: tls qux (200; 7.75133ms)
Sep 16 23:29:53.097: INFO: (3) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname1/proxy/: foo (200; 7.82461ms)
Sep 16 23:29:53.097: INFO: (3) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname2/proxy/: bar (200; 7.787612ms)
Sep 16 23:29:53.099: INFO: (4) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:460/proxy/: tls baz (200; 2.140953ms)
Sep 16 23:29:53.100: INFO: (4) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 2.856197ms)
Sep 16 23:29:53.100: INFO: (4) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">test<... (200; 2.874927ms)
Sep 16 23:29:53.100: INFO: (4) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 2.96111ms)
Sep 16 23:29:53.100: INFO: (4) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 3.238311ms)
Sep 16 23:29:53.100: INFO: (4) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">... (200; 3.32944ms)
Sep 16 23:29:53.100: INFO: (4) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 3.218833ms)
Sep 16 23:29:53.101: INFO: (4) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:462/proxy/: tls qux (200; 3.546062ms)
Sep 16 23:29:53.101: INFO: (4) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/tlsrewritem... (200; 3.447737ms)
Sep 16 23:29:53.101: INFO: (4) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/rewriteme">test</a> (200; 3.535428ms)
Sep 16 23:29:53.101: INFO: (4) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname1/proxy/: tls baz (200; 3.855373ms)
Sep 16 23:29:53.102: INFO: (4) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname2/proxy/: bar (200; 4.781238ms)
Sep 16 23:29:53.102: INFO: (4) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname2/proxy/: bar (200; 4.812403ms)
Sep 16 23:29:53.102: INFO: (4) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname1/proxy/: foo (200; 4.882405ms)
Sep 16 23:29:53.102: INFO: (4) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname1/proxy/: foo (200; 4.792441ms)
Sep 16 23:29:53.102: INFO: (4) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname2/proxy/: tls qux (200; 4.80904ms)
Sep 16 23:29:53.104: INFO: (5) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:462/proxy/: tls qux (200; 2.248339ms)
Sep 16 23:29:53.105: INFO: (5) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:460/proxy/: tls baz (200; 2.703448ms)
Sep 16 23:29:53.105: INFO: (5) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">test<... (200; 3.050021ms)
Sep 16 23:29:53.105: INFO: (5) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 3.068196ms)
Sep 16 23:29:53.105: INFO: (5) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 3.282503ms)
Sep 16 23:29:53.106: INFO: (5) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 3.386515ms)
Sep 16 23:29:53.106: INFO: (5) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 3.413596ms)
Sep 16 23:29:53.106: INFO: (5) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/rewriteme">test</a> (200; 3.378827ms)
Sep 16 23:29:53.106: INFO: (5) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/tlsrewritem... (200; 3.499622ms)
Sep 16 23:29:53.106: INFO: (5) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">... (200; 3.422796ms)
Sep 16 23:29:53.106: INFO: (5) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname1/proxy/: foo (200; 4.171829ms)
Sep 16 23:29:53.106: INFO: (5) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname2/proxy/: tls qux (200; 4.133337ms)
Sep 16 23:29:53.106: INFO: (5) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname1/proxy/: tls baz (200; 4.225152ms)
Sep 16 23:29:53.107: INFO: (5) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname2/proxy/: bar (200; 4.629654ms)
Sep 16 23:29:53.107: INFO: (5) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname2/proxy/: bar (200; 4.658401ms)
Sep 16 23:29:53.107: INFO: (5) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname1/proxy/: foo (200; 4.654064ms)
Sep 16 23:29:53.109: INFO: (6) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">... (200; 2.337006ms)
Sep 16 23:29:53.110: INFO: (6) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 2.873653ms)
Sep 16 23:29:53.110: INFO: (6) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 3.135226ms)
Sep 16 23:29:53.110: INFO: (6) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:460/proxy/: tls baz (200; 3.107696ms)
Sep 16 23:29:53.110: INFO: (6) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 3.260076ms)
Sep 16 23:29:53.110: INFO: (6) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/tlsrewritem... (200; 3.313039ms)
Sep 16 23:29:53.111: INFO: (6) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 3.797499ms)
Sep 16 23:29:53.111: INFO: (6) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/rewriteme">test</a> (200; 3.871584ms)
Sep 16 23:29:53.111: INFO: (6) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname2/proxy/: bar (200; 4.026664ms)
Sep 16 23:29:53.111: INFO: (6) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:462/proxy/: tls qux (200; 3.927231ms)
Sep 16 23:29:53.111: INFO: (6) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">test<... (200; 4.035413ms)
Sep 16 23:29:53.111: INFO: (6) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname2/proxy/: tls qux (200; 4.168347ms)
Sep 16 23:29:53.112: INFO: (6) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname2/proxy/: bar (200; 4.992782ms)
Sep 16 23:29:53.112: INFO: (6) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname1/proxy/: tls baz (200; 4.982341ms)
Sep 16 23:29:53.112: INFO: (6) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname1/proxy/: foo (200; 5.04503ms)
Sep 16 23:29:53.112: INFO: (6) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname1/proxy/: foo (200; 5.024566ms)
Sep 16 23:29:53.114: INFO: (7) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 2.130452ms)
Sep 16 23:29:53.115: INFO: (7) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 2.854867ms)
Sep 16 23:29:53.115: INFO: (7) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/rewriteme">test</a> (200; 2.91398ms)
Sep 16 23:29:53.115: INFO: (7) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/tlsrewritem... (200; 2.942578ms)
Sep 16 23:29:53.115: INFO: (7) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">test<... (200; 2.966503ms)
Sep 16 23:29:53.115: INFO: (7) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 2.979833ms)
Sep 16 23:29:53.115: INFO: (7) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:462/proxy/: tls qux (200; 3.268079ms)
Sep 16 23:29:53.115: INFO: (7) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 3.317987ms)
Sep 16 23:29:53.116: INFO: (7) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">... (200; 3.468499ms)
Sep 16 23:29:53.116: INFO: (7) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:460/proxy/: tls baz (200; 3.475029ms)
Sep 16 23:29:53.116: INFO: (7) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname1/proxy/: foo (200; 3.853116ms)
Sep 16 23:29:53.116: INFO: (7) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname2/proxy/: bar (200; 3.96481ms)
Sep 16 23:29:53.117: INFO: (7) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname2/proxy/: bar (200; 4.555573ms)
Sep 16 23:29:53.117: INFO: (7) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname1/proxy/: tls baz (200; 4.528854ms)
Sep 16 23:29:53.117: INFO: (7) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname2/proxy/: tls qux (200; 4.623343ms)
Sep 16 23:29:53.117: INFO: (7) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname1/proxy/: foo (200; 4.649804ms)
Sep 16 23:29:53.119: INFO: (8) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 2.713441ms)
Sep 16 23:29:53.120: INFO: (8) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 2.996575ms)
Sep 16 23:29:53.120: INFO: (8) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/tlsrewritem... (200; 2.968731ms)
Sep 16 23:29:53.120: INFO: (8) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">test<... (200; 3.113572ms)
Sep 16 23:29:53.120: INFO: (8) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:460/proxy/: tls baz (200; 3.198472ms)
Sep 16 23:29:53.120: INFO: (8) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 3.17767ms)
Sep 16 23:29:53.120: INFO: (8) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 3.354976ms)
Sep 16 23:29:53.120: INFO: (8) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/rewriteme">test</a> (200; 3.526138ms)
Sep 16 23:29:53.120: INFO: (8) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname1/proxy/: tls baz (200; 3.623696ms)
Sep 16 23:29:53.120: INFO: (8) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">... (200; 3.616568ms)
Sep 16 23:29:53.121: INFO: (8) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname1/proxy/: foo (200; 3.770604ms)
Sep 16 23:29:53.121: INFO: (8) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:462/proxy/: tls qux (200; 3.743384ms)
Sep 16 23:29:53.121: INFO: (8) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname2/proxy/: tls qux (200; 4.5325ms)
Sep 16 23:29:53.121: INFO: (8) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname2/proxy/: bar (200; 4.525535ms)
Sep 16 23:29:53.122: INFO: (8) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname1/proxy/: foo (200; 4.776885ms)
Sep 16 23:29:53.122: INFO: (8) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname2/proxy/: bar (200; 4.767538ms)
Sep 16 23:29:53.124: INFO: (9) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 2.716106ms)
Sep 16 23:29:53.124: INFO: (9) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/rewriteme">test</a> (200; 2.637334ms)
Sep 16 23:29:53.124: INFO: (9) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 2.849929ms)
Sep 16 23:29:53.124: INFO: (9) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:462/proxy/: tls qux (200; 2.864661ms)
Sep 16 23:29:53.124: INFO: (9) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">... (200; 2.763292ms)
Sep 16 23:29:53.125: INFO: (9) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:460/proxy/: tls baz (200; 3.066707ms)
Sep 16 23:29:53.125: INFO: (9) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname2/proxy/: bar (200; 3.485012ms)
Sep 16 23:29:53.125: INFO: (9) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/tlsrewritem... (200; 3.695878ms)
Sep 16 23:29:53.125: INFO: (9) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">test<... (200; 3.675079ms)
Sep 16 23:29:53.125: INFO: (9) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 3.643207ms)
Sep 16 23:29:53.125: INFO: (9) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 3.690909ms)
Sep 16 23:29:53.126: INFO: (9) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname2/proxy/: bar (200; 4.155835ms)
Sep 16 23:29:53.126: INFO: (9) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname1/proxy/: foo (200; 4.736083ms)
Sep 16 23:29:53.126: INFO: (9) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname1/proxy/: foo (200; 4.873715ms)
Sep 16 23:29:53.126: INFO: (9) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname2/proxy/: tls qux (200; 4.806788ms)
Sep 16 23:29:53.126: INFO: (9) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname1/proxy/: tls baz (200; 4.772167ms)
Sep 16 23:29:53.129: INFO: (10) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:460/proxy/: tls baz (200; 2.437209ms)
Sep 16 23:29:53.130: INFO: (10) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 3.310132ms)
Sep 16 23:29:53.130: INFO: (10) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 3.315479ms)
Sep 16 23:29:53.130: INFO: (10) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">test<... (200; 3.399161ms)
Sep 16 23:29:53.130: INFO: (10) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">... (200; 3.464299ms)
Sep 16 23:29:53.130: INFO: (10) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/rewriteme">test</a> (200; 3.493127ms)
Sep 16 23:29:53.130: INFO: (10) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 3.497306ms)
Sep 16 23:29:53.130: INFO: (10) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:462/proxy/: tls qux (200; 3.53528ms)
Sep 16 23:29:53.130: INFO: (10) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/tlsrewritem... (200; 3.577639ms)
Sep 16 23:29:53.130: INFO: (10) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 3.692478ms)
Sep 16 23:29:53.131: INFO: (10) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname2/proxy/: bar (200; 4.66038ms)
Sep 16 23:29:53.132: INFO: (10) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname1/proxy/: foo (200; 5.422081ms)
Sep 16 23:29:53.132: INFO: (10) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname2/proxy/: tls qux (200; 5.334012ms)
Sep 16 23:29:53.132: INFO: (10) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname2/proxy/: bar (200; 5.408464ms)
Sep 16 23:29:53.132: INFO: (10) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname1/proxy/: foo (200; 5.358771ms)
Sep 16 23:29:53.132: INFO: (10) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname1/proxy/: tls baz (200; 5.419819ms)
Sep 16 23:29:53.135: INFO: (11) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 2.832556ms)
Sep 16 23:29:53.135: INFO: (11) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 2.922302ms)
Sep 16 23:29:53.135: INFO: (11) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 3.030327ms)
Sep 16 23:29:53.136: INFO: (11) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">... (200; 3.498821ms)
Sep 16 23:29:53.136: INFO: (11) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname1/proxy/: foo (200; 3.518581ms)
Sep 16 23:29:53.136: INFO: (11) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:460/proxy/: tls baz (200; 3.502241ms)
Sep 16 23:29:53.136: INFO: (11) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/rewriteme">test</a> (200; 3.688261ms)
Sep 16 23:29:53.136: INFO: (11) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/tlsrewritem... (200; 3.854706ms)
Sep 16 23:29:53.136: INFO: (11) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 3.909448ms)
Sep 16 23:29:53.136: INFO: (11) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">test<... (200; 3.906786ms)
Sep 16 23:29:53.136: INFO: (11) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname1/proxy/: foo (200; 4.022057ms)
Sep 16 23:29:53.136: INFO: (11) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:462/proxy/: tls qux (200; 4.050106ms)
Sep 16 23:29:53.136: INFO: (11) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname1/proxy/: tls baz (200; 3.988597ms)
Sep 16 23:29:53.137: INFO: (11) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname2/proxy/: tls qux (200; 4.637169ms)
Sep 16 23:29:53.137: INFO: (11) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname2/proxy/: bar (200; 4.644996ms)
Sep 16 23:29:53.137: INFO: (11) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname2/proxy/: bar (200; 4.889814ms)
Sep 16 23:29:53.140: INFO: (12) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 3.1893ms)
Sep 16 23:29:53.141: INFO: (12) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 3.538768ms)
Sep 16 23:29:53.141: INFO: (12) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 3.502919ms)
Sep 16 23:29:53.141: INFO: (12) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">test<... (200; 3.533921ms)
Sep 16 23:29:53.141: INFO: (12) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:462/proxy/: tls qux (200; 3.504217ms)
Sep 16 23:29:53.142: INFO: (12) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">... (200; 4.377016ms)
Sep 16 23:29:53.142: INFO: (12) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 4.464461ms)
Sep 16 23:29:53.142: INFO: (12) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/rewriteme">test</a> (200; 4.511422ms)
Sep 16 23:29:53.142: INFO: (12) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname1/proxy/: tls baz (200; 4.583802ms)
Sep 16 23:29:53.142: INFO: (12) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/tlsrewritem... (200; 4.575441ms)
Sep 16 23:29:53.142: INFO: (12) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname2/proxy/: bar (200; 4.567417ms)
Sep 16 23:29:53.142: INFO: (12) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:460/proxy/: tls baz (200; 4.541272ms)
Sep 16 23:29:53.143: INFO: (12) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname1/proxy/: foo (200; 5.67118ms)
Sep 16 23:29:53.143: INFO: (12) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname1/proxy/: foo (200; 5.612905ms)
Sep 16 23:29:53.143: INFO: (12) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname2/proxy/: tls qux (200; 5.613602ms)
Sep 16 23:29:53.143: INFO: (12) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname2/proxy/: bar (200; 5.67394ms)
Sep 16 23:29:53.145: INFO: (13) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/tlsrewritem... (200; 2.487089ms)
Sep 16 23:29:53.146: INFO: (13) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">test<... (200; 3.110719ms)
Sep 16 23:29:53.146: INFO: (13) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 3.231966ms)
Sep 16 23:29:53.146: INFO: (13) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 3.197462ms)
Sep 16 23:29:53.146: INFO: (13) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:462/proxy/: tls qux (200; 3.288028ms)
Sep 16 23:29:53.146: INFO: (13) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/rewriteme">test</a> (200; 3.312498ms)
Sep 16 23:29:53.146: INFO: (13) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:460/proxy/: tls baz (200; 3.269084ms)
Sep 16 23:29:53.147: INFO: (13) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">... (200; 3.716499ms)
Sep 16 23:29:53.147: INFO: (13) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 3.752056ms)
Sep 16 23:29:53.147: INFO: (13) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 3.813792ms)
Sep 16 23:29:53.147: INFO: (13) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname1/proxy/: foo (200; 4.305988ms)
Sep 16 23:29:53.148: INFO: (13) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname1/proxy/: foo (200; 5.146144ms)
Sep 16 23:29:53.148: INFO: (13) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname2/proxy/: bar (200; 5.107777ms)
Sep 16 23:29:53.148: INFO: (13) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname2/proxy/: tls qux (200; 5.265838ms)
Sep 16 23:29:53.148: INFO: (13) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname1/proxy/: tls baz (200; 5.145473ms)
Sep 16 23:29:53.148: INFO: (13) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname2/proxy/: bar (200; 5.143091ms)
Sep 16 23:29:53.155: INFO: (14) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 6.6924ms)
Sep 16 23:29:53.156: INFO: (14) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 7.711836ms)
Sep 16 23:29:53.156: INFO: (14) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:462/proxy/: tls qux (200; 7.692941ms)
Sep 16 23:29:53.156: INFO: (14) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:460/proxy/: tls baz (200; 7.836876ms)
Sep 16 23:29:53.156: INFO: (14) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">... (200; 8.259864ms)
Sep 16 23:29:53.156: INFO: (14) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/rewriteme">test</a> (200; 8.16364ms)
Sep 16 23:29:53.156: INFO: (14) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 8.208092ms)
Sep 16 23:29:53.156: INFO: (14) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname1/proxy/: foo (200; 8.330445ms)
Sep 16 23:29:53.156: INFO: (14) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">test<... (200; 8.245128ms)
Sep 16 23:29:53.157: INFO: (14) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/tlsrewritem... (200; 8.320367ms)
Sep 16 23:29:53.157: INFO: (14) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 8.419163ms)
Sep 16 23:29:53.157: INFO: (14) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname1/proxy/: tls baz (200; 8.764816ms)
Sep 16 23:29:53.157: INFO: (14) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname2/proxy/: bar (200; 8.815207ms)
Sep 16 23:29:53.158: INFO: (14) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname1/proxy/: foo (200; 9.520714ms)
Sep 16 23:29:53.158: INFO: (14) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname2/proxy/: bar (200; 9.493824ms)
Sep 16 23:29:53.158: INFO: (14) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname2/proxy/: tls qux (200; 9.553491ms)
Sep 16 23:29:53.172: INFO: (15) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 14.19423ms)
Sep 16 23:29:53.173: INFO: (15) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 15.023796ms)
Sep 16 23:29:53.173: INFO: (15) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/tlsrewritem... (200; 15.041852ms)
Sep 16 23:29:53.173: INFO: (15) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:460/proxy/: tls baz (200; 15.12369ms)
Sep 16 23:29:53.173: INFO: (15) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:462/proxy/: tls qux (200; 15.027887ms)
Sep 16 23:29:53.173: INFO: (15) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 15.050453ms)
Sep 16 23:29:53.173: INFO: (15) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">... (200; 15.287241ms)
Sep 16 23:29:53.173: INFO: (15) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/rewriteme">test</a> (200; 15.348206ms)
Sep 16 23:29:53.173: INFO: (15) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 15.392083ms)
Sep 16 23:29:53.173: INFO: (15) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">test<... (200; 15.541727ms)
Sep 16 23:29:53.174: INFO: (15) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname1/proxy/: foo (200; 15.953292ms)
Sep 16 23:29:53.175: INFO: (15) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname2/proxy/: bar (200; 16.836984ms)
Sep 16 23:29:53.175: INFO: (15) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname1/proxy/: foo (200; 16.852683ms)
Sep 16 23:29:53.175: INFO: (15) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname1/proxy/: tls baz (200; 16.829643ms)
Sep 16 23:29:53.175: INFO: (15) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname2/proxy/: bar (200; 16.802976ms)
Sep 16 23:29:53.175: INFO: (15) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname2/proxy/: tls qux (200; 16.848087ms)
Sep 16 23:29:53.177: INFO: (16) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 2.286414ms)
Sep 16 23:29:53.178: INFO: (16) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 2.928737ms)
Sep 16 23:29:53.178: INFO: (16) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">... (200; 3.202551ms)
Sep 16 23:29:53.178: INFO: (16) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:462/proxy/: tls qux (200; 3.24563ms)
Sep 16 23:29:53.178: INFO: (16) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/rewriteme">test</a> (200; 3.269035ms)
Sep 16 23:29:53.179: INFO: (16) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">test<... (200; 3.721549ms)
Sep 16 23:29:53.179: INFO: (16) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:460/proxy/: tls baz (200; 3.665883ms)
Sep 16 23:29:53.179: INFO: (16) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 3.749669ms)
Sep 16 23:29:53.179: INFO: (16) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/tlsrewritem... (200; 3.803481ms)
Sep 16 23:29:53.190: INFO: (16) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname2/proxy/: bar (200; 15.514959ms)
Sep 16 23:29:53.190: INFO: (16) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname2/proxy/: tls qux (200; 15.480668ms)
Sep 16 23:29:53.190: INFO: (16) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname2/proxy/: bar (200; 15.488773ms)
Sep 16 23:29:53.190: INFO: (16) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname1/proxy/: foo (200; 15.622551ms)
Sep 16 23:29:53.190: INFO: (16) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname1/proxy/: foo (200; 15.559052ms)
Sep 16 23:29:53.190: INFO: (16) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname1/proxy/: tls baz (200; 15.66118ms)
Sep 16 23:29:53.191: INFO: (16) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 15.703738ms)
Sep 16 23:29:53.193: INFO: (17) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/tlsrewritem... (200; 2.362401ms)
Sep 16 23:29:53.194: INFO: (17) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 2.947255ms)
Sep 16 23:29:53.194: INFO: (17) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:462/proxy/: tls qux (200; 3.152487ms)
Sep 16 23:29:53.194: INFO: (17) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 3.329038ms)
Sep 16 23:29:53.194: INFO: (17) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">... (200; 3.207277ms)
Sep 16 23:29:53.194: INFO: (17) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/rewriteme">test</a> (200; 3.691588ms)
Sep 16 23:29:53.194: INFO: (17) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 3.667683ms)
Sep 16 23:29:53.194: INFO: (17) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">test<... (200; 3.763396ms)
Sep 16 23:29:53.194: INFO: (17) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 3.81917ms)
Sep 16 23:29:53.194: INFO: (17) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:460/proxy/: tls baz (200; 3.755756ms)
Sep 16 23:29:53.195: INFO: (17) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname1/proxy/: tls baz (200; 3.871936ms)
Sep 16 23:29:53.195: INFO: (17) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname2/proxy/: bar (200; 4.437082ms)
Sep 16 23:29:53.196: INFO: (17) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname2/proxy/: bar (200; 4.776917ms)
Sep 16 23:29:53.196: INFO: (17) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname2/proxy/: tls qux (200; 4.808222ms)
Sep 16 23:29:53.196: INFO: (17) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname1/proxy/: foo (200; 4.914109ms)
Sep 16 23:29:53.196: INFO: (17) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname1/proxy/: foo (200; 4.792242ms)
Sep 16 23:29:53.198: INFO: (18) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 2.191736ms)
Sep 16 23:29:53.199: INFO: (18) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/tlsrewritem... (200; 3.041786ms)
Sep 16 23:29:53.199: INFO: (18) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:462/proxy/: tls qux (200; 3.154345ms)
Sep 16 23:29:53.199: INFO: (18) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">test<... (200; 3.142392ms)
Sep 16 23:29:53.199: INFO: (18) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 3.325001ms)
Sep 16 23:29:53.199: INFO: (18) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:460/proxy/: tls baz (200; 3.225279ms)
Sep 16 23:29:53.199: INFO: (18) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 3.556829ms)
Sep 16 23:29:53.199: INFO: (18) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">... (200; 3.606199ms)
Sep 16 23:29:53.199: INFO: (18) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/rewriteme">test</a> (200; 3.696782ms)
Sep 16 23:29:53.199: INFO: (18) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 3.691968ms)
Sep 16 23:29:53.200: INFO: (18) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname2/proxy/: bar (200; 3.811758ms)
Sep 16 23:29:53.200: INFO: (18) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname1/proxy/: foo (200; 4.630099ms)
Sep 16 23:29:53.200: INFO: (18) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname1/proxy/: foo (200; 4.700498ms)
Sep 16 23:29:53.200: INFO: (18) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname2/proxy/: tls qux (200; 4.679172ms)
Sep 16 23:29:53.200: INFO: (18) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname2/proxy/: bar (200; 4.797554ms)
Sep 16 23:29:53.201: INFO: (18) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname1/proxy/: tls baz (200; 5.041555ms)
Sep 16 23:29:53.203: INFO: (19) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">... (200; 2.24849ms)
Sep 16 23:29:53.204: INFO: (19) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 2.966426ms)
Sep 16 23:29:53.204: INFO: (19) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 3.016772ms)
Sep 16 23:29:53.204: INFO: (19) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:1080/proxy/rewriteme">test<... (200; 3.059505ms)
Sep 16 23:29:53.204: INFO: (19) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:462/proxy/: tls qux (200; 3.40117ms)
Sep 16 23:29:53.204: INFO: (19) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:460/proxy/: tls baz (200; 3.514244ms)
Sep 16 23:29:53.205: INFO: (19) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt/proxy/rewriteme">test</a> (200; 3.711667ms)
Sep 16 23:29:53.205: INFO: (19) /api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/: <a href="/api/v1/namespaces/proxy-7726/pods/https:proxy-service-9bk4n-cctvt:443/proxy/tlsrewritem... (200; 3.717878ms)
Sep 16 23:29:53.205: INFO: (19) /api/v1/namespaces/proxy-7726/pods/http:proxy-service-9bk4n-cctvt:162/proxy/: bar (200; 3.951686ms)
Sep 16 23:29:53.205: INFO: (19) /api/v1/namespaces/proxy-7726/pods/proxy-service-9bk4n-cctvt:160/proxy/: foo (200; 3.990229ms)
Sep 16 23:29:53.205: INFO: (19) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname2/proxy/: bar (200; 4.327714ms)
Sep 16 23:29:53.205: INFO: (19) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname1/proxy/: foo (200; 4.343281ms)
Sep 16 23:29:53.206: INFO: (19) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname2/proxy/: tls qux (200; 4.856516ms)
Sep 16 23:29:53.207: INFO: (19) /api/v1/namespaces/proxy-7726/services/http:proxy-service-9bk4n:portname2/proxy/: bar (200; 5.598085ms)
Sep 16 23:29:53.207: INFO: (19) /api/v1/namespaces/proxy-7726/services/https:proxy-service-9bk4n:tlsportname1/proxy/: tls baz (200; 5.614709ms)
Sep 16 23:29:53.207: INFO: (19) /api/v1/namespaces/proxy-7726/services/proxy-service-9bk4n:portname1/proxy/: foo (200; 5.594674ms)
STEP: deleting ReplicationController proxy-service-9bk4n in namespace proxy-7726, will wait for the garbage collector to delete the pods
Sep 16 23:29:53.269: INFO: Deleting ReplicationController proxy-service-9bk4n took: 10.010854ms
Sep 16 23:29:53.369: INFO: Terminating ReplicationController proxy-service-9bk4n pods took: 100.17798ms
[AfterEach] version v1
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:29:57.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7726" for this suite.

• [SLOW TEST:9.668 seconds]
[sig-network] Proxy
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":277,"completed":75,"skipped":1300,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:29:57.580: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-299420a1-e9a1-4b2e-b6d5-d77dd849d96a
STEP: Creating a pod to test consume secrets
Sep 16 23:29:57.715: INFO: Waiting up to 5m0s for pod "pod-secrets-8d8112f2-9db8-4149-8e0b-5f8d695f3f64" in namespace "secrets-8589" to be "Succeeded or Failed"
Sep 16 23:29:57.717: INFO: Pod "pod-secrets-8d8112f2-9db8-4149-8e0b-5f8d695f3f64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036568ms
Sep 16 23:29:59.720: INFO: Pod "pod-secrets-8d8112f2-9db8-4149-8e0b-5f8d695f3f64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005248642s
Sep 16 23:30:01.723: INFO: Pod "pod-secrets-8d8112f2-9db8-4149-8e0b-5f8d695f3f64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008352057s
STEP: Saw pod success
Sep 16 23:30:01.723: INFO: Pod "pod-secrets-8d8112f2-9db8-4149-8e0b-5f8d695f3f64" satisfied condition "Succeeded or Failed"
Sep 16 23:30:01.725: INFO: Trying to get logs from node eqx03-flash07 pod pod-secrets-8d8112f2-9db8-4149-8e0b-5f8d695f3f64 container secret-volume-test: <nil>
STEP: delete the pod
Sep 16 23:30:01.828: INFO: Waiting for pod pod-secrets-8d8112f2-9db8-4149-8e0b-5f8d695f3f64 to disappear
Sep 16 23:30:01.829: INFO: Pod pod-secrets-8d8112f2-9db8-4149-8e0b-5f8d695f3f64 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:30:01.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8589" for this suite.
STEP: Destroying namespace "secret-namespace-2858" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":277,"completed":76,"skipped":1313,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:30:01.855: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:30:18.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2953" for this suite.

• [SLOW TEST:16.192 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":277,"completed":77,"skipped":1322,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:30:18.048: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Sep 16 23:30:18.112: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 16 23:31:18.196: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 16 23:31:18.199: INFO: Starting informer...
STEP: Starting pod...
Sep 16 23:31:18.421: INFO: Pod is running on eqx03-flash06. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Sep 16 23:31:18.441: INFO: Pod wasn't evicted. Proceeding
Sep 16 23:31:18.441: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Sep 16 23:32:33.533: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:32:33.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-6993" for this suite.

• [SLOW TEST:135.498 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":277,"completed":78,"skipped":1354,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:32:33.547: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 16 23:32:34.367: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 16 23:32:36.374: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735895954, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735895954, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735895954, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735895954, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 16 23:32:39.418: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:32:39.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6322" for this suite.
STEP: Destroying namespace "webhook-6322-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.976 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":277,"completed":79,"skipped":1370,"failed":0}
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:32:39.523: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep 16 23:32:47.715: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 16 23:32:47.718: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 16 23:32:49.718: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 16 23:32:49.721: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 16 23:32:51.718: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 16 23:32:51.721: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:32:51.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-112" for this suite.

• [SLOW TEST:12.213 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":277,"completed":80,"skipped":1371,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:32:51.736: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Sep 16 23:32:51.786: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 16 23:33:51.853: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 16 23:33:51.856: INFO: Starting informer...
STEP: Starting pods...
Sep 16 23:33:52.076: INFO: Pod1 is running on eqx03-flash06. Tainting Node
Sep 16 23:33:56.299: INFO: Pod2 is running on eqx03-flash06. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Sep 16 23:34:10.957: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Sep 16 23:34:31.405: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:34:31.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-5634" for this suite.

• [SLOW TEST:99.709 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":277,"completed":81,"skipped":1391,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:34:31.446: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 16 23:34:31.800: INFO: Creating ReplicaSet my-hostname-basic-646e2062-61b9-4861-9cba-2b2777d9bd81
Sep 16 23:34:31.822: INFO: Pod name my-hostname-basic-646e2062-61b9-4861-9cba-2b2777d9bd81: Found 0 pods out of 1
Sep 16 23:34:36.825: INFO: Pod name my-hostname-basic-646e2062-61b9-4861-9cba-2b2777d9bd81: Found 1 pods out of 1
Sep 16 23:34:36.826: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-646e2062-61b9-4861-9cba-2b2777d9bd81" is running
Sep 16 23:34:36.828: INFO: Pod "my-hostname-basic-646e2062-61b9-4861-9cba-2b2777d9bd81-lm75x" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-16 23:34:31 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-16 23:34:34 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-16 23:34:34 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-16 23:34:31 +0000 UTC Reason: Message:}])
Sep 16 23:34:36.828: INFO: Trying to dial the pod
Sep 16 23:34:41.837: INFO: Controller my-hostname-basic-646e2062-61b9-4861-9cba-2b2777d9bd81: Got expected result from replica 1 [my-hostname-basic-646e2062-61b9-4861-9cba-2b2777d9bd81-lm75x]: "my-hostname-basic-646e2062-61b9-4861-9cba-2b2777d9bd81-lm75x", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:34:41.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-765" for this suite.

• [SLOW TEST:10.413 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":277,"completed":82,"skipped":1500,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:34:41.860: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: validating api versions
Sep 16 23:34:41.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 api-versions'
Sep 16 23:34:42.000: INFO: stderr: ""
Sep 16 23:34:42.000: INFO: stdout: "acme.cert-manager.io/v1alpha2\nacme.cert-manager.io/v1alpha3\nacme.cert-manager.io/v1beta1\nadmissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncert-manager.io/v1alpha2\ncert-manager.io/v1alpha3\ncert-manager.io/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ncsi.storage.k8s.io/v1alpha1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nk8s.cni.cncf.io/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nrobin.io/v1alpha1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:34:42.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-892" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":277,"completed":83,"skipped":1522,"failed":0}

------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:34:42.016: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:34:46.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2644" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":277,"completed":84,"skipped":1522,"failed":0}

------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:34:46.143: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Sep 16 23:34:46.194: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
Sep 16 23:34:50.977: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:35:10.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2232" for this suite.

• [SLOW TEST:24.004 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":277,"completed":85,"skipped":1522,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:35:10.147: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating Agnhost RC
Sep 16 23:35:10.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 create -f - --namespace=kubectl-9557'
Sep 16 23:35:10.611: INFO: stderr: ""
Sep 16 23:35:10.611: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Sep 16 23:35:11.633: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 16 23:35:11.633: INFO: Found 0 / 1
Sep 16 23:35:12.614: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 16 23:35:12.614: INFO: Found 0 / 1
Sep 16 23:35:13.615: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 16 23:35:13.615: INFO: Found 1 / 1
Sep 16 23:35:13.615: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Sep 16 23:35:13.617: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 16 23:35:13.617: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 16 23:35:13.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 patch pod agnhost-master-29c97 --namespace=kubectl-9557 -p {"metadata":{"annotations":{"x":"y"}}}'
Sep 16 23:35:13.734: INFO: stderr: ""
Sep 16 23:35:13.734: INFO: stdout: "pod/agnhost-master-29c97 patched\n"
STEP: checking annotations
Sep 16 23:35:13.737: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 16 23:35:13.737: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:35:13.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9557" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":277,"completed":86,"skipped":1526,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:35:13.767: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 16 23:35:13.830: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bd3e8f1f-2cb7-4a69-9135-20bfaa6f2706" in namespace "projected-3280" to be "Succeeded or Failed"
Sep 16 23:35:13.832: INFO: Pod "downwardapi-volume-bd3e8f1f-2cb7-4a69-9135-20bfaa6f2706": Phase="Pending", Reason="", readiness=false. Elapsed: 1.93233ms
Sep 16 23:35:15.836: INFO: Pod "downwardapi-volume-bd3e8f1f-2cb7-4a69-9135-20bfaa6f2706": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005295498s
Sep 16 23:35:17.839: INFO: Pod "downwardapi-volume-bd3e8f1f-2cb7-4a69-9135-20bfaa6f2706": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008724414s
STEP: Saw pod success
Sep 16 23:35:17.839: INFO: Pod "downwardapi-volume-bd3e8f1f-2cb7-4a69-9135-20bfaa6f2706" satisfied condition "Succeeded or Failed"
Sep 16 23:35:17.841: INFO: Trying to get logs from node eqx03-flash06 pod downwardapi-volume-bd3e8f1f-2cb7-4a69-9135-20bfaa6f2706 container client-container: <nil>
STEP: delete the pod
Sep 16 23:35:17.880: INFO: Waiting for pod downwardapi-volume-bd3e8f1f-2cb7-4a69-9135-20bfaa6f2706 to disappear
Sep 16 23:35:17.882: INFO: Pod downwardapi-volume-bd3e8f1f-2cb7-4a69-9135-20bfaa6f2706 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:35:17.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3280" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":277,"completed":87,"skipped":1569,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:35:17.894: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep 16 23:35:22.475: INFO: Successfully updated pod "pod-update-activedeadlineseconds-73ac3103-f3f9-4535-98ff-ceef4ead34b7"
Sep 16 23:35:22.475: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-73ac3103-f3f9-4535-98ff-ceef4ead34b7" in namespace "pods-4316" to be "terminated due to deadline exceeded"
Sep 16 23:35:22.477: INFO: Pod "pod-update-activedeadlineseconds-73ac3103-f3f9-4535-98ff-ceef4ead34b7": Phase="Running", Reason="", readiness=true. Elapsed: 1.921007ms
Sep 16 23:35:24.481: INFO: Pod "pod-update-activedeadlineseconds-73ac3103-f3f9-4535-98ff-ceef4ead34b7": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.006442267s
Sep 16 23:35:24.481: INFO: Pod "pod-update-activedeadlineseconds-73ac3103-f3f9-4535-98ff-ceef4ead34b7" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:35:24.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4316" for this suite.

• [SLOW TEST:6.597 seconds]
[k8s.io] Pods
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":277,"completed":88,"skipped":1598,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:35:24.492: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 16 23:35:24.551: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f664e223-947c-4efe-a7c6-d6780a74c619" in namespace "projected-293" to be "Succeeded or Failed"
Sep 16 23:35:24.579: INFO: Pod "downwardapi-volume-f664e223-947c-4efe-a7c6-d6780a74c619": Phase="Pending", Reason="", readiness=false. Elapsed: 28.247323ms
Sep 16 23:35:26.583: INFO: Pod "downwardapi-volume-f664e223-947c-4efe-a7c6-d6780a74c619": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032182468s
Sep 16 23:35:28.586: INFO: Pod "downwardapi-volume-f664e223-947c-4efe-a7c6-d6780a74c619": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035528277s
STEP: Saw pod success
Sep 16 23:35:28.586: INFO: Pod "downwardapi-volume-f664e223-947c-4efe-a7c6-d6780a74c619" satisfied condition "Succeeded or Failed"
Sep 16 23:35:28.589: INFO: Trying to get logs from node eqx03-flash06 pod downwardapi-volume-f664e223-947c-4efe-a7c6-d6780a74c619 container client-container: <nil>
STEP: delete the pod
Sep 16 23:35:28.625: INFO: Waiting for pod downwardapi-volume-f664e223-947c-4efe-a7c6-d6780a74c619 to disappear
Sep 16 23:35:28.627: INFO: Pod downwardapi-volume-f664e223-947c-4efe-a7c6-d6780a74c619 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:35:28.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-293" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":277,"completed":89,"skipped":1632,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:35:28.637: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep 16 23:35:28.711: INFO: Waiting up to 5m0s for pod "pod-63a139e9-fc55-4aad-b34c-f26bb5f509a2" in namespace "emptydir-9660" to be "Succeeded or Failed"
Sep 16 23:35:28.713: INFO: Pod "pod-63a139e9-fc55-4aad-b34c-f26bb5f509a2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.972694ms
Sep 16 23:35:30.715: INFO: Pod "pod-63a139e9-fc55-4aad-b34c-f26bb5f509a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004532073s
Sep 16 23:35:32.719: INFO: Pod "pod-63a139e9-fc55-4aad-b34c-f26bb5f509a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008142473s
STEP: Saw pod success
Sep 16 23:35:32.719: INFO: Pod "pod-63a139e9-fc55-4aad-b34c-f26bb5f509a2" satisfied condition "Succeeded or Failed"
Sep 16 23:35:32.721: INFO: Trying to get logs from node eqx03-flash06 pod pod-63a139e9-fc55-4aad-b34c-f26bb5f509a2 container test-container: <nil>
STEP: delete the pod
Sep 16 23:35:32.775: INFO: Waiting for pod pod-63a139e9-fc55-4aad-b34c-f26bb5f509a2 to disappear
Sep 16 23:35:32.777: INFO: Pod pod-63a139e9-fc55-4aad-b34c-f26bb5f509a2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:35:32.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9660" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":90,"skipped":1678,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:35:32.791: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating service nodeport-test with type=NodePort in namespace services-6956
STEP: creating replication controller nodeport-test in namespace services-6956
I0916 23:35:32.905867      23 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-6956, replica count: 2
Sep 16 23:35:35.956: INFO: Creating new exec pod
I0916 23:35:35.956274      23 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 16 23:35:38.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=services-6956 execpodw7vm7 -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Sep 16 23:35:39.329: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Sep 16 23:35:39.329: INFO: stdout: ""
Sep 16 23:35:39.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=services-6956 execpodw7vm7 -- /bin/sh -x -c nc -zv -t -w 2 172.19.16.30 80'
Sep 16 23:35:39.665: INFO: stderr: "+ nc -zv -t -w 2 172.19.16.30 80\nConnection to 172.19.16.30 80 port [tcp/http] succeeded!\n"
Sep 16 23:35:39.665: INFO: stdout: ""
Sep 16 23:35:39.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=services-6956 execpodw7vm7 -- /bin/sh -x -c nc -zv -t -w 2 10.9.140.107 31810'
Sep 16 23:35:40.012: INFO: stderr: "+ nc -zv -t -w 2 10.9.140.107 31810\nConnection to 10.9.140.107 31810 port [tcp/31810] succeeded!\n"
Sep 16 23:35:40.012: INFO: stdout: ""
Sep 16 23:35:40.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=services-6956 execpodw7vm7 -- /bin/sh -x -c nc -zv -t -w 2 10.9.40.106 31810'
Sep 16 23:35:40.417: INFO: stderr: "+ nc -zv -t -w 2 10.9.40.106 31810\nConnection to 10.9.40.106 31810 port [tcp/31810] succeeded!\n"
Sep 16 23:35:40.417: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:35:40.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6956" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:7.639 seconds]
[sig-network] Services
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":277,"completed":91,"skipped":1679,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:35:40.431: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:35:57.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7145" for this suite.

• [SLOW TEST:17.145 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":277,"completed":92,"skipped":1694,"failed":0}
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:35:57.577: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Sep 16 23:36:02.703: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:36:03.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2736" for this suite.

• [SLOW TEST:6.154 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":277,"completed":93,"skipped":1694,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:36:03.730: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-map-e7ff3aa9-0c9e-479f-9074-a12099a492f2
STEP: Creating a pod to test consume secrets
Sep 16 23:36:03.847: INFO: Waiting up to 5m0s for pod "pod-secrets-cf5de488-0e84-466a-9fad-4b922357a57d" in namespace "secrets-1357" to be "Succeeded or Failed"
Sep 16 23:36:03.849: INFO: Pod "pod-secrets-cf5de488-0e84-466a-9fad-4b922357a57d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063678ms
Sep 16 23:36:05.854: INFO: Pod "pod-secrets-cf5de488-0e84-466a-9fad-4b922357a57d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007077684s
Sep 16 23:36:07.861: INFO: Pod "pod-secrets-cf5de488-0e84-466a-9fad-4b922357a57d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013916371s
STEP: Saw pod success
Sep 16 23:36:07.861: INFO: Pod "pod-secrets-cf5de488-0e84-466a-9fad-4b922357a57d" satisfied condition "Succeeded or Failed"
Sep 16 23:36:07.863: INFO: Trying to get logs from node eqx03-flash06 pod pod-secrets-cf5de488-0e84-466a-9fad-4b922357a57d container secret-volume-test: <nil>
STEP: delete the pod
Sep 16 23:36:07.898: INFO: Waiting for pod pod-secrets-cf5de488-0e84-466a-9fad-4b922357a57d to disappear
Sep 16 23:36:07.900: INFO: Pod pod-secrets-cf5de488-0e84-466a-9fad-4b922357a57d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:36:07.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1357" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":94,"skipped":1698,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:36:07.910: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:36:26.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7664" for this suite.

• [SLOW TEST:18.982 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":277,"completed":95,"skipped":1706,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:36:26.894: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating service multi-endpoint-test in namespace services-9244
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9244 to expose endpoints map[]
Sep 16 23:36:27.052: INFO: Get endpoints failed (2.210543ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Sep 16 23:36:28.055: INFO: successfully validated that service multi-endpoint-test in namespace services-9244 exposes endpoints map[] (1.005595587s elapsed)
STEP: Creating pod pod1 in namespace services-9244
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9244 to expose endpoints map[pod1:[100]]
Sep 16 23:36:31.109: INFO: successfully validated that service multi-endpoint-test in namespace services-9244 exposes endpoints map[pod1:[100]] (3.02799505s elapsed)
STEP: Creating pod pod2 in namespace services-9244
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9244 to expose endpoints map[pod1:[100] pod2:[101]]
Sep 16 23:36:34.164: INFO: successfully validated that service multi-endpoint-test in namespace services-9244 exposes endpoints map[pod1:[100] pod2:[101]] (3.044362447s elapsed)
STEP: Deleting pod pod1 in namespace services-9244
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9244 to expose endpoints map[pod2:[101]]
Sep 16 23:36:35.199: INFO: successfully validated that service multi-endpoint-test in namespace services-9244 exposes endpoints map[pod2:[101]] (1.019803595s elapsed)
STEP: Deleting pod pod2 in namespace services-9244
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9244 to expose endpoints map[]
Sep 16 23:36:36.214: INFO: successfully validated that service multi-endpoint-test in namespace services-9244 exposes endpoints map[] (1.005015798s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:36:36.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9244" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:9.359 seconds]
[sig-network] Services
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":277,"completed":96,"skipped":1754,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:36:36.255: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep 16 23:36:36.336: INFO: Waiting up to 5m0s for pod "pod-8aa9426c-85d0-41c0-8b1d-7c3248f00227" in namespace "emptydir-214" to be "Succeeded or Failed"
Sep 16 23:36:36.338: INFO: Pod "pod-8aa9426c-85d0-41c0-8b1d-7c3248f00227": Phase="Pending", Reason="", readiness=false. Elapsed: 2.903903ms
Sep 16 23:36:38.342: INFO: Pod "pod-8aa9426c-85d0-41c0-8b1d-7c3248f00227": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006328863s
Sep 16 23:36:40.345: INFO: Pod "pod-8aa9426c-85d0-41c0-8b1d-7c3248f00227": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009891883s
STEP: Saw pod success
Sep 16 23:36:40.345: INFO: Pod "pod-8aa9426c-85d0-41c0-8b1d-7c3248f00227" satisfied condition "Succeeded or Failed"
Sep 16 23:36:40.348: INFO: Trying to get logs from node eqx03-flash06 pod pod-8aa9426c-85d0-41c0-8b1d-7c3248f00227 container test-container: <nil>
STEP: delete the pod
Sep 16 23:36:40.380: INFO: Waiting for pod pod-8aa9426c-85d0-41c0-8b1d-7c3248f00227 to disappear
Sep 16 23:36:40.382: INFO: Pod pod-8aa9426c-85d0-41c0-8b1d-7c3248f00227 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:36:40.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-214" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":97,"skipped":1882,"failed":0}

------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:36:40.391: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 16 23:36:40.467: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d2a3a10f-9205-4ecc-b782-25cc904afe02" in namespace "projected-5505" to be "Succeeded or Failed"
Sep 16 23:36:40.469: INFO: Pod "downwardapi-volume-d2a3a10f-9205-4ecc-b782-25cc904afe02": Phase="Pending", Reason="", readiness=false. Elapsed: 1.974915ms
Sep 16 23:36:42.472: INFO: Pod "downwardapi-volume-d2a3a10f-9205-4ecc-b782-25cc904afe02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005067223s
STEP: Saw pod success
Sep 16 23:36:42.472: INFO: Pod "downwardapi-volume-d2a3a10f-9205-4ecc-b782-25cc904afe02" satisfied condition "Succeeded or Failed"
Sep 16 23:36:42.474: INFO: Trying to get logs from node eqx03-flash06 pod downwardapi-volume-d2a3a10f-9205-4ecc-b782-25cc904afe02 container client-container: <nil>
STEP: delete the pod
Sep 16 23:36:42.521: INFO: Waiting for pod downwardapi-volume-d2a3a10f-9205-4ecc-b782-25cc904afe02 to disappear
Sep 16 23:36:42.523: INFO: Pod downwardapi-volume-d2a3a10f-9205-4ecc-b782-25cc904afe02 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:36:42.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5505" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":277,"completed":98,"skipped":1882,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:36:42.535: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep 16 23:36:42.601: INFO: Waiting up to 5m0s for pod "pod-58756cc2-6a57-4301-8beb-d52d381d2367" in namespace "emptydir-4066" to be "Succeeded or Failed"
Sep 16 23:36:42.603: INFO: Pod "pod-58756cc2-6a57-4301-8beb-d52d381d2367": Phase="Pending", Reason="", readiness=false. Elapsed: 2.172652ms
Sep 16 23:36:44.607: INFO: Pod "pod-58756cc2-6a57-4301-8beb-d52d381d2367": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005391957s
Sep 16 23:36:46.610: INFO: Pod "pod-58756cc2-6a57-4301-8beb-d52d381d2367": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008762386s
STEP: Saw pod success
Sep 16 23:36:46.610: INFO: Pod "pod-58756cc2-6a57-4301-8beb-d52d381d2367" satisfied condition "Succeeded or Failed"
Sep 16 23:36:46.612: INFO: Trying to get logs from node eqx03-flash06 pod pod-58756cc2-6a57-4301-8beb-d52d381d2367 container test-container: <nil>
STEP: delete the pod
Sep 16 23:36:46.652: INFO: Waiting for pod pod-58756cc2-6a57-4301-8beb-d52d381d2367 to disappear
Sep 16 23:36:46.654: INFO: Pod pod-58756cc2-6a57-4301-8beb-d52d381d2367 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:36:46.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4066" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":99,"skipped":1894,"failed":0}

------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:36:46.666: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-a63a7da5-ac0c-4ac4-94bd-cffd764e1c5c
STEP: Creating a pod to test consume configMaps
Sep 16 23:36:46.823: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c64b57ed-fe39-4c00-8030-9ff8caed5344" in namespace "projected-4430" to be "Succeeded or Failed"
Sep 16 23:36:46.825: INFO: Pod "pod-projected-configmaps-c64b57ed-fe39-4c00-8030-9ff8caed5344": Phase="Pending", Reason="", readiness=false. Elapsed: 2.224664ms
Sep 16 23:36:48.828: INFO: Pod "pod-projected-configmaps-c64b57ed-fe39-4c00-8030-9ff8caed5344": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005358937s
Sep 16 23:36:50.832: INFO: Pod "pod-projected-configmaps-c64b57ed-fe39-4c00-8030-9ff8caed5344": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008630092s
STEP: Saw pod success
Sep 16 23:36:50.832: INFO: Pod "pod-projected-configmaps-c64b57ed-fe39-4c00-8030-9ff8caed5344" satisfied condition "Succeeded or Failed"
Sep 16 23:36:50.834: INFO: Trying to get logs from node eqx03-flash06 pod pod-projected-configmaps-c64b57ed-fe39-4c00-8030-9ff8caed5344 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 16 23:36:50.863: INFO: Waiting for pod pod-projected-configmaps-c64b57ed-fe39-4c00-8030-9ff8caed5344 to disappear
Sep 16 23:36:50.865: INFO: Pod pod-projected-configmaps-c64b57ed-fe39-4c00-8030-9ff8caed5344 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:36:50.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4430" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":277,"completed":100,"skipped":1894,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:36:50.877: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 16 23:36:50.926: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:36:51.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5317" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":277,"completed":101,"skipped":1916,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:36:51.075: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-a7f168d4-da43-4254-8f8c-9c5bb91dcbcc
STEP: Creating a pod to test consume configMaps
Sep 16 23:36:51.142: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-78e3090c-6bfb-4596-99a8-528f58791114" in namespace "projected-1084" to be "Succeeded or Failed"
Sep 16 23:36:51.147: INFO: Pod "pod-projected-configmaps-78e3090c-6bfb-4596-99a8-528f58791114": Phase="Pending", Reason="", readiness=false. Elapsed: 4.962136ms
Sep 16 23:36:53.150: INFO: Pod "pod-projected-configmaps-78e3090c-6bfb-4596-99a8-528f58791114": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00810669s
Sep 16 23:36:55.153: INFO: Pod "pod-projected-configmaps-78e3090c-6bfb-4596-99a8-528f58791114": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011282788s
STEP: Saw pod success
Sep 16 23:36:55.153: INFO: Pod "pod-projected-configmaps-78e3090c-6bfb-4596-99a8-528f58791114" satisfied condition "Succeeded or Failed"
Sep 16 23:36:55.156: INFO: Trying to get logs from node eqx03-flash06 pod pod-projected-configmaps-78e3090c-6bfb-4596-99a8-528f58791114 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 16 23:36:55.215: INFO: Waiting for pod pod-projected-configmaps-78e3090c-6bfb-4596-99a8-528f58791114 to disappear
Sep 16 23:36:55.217: INFO: Pod pod-projected-configmaps-78e3090c-6bfb-4596-99a8-528f58791114 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:36:55.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1084" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":277,"completed":102,"skipped":1918,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:36:55.228: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 16 23:36:55.307: INFO: Waiting up to 5m0s for pod "downwardapi-volume-afb1688a-20e4-439f-b78f-29fe15bbd428" in namespace "downward-api-3495" to be "Succeeded or Failed"
Sep 16 23:36:55.309: INFO: Pod "downwardapi-volume-afb1688a-20e4-439f-b78f-29fe15bbd428": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009458ms
Sep 16 23:36:57.313: INFO: Pod "downwardapi-volume-afb1688a-20e4-439f-b78f-29fe15bbd428": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005405457s
Sep 16 23:36:59.316: INFO: Pod "downwardapi-volume-afb1688a-20e4-439f-b78f-29fe15bbd428": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008400642s
STEP: Saw pod success
Sep 16 23:36:59.316: INFO: Pod "downwardapi-volume-afb1688a-20e4-439f-b78f-29fe15bbd428" satisfied condition "Succeeded or Failed"
Sep 16 23:36:59.318: INFO: Trying to get logs from node eqx03-flash06 pod downwardapi-volume-afb1688a-20e4-439f-b78f-29fe15bbd428 container client-container: <nil>
STEP: delete the pod
Sep 16 23:36:59.373: INFO: Waiting for pod downwardapi-volume-afb1688a-20e4-439f-b78f-29fe15bbd428 to disappear
Sep 16 23:36:59.375: INFO: Pod downwardapi-volume-afb1688a-20e4-439f-b78f-29fe15bbd428 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:36:59.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3495" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":277,"completed":103,"skipped":1936,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:36:59.385: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Sep 16 23:37:39.480: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0916 23:37:39.480415      23 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:37:39.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7871" for this suite.

• [SLOW TEST:40.109 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":277,"completed":104,"skipped":1946,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:37:39.495: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:37:43.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4485" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":105,"skipped":1960,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:37:43.606: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 16 23:37:43.680: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:37:50.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4437" for this suite.

• [SLOW TEST:6.481 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":277,"completed":106,"skipped":1965,"failed":0}
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:37:50.087: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service nodeport-service with the type=NodePort in namespace services-7688
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-7688
STEP: creating replication controller externalsvc in namespace services-7688
I0916 23:37:50.216502      23 runners.go:190] Created replication controller with name: externalsvc, namespace: services-7688, replica count: 2
I0916 23:37:53.267091      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0916 23:37:56.267304      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Sep 16 23:37:56.305: INFO: Creating new exec pod
Sep 16 23:38:00.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=services-7688 execpodvttxt -- /bin/sh -x -c nslookup nodeport-service'
Sep 16 23:38:00.670: INFO: stderr: "+ nslookup nodeport-service\n"
Sep 16 23:38:00.670: INFO: stdout: "Server:\t\t172.19.0.10\nAddress:\t172.19.0.10#53\n\nnodeport-service.services-7688.svc.cluster.local\tcanonical name = externalsvc.services-7688.svc.cluster.local.\nName:\texternalsvc.services-7688.svc.cluster.local\nAddress: 172.19.209.225\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7688, will wait for the garbage collector to delete the pods
Sep 16 23:38:00.733: INFO: Deleting ReplicationController externalsvc took: 9.892598ms
Sep 16 23:38:01.633: INFO: Terminating ReplicationController externalsvc pods took: 900.230674ms
Sep 16 23:38:11.069: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:38:11.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7688" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:21.032 seconds]
[sig-network] Services
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":277,"completed":107,"skipped":1965,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:38:11.120: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-7448
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 16 23:38:11.166: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 16 23:38:11.269: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 16 23:38:13.272: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 16 23:38:15.273: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 16 23:38:17.273: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 16 23:38:19.273: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 16 23:38:21.273: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 16 23:38:23.273: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 16 23:38:25.273: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 16 23:38:27.272: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 16 23:38:29.273: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep 16 23:38:29.278: INFO: The status of Pod netserver-1 is Running (Ready = true)
Sep 16 23:38:29.282: INFO: The status of Pod netserver-2 is Running (Ready = true)
Sep 16 23:38:29.286: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
Sep 16 23:38:31.341: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.21.13.123:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7448 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 16 23:38:31.341: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
Sep 16 23:38:31.585: INFO: Found all expected endpoints: [netserver-0]
Sep 16 23:38:31.587: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.21.4.18:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7448 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 16 23:38:31.587: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
Sep 16 23:38:31.829: INFO: Found all expected endpoints: [netserver-1]
Sep 16 23:38:31.831: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.21.3.105:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7448 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 16 23:38:31.831: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
Sep 16 23:38:32.081: INFO: Found all expected endpoints: [netserver-2]
Sep 16 23:38:32.083: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.21.4.78:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7448 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 16 23:38:32.084: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
Sep 16 23:38:32.324: INFO: Found all expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:38:32.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7448" for this suite.

• [SLOW TEST:21.272 seconds]
[sig-network] Networking
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":108,"skipped":2016,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:38:32.391: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 16 23:38:33.347: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 16 23:38:35.355: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735896313, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735896313, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735896313, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735896313, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 16 23:38:38.377: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Sep 16 23:38:42.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 attach --namespace=webhook-4983 to-be-attached-pod -i -c=container1'
Sep 16 23:38:42.572: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:38:42.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4983" for this suite.
STEP: Destroying namespace "webhook-4983-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.294 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":277,"completed":109,"skipped":2026,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:38:42.685: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep 16 23:38:42.748: INFO: Waiting up to 5m0s for pod "pod-3114b764-d9e4-4196-8118-1259d5afbaf3" in namespace "emptydir-5513" to be "Succeeded or Failed"
Sep 16 23:38:42.750: INFO: Pod "pod-3114b764-d9e4-4196-8118-1259d5afbaf3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.962283ms
Sep 16 23:38:44.753: INFO: Pod "pod-3114b764-d9e4-4196-8118-1259d5afbaf3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005250841s
Sep 16 23:38:46.756: INFO: Pod "pod-3114b764-d9e4-4196-8118-1259d5afbaf3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008148386s
STEP: Saw pod success
Sep 16 23:38:46.756: INFO: Pod "pod-3114b764-d9e4-4196-8118-1259d5afbaf3" satisfied condition "Succeeded or Failed"
Sep 16 23:38:46.758: INFO: Trying to get logs from node eqx03-flash06 pod pod-3114b764-d9e4-4196-8118-1259d5afbaf3 container test-container: <nil>
STEP: delete the pod
Sep 16 23:38:46.797: INFO: Waiting for pod pod-3114b764-d9e4-4196-8118-1259d5afbaf3 to disappear
Sep 16 23:38:46.798: INFO: Pod pod-3114b764-d9e4-4196-8118-1259d5afbaf3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:38:46.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5513" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":110,"skipped":2030,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:38:46.827: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 16 23:38:49.916: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:38:49.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2040" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":277,"completed":111,"skipped":2037,"failed":0}

------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:38:49.981: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Sep 16 23:38:50.043: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-202 /api/v1/namespaces/watch-202/configmaps/e2e-watch-test-watch-closed 7cb3c224-1859-4109-8ba9-2d9d8f4f5970 4233420 0 2020-09-16 23:38:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-09-16 23:38:50 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 16 23:38:50.043: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-202 /api/v1/namespaces/watch-202/configmaps/e2e-watch-test-watch-closed 7cb3c224-1859-4109-8ba9-2d9d8f4f5970 4233421 0 2020-09-16 23:38:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-09-16 23:38:50 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Sep 16 23:38:50.060: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-202 /api/v1/namespaces/watch-202/configmaps/e2e-watch-test-watch-closed 7cb3c224-1859-4109-8ba9-2d9d8f4f5970 4233422 0 2020-09-16 23:38:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-09-16 23:38:50 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 16 23:38:50.060: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-202 /api/v1/namespaces/watch-202/configmaps/e2e-watch-test-watch-closed 7cb3c224-1859-4109-8ba9-2d9d8f4f5970 4233423 0 2020-09-16 23:38:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-09-16 23:38:50 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:38:50.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-202" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":277,"completed":112,"skipped":2037,"failed":0}
SSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:38:50.087: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Sep 16 23:38:50.149: INFO: Pod name pod-release: Found 0 pods out of 1
Sep 16 23:38:55.155: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:38:56.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4880" for this suite.

• [SLOW TEST:6.094 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":277,"completed":113,"skipped":2040,"failed":0}
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:38:56.181: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-3757
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating statefulset ss in namespace statefulset-3757
Sep 16 23:38:56.292: INFO: Found 0 stateful pods, waiting for 1
Sep 16 23:39:06.295: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Sep 16 23:39:06.309: INFO: Deleting all statefulset in ns statefulset-3757
Sep 16 23:39:06.311: INFO: Scaling statefulset ss to 0
Sep 16 23:39:36.365: INFO: Waiting for statefulset status.replicas updated to 0
Sep 16 23:39:36.368: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:39:36.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3757" for this suite.

• [SLOW TEST:40.214 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":277,"completed":114,"skipped":2045,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:39:36.396: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 16 23:39:36.476: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f99e05e8-cbbe-4932-addb-2e942a4af1cb" in namespace "projected-1245" to be "Succeeded or Failed"
Sep 16 23:39:36.478: INFO: Pod "downwardapi-volume-f99e05e8-cbbe-4932-addb-2e942a4af1cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.104ms
Sep 16 23:39:38.481: INFO: Pod "downwardapi-volume-f99e05e8-cbbe-4932-addb-2e942a4af1cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004799819s
STEP: Saw pod success
Sep 16 23:39:38.481: INFO: Pod "downwardapi-volume-f99e05e8-cbbe-4932-addb-2e942a4af1cb" satisfied condition "Succeeded or Failed"
Sep 16 23:39:38.483: INFO: Trying to get logs from node eqx03-flash06 pod downwardapi-volume-f99e05e8-cbbe-4932-addb-2e942a4af1cb container client-container: <nil>
STEP: delete the pod
Sep 16 23:39:38.521: INFO: Waiting for pod downwardapi-volume-f99e05e8-cbbe-4932-addb-2e942a4af1cb to disappear
Sep 16 23:39:38.523: INFO: Pod downwardapi-volume-f99e05e8-cbbe-4932-addb-2e942a4af1cb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:39:38.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1245" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":277,"completed":115,"skipped":2071,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:39:38.533: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep 16 23:39:38.616: INFO: Waiting up to 5m0s for pod "pod-9f21270e-484e-4673-b185-37ee95a7c7cc" in namespace "emptydir-3433" to be "Succeeded or Failed"
Sep 16 23:39:38.618: INFO: Pod "pod-9f21270e-484e-4673-b185-37ee95a7c7cc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.928888ms
Sep 16 23:39:40.620: INFO: Pod "pod-9f21270e-484e-4673-b185-37ee95a7c7cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004848812s
Sep 16 23:39:42.624: INFO: Pod "pod-9f21270e-484e-4673-b185-37ee95a7c7cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008761362s
STEP: Saw pod success
Sep 16 23:39:42.624: INFO: Pod "pod-9f21270e-484e-4673-b185-37ee95a7c7cc" satisfied condition "Succeeded or Failed"
Sep 16 23:39:42.627: INFO: Trying to get logs from node eqx03-flash06 pod pod-9f21270e-484e-4673-b185-37ee95a7c7cc container test-container: <nil>
STEP: delete the pod
Sep 16 23:39:42.680: INFO: Waiting for pod pod-9f21270e-484e-4673-b185-37ee95a7c7cc to disappear
Sep 16 23:39:42.682: INFO: Pod pod-9f21270e-484e-4673-b185-37ee95a7c7cc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:39:42.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3433" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":116,"skipped":2077,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:39:42.694: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 16 23:39:42.759: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Sep 16 23:39:47.762: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 16 23:39:47.762: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Sep 16 23:39:47.779: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4054 /apis/apps/v1/namespaces/deployment-4054/deployments/test-cleanup-deployment 3feed343-4fec-4e44-a0cc-703043f84dd2 4233897 1 2020-09-16 23:39:47 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2020-09-16 23:39:47 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0078720e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Sep 16 23:39:47.786: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Sep 16 23:39:47.786: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Sep 16 23:39:47.787: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4054 /apis/apps/v1/namespaces/deployment-4054/replicasets/test-cleanup-controller 7fce7799-f537-4bfe-9581-ad75000f11a9 4233898 1 2020-09-16 23:39:42 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 3feed343-4fec-4e44-a0cc-703043f84dd2 0xc0078724bf 0xc0078724d0}] []  [{e2e.test Update apps/v1 2020-09-16 23:39:42 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-09-16 23:39:47 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 51 102 101 101 100 51 52 51 45 52 102 101 99 45 52 101 52 52 45 97 48 99 99 45 55 48 51 48 52 51 102 56 52 100 100 50 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc007872568 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 16 23:39:47.823: INFO: Pod "test-cleanup-controller-khqhc" is available:
&Pod{ObjectMeta:{test-cleanup-controller-khqhc test-cleanup-controller- deployment-4054 /api/v1/namespaces/deployment-4054/pods/test-cleanup-controller-khqhc f688befc-c90c-4c34-9015-61f6c661689c 4233889 0 2020-09-16 23:39:42 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:172.21.4.78/32 cni.projectcalico.org/podIPs:172.21.4.78/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "calico",
    "ips": [
        "172.21.4.78"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "calico",
    "ips": [
        "172.21.4.78"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-cleanup-controller 7fce7799-f537-4bfe-9581-ad75000f11a9 0xc0078c6a17 0xc0078c6a18}] []  [{kube-controller-manager Update v1 2020-09-16 23:39:42 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 102 99 101 55 55 57 57 45 102 53 51 55 45 52 98 102 101 45 57 53 56 49 45 97 100 55 53 48 48 48 102 49 49 97 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-09-16 23:39:44 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {multus Update v1 2020-09-16 23:39:44 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 107 56 115 46 118 49 46 99 110 105 46 99 110 99 102 46 105 111 47 110 101 116 119 111 114 107 45 115 116 97 116 117 115 34 58 123 125 44 34 102 58 107 56 115 46 118 49 46 99 110 105 46 99 110 99 102 46 105 111 47 110 101 116 119 111 114 107 115 45 115 116 97 116 117 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-09-16 23:39:45 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 50 49 46 52 46 55 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zwlwk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zwlwk,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zwlwk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-16 23:39:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-16 23:39:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-16 23:39:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-16 23:39:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.140.106,PodIP:172.21.4.78,StartTime:2020-09-16 23:39:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-16 23:39:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:robin://989bbd4dbf6b9d90bb90c97fcec9c1b3bb6cf07a49def9c547f35f01574b307f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.4.78,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:39:47.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4054" for this suite.

• [SLOW TEST:5.202 seconds]
[sig-apps] Deployment
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":277,"completed":117,"skipped":2084,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:39:47.896: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-1087
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 16 23:39:47.967: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 16 23:39:48.092: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 16 23:39:50.095: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 16 23:39:52.095: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 16 23:39:54.095: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 16 23:39:56.101: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 16 23:39:58.095: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 16 23:40:00.095: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 16 23:40:02.095: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 16 23:40:04.095: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 16 23:40:06.095: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep 16 23:40:06.100: INFO: The status of Pod netserver-1 is Running (Ready = true)
Sep 16 23:40:06.104: INFO: The status of Pod netserver-2 is Running (Ready = true)
Sep 16 23:40:06.108: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
Sep 16 23:40:10.128: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.4.78:8080/dial?request=hostname&protocol=udp&host=172.21.12.228&port=8081&tries=1'] Namespace:pod-network-test-1087 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 16 23:40:10.128: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
Sep 16 23:40:10.483: INFO: Waiting for responses: map[]
Sep 16 23:40:10.486: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.4.78:8080/dial?request=hostname&protocol=udp&host=172.21.4.18&port=8081&tries=1'] Namespace:pod-network-test-1087 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 16 23:40:10.486: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
Sep 16 23:40:10.744: INFO: Waiting for responses: map[]
Sep 16 23:40:10.746: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.4.78:8080/dial?request=hostname&protocol=udp&host=172.21.13.123&port=8081&tries=1'] Namespace:pod-network-test-1087 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 16 23:40:10.746: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
Sep 16 23:40:10.984: INFO: Waiting for responses: map[]
Sep 16 23:40:10.986: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.4.78:8080/dial?request=hostname&protocol=udp&host=172.21.3.105&port=8081&tries=1'] Namespace:pod-network-test-1087 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 16 23:40:10.986: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
Sep 16 23:40:11.350: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:40:11.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1087" for this suite.

• [SLOW TEST:23.474 seconds]
[sig-network] Networking
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":277,"completed":118,"skipped":2104,"failed":0}
SS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:40:11.371: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 16 23:40:11.427: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: creating replication controller svc-latency-rc in namespace svc-latency-2033
I0916 23:40:11.448065      23 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2033, replica count: 1
I0916 23:40:12.498534      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0916 23:40:13.498811      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0916 23:40:14.499049      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 16 23:40:14.623: INFO: Created: latency-svc-xxkt4
Sep 16 23:40:14.633: INFO: Got endpoints: latency-svc-xxkt4 [34.030155ms]
Sep 16 23:40:14.652: INFO: Created: latency-svc-z5lt9
Sep 16 23:40:14.661: INFO: Got endpoints: latency-svc-z5lt9 [27.638378ms]
Sep 16 23:40:14.671: INFO: Created: latency-svc-g7g5b
Sep 16 23:40:14.682: INFO: Got endpoints: latency-svc-g7g5b [48.80466ms]
Sep 16 23:40:14.700: INFO: Created: latency-svc-46hth
Sep 16 23:40:14.712: INFO: Got endpoints: latency-svc-46hth [79.12914ms]
Sep 16 23:40:14.731: INFO: Created: latency-svc-hxq47
Sep 16 23:40:14.739: INFO: Got endpoints: latency-svc-hxq47 [106.214202ms]
Sep 16 23:40:14.754: INFO: Created: latency-svc-nj2hp
Sep 16 23:40:14.768: INFO: Got endpoints: latency-svc-nj2hp [134.750866ms]
Sep 16 23:40:14.785: INFO: Created: latency-svc-hztfx
Sep 16 23:40:14.797: INFO: Got endpoints: latency-svc-hztfx [163.935662ms]
Sep 16 23:40:14.808: INFO: Created: latency-svc-xtgc4
Sep 16 23:40:14.817: INFO: Got endpoints: latency-svc-xtgc4 [183.917989ms]
Sep 16 23:40:14.824: INFO: Created: latency-svc-g4s56
Sep 16 23:40:14.855: INFO: Got endpoints: latency-svc-g4s56 [222.183681ms]
Sep 16 23:40:14.862: INFO: Created: latency-svc-9z9jh
Sep 16 23:40:14.874: INFO: Got endpoints: latency-svc-9z9jh [240.895882ms]
Sep 16 23:40:14.886: INFO: Created: latency-svc-lpv8f
Sep 16 23:40:14.894: INFO: Got endpoints: latency-svc-lpv8f [260.988543ms]
Sep 16 23:40:14.904: INFO: Created: latency-svc-xhrv8
Sep 16 23:40:14.915: INFO: Got endpoints: latency-svc-xhrv8 [282.357517ms]
Sep 16 23:40:14.928: INFO: Created: latency-svc-jg2dq
Sep 16 23:40:14.934: INFO: Got endpoints: latency-svc-jg2dq [300.98005ms]
Sep 16 23:40:14.980: INFO: Created: latency-svc-hk27m
Sep 16 23:40:14.989: INFO: Got endpoints: latency-svc-hk27m [355.750975ms]
Sep 16 23:40:15.013: INFO: Created: latency-svc-5v2jg
Sep 16 23:40:15.018: INFO: Got endpoints: latency-svc-5v2jg [384.601021ms]
Sep 16 23:40:15.036: INFO: Created: latency-svc-nblln
Sep 16 23:40:15.044: INFO: Got endpoints: latency-svc-nblln [411.29294ms]
Sep 16 23:40:15.055: INFO: Created: latency-svc-4vfvb
Sep 16 23:40:15.066: INFO: Got endpoints: latency-svc-4vfvb [405.016774ms]
Sep 16 23:40:15.100: INFO: Created: latency-svc-sbqvj
Sep 16 23:40:15.110: INFO: Got endpoints: latency-svc-sbqvj [428.316964ms]
Sep 16 23:40:15.120: INFO: Created: latency-svc-ljjcq
Sep 16 23:40:15.129: INFO: Got endpoints: latency-svc-ljjcq [416.41054ms]
Sep 16 23:40:15.136: INFO: Created: latency-svc-5hvkm
Sep 16 23:40:15.149: INFO: Got endpoints: latency-svc-5hvkm [409.958075ms]
Sep 16 23:40:15.167: INFO: Created: latency-svc-wvm5w
Sep 16 23:40:15.178: INFO: Got endpoints: latency-svc-wvm5w [410.703711ms]
Sep 16 23:40:15.191: INFO: Created: latency-svc-brzsn
Sep 16 23:40:15.199: INFO: Got endpoints: latency-svc-brzsn [402.590685ms]
Sep 16 23:40:15.219: INFO: Created: latency-svc-59jrz
Sep 16 23:40:15.233: INFO: Got endpoints: latency-svc-59jrz [416.0737ms]
Sep 16 23:40:15.245: INFO: Created: latency-svc-wnfj8
Sep 16 23:40:15.257: INFO: Got endpoints: latency-svc-wnfj8 [401.442408ms]
Sep 16 23:40:15.269: INFO: Created: latency-svc-2hq9g
Sep 16 23:40:15.277: INFO: Got endpoints: latency-svc-2hq9g [403.47272ms]
Sep 16 23:40:15.288: INFO: Created: latency-svc-njhn9
Sep 16 23:40:15.298: INFO: Got endpoints: latency-svc-njhn9 [404.311929ms]
Sep 16 23:40:15.317: INFO: Created: latency-svc-8x2gs
Sep 16 23:40:15.343: INFO: Got endpoints: latency-svc-8x2gs [428.011926ms]
Sep 16 23:40:15.354: INFO: Created: latency-svc-qsq6c
Sep 16 23:40:15.362: INFO: Got endpoints: latency-svc-qsq6c [427.799755ms]
Sep 16 23:40:15.375: INFO: Created: latency-svc-d88n9
Sep 16 23:40:15.389: INFO: Got endpoints: latency-svc-d88n9 [400.688137ms]
Sep 16 23:40:15.418: INFO: Created: latency-svc-grpfj
Sep 16 23:40:15.425: INFO: Got endpoints: latency-svc-grpfj [407.495587ms]
Sep 16 23:40:15.462: INFO: Created: latency-svc-dc7zq
Sep 16 23:40:15.476: INFO: Got endpoints: latency-svc-dc7zq [431.309385ms]
Sep 16 23:40:15.491: INFO: Created: latency-svc-545ff
Sep 16 23:40:15.500: INFO: Got endpoints: latency-svc-545ff [434.198392ms]
Sep 16 23:40:15.510: INFO: Created: latency-svc-qh4c2
Sep 16 23:40:15.518: INFO: Got endpoints: latency-svc-qh4c2 [408.20072ms]
Sep 16 23:40:15.534: INFO: Created: latency-svc-jnwhx
Sep 16 23:40:15.548: INFO: Got endpoints: latency-svc-jnwhx [419.481262ms]
Sep 16 23:40:15.559: INFO: Created: latency-svc-29v4t
Sep 16 23:40:15.583: INFO: Got endpoints: latency-svc-29v4t [434.009547ms]
Sep 16 23:40:15.588: INFO: Created: latency-svc-bq2s7
Sep 16 23:40:15.596: INFO: Got endpoints: latency-svc-bq2s7 [417.180945ms]
Sep 16 23:40:15.610: INFO: Created: latency-svc-trjjt
Sep 16 23:40:15.624: INFO: Got endpoints: latency-svc-trjjt [424.56363ms]
Sep 16 23:40:15.640: INFO: Created: latency-svc-cdpqj
Sep 16 23:40:15.652: INFO: Got endpoints: latency-svc-cdpqj [418.532525ms]
Sep 16 23:40:15.659: INFO: Created: latency-svc-5wf5f
Sep 16 23:40:15.672: INFO: Got endpoints: latency-svc-5wf5f [415.47589ms]
Sep 16 23:40:15.704: INFO: Created: latency-svc-m768n
Sep 16 23:40:15.716: INFO: Got endpoints: latency-svc-m768n [438.676051ms]
Sep 16 23:40:15.730: INFO: Created: latency-svc-xwhmb
Sep 16 23:40:15.738: INFO: Got endpoints: latency-svc-xwhmb [440.067811ms]
Sep 16 23:40:15.749: INFO: Created: latency-svc-tnlv5
Sep 16 23:40:15.765: INFO: Got endpoints: latency-svc-tnlv5 [421.551939ms]
Sep 16 23:40:15.777: INFO: Created: latency-svc-b5c2q
Sep 16 23:40:15.790: INFO: Got endpoints: latency-svc-b5c2q [428.127536ms]
Sep 16 23:40:15.815: INFO: Created: latency-svc-kfd26
Sep 16 23:40:15.823: INFO: Got endpoints: latency-svc-kfd26 [433.820739ms]
Sep 16 23:40:15.835: INFO: Created: latency-svc-4njfp
Sep 16 23:40:15.850: INFO: Got endpoints: latency-svc-4njfp [424.426662ms]
Sep 16 23:40:15.867: INFO: Created: latency-svc-c8lvh
Sep 16 23:40:15.880: INFO: Got endpoints: latency-svc-c8lvh [404.196638ms]
Sep 16 23:40:15.888: INFO: Created: latency-svc-q72hm
Sep 16 23:40:15.896: INFO: Got endpoints: latency-svc-q72hm [396.354753ms]
Sep 16 23:40:15.907: INFO: Created: latency-svc-hg6rj
Sep 16 23:40:15.938: INFO: Got endpoints: latency-svc-hg6rj [419.538081ms]
Sep 16 23:40:15.944: INFO: Created: latency-svc-86cct
Sep 16 23:40:15.956: INFO: Got endpoints: latency-svc-86cct [407.588622ms]
Sep 16 23:40:15.968: INFO: Created: latency-svc-cpld8
Sep 16 23:40:15.975: INFO: Got endpoints: latency-svc-cpld8 [392.167907ms]
Sep 16 23:40:15.990: INFO: Created: latency-svc-bkzc5
Sep 16 23:40:16.001: INFO: Got endpoints: latency-svc-bkzc5 [405.624116ms]
Sep 16 23:40:16.020: INFO: Created: latency-svc-rj8s8
Sep 16 23:40:16.032: INFO: Got endpoints: latency-svc-rj8s8 [408.180553ms]
Sep 16 23:40:16.054: INFO: Created: latency-svc-b95nv
Sep 16 23:40:16.067: INFO: Got endpoints: latency-svc-b95nv [415.856595ms]
Sep 16 23:40:16.079: INFO: Created: latency-svc-nqntg
Sep 16 23:40:16.093: INFO: Got endpoints: latency-svc-nqntg [420.534696ms]
Sep 16 23:40:16.105: INFO: Created: latency-svc-pbv44
Sep 16 23:40:16.117: INFO: Got endpoints: latency-svc-pbv44 [400.614789ms]
Sep 16 23:40:16.130: INFO: Created: latency-svc-hng2p
Sep 16 23:40:16.138: INFO: Got endpoints: latency-svc-hng2p [399.44497ms]
Sep 16 23:40:16.153: INFO: Created: latency-svc-8pn42
Sep 16 23:40:16.184: INFO: Got endpoints: latency-svc-8pn42 [419.134797ms]
Sep 16 23:40:16.200: INFO: Created: latency-svc-5f5zf
Sep 16 23:40:16.208: INFO: Got endpoints: latency-svc-5f5zf [418.355322ms]
Sep 16 23:40:16.215: INFO: Created: latency-svc-w7gvt
Sep 16 23:40:16.224: INFO: Got endpoints: latency-svc-w7gvt [401.003298ms]
Sep 16 23:40:16.245: INFO: Created: latency-svc-p9pzb
Sep 16 23:40:16.258: INFO: Got endpoints: latency-svc-p9pzb [408.806301ms]
Sep 16 23:40:16.274: INFO: Created: latency-svc-nhp87
Sep 16 23:40:16.300: INFO: Got endpoints: latency-svc-nhp87 [419.641548ms]
Sep 16 23:40:16.304: INFO: Created: latency-svc-lb4fw
Sep 16 23:40:16.314: INFO: Got endpoints: latency-svc-lb4fw [417.631345ms]
Sep 16 23:40:16.326: INFO: Created: latency-svc-vv8f4
Sep 16 23:40:16.340: INFO: Got endpoints: latency-svc-vv8f4 [402.564751ms]
Sep 16 23:40:16.358: INFO: Created: latency-svc-8qtb8
Sep 16 23:40:16.365: INFO: Got endpoints: latency-svc-8qtb8 [409.383776ms]
Sep 16 23:40:16.381: INFO: Created: latency-svc-knnv6
Sep 16 23:40:16.396: INFO: Got endpoints: latency-svc-knnv6 [420.448018ms]
Sep 16 23:40:16.441: INFO: Created: latency-svc-9tjxr
Sep 16 23:40:16.452: INFO: Got endpoints: latency-svc-9tjxr [450.402699ms]
Sep 16 23:40:16.463: INFO: Created: latency-svc-zhl5t
Sep 16 23:40:16.485: INFO: Got endpoints: latency-svc-zhl5t [452.973123ms]
Sep 16 23:40:16.507: INFO: Created: latency-svc-kctf4
Sep 16 23:40:16.546: INFO: Got endpoints: latency-svc-kctf4 [478.294575ms]
Sep 16 23:40:16.564: INFO: Created: latency-svc-z9jvs
Sep 16 23:40:16.579: INFO: Got endpoints: latency-svc-z9jvs [486.698012ms]
Sep 16 23:40:16.602: INFO: Created: latency-svc-nqnhk
Sep 16 23:40:16.627: INFO: Got endpoints: latency-svc-nqnhk [510.703131ms]
Sep 16 23:40:16.644: INFO: Created: latency-svc-zstv6
Sep 16 23:40:16.688: INFO: Got endpoints: latency-svc-zstv6 [550.183301ms]
Sep 16 23:40:16.712: INFO: Created: latency-svc-4cp4j
Sep 16 23:40:16.734: INFO: Got endpoints: latency-svc-4cp4j [549.838966ms]
Sep 16 23:40:16.748: INFO: Created: latency-svc-42k2v
Sep 16 23:40:16.761: INFO: Created: latency-svc-xznmn
Sep 16 23:40:16.797: INFO: Got endpoints: latency-svc-42k2v [588.823327ms]
Sep 16 23:40:16.810: INFO: Created: latency-svc-8ggd8
Sep 16 23:40:16.837: INFO: Created: latency-svc-r2h9t
Sep 16 23:40:16.837: INFO: Got endpoints: latency-svc-xznmn [612.714443ms]
Sep 16 23:40:16.882: INFO: Created: latency-svc-n6xjl
Sep 16 23:40:16.919: INFO: Got endpoints: latency-svc-8ggd8 [660.544473ms]
Sep 16 23:40:16.928: INFO: Got endpoints: latency-svc-r2h9t [628.280279ms]
Sep 16 23:40:16.947: INFO: Created: latency-svc-7snbf
Sep 16 23:40:16.981: INFO: Created: latency-svc-nklvq
Sep 16 23:40:16.981: INFO: Got endpoints: latency-svc-n6xjl [667.31254ms]
Sep 16 23:40:16.996: INFO: Created: latency-svc-72nbc
Sep 16 23:40:17.042: INFO: Created: latency-svc-2l6rm
Sep 16 23:40:17.042: INFO: Got endpoints: latency-svc-7snbf [701.194973ms]
Sep 16 23:40:17.060: INFO: Created: latency-svc-952r6
Sep 16 23:40:17.077: INFO: Created: latency-svc-4gw4w
Sep 16 23:40:17.077: INFO: Got endpoints: latency-svc-nklvq [712.034252ms]
Sep 16 23:40:17.098: INFO: Created: latency-svc-nhnxz
Sep 16 23:40:17.130: INFO: Got endpoints: latency-svc-72nbc [734.181897ms]
Sep 16 23:40:17.130: INFO: Created: latency-svc-zmbc2
Sep 16 23:40:17.156: INFO: Created: latency-svc-xsdlk
Sep 16 23:40:17.179: INFO: Created: latency-svc-9fp8d
Sep 16 23:40:17.185: INFO: Got endpoints: latency-svc-2l6rm [733.104874ms]
Sep 16 23:40:17.203: INFO: Created: latency-svc-vlqp8
Sep 16 23:40:17.223: INFO: Created: latency-svc-77hdh
Sep 16 23:40:17.231: INFO: Got endpoints: latency-svc-952r6 [745.738398ms]
Sep 16 23:40:17.238: INFO: Created: latency-svc-4pkg8
Sep 16 23:40:17.275: INFO: Created: latency-svc-gvfsk
Sep 16 23:40:17.281: INFO: Got endpoints: latency-svc-4gw4w [735.521174ms]
Sep 16 23:40:17.301: INFO: Created: latency-svc-xwfs7
Sep 16 23:40:17.318: INFO: Created: latency-svc-6zcft
Sep 16 23:40:17.330: INFO: Got endpoints: latency-svc-nhnxz [750.536402ms]
Sep 16 23:40:17.342: INFO: Created: latency-svc-krghb
Sep 16 23:40:17.371: INFO: Created: latency-svc-q2gc2
Sep 16 23:40:17.387: INFO: Got endpoints: latency-svc-zmbc2 [760.003378ms]
Sep 16 23:40:17.399: INFO: Created: latency-svc-kk8cz
Sep 16 23:40:17.422: INFO: Created: latency-svc-59qcz
Sep 16 23:40:17.434: INFO: Got endpoints: latency-svc-xsdlk [745.738047ms]
Sep 16 23:40:17.448: INFO: Created: latency-svc-xndn2
Sep 16 23:40:17.461: INFO: Created: latency-svc-bb7b2
Sep 16 23:40:17.487: INFO: Got endpoints: latency-svc-9fp8d [752.881842ms]
Sep 16 23:40:17.487: INFO: Created: latency-svc-hpxg6
Sep 16 23:40:17.533: INFO: Got endpoints: latency-svc-vlqp8 [735.758025ms]
Sep 16 23:40:17.543: INFO: Created: latency-svc-vwk8v
Sep 16 23:40:17.569: INFO: Created: latency-svc-jk7h8
Sep 16 23:40:17.582: INFO: Got endpoints: latency-svc-77hdh [744.743019ms]
Sep 16 23:40:17.598: INFO: Created: latency-svc-k59sr
Sep 16 23:40:17.618: INFO: Created: latency-svc-54dsf
Sep 16 23:40:17.649: INFO: Got endpoints: latency-svc-4pkg8 [729.694788ms]
Sep 16 23:40:17.678: INFO: Created: latency-svc-mh5ml
Sep 16 23:40:17.681: INFO: Got endpoints: latency-svc-gvfsk [752.995718ms]
Sep 16 23:40:17.714: INFO: Created: latency-svc-x87zb
Sep 16 23:40:17.744: INFO: Got endpoints: latency-svc-xwfs7 [762.95399ms]
Sep 16 23:40:17.774: INFO: Created: latency-svc-l7nj4
Sep 16 23:40:17.788: INFO: Got endpoints: latency-svc-6zcft [746.385995ms]
Sep 16 23:40:17.818: INFO: Created: latency-svc-ml59f
Sep 16 23:40:17.829: INFO: Got endpoints: latency-svc-krghb [751.874652ms]
Sep 16 23:40:17.849: INFO: Created: latency-svc-ts7x4
Sep 16 23:40:17.886: INFO: Got endpoints: latency-svc-q2gc2 [756.004958ms]
Sep 16 23:40:17.916: INFO: Created: latency-svc-hzxdq
Sep 16 23:40:17.936: INFO: Got endpoints: latency-svc-kk8cz [750.886001ms]
Sep 16 23:40:17.951: INFO: Created: latency-svc-pnvnw
Sep 16 23:40:18.000: INFO: Got endpoints: latency-svc-59qcz [768.902051ms]
Sep 16 23:40:18.023: INFO: Created: latency-svc-76nss
Sep 16 23:40:18.031: INFO: Got endpoints: latency-svc-xndn2 [749.546685ms]
Sep 16 23:40:18.054: INFO: Created: latency-svc-66s86
Sep 16 23:40:18.079: INFO: Got endpoints: latency-svc-bb7b2 [749.223047ms]
Sep 16 23:40:18.097: INFO: Created: latency-svc-cfmgw
Sep 16 23:40:18.131: INFO: Got endpoints: latency-svc-hpxg6 [743.336302ms]
Sep 16 23:40:18.156: INFO: Created: latency-svc-nj86k
Sep 16 23:40:18.179: INFO: Got endpoints: latency-svc-vwk8v [744.793387ms]
Sep 16 23:40:18.195: INFO: Created: latency-svc-qkgtb
Sep 16 23:40:18.243: INFO: Got endpoints: latency-svc-jk7h8 [756.159109ms]
Sep 16 23:40:18.265: INFO: Created: latency-svc-jdmlg
Sep 16 23:40:18.279: INFO: Got endpoints: latency-svc-k59sr [745.537769ms]
Sep 16 23:40:18.300: INFO: Created: latency-svc-9cc2l
Sep 16 23:40:18.333: INFO: Got endpoints: latency-svc-54dsf [751.429264ms]
Sep 16 23:40:18.376: INFO: Created: latency-svc-lj8ct
Sep 16 23:40:18.380: INFO: Got endpoints: latency-svc-mh5ml [731.658488ms]
Sep 16 23:40:18.397: INFO: Created: latency-svc-dj5s7
Sep 16 23:40:18.439: INFO: Got endpoints: latency-svc-x87zb [757.572502ms]
Sep 16 23:40:18.474: INFO: Created: latency-svc-lpqzs
Sep 16 23:40:18.486: INFO: Got endpoints: latency-svc-l7nj4 [741.642849ms]
Sep 16 23:40:18.501: INFO: Created: latency-svc-fgr97
Sep 16 23:40:18.531: INFO: Got endpoints: latency-svc-ml59f [742.522746ms]
Sep 16 23:40:18.557: INFO: Created: latency-svc-vx9zs
Sep 16 23:40:18.590: INFO: Got endpoints: latency-svc-ts7x4 [760.446481ms]
Sep 16 23:40:18.613: INFO: Created: latency-svc-2tgd2
Sep 16 23:40:18.640: INFO: Got endpoints: latency-svc-hzxdq [753.340059ms]
Sep 16 23:40:18.667: INFO: Created: latency-svc-pcgpx
Sep 16 23:40:18.686: INFO: Got endpoints: latency-svc-pnvnw [750.208857ms]
Sep 16 23:40:18.709: INFO: Created: latency-svc-pnd7p
Sep 16 23:40:18.732: INFO: Got endpoints: latency-svc-76nss [732.180452ms]
Sep 16 23:40:18.766: INFO: Created: latency-svc-w9dn8
Sep 16 23:40:18.781: INFO: Got endpoints: latency-svc-66s86 [750.04655ms]
Sep 16 23:40:18.803: INFO: Created: latency-svc-5wspf
Sep 16 23:40:18.832: INFO: Got endpoints: latency-svc-cfmgw [752.930569ms]
Sep 16 23:40:18.856: INFO: Created: latency-svc-c4nqz
Sep 16 23:40:18.882: INFO: Got endpoints: latency-svc-nj86k [750.7248ms]
Sep 16 23:40:18.911: INFO: Created: latency-svc-6qwvg
Sep 16 23:40:18.928: INFO: Got endpoints: latency-svc-qkgtb [749.692084ms]
Sep 16 23:40:18.946: INFO: Created: latency-svc-tmgpr
Sep 16 23:40:18.982: INFO: Got endpoints: latency-svc-jdmlg [738.522109ms]
Sep 16 23:40:19.010: INFO: Created: latency-svc-6n42g
Sep 16 23:40:19.037: INFO: Got endpoints: latency-svc-9cc2l [757.990642ms]
Sep 16 23:40:19.053: INFO: Created: latency-svc-rm2mv
Sep 16 23:40:19.088: INFO: Got endpoints: latency-svc-lj8ct [754.507287ms]
Sep 16 23:40:19.113: INFO: Created: latency-svc-sjhk4
Sep 16 23:40:19.131: INFO: Got endpoints: latency-svc-dj5s7 [750.308464ms]
Sep 16 23:40:19.177: INFO: Created: latency-svc-x76zc
Sep 16 23:40:19.189: INFO: Got endpoints: latency-svc-lpqzs [750.317222ms]
Sep 16 23:40:19.228: INFO: Created: latency-svc-krsdx
Sep 16 23:40:19.234: INFO: Got endpoints: latency-svc-fgr97 [748.020244ms]
Sep 16 23:40:19.288: INFO: Got endpoints: latency-svc-vx9zs [757.422156ms]
Sep 16 23:40:19.295: INFO: Created: latency-svc-78trn
Sep 16 23:40:19.318: INFO: Created: latency-svc-65pnp
Sep 16 23:40:19.334: INFO: Got endpoints: latency-svc-2tgd2 [744.358661ms]
Sep 16 23:40:19.363: INFO: Created: latency-svc-dvbg8
Sep 16 23:40:19.393: INFO: Got endpoints: latency-svc-pcgpx [752.990475ms]
Sep 16 23:40:19.436: INFO: Created: latency-svc-7chrv
Sep 16 23:40:19.436: INFO: Got endpoints: latency-svc-pnd7p [749.45727ms]
Sep 16 23:40:19.468: INFO: Created: latency-svc-m95m6
Sep 16 23:40:19.485: INFO: Got endpoints: latency-svc-w9dn8 [753.127246ms]
Sep 16 23:40:19.543: INFO: Got endpoints: latency-svc-5wspf [761.534ms]
Sep 16 23:40:19.549: INFO: Created: latency-svc-t77ls
Sep 16 23:40:19.582: INFO: Created: latency-svc-m7jh6
Sep 16 23:40:19.582: INFO: Got endpoints: latency-svc-c4nqz [749.611176ms]
Sep 16 23:40:19.613: INFO: Created: latency-svc-zd2n9
Sep 16 23:40:19.640: INFO: Got endpoints: latency-svc-6qwvg [758.579631ms]
Sep 16 23:40:19.656: INFO: Created: latency-svc-pbrnd
Sep 16 23:40:19.679: INFO: Got endpoints: latency-svc-tmgpr [750.40501ms]
Sep 16 23:40:19.722: INFO: Created: latency-svc-fpzg2
Sep 16 23:40:19.734: INFO: Got endpoints: latency-svc-6n42g [752.548912ms]
Sep 16 23:40:19.763: INFO: Created: latency-svc-hprdz
Sep 16 23:40:19.782: INFO: Got endpoints: latency-svc-rm2mv [745.336026ms]
Sep 16 23:40:19.809: INFO: Created: latency-svc-9fhvq
Sep 16 23:40:19.847: INFO: Got endpoints: latency-svc-sjhk4 [758.760918ms]
Sep 16 23:40:19.882: INFO: Created: latency-svc-fvcw2
Sep 16 23:40:19.894: INFO: Got endpoints: latency-svc-x76zc [763.267641ms]
Sep 16 23:40:19.923: INFO: Created: latency-svc-58zlt
Sep 16 23:40:19.929: INFO: Got endpoints: latency-svc-krsdx [739.556376ms]
Sep 16 23:40:19.950: INFO: Created: latency-svc-pv75g
Sep 16 23:40:19.994: INFO: Got endpoints: latency-svc-78trn [759.74768ms]
Sep 16 23:40:20.018: INFO: Created: latency-svc-pm7j8
Sep 16 23:40:20.030: INFO: Got endpoints: latency-svc-65pnp [742.018634ms]
Sep 16 23:40:20.057: INFO: Created: latency-svc-9pz2m
Sep 16 23:40:20.086: INFO: Got endpoints: latency-svc-dvbg8 [752.280524ms]
Sep 16 23:40:20.113: INFO: Created: latency-svc-ggm7g
Sep 16 23:40:20.141: INFO: Got endpoints: latency-svc-7chrv [747.895484ms]
Sep 16 23:40:20.164: INFO: Created: latency-svc-w9pzz
Sep 16 23:40:20.181: INFO: Got endpoints: latency-svc-m95m6 [745.575683ms]
Sep 16 23:40:20.206: INFO: Created: latency-svc-6c4n2
Sep 16 23:40:20.232: INFO: Got endpoints: latency-svc-t77ls [746.713881ms]
Sep 16 23:40:20.260: INFO: Created: latency-svc-bzxgf
Sep 16 23:40:20.281: INFO: Got endpoints: latency-svc-m7jh6 [738.061493ms]
Sep 16 23:40:20.302: INFO: Created: latency-svc-gwqb8
Sep 16 23:40:20.338: INFO: Got endpoints: latency-svc-zd2n9 [755.628355ms]
Sep 16 23:40:20.370: INFO: Created: latency-svc-xx88p
Sep 16 23:40:20.384: INFO: Got endpoints: latency-svc-pbrnd [743.788698ms]
Sep 16 23:40:20.400: INFO: Created: latency-svc-s8c8x
Sep 16 23:40:20.432: INFO: Got endpoints: latency-svc-fpzg2 [753.1201ms]
Sep 16 23:40:20.479: INFO: Created: latency-svc-sz964
Sep 16 23:40:20.485: INFO: Got endpoints: latency-svc-hprdz [751.069467ms]
Sep 16 23:40:20.502: INFO: Created: latency-svc-xc2d6
Sep 16 23:40:20.532: INFO: Got endpoints: latency-svc-9fhvq [750.263274ms]
Sep 16 23:40:20.577: INFO: Created: latency-svc-xkznf
Sep 16 23:40:20.584: INFO: Got endpoints: latency-svc-fvcw2 [736.925172ms]
Sep 16 23:40:20.598: INFO: Created: latency-svc-vp4gw
Sep 16 23:40:20.640: INFO: Got endpoints: latency-svc-58zlt [745.518935ms]
Sep 16 23:40:20.662: INFO: Created: latency-svc-wrhqc
Sep 16 23:40:20.692: INFO: Got endpoints: latency-svc-pv75g [762.917318ms]
Sep 16 23:40:20.710: INFO: Created: latency-svc-66l94
Sep 16 23:40:20.733: INFO: Got endpoints: latency-svc-pm7j8 [739.164382ms]
Sep 16 23:40:20.756: INFO: Created: latency-svc-n5zdr
Sep 16 23:40:20.788: INFO: Got endpoints: latency-svc-9pz2m [758.339102ms]
Sep 16 23:40:20.819: INFO: Created: latency-svc-xwdkp
Sep 16 23:40:20.831: INFO: Got endpoints: latency-svc-ggm7g [744.592287ms]
Sep 16 23:40:20.862: INFO: Created: latency-svc-56hfh
Sep 16 23:40:20.888: INFO: Got endpoints: latency-svc-w9pzz [747.701479ms]
Sep 16 23:40:20.903: INFO: Created: latency-svc-xk5sm
Sep 16 23:40:20.930: INFO: Got endpoints: latency-svc-6c4n2 [748.724168ms]
Sep 16 23:40:20.958: INFO: Created: latency-svc-dfnxk
Sep 16 23:40:20.981: INFO: Got endpoints: latency-svc-bzxgf [748.619127ms]
Sep 16 23:40:21.001: INFO: Created: latency-svc-68k4j
Sep 16 23:40:21.032: INFO: Got endpoints: latency-svc-gwqb8 [751.771393ms]
Sep 16 23:40:21.049: INFO: Created: latency-svc-5tr8j
Sep 16 23:40:21.082: INFO: Got endpoints: latency-svc-xx88p [744.115823ms]
Sep 16 23:40:21.113: INFO: Created: latency-svc-4f28v
Sep 16 23:40:21.140: INFO: Got endpoints: latency-svc-s8c8x [755.718616ms]
Sep 16 23:40:21.158: INFO: Created: latency-svc-vlclh
Sep 16 23:40:21.179: INFO: Got endpoints: latency-svc-sz964 [746.806645ms]
Sep 16 23:40:21.202: INFO: Created: latency-svc-lk972
Sep 16 23:40:21.237: INFO: Got endpoints: latency-svc-xc2d6 [751.823951ms]
Sep 16 23:40:21.269: INFO: Created: latency-svc-zknsc
Sep 16 23:40:21.280: INFO: Got endpoints: latency-svc-xkznf [747.687759ms]
Sep 16 23:40:21.305: INFO: Created: latency-svc-79qr6
Sep 16 23:40:21.331: INFO: Got endpoints: latency-svc-vp4gw [747.450616ms]
Sep 16 23:40:21.373: INFO: Created: latency-svc-gf8m4
Sep 16 23:40:21.379: INFO: Got endpoints: latency-svc-wrhqc [738.937709ms]
Sep 16 23:40:21.406: INFO: Created: latency-svc-s246n
Sep 16 23:40:21.441: INFO: Got endpoints: latency-svc-66l94 [749.70069ms]
Sep 16 23:40:21.458: INFO: Created: latency-svc-47446
Sep 16 23:40:21.485: INFO: Got endpoints: latency-svc-n5zdr [751.622228ms]
Sep 16 23:40:21.510: INFO: Created: latency-svc-bmks8
Sep 16 23:40:21.531: INFO: Got endpoints: latency-svc-xwdkp [742.454224ms]
Sep 16 23:40:21.555: INFO: Created: latency-svc-47bgt
Sep 16 23:40:21.583: INFO: Got endpoints: latency-svc-56hfh [752.188791ms]
Sep 16 23:40:21.616: INFO: Created: latency-svc-4cffp
Sep 16 23:40:21.631: INFO: Got endpoints: latency-svc-xk5sm [742.961127ms]
Sep 16 23:40:21.661: INFO: Created: latency-svc-ppld5
Sep 16 23:40:21.681: INFO: Got endpoints: latency-svc-dfnxk [750.932497ms]
Sep 16 23:40:21.710: INFO: Created: latency-svc-scglz
Sep 16 23:40:21.735: INFO: Got endpoints: latency-svc-68k4j [754.27905ms]
Sep 16 23:40:21.751: INFO: Created: latency-svc-b5cqx
Sep 16 23:40:21.784: INFO: Got endpoints: latency-svc-5tr8j [751.451159ms]
Sep 16 23:40:21.800: INFO: Created: latency-svc-n56pv
Sep 16 23:40:21.834: INFO: Got endpoints: latency-svc-4f28v [751.779378ms]
Sep 16 23:40:21.855: INFO: Created: latency-svc-67njs
Sep 16 23:40:21.879: INFO: Got endpoints: latency-svc-vlclh [738.917092ms]
Sep 16 23:40:21.899: INFO: Created: latency-svc-ljv2s
Sep 16 23:40:21.943: INFO: Got endpoints: latency-svc-lk972 [764.067481ms]
Sep 16 23:40:21.971: INFO: Created: latency-svc-zrj98
Sep 16 23:40:21.985: INFO: Got endpoints: latency-svc-zknsc [748.148749ms]
Sep 16 23:40:22.003: INFO: Created: latency-svc-bxfrw
Sep 16 23:40:22.029: INFO: Got endpoints: latency-svc-79qr6 [748.549223ms]
Sep 16 23:40:22.075: INFO: Created: latency-svc-xhvxj
Sep 16 23:40:22.085: INFO: Got endpoints: latency-svc-gf8m4 [753.703127ms]
Sep 16 23:40:22.100: INFO: Created: latency-svc-lf8fr
Sep 16 23:40:22.132: INFO: Got endpoints: latency-svc-s246n [753.339917ms]
Sep 16 23:40:22.153: INFO: Created: latency-svc-bt8t7
Sep 16 23:40:22.192: INFO: Got endpoints: latency-svc-47446 [750.260776ms]
Sep 16 23:40:22.212: INFO: Created: latency-svc-hsbql
Sep 16 23:40:22.234: INFO: Got endpoints: latency-svc-bmks8 [748.950539ms]
Sep 16 23:40:22.251: INFO: Created: latency-svc-jrkc4
Sep 16 23:40:22.302: INFO: Got endpoints: latency-svc-47bgt [771.262369ms]
Sep 16 23:40:22.329: INFO: Got endpoints: latency-svc-4cffp [746.127664ms]
Sep 16 23:40:22.329: INFO: Created: latency-svc-t2c6j
Sep 16 23:40:22.347: INFO: Created: latency-svc-xpjh7
Sep 16 23:40:22.386: INFO: Got endpoints: latency-svc-ppld5 [754.726183ms]
Sep 16 23:40:22.424: INFO: Created: latency-svc-qbbg7
Sep 16 23:40:22.431: INFO: Got endpoints: latency-svc-scglz [750.510025ms]
Sep 16 23:40:22.459: INFO: Created: latency-svc-tmkjk
Sep 16 23:40:22.479: INFO: Got endpoints: latency-svc-b5cqx [744.24536ms]
Sep 16 23:40:22.556: INFO: Got endpoints: latency-svc-n56pv [772.376219ms]
Sep 16 23:40:22.588: INFO: Got endpoints: latency-svc-67njs [754.091359ms]
Sep 16 23:40:22.639: INFO: Got endpoints: latency-svc-ljv2s [760.329818ms]
Sep 16 23:40:22.682: INFO: Got endpoints: latency-svc-zrj98 [739.344801ms]
Sep 16 23:40:22.732: INFO: Got endpoints: latency-svc-bxfrw [746.341129ms]
Sep 16 23:40:22.785: INFO: Got endpoints: latency-svc-xhvxj [756.164973ms]
Sep 16 23:40:22.841: INFO: Got endpoints: latency-svc-lf8fr [756.356768ms]
Sep 16 23:40:22.879: INFO: Got endpoints: latency-svc-bt8t7 [747.00182ms]
Sep 16 23:40:22.938: INFO: Got endpoints: latency-svc-hsbql [746.658694ms]
Sep 16 23:40:22.981: INFO: Got endpoints: latency-svc-jrkc4 [747.203625ms]
Sep 16 23:40:23.035: INFO: Got endpoints: latency-svc-t2c6j [733.054ms]
Sep 16 23:40:23.082: INFO: Got endpoints: latency-svc-xpjh7 [752.422327ms]
Sep 16 23:40:23.132: INFO: Got endpoints: latency-svc-qbbg7 [745.456188ms]
Sep 16 23:40:23.182: INFO: Got endpoints: latency-svc-tmkjk [750.326236ms]
Sep 16 23:40:23.182: INFO: Latencies: [27.638378ms 48.80466ms 79.12914ms 106.214202ms 134.750866ms 163.935662ms 183.917989ms 222.183681ms 240.895882ms 260.988543ms 282.357517ms 300.98005ms 355.750975ms 384.601021ms 392.167907ms 396.354753ms 399.44497ms 400.614789ms 400.688137ms 401.003298ms 401.442408ms 402.564751ms 402.590685ms 403.47272ms 404.196638ms 404.311929ms 405.016774ms 405.624116ms 407.495587ms 407.588622ms 408.180553ms 408.20072ms 408.806301ms 409.383776ms 409.958075ms 410.703711ms 411.29294ms 415.47589ms 415.856595ms 416.0737ms 416.41054ms 417.180945ms 417.631345ms 418.355322ms 418.532525ms 419.134797ms 419.481262ms 419.538081ms 419.641548ms 420.448018ms 420.534696ms 421.551939ms 424.426662ms 424.56363ms 427.799755ms 428.011926ms 428.127536ms 428.316964ms 431.309385ms 433.820739ms 434.009547ms 434.198392ms 438.676051ms 440.067811ms 450.402699ms 452.973123ms 478.294575ms 486.698012ms 510.703131ms 549.838966ms 550.183301ms 588.823327ms 612.714443ms 628.280279ms 660.544473ms 667.31254ms 701.194973ms 712.034252ms 729.694788ms 731.658488ms 732.180452ms 733.054ms 733.104874ms 734.181897ms 735.521174ms 735.758025ms 736.925172ms 738.061493ms 738.522109ms 738.917092ms 738.937709ms 739.164382ms 739.344801ms 739.556376ms 741.642849ms 742.018634ms 742.454224ms 742.522746ms 742.961127ms 743.336302ms 743.788698ms 744.115823ms 744.24536ms 744.358661ms 744.592287ms 744.743019ms 744.793387ms 745.336026ms 745.456188ms 745.518935ms 745.537769ms 745.575683ms 745.738047ms 745.738398ms 746.127664ms 746.341129ms 746.385995ms 746.658694ms 746.713881ms 746.806645ms 747.00182ms 747.203625ms 747.450616ms 747.687759ms 747.701479ms 747.895484ms 748.020244ms 748.148749ms 748.549223ms 748.619127ms 748.724168ms 748.950539ms 749.223047ms 749.45727ms 749.546685ms 749.611176ms 749.692084ms 749.70069ms 750.04655ms 750.208857ms 750.260776ms 750.263274ms 750.308464ms 750.317222ms 750.326236ms 750.40501ms 750.510025ms 750.536402ms 750.7248ms 750.886001ms 750.932497ms 751.069467ms 751.429264ms 751.451159ms 751.622228ms 751.771393ms 751.779378ms 751.823951ms 751.874652ms 752.188791ms 752.280524ms 752.422327ms 752.548912ms 752.881842ms 752.930569ms 752.990475ms 752.995718ms 753.1201ms 753.127246ms 753.339917ms 753.340059ms 753.703127ms 754.091359ms 754.27905ms 754.507287ms 754.726183ms 755.628355ms 755.718616ms 756.004958ms 756.159109ms 756.164973ms 756.356768ms 757.422156ms 757.572502ms 757.990642ms 758.339102ms 758.579631ms 758.760918ms 759.74768ms 760.003378ms 760.329818ms 760.446481ms 761.534ms 762.917318ms 762.95399ms 763.267641ms 764.067481ms 768.902051ms 771.262369ms 772.376219ms]
Sep 16 23:40:23.182: INFO: 50 %ile: 743.788698ms
Sep 16 23:40:23.182: INFO: 90 %ile: 756.164973ms
Sep 16 23:40:23.182: INFO: 99 %ile: 771.262369ms
Sep 16 23:40:23.182: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:40:23.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-2033" for this suite.

• [SLOW TEST:11.833 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":277,"completed":119,"skipped":2106,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:40:23.204: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep 16 23:40:23.267: INFO: Waiting up to 5m0s for pod "pod-4a5ea31c-e69d-4047-8b03-939a26e5d857" in namespace "emptydir-5765" to be "Succeeded or Failed"
Sep 16 23:40:23.269: INFO: Pod "pod-4a5ea31c-e69d-4047-8b03-939a26e5d857": Phase="Pending", Reason="", readiness=false. Elapsed: 1.984656ms
Sep 16 23:40:25.272: INFO: Pod "pod-4a5ea31c-e69d-4047-8b03-939a26e5d857": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005034504s
Sep 16 23:40:27.275: INFO: Pod "pod-4a5ea31c-e69d-4047-8b03-939a26e5d857": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008648372s
STEP: Saw pod success
Sep 16 23:40:27.275: INFO: Pod "pod-4a5ea31c-e69d-4047-8b03-939a26e5d857" satisfied condition "Succeeded or Failed"
Sep 16 23:40:27.278: INFO: Trying to get logs from node eqx03-flash06 pod pod-4a5ea31c-e69d-4047-8b03-939a26e5d857 container test-container: <nil>
STEP: delete the pod
Sep 16 23:40:27.323: INFO: Waiting for pod pod-4a5ea31c-e69d-4047-8b03-939a26e5d857 to disappear
Sep 16 23:40:27.325: INFO: Pod pod-4a5ea31c-e69d-4047-8b03-939a26e5d857 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:40:27.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5765" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":120,"skipped":2108,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:40:27.342: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 16 23:40:27.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 version'
Sep 16 23:40:27.459: INFO: stderr: ""
Sep 16 23:40:27.459: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"18\", GitVersion:\"v1.18.6\", GitCommit:\"dff82dc0de47299ab66c83c626e08b245ab19037\", GitTreeState:\"clean\", BuildDate:\"2020-07-15T16:58:53Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"18\", GitVersion:\"v1.18.6\", GitCommit:\"dff82dc0de47299ab66c83c626e08b245ab19037\", GitTreeState:\"clean\", BuildDate:\"2020-07-15T16:51:04Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:40:27.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5655" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":277,"completed":121,"skipped":2112,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:40:27.470: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 16 23:40:28.173: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 16 23:40:30.288: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735896428, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735896428, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735896428, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735896428, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 16 23:40:33.511: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Sep 16 23:40:34.511: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Sep 16 23:40:35.511: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Sep 16 23:40:36.511: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Sep 16 23:40:37.511: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Sep 16 23:40:38.511: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Sep 16 23:40:39.511: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Sep 16 23:40:40.511: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:40:40.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4250" for this suite.
STEP: Destroying namespace "webhook-4250-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.462 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":277,"completed":122,"skipped":2122,"failed":0}
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:40:40.933: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Sep 16 23:40:41.008: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9410 /api/v1/namespaces/watch-9410/configmaps/e2e-watch-test-configmap-a 1481c362-be35-4017-bb4d-737e2330d511 4236183 0 2020-09-16 23:40:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-09-16 23:40:40 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 16 23:40:41.008: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9410 /api/v1/namespaces/watch-9410/configmaps/e2e-watch-test-configmap-a 1481c362-be35-4017-bb4d-737e2330d511 4236183 0 2020-09-16 23:40:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-09-16 23:40:40 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Sep 16 23:40:51.020: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9410 /api/v1/namespaces/watch-9410/configmaps/e2e-watch-test-configmap-a 1481c362-be35-4017-bb4d-737e2330d511 4236245 0 2020-09-16 23:40:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-09-16 23:40:51 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 16 23:40:51.020: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9410 /api/v1/namespaces/watch-9410/configmaps/e2e-watch-test-configmap-a 1481c362-be35-4017-bb4d-737e2330d511 4236245 0 2020-09-16 23:40:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-09-16 23:40:51 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Sep 16 23:41:01.032: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9410 /api/v1/namespaces/watch-9410/configmaps/e2e-watch-test-configmap-a 1481c362-be35-4017-bb4d-737e2330d511 4236287 0 2020-09-16 23:40:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-09-16 23:41:01 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 16 23:41:01.032: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9410 /api/v1/namespaces/watch-9410/configmaps/e2e-watch-test-configmap-a 1481c362-be35-4017-bb4d-737e2330d511 4236287 0 2020-09-16 23:40:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-09-16 23:41:01 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Sep 16 23:41:11.041: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9410 /api/v1/namespaces/watch-9410/configmaps/e2e-watch-test-configmap-a 1481c362-be35-4017-bb4d-737e2330d511 4236328 0 2020-09-16 23:40:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-09-16 23:41:01 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 16 23:41:11.041: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9410 /api/v1/namespaces/watch-9410/configmaps/e2e-watch-test-configmap-a 1481c362-be35-4017-bb4d-737e2330d511 4236328 0 2020-09-16 23:40:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-09-16 23:41:01 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Sep 16 23:41:21.052: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9410 /api/v1/namespaces/watch-9410/configmaps/e2e-watch-test-configmap-b e86d77d3-5589-45dd-bfd0-dd9bc3db432d 4236372 0 2020-09-16 23:41:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-09-16 23:41:21 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 16 23:41:21.052: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9410 /api/v1/namespaces/watch-9410/configmaps/e2e-watch-test-configmap-b e86d77d3-5589-45dd-bfd0-dd9bc3db432d 4236372 0 2020-09-16 23:41:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-09-16 23:41:21 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Sep 16 23:41:31.064: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9410 /api/v1/namespaces/watch-9410/configmaps/e2e-watch-test-configmap-b e86d77d3-5589-45dd-bfd0-dd9bc3db432d 4236417 0 2020-09-16 23:41:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-09-16 23:41:21 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 16 23:41:31.065: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9410 /api/v1/namespaces/watch-9410/configmaps/e2e-watch-test-configmap-b e86d77d3-5589-45dd-bfd0-dd9bc3db432d 4236417 0 2020-09-16 23:41:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-09-16 23:41:21 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:41:41.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9410" for this suite.

• [SLOW TEST:60.157 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":277,"completed":123,"skipped":2122,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:41:41.090: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-a7b96b22-8d47-4263-860c-fbbe71f7b7f8
STEP: Creating a pod to test consume secrets
Sep 16 23:41:41.166: INFO: Waiting up to 5m0s for pod "pod-secrets-91d303b4-f7a9-4715-83cd-6a0b9c5a8f13" in namespace "secrets-4116" to be "Succeeded or Failed"
Sep 16 23:41:41.168: INFO: Pod "pod-secrets-91d303b4-f7a9-4715-83cd-6a0b9c5a8f13": Phase="Pending", Reason="", readiness=false. Elapsed: 1.805803ms
Sep 16 23:41:43.171: INFO: Pod "pod-secrets-91d303b4-f7a9-4715-83cd-6a0b9c5a8f13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004823982s
Sep 16 23:41:45.174: INFO: Pod "pod-secrets-91d303b4-f7a9-4715-83cd-6a0b9c5a8f13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007812685s
STEP: Saw pod success
Sep 16 23:41:45.174: INFO: Pod "pod-secrets-91d303b4-f7a9-4715-83cd-6a0b9c5a8f13" satisfied condition "Succeeded or Failed"
Sep 16 23:41:45.176: INFO: Trying to get logs from node eqx03-flash06 pod pod-secrets-91d303b4-f7a9-4715-83cd-6a0b9c5a8f13 container secret-volume-test: <nil>
STEP: delete the pod
Sep 16 23:41:45.214: INFO: Waiting for pod pod-secrets-91d303b4-f7a9-4715-83cd-6a0b9c5a8f13 to disappear
Sep 16 23:41:45.216: INFO: Pod pod-secrets-91d303b4-f7a9-4715-83cd-6a0b9c5a8f13 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:41:45.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4116" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":124,"skipped":2129,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:41:45.227: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Sep 16 23:41:45.272: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 16 23:41:45.282: INFO: Waiting for terminating namespaces to be deleted...
Sep 16 23:41:45.285: INFO: 
Logging pods the kubelet thinks is on node eqx03-flash06 before test
Sep 16 23:41:45.297: INFO: etcd-eqx03-flash06 from kube-system started at 2020-09-04 20:35:41 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.297: INFO: 	Container etcd ready: true, restart count 0
Sep 16 23:41:45.297: INFO: calico-node-zqtk2 from kube-system started at 2020-09-09 19:40:56 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.297: INFO: 	Container calico-node ready: true, restart count 0
Sep 16 23:41:45.297: INFO: kube-apiserver-eqx03-flash06 from kube-system started at 2020-09-04 20:36:01 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.298: INFO: 	Container kube-apiserver ready: true, restart count 0
Sep 16 23:41:45.298: INFO: csi-nodeplugin-robin-rnhf4 from robinio started at 2020-09-16 23:34:31 +0000 UTC (2 container statuses recorded)
Sep 16 23:41:45.298: INFO: 	Container driver-registrar ready: true, restart count 0
Sep 16 23:41:45.298: INFO: 	Container robin ready: true, restart count 0
Sep 16 23:41:45.298: INFO: kube-controller-manager-eqx03-flash06 from kube-system started at 2020-09-04 20:36:01 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.298: INFO: 	Container kube-controller-manager ready: true, restart count 1
Sep 16 23:41:45.298: INFO: kube-sriov-device-plugin-amd64-29hgs from kube-system started at 2020-09-16 23:34:31 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.298: INFO: 	Container kube-sriovdp ready: true, restart count 0
Sep 16 23:41:45.298: INFO: kube-multus-ds-amd64-wjsv5 from kube-system started at 2020-09-16 23:34:31 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.298: INFO: 	Container kube-multus ready: true, restart count 0
Sep 16 23:41:45.298: INFO: robin-master-hr288 from robinio started at 2020-09-16 23:34:41 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.298: INFO: 	Container robinrcm ready: true, restart count 0
Sep 16 23:41:45.298: INFO: kube-scheduler-eqx03-flash06 from kube-system started at 2020-09-04 20:35:41 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.298: INFO: 	Container kube-scheduler ready: true, restart count 0
Sep 16 23:41:45.298: INFO: kube-proxy-mchgt from kube-system started at 2020-09-04 20:35:41 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.298: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 16 23:41:45.298: INFO: csi-snapshotter-robin-0 from robinio started at 2020-09-16 23:34:41 +0000 UTC (2 container statuses recorded)
Sep 16 23:41:45.298: INFO: 	Container csi-snapshotter ready: true, restart count 0
Sep 16 23:41:45.298: INFO: 	Container robin ready: true, restart count 0
Sep 16 23:41:45.298: INFO: sonobuoy-e2e-job-362cab5d438b4e90 from sonobuoy started at 2020-09-16 23:06:47 +0000 UTC (2 container statuses recorded)
Sep 16 23:41:45.298: INFO: 	Container e2e ready: true, restart count 0
Sep 16 23:41:45.298: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 16 23:41:45.298: INFO: sonobuoy-systemd-logs-daemon-set-71797a053a4d44ec-pp8g4 from sonobuoy started at 2020-09-16 23:06:47 +0000 UTC (2 container statuses recorded)
Sep 16 23:41:45.298: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 16 23:41:45.298: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 16 23:41:45.298: INFO: 
Logging pods the kubelet thinks is on node eqx03-flash07 before test
Sep 16 23:41:45.340: INFO: csi-resizer-robin-7d566f9df9-sq477 from robinio started at 2020-09-09 19:56:05 +0000 UTC (2 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container csi-resizer ready: true, restart count 0
Sep 16 23:41:45.340: INFO: 	Container robin ready: true, restart count 0
Sep 16 23:41:45.340: INFO: sonobuoy-systemd-logs-daemon-set-71797a053a4d44ec-4zr6m from sonobuoy started at 2020-09-16 23:06:47 +0000 UTC (2 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 16 23:41:45.340: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 16 23:41:45.340: INFO: neo-neo4j-replica-0 from ripul started at 2020-09-16 23:34:01 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container neo4j ready: true, restart count 0
Sep 16 23:41:45.340: INFO: elkname-logstash-0 from t001-u000003 started at 2020-09-05 06:08:19 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container logstash ready: true, restart count 0
Sep 16 23:41:45.340: INFO: hari-kibana-b6f7d64-j96s2 from spk started at 2020-09-05 06:37:23 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container kibana ready: true, restart count 0
Sep 16 23:41:45.340: INFO: coredns-6878cb8f64-7rgtp from kube-system started at 2020-09-04 20:25:00 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container coredns ready: true, restart count 0
Sep 16 23:41:45.340: INFO: csi-provisioner-robin-6646bdd46b-kl2ft from robinio started at 2020-09-16 23:31:18 +0000 UTC (2 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container csi-provisioner ready: true, restart count 0
Sep 16 23:41:45.340: INFO: 	Container robin ready: true, restart count 0
Sep 16 23:41:45.340: INFO: robin-master-w2m7f from robinio started at 2020-09-09 19:47:17 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container robinrcm ready: true, restart count 0
Sep 16 23:41:45.340: INFO: cert-manager-webhook-556b9d7dfd-swlll from cert-manager started at 2020-09-04 20:38:07 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container cert-manager ready: true, restart count 0
Sep 16 23:41:45.340: INFO: elkname-elasticsearch-client-6768458985-6sgxz from t001-u000003 started at 2020-09-05 06:08:19 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:41:45.340: INFO: elkname-elasticsearch-master-0 from t001-u000003 started at 2020-09-05 06:08:19 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:41:45.340: INFO: kube-multus-ds-amd64-lf8vr from kube-system started at 2020-09-04 20:24:11 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container kube-multus ready: true, restart count 0
Sep 16 23:41:45.340: INFO: kube-apiserver-eqx03-flash07 from kube-system started at 2020-09-04 20:24:11 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container kube-apiserver ready: true, restart count 0
Sep 16 23:41:45.340: INFO: csi-attacher-robin-5d956884cf-nqxfg from robinio started at 2020-09-16 23:31:18 +0000 UTC (2 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container csi-attacher ready: true, restart count 0
Sep 16 23:41:45.340: INFO: 	Container robin ready: true, restart count 0
Sep 16 23:41:45.340: INFO: neo-neo4j-replica-0 from madhura started at 2020-09-16 23:33:58 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container neo4j ready: true, restart count 0
Sep 16 23:41:45.340: INFO: hari-elasticsearch-client-c59586c8-k84fl from spk started at 2020-09-05 06:37:23 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:41:45.340: INFO: neo-neo4j-core-0 from ripul started at 2020-09-16 23:34:03 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 16 23:41:45.340: INFO: snapshot-controller-0 from robinio started at 2020-09-09 19:56:15 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container snapshot-controller ready: true, restart count 0
Sep 16 23:41:45.340: INFO: kube-controller-manager-eqx03-flash07 from kube-system started at 2020-09-04 20:24:11 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container kube-controller-manager ready: true, restart count 1
Sep 16 23:41:45.340: INFO: test1-logstash-0 from t001-u000003 started at 2020-09-05 06:42:09 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container logstash ready: true, restart count 0
Sep 16 23:41:45.340: INFO: csi-nodeplugin-robin-bcq5b from robinio started at 2020-09-09 19:56:04 +0000 UTC (2 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container driver-registrar ready: true, restart count 0
Sep 16 23:41:45.340: INFO: 	Container robin ready: true, restart count 0
Sep 16 23:41:45.340: INFO: neo-neo4j-core-1 from default started at 2020-09-16 23:34:17 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 16 23:41:45.340: INFO: hari-logstash-0 from spk started at 2020-09-05 06:37:24 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container logstash ready: true, restart count 0
Sep 16 23:41:45.340: INFO: test1-kibana-84f6c46bf6-mndsz from t001-u000003 started at 2020-09-05 06:42:07 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container kibana ready: true, restart count 0
Sep 16 23:41:45.340: INFO: neo-neo4j-core-2 from default started at 2020-09-16 23:34:00 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 16 23:41:45.340: INFO: elkname-elasticsearch-data-0 from t001-u000003 started at 2020-09-05 06:08:19 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:41:45.340: INFO: neo-neo4j-core-2 from ripul started at 2020-09-16 23:33:57 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 16 23:41:45.340: INFO: mg-runner-gitlab-runner-97dd45f4-8z4zg from gitlab-runner started at 2020-09-10 19:25:18 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container mg-runner-gitlab-runner ready: false, restart count 1147
Sep 16 23:41:45.340: INFO: neo-neo4j-core-1 from madhura started at 2020-09-16 23:33:58 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 16 23:41:45.340: INFO: neo-neo4j-core-0 from default started at 2020-09-16 23:34:12 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 16 23:41:45.340: INFO: kube-scheduler-eqx03-flash07 from kube-system started at 2020-09-04 20:24:11 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container kube-scheduler ready: true, restart count 0
Sep 16 23:41:45.340: INFO: cert-manager-cainjector-5ffff9dd7c-7fv5l from cert-manager started at 2020-09-04 20:38:07 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container cert-manager ready: true, restart count 0
Sep 16 23:41:45.340: INFO: calico-node-8t2kf from kube-system started at 2020-09-09 19:41:23 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container calico-node ready: true, restart count 0
Sep 16 23:41:45.340: INFO: hari-elasticsearch-data-0 from spk started at 2020-09-05 06:37:25 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:41:45.340: INFO: test1-elasticsearch-client-df46b4cd8-2knzt from t001-u000003 started at 2020-09-05 06:42:07 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:41:45.340: INFO: kube-sriov-device-plugin-amd64-rxs7g from kube-system started at 2020-09-04 20:25:01 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container kube-sriovdp ready: true, restart count 0
Sep 16 23:41:45.340: INFO: cert-manager-578cd6d964-sq59x from cert-manager started at 2020-09-04 20:38:07 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container cert-manager ready: true, restart count 0
Sep 16 23:41:45.340: INFO: elkname-kibana-56c986874d-nx4t6 from t001-u000003 started at 2020-09-05 06:08:19 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container kibana ready: true, restart count 0
Sep 16 23:41:45.340: INFO: test1-elasticsearch-data-0 from t001-u000003 started at 2020-09-05 06:42:08 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:41:45.340: INFO: sonobuoy from sonobuoy started at 2020-09-16 23:06:40 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 16 23:41:45.340: INFO: neo-neo4j-core-1 from ripul started at 2020-09-16 23:34:02 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 16 23:41:45.340: INFO: kube-proxy-9zgnq from kube-system started at 2020-09-04 20:24:11 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 16 23:41:45.340: INFO: etcd-eqx03-flash07 from kube-system started at 2020-09-04 20:24:37 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.340: INFO: 	Container etcd ready: true, restart count 0
Sep 16 23:41:45.340: INFO: 
Logging pods the kubelet thinks is on node eqx04-flash04 before test
Sep 16 23:41:45.366: INFO: robin-master-hkj7r from robinio started at 2020-09-09 19:48:36 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.366: INFO: 	Container robinrcm ready: true, restart count 0
Sep 16 23:41:45.366: INFO: kube-controller-manager-eqx04-flash04 from kube-system started at 2020-09-04 20:12:22 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.366: INFO: 	Container kube-controller-manager ready: true, restart count 1
Sep 16 23:41:45.366: INFO: kube-multus-ds-amd64-mzx44 from kube-system started at 2020-09-04 20:12:27 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.366: INFO: 	Container kube-multus ready: true, restart count 0
Sep 16 23:41:45.366: INFO: sonobuoy-systemd-logs-daemon-set-71797a053a4d44ec-dx2j7 from sonobuoy started at 2020-09-16 23:06:47 +0000 UTC (2 container statuses recorded)
Sep 16 23:41:45.366: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 16 23:41:45.366: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 16 23:41:45.366: INFO: kube-apiserver-eqx04-flash04 from kube-system started at 2020-09-04 20:12:22 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.366: INFO: 	Container kube-apiserver ready: true, restart count 0
Sep 16 23:41:45.366: INFO: hari-elasticsearch-master-0 from spk started at 2020-09-05 06:37:24 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.366: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:41:45.366: INFO: csi-nodeplugin-robin-6xzkh from robinio started at 2020-09-09 19:56:04 +0000 UTC (2 container statuses recorded)
Sep 16 23:41:45.366: INFO: 	Container driver-registrar ready: true, restart count 0
Sep 16 23:41:45.366: INFO: 	Container robin ready: true, restart count 0
Sep 16 23:41:45.366: INFO: neo-neo4j-replica-0 from default started at 2020-09-16 23:34:03 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.366: INFO: 	Container neo4j ready: true, restart count 0
Sep 16 23:41:45.366: INFO: kube-proxy-2mmk7 from kube-system started at 2020-09-04 20:12:22 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.366: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 16 23:41:45.366: INFO: etcd-eqx04-flash04 from kube-system started at 2020-09-04 20:12:22 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.366: INFO: 	Container etcd ready: true, restart count 0
Sep 16 23:41:45.366: INFO: coredns-6878cb8f64-n75hx from kube-system started at 2020-09-04 20:12:55 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.366: INFO: 	Container coredns ready: true, restart count 0
Sep 16 23:41:45.366: INFO: neo-neo4j-core-0 from madhura started at 2020-09-16 23:34:02 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.366: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 16 23:41:45.366: INFO: elkname-elasticsearch-master-2 from t001-u000003 started at 2020-09-05 06:09:50 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.366: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:41:45.366: INFO: test1-elasticsearch-data-1 from t001-u000003 started at 2020-09-05 06:44:40 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.366: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:41:45.366: INFO: calico-node-wpq99 from kube-system started at 2020-09-09 19:41:33 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.366: INFO: 	Container calico-node ready: true, restart count 0
Sep 16 23:41:45.366: INFO: kube-scheduler-eqx04-flash04 from kube-system started at 2020-09-04 20:12:22 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.366: INFO: 	Container kube-scheduler ready: true, restart count 2
Sep 16 23:41:45.366: INFO: calico-kube-controllers-6c49f88586-jzdjm from kube-system started at 2020-09-04 20:12:28 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.366: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep 16 23:41:45.366: INFO: kube-sriov-device-plugin-amd64-xlvxl from kube-system started at 2020-09-04 20:12:30 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.366: INFO: 	Container kube-sriovdp ready: true, restart count 0
Sep 16 23:41:45.366: INFO: neo-neo4j-core-2 from madhura started at 2020-09-16 23:34:01 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.366: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 16 23:41:45.366: INFO: 
Logging pods the kubelet thinks is on node eqx04-flash06 before test
Sep 16 23:41:45.391: INFO: elkname-elasticsearch-master-1 from t001-u000003 started at 2020-09-05 06:09:25 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.391: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:41:45.391: INFO: test1-elasticsearch-master-1 from t001-u000003 started at 2020-09-05 06:43:08 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.391: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:41:45.391: INFO: test1-elasticsearch-master-2 from t001-u000003 started at 2020-09-05 06:43:44 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.391: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:41:45.391: INFO: calico-node-x8tqn from kube-system started at 2020-09-09 19:41:10 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.391: INFO: 	Container calico-node ready: true, restart count 0
Sep 16 23:41:45.391: INFO: kube-multus-ds-amd64-j9jmp from kube-system started at 2020-09-04 20:34:34 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.391: INFO: 	Container kube-multus ready: true, restart count 0
Sep 16 23:41:45.391: INFO: elkname-elasticsearch-data-1 from t001-u000003 started at 2020-09-05 06:10:51 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.391: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:41:45.391: INFO: test1-elasticsearch-master-0 from t001-u000003 started at 2020-09-05 06:42:08 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.391: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:41:45.391: INFO: sonobuoy-systemd-logs-daemon-set-71797a053a4d44ec-w59x2 from sonobuoy started at 2020-09-16 23:06:47 +0000 UTC (2 container statuses recorded)
Sep 16 23:41:45.391: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 16 23:41:45.391: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 16 23:41:45.391: INFO: gitlab-runner-gitlab-runner-69dbb57dd4-sptqz from gitlab-runner started at 2020-09-10 19:12:45 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.391: INFO: 	Container gitlab-runner-gitlab-runner ready: false, restart count 0
Sep 16 23:41:45.391: INFO: hari-elasticsearch-client-c59586c8-xvm2k from spk started at 2020-09-05 06:37:23 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.391: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:41:45.391: INFO: hari-elasticsearch-master-1 from spk started at 2020-09-05 06:38:21 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.391: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:41:45.391: INFO: elkname-elasticsearch-client-6768458985-j2fnt from t001-u000003 started at 2020-09-05 06:08:19 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.391: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:41:45.391: INFO: hari-elasticsearch-master-2 from spk started at 2020-09-05 06:38:48 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.391: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:41:45.391: INFO: test1-elasticsearch-client-df46b4cd8-52bhb from t001-u000003 started at 2020-09-05 06:42:07 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.391: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:41:45.391: INFO: csi-nodeplugin-robin-wk7pp from robinio started at 2020-09-09 19:56:04 +0000 UTC (2 container statuses recorded)
Sep 16 23:41:45.391: INFO: 	Container driver-registrar ready: true, restart count 0
Sep 16 23:41:45.391: INFO: 	Container robin ready: true, restart count 0
Sep 16 23:41:45.391: INFO: kube-proxy-n5ckt from kube-system started at 2020-09-04 20:34:34 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.391: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 16 23:41:45.391: INFO: robin-worker-6zwcm from robinio started at 2020-09-09 19:53:16 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.391: INFO: 	Container robinrcm ready: true, restart count 0
Sep 16 23:41:45.391: INFO: kube-sriov-device-plugin-amd64-wwdl2 from kube-system started at 2020-09-04 20:34:39 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.391: INFO: 	Container kube-sriovdp ready: true, restart count 0
Sep 16 23:41:45.391: INFO: hari-elasticsearch-data-1 from spk started at 2020-09-05 06:40:02 +0000 UTC (1 container statuses recorded)
Sep 16 23:41:45.391: INFO: 	Container elasticsearch ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-b8a289c9-1c97-4f6e-8c5b-6836683f3276 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-b8a289c9-1c97-4f6e-8c5b-6836683f3276 off the node eqx03-flash06
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b8a289c9-1c97-4f6e-8c5b-6836683f3276
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:46:51.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-444" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

• [SLOW TEST:306.318 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":277,"completed":125,"skipped":2156,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:46:51.545: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap that has name configmap-test-emptyKey-e57041e1-a023-4773-81c5-ec2d896c493a
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:46:51.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2925" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":277,"completed":126,"skipped":2189,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:46:51.625: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-map-94f9f29c-d95d-4d94-9d6d-65b12b15ac52
STEP: Creating a pod to test consume secrets
Sep 16 23:46:51.695: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d24b9c69-803d-475d-a56f-73a98a02c796" in namespace "projected-7527" to be "Succeeded or Failed"
Sep 16 23:46:51.697: INFO: Pod "pod-projected-secrets-d24b9c69-803d-475d-a56f-73a98a02c796": Phase="Pending", Reason="", readiness=false. Elapsed: 2.139675ms
Sep 16 23:46:53.700: INFO: Pod "pod-projected-secrets-d24b9c69-803d-475d-a56f-73a98a02c796": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005198165s
STEP: Saw pod success
Sep 16 23:46:53.700: INFO: Pod "pod-projected-secrets-d24b9c69-803d-475d-a56f-73a98a02c796" satisfied condition "Succeeded or Failed"
Sep 16 23:46:53.702: INFO: Trying to get logs from node eqx03-flash06 pod pod-projected-secrets-d24b9c69-803d-475d-a56f-73a98a02c796 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 16 23:46:53.776: INFO: Waiting for pod pod-projected-secrets-d24b9c69-803d-475d-a56f-73a98a02c796 to disappear
Sep 16 23:46:53.778: INFO: Pod pod-projected-secrets-d24b9c69-803d-475d-a56f-73a98a02c796 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:46:53.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7527" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":277,"completed":127,"skipped":2201,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:46:53.789: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0916 23:46:55.377290      23 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep 16 23:46:55.377: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:46:55.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7726" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":277,"completed":128,"skipped":2219,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:46:55.389: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating secret secrets-2147/secret-test-e40f6cfc-0b91-4632-a557-ba7a8d072b38
STEP: Creating a pod to test consume secrets
Sep 16 23:46:55.477: INFO: Waiting up to 5m0s for pod "pod-configmaps-d5dc1ebf-8d3f-43f1-bfc0-beac16f8fa5d" in namespace "secrets-2147" to be "Succeeded or Failed"
Sep 16 23:46:55.479: INFO: Pod "pod-configmaps-d5dc1ebf-8d3f-43f1-bfc0-beac16f8fa5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.144706ms
Sep 16 23:46:57.482: INFO: Pod "pod-configmaps-d5dc1ebf-8d3f-43f1-bfc0-beac16f8fa5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005066604s
Sep 16 23:46:59.485: INFO: Pod "pod-configmaps-d5dc1ebf-8d3f-43f1-bfc0-beac16f8fa5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00841204s
STEP: Saw pod success
Sep 16 23:46:59.485: INFO: Pod "pod-configmaps-d5dc1ebf-8d3f-43f1-bfc0-beac16f8fa5d" satisfied condition "Succeeded or Failed"
Sep 16 23:46:59.487: INFO: Trying to get logs from node eqx03-flash06 pod pod-configmaps-d5dc1ebf-8d3f-43f1-bfc0-beac16f8fa5d container env-test: <nil>
STEP: delete the pod
Sep 16 23:46:59.521: INFO: Waiting for pod pod-configmaps-d5dc1ebf-8d3f-43f1-bfc0-beac16f8fa5d to disappear
Sep 16 23:46:59.523: INFO: Pod pod-configmaps-d5dc1ebf-8d3f-43f1-bfc0-beac16f8fa5d no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:46:59.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2147" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":277,"completed":129,"skipped":2238,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:46:59.537: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:47:05.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2142" for this suite.
STEP: Destroying namespace "nsdeletetest-5127" for this suite.
Sep 16 23:47:05.736: INFO: Namespace nsdeletetest-5127 was already deleted
STEP: Destroying namespace "nsdeletetest-5290" for this suite.

• [SLOW TEST:6.208 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":277,"completed":130,"skipped":2381,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:47:05.746: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep 16 23:47:11.867: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 16 23:47:11.869: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 16 23:47:13.869: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 16 23:47:13.873: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 16 23:47:15.869: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 16 23:47:15.873: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 16 23:47:17.869: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 16 23:47:17.872: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:47:17.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9974" for this suite.

• [SLOW TEST:12.151 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":277,"completed":131,"skipped":2413,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:47:17.898: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 16 23:47:17.960: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:47:22.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3835" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":277,"completed":132,"skipped":2423,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:47:22.023: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: validating cluster-info
Sep 16 23:47:22.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 cluster-info'
Sep 16 23:47:22.266: INFO: stderr: ""
Sep 16 23:47:22.266: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://172.19.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://172.19.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:47:22.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1663" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":277,"completed":133,"skipped":2445,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:47:22.278: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-secret-5k7b
STEP: Creating a pod to test atomic-volume-subpath
Sep 16 23:47:22.355: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-5k7b" in namespace "subpath-4128" to be "Succeeded or Failed"
Sep 16 23:47:22.357: INFO: Pod "pod-subpath-test-secret-5k7b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.877888ms
Sep 16 23:47:24.365: INFO: Pod "pod-subpath-test-secret-5k7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009848305s
Sep 16 23:47:26.368: INFO: Pod "pod-subpath-test-secret-5k7b": Phase="Running", Reason="", readiness=true. Elapsed: 4.012799509s
Sep 16 23:47:28.371: INFO: Pod "pod-subpath-test-secret-5k7b": Phase="Running", Reason="", readiness=true. Elapsed: 6.015761299s
Sep 16 23:47:30.375: INFO: Pod "pod-subpath-test-secret-5k7b": Phase="Running", Reason="", readiness=true. Elapsed: 8.019608028s
Sep 16 23:47:32.377: INFO: Pod "pod-subpath-test-secret-5k7b": Phase="Running", Reason="", readiness=true. Elapsed: 10.022356879s
Sep 16 23:47:34.381: INFO: Pod "pod-subpath-test-secret-5k7b": Phase="Running", Reason="", readiness=true. Elapsed: 12.025614905s
Sep 16 23:47:36.384: INFO: Pod "pod-subpath-test-secret-5k7b": Phase="Running", Reason="", readiness=true. Elapsed: 14.028838949s
Sep 16 23:47:38.387: INFO: Pod "pod-subpath-test-secret-5k7b": Phase="Running", Reason="", readiness=true. Elapsed: 16.032063058s
Sep 16 23:47:40.391: INFO: Pod "pod-subpath-test-secret-5k7b": Phase="Running", Reason="", readiness=true. Elapsed: 18.035683736s
Sep 16 23:47:42.394: INFO: Pod "pod-subpath-test-secret-5k7b": Phase="Running", Reason="", readiness=true. Elapsed: 20.038834996s
Sep 16 23:47:44.397: INFO: Pod "pod-subpath-test-secret-5k7b": Phase="Running", Reason="", readiness=true. Elapsed: 22.041915278s
Sep 16 23:47:46.400: INFO: Pod "pod-subpath-test-secret-5k7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.044949354s
STEP: Saw pod success
Sep 16 23:47:46.400: INFO: Pod "pod-subpath-test-secret-5k7b" satisfied condition "Succeeded or Failed"
Sep 16 23:47:46.402: INFO: Trying to get logs from node eqx03-flash06 pod pod-subpath-test-secret-5k7b container test-container-subpath-secret-5k7b: <nil>
STEP: delete the pod
Sep 16 23:47:46.455: INFO: Waiting for pod pod-subpath-test-secret-5k7b to disappear
Sep 16 23:47:46.457: INFO: Pod pod-subpath-test-secret-5k7b no longer exists
STEP: Deleting pod pod-subpath-test-secret-5k7b
Sep 16 23:47:46.457: INFO: Deleting pod "pod-subpath-test-secret-5k7b" in namespace "subpath-4128"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:47:46.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4128" for this suite.

• [SLOW TEST:24.193 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":277,"completed":134,"skipped":2449,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:47:46.471: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-0233b0e0-7bfc-436a-acfe-edde6c47a238
STEP: Creating a pod to test consume secrets
Sep 16 23:47:46.563: INFO: Waiting up to 5m0s for pod "pod-secrets-d8fbdb6f-e99d-4b68-a914-6fe4d2aa6f72" in namespace "secrets-3986" to be "Succeeded or Failed"
Sep 16 23:47:46.566: INFO: Pod "pod-secrets-d8fbdb6f-e99d-4b68-a914-6fe4d2aa6f72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.823377ms
Sep 16 23:47:48.569: INFO: Pod "pod-secrets-d8fbdb6f-e99d-4b68-a914-6fe4d2aa6f72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005688112s
STEP: Saw pod success
Sep 16 23:47:48.569: INFO: Pod "pod-secrets-d8fbdb6f-e99d-4b68-a914-6fe4d2aa6f72" satisfied condition "Succeeded or Failed"
Sep 16 23:47:48.571: INFO: Trying to get logs from node eqx03-flash06 pod pod-secrets-d8fbdb6f-e99d-4b68-a914-6fe4d2aa6f72 container secret-volume-test: <nil>
STEP: delete the pod
Sep 16 23:47:48.621: INFO: Waiting for pod pod-secrets-d8fbdb6f-e99d-4b68-a914-6fe4d2aa6f72 to disappear
Sep 16 23:47:48.623: INFO: Pod pod-secrets-d8fbdb6f-e99d-4b68-a914-6fe4d2aa6f72 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:47:48.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3986" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":277,"completed":135,"skipped":2453,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:47:48.644: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test hostPath mode
Sep 16 23:47:48.699: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-5976" to be "Succeeded or Failed"
Sep 16 23:47:48.701: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 1.820111ms
Sep 16 23:47:50.704: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005172074s
Sep 16 23:47:52.707: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008276111s
STEP: Saw pod success
Sep 16 23:47:52.707: INFO: Pod "pod-host-path-test" satisfied condition "Succeeded or Failed"
Sep 16 23:47:52.710: INFO: Trying to get logs from node eqx03-flash06 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Sep 16 23:47:52.768: INFO: Waiting for pod pod-host-path-test to disappear
Sep 16 23:47:52.771: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:47:52.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-5976" for this suite.
•{"msg":"PASSED [sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":136,"skipped":2486,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:47:52.791: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating replication controller my-hostname-basic-1933369d-0dd2-44c2-9e9f-a623b88034e4
Sep 16 23:47:52.847: INFO: Pod name my-hostname-basic-1933369d-0dd2-44c2-9e9f-a623b88034e4: Found 0 pods out of 1
Sep 16 23:47:57.851: INFO: Pod name my-hostname-basic-1933369d-0dd2-44c2-9e9f-a623b88034e4: Found 1 pods out of 1
Sep 16 23:47:57.851: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-1933369d-0dd2-44c2-9e9f-a623b88034e4" are running
Sep 16 23:47:57.857: INFO: Pod "my-hostname-basic-1933369d-0dd2-44c2-9e9f-a623b88034e4-xkdgr" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-16 23:47:52 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-16 23:47:55 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-16 23:47:55 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-16 23:47:52 +0000 UTC Reason: Message:}])
Sep 16 23:47:57.857: INFO: Trying to dial the pod
Sep 16 23:48:02.867: INFO: Controller my-hostname-basic-1933369d-0dd2-44c2-9e9f-a623b88034e4: Got expected result from replica 1 [my-hostname-basic-1933369d-0dd2-44c2-9e9f-a623b88034e4-xkdgr]: "my-hostname-basic-1933369d-0dd2-44c2-9e9f-a623b88034e4-xkdgr", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:48:02.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-207" for this suite.

• [SLOW TEST:10.090 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":277,"completed":137,"skipped":2511,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:48:02.881: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0916 23:48:04.505864      23 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep 16 23:48:04.505: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:48:04.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3000" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":277,"completed":138,"skipped":2520,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:48:04.516: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-upd-15b8e8cd-eeed-4f7a-ac1e-dc56a8bcbd61
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:48:08.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6358" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":139,"skipped":2521,"failed":0}
S
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:48:08.708: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Sep 16 23:48:08.755: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Sep 16 23:48:17.822: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:48:17.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9907" for this suite.

• [SLOW TEST:9.131 seconds]
[k8s.io] Pods
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":277,"completed":140,"skipped":2522,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:48:17.840: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name s-test-opt-del-3a192611-47fb-43ed-8082-ecc31394f655
STEP: Creating secret with name s-test-opt-upd-643ac0b3-f356-43bd-a62a-1d3c47c7411d
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-3a192611-47fb-43ed-8082-ecc31394f655
STEP: Updating secret s-test-opt-upd-643ac0b3-f356-43bd-a62a-1d3c47c7411d
STEP: Creating secret with name s-test-opt-create-9b33fd7d-9e67-459c-a8e8-57e4ec35b708
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:48:26.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7785" for this suite.

• [SLOW TEST:8.303 seconds]
[sig-storage] Secrets
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":141,"skipped":2545,"failed":0}
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:48:26.142: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 16 23:48:26.221: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Sep 16 23:48:26.244: INFO: Number of nodes with available pods: 0
Sep 16 23:48:26.244: INFO: Node eqx03-flash06 is running more than one daemon pod
Sep 16 23:48:27.252: INFO: Number of nodes with available pods: 0
Sep 16 23:48:27.252: INFO: Node eqx03-flash06 is running more than one daemon pod
Sep 16 23:48:28.251: INFO: Number of nodes with available pods: 1
Sep 16 23:48:28.251: INFO: Node eqx03-flash07 is running more than one daemon pod
Sep 16 23:48:29.252: INFO: Number of nodes with available pods: 3
Sep 16 23:48:29.252: INFO: Node eqx03-flash07 is running more than one daemon pod
Sep 16 23:48:30.252: INFO: Number of nodes with available pods: 4
Sep 16 23:48:30.252: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Sep 16 23:48:30.301: INFO: Wrong image for pod: daemon-set-28t8q. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:30.301: INFO: Wrong image for pod: daemon-set-fg64f. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:30.301: INFO: Wrong image for pod: daemon-set-rb5qp. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:30.301: INFO: Wrong image for pod: daemon-set-tbbwk. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:31.327: INFO: Wrong image for pod: daemon-set-28t8q. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:31.327: INFO: Wrong image for pod: daemon-set-fg64f. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:31.327: INFO: Wrong image for pod: daemon-set-rb5qp. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:31.327: INFO: Wrong image for pod: daemon-set-tbbwk. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:32.309: INFO: Wrong image for pod: daemon-set-28t8q. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:32.309: INFO: Pod daemon-set-28t8q is not available
Sep 16 23:48:32.309: INFO: Wrong image for pod: daemon-set-fg64f. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:32.309: INFO: Wrong image for pod: daemon-set-rb5qp. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:32.309: INFO: Wrong image for pod: daemon-set-tbbwk. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:33.308: INFO: Wrong image for pod: daemon-set-28t8q. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:33.308: INFO: Pod daemon-set-28t8q is not available
Sep 16 23:48:33.308: INFO: Wrong image for pod: daemon-set-fg64f. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:33.308: INFO: Wrong image for pod: daemon-set-rb5qp. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:33.308: INFO: Wrong image for pod: daemon-set-tbbwk. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:34.309: INFO: Wrong image for pod: daemon-set-28t8q. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:34.309: INFO: Pod daemon-set-28t8q is not available
Sep 16 23:48:34.309: INFO: Wrong image for pod: daemon-set-fg64f. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:34.309: INFO: Wrong image for pod: daemon-set-rb5qp. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:34.309: INFO: Wrong image for pod: daemon-set-tbbwk. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:35.308: INFO: Wrong image for pod: daemon-set-28t8q. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:35.308: INFO: Pod daemon-set-28t8q is not available
Sep 16 23:48:35.308: INFO: Wrong image for pod: daemon-set-fg64f. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:35.308: INFO: Wrong image for pod: daemon-set-rb5qp. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:35.308: INFO: Wrong image for pod: daemon-set-tbbwk. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:36.309: INFO: Wrong image for pod: daemon-set-28t8q. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:36.309: INFO: Pod daemon-set-28t8q is not available
Sep 16 23:48:36.309: INFO: Wrong image for pod: daemon-set-fg64f. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:36.309: INFO: Wrong image for pod: daemon-set-rb5qp. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:36.309: INFO: Wrong image for pod: daemon-set-tbbwk. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:37.309: INFO: Wrong image for pod: daemon-set-28t8q. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:37.309: INFO: Pod daemon-set-28t8q is not available
Sep 16 23:48:37.309: INFO: Wrong image for pod: daemon-set-fg64f. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:37.309: INFO: Wrong image for pod: daemon-set-rb5qp. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:37.309: INFO: Wrong image for pod: daemon-set-tbbwk. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:38.308: INFO: Pod daemon-set-6kv6t is not available
Sep 16 23:48:38.308: INFO: Wrong image for pod: daemon-set-fg64f. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:38.308: INFO: Wrong image for pod: daemon-set-rb5qp. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:38.308: INFO: Wrong image for pod: daemon-set-tbbwk. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:39.404: INFO: Pod daemon-set-6kv6t is not available
Sep 16 23:48:39.404: INFO: Wrong image for pod: daemon-set-fg64f. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:39.404: INFO: Wrong image for pod: daemon-set-rb5qp. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:39.404: INFO: Wrong image for pod: daemon-set-tbbwk. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:40.308: INFO: Pod daemon-set-6kv6t is not available
Sep 16 23:48:40.308: INFO: Wrong image for pod: daemon-set-fg64f. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:40.308: INFO: Wrong image for pod: daemon-set-rb5qp. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:40.308: INFO: Wrong image for pod: daemon-set-tbbwk. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:41.309: INFO: Wrong image for pod: daemon-set-fg64f. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:41.309: INFO: Wrong image for pod: daemon-set-rb5qp. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:41.309: INFO: Wrong image for pod: daemon-set-tbbwk. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:42.309: INFO: Wrong image for pod: daemon-set-fg64f. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:42.309: INFO: Wrong image for pod: daemon-set-rb5qp. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:42.309: INFO: Wrong image for pod: daemon-set-tbbwk. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:43.308: INFO: Wrong image for pod: daemon-set-fg64f. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:43.308: INFO: Pod daemon-set-fg64f is not available
Sep 16 23:48:43.308: INFO: Wrong image for pod: daemon-set-rb5qp. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:43.308: INFO: Wrong image for pod: daemon-set-tbbwk. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:44.309: INFO: Pod daemon-set-csjpt is not available
Sep 16 23:48:44.309: INFO: Wrong image for pod: daemon-set-rb5qp. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:44.309: INFO: Wrong image for pod: daemon-set-tbbwk. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:45.309: INFO: Pod daemon-set-csjpt is not available
Sep 16 23:48:45.309: INFO: Wrong image for pod: daemon-set-rb5qp. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:45.309: INFO: Wrong image for pod: daemon-set-tbbwk. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:46.309: INFO: Wrong image for pod: daemon-set-rb5qp. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:46.309: INFO: Wrong image for pod: daemon-set-tbbwk. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:47.309: INFO: Wrong image for pod: daemon-set-rb5qp. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:47.309: INFO: Wrong image for pod: daemon-set-tbbwk. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:48.308: INFO: Wrong image for pod: daemon-set-rb5qp. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:48.308: INFO: Wrong image for pod: daemon-set-tbbwk. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:48.308: INFO: Pod daemon-set-tbbwk is not available
Sep 16 23:48:49.308: INFO: Wrong image for pod: daemon-set-rb5qp. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:49.308: INFO: Wrong image for pod: daemon-set-tbbwk. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:49.308: INFO: Pod daemon-set-tbbwk is not available
Sep 16 23:48:50.308: INFO: Wrong image for pod: daemon-set-rb5qp. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:50.308: INFO: Wrong image for pod: daemon-set-tbbwk. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:50.308: INFO: Pod daemon-set-tbbwk is not available
Sep 16 23:48:51.310: INFO: Wrong image for pod: daemon-set-rb5qp. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:51.310: INFO: Wrong image for pod: daemon-set-tbbwk. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:51.310: INFO: Pod daemon-set-tbbwk is not available
Sep 16 23:48:52.308: INFO: Pod daemon-set-27rh7 is not available
Sep 16 23:48:52.308: INFO: Wrong image for pod: daemon-set-rb5qp. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:53.308: INFO: Pod daemon-set-27rh7 is not available
Sep 16 23:48:53.308: INFO: Wrong image for pod: daemon-set-rb5qp. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:54.309: INFO: Wrong image for pod: daemon-set-rb5qp. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:55.309: INFO: Wrong image for pod: daemon-set-rb5qp. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:56.308: INFO: Wrong image for pod: daemon-set-rb5qp. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:57.308: INFO: Wrong image for pod: daemon-set-rb5qp. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Sep 16 23:48:57.308: INFO: Pod daemon-set-rb5qp is not available
Sep 16 23:48:58.309: INFO: Pod daemon-set-jfc8d is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Sep 16 23:48:58.319: INFO: Number of nodes with available pods: 3
Sep 16 23:48:58.319: INFO: Node eqx03-flash07 is running more than one daemon pod
Sep 16 23:48:59.327: INFO: Number of nodes with available pods: 3
Sep 16 23:48:59.327: INFO: Node eqx03-flash07 is running more than one daemon pod
Sep 16 23:49:00.327: INFO: Number of nodes with available pods: 3
Sep 16 23:49:00.327: INFO: Node eqx03-flash07 is running more than one daemon pod
Sep 16 23:49:01.327: INFO: Number of nodes with available pods: 4
Sep 16 23:49:01.327: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-918, will wait for the garbage collector to delete the pods
Sep 16 23:49:01.403: INFO: Deleting DaemonSet.extensions daemon-set took: 11.303685ms
Sep 16 23:49:02.304: INFO: Terminating DaemonSet.extensions daemon-set pods took: 900.281278ms
Sep 16 23:49:11.806: INFO: Number of nodes with available pods: 0
Sep 16 23:49:11.806: INFO: Number of running nodes: 0, number of available pods: 0
Sep 16 23:49:11.808: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-918/daemonsets","resourceVersion":"4239053"},"items":null}

Sep 16 23:49:11.810: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-918/pods","resourceVersion":"4239053"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:49:11.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-918" for this suite.

• [SLOW TEST:45.692 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":277,"completed":142,"skipped":2545,"failed":0}
SSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:49:11.834: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 16 23:49:13.925: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:49:13.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6748" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":277,"completed":143,"skipped":2550,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:49:13.976: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: getting the auto-created API token
STEP: reading a file in the container
Sep 16 23:49:18.565: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2466 pod-service-account-740d5dd6-927d-4582-9a07-1cc31d3df554 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Sep 16 23:49:18.899: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2466 pod-service-account-740d5dd6-927d-4582-9a07-1cc31d3df554 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Sep 16 23:49:19.265: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2466 pod-service-account-740d5dd6-927d-4582-9a07-1cc31d3df554 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:49:19.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2466" for this suite.

• [SLOW TEST:5.725 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":277,"completed":144,"skipped":2569,"failed":0}
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:49:19.702: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-downwardapi-vhqw
STEP: Creating a pod to test atomic-volume-subpath
Sep 16 23:49:19.800: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-vhqw" in namespace "subpath-3540" to be "Succeeded or Failed"
Sep 16 23:49:19.802: INFO: Pod "pod-subpath-test-downwardapi-vhqw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.077742ms
Sep 16 23:49:21.805: INFO: Pod "pod-subpath-test-downwardapi-vhqw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004707884s
Sep 16 23:49:23.811: INFO: Pod "pod-subpath-test-downwardapi-vhqw": Phase="Running", Reason="", readiness=true. Elapsed: 4.010594912s
Sep 16 23:49:25.814: INFO: Pod "pod-subpath-test-downwardapi-vhqw": Phase="Running", Reason="", readiness=true. Elapsed: 6.013703792s
Sep 16 23:49:27.816: INFO: Pod "pod-subpath-test-downwardapi-vhqw": Phase="Running", Reason="", readiness=true. Elapsed: 8.016425015s
Sep 16 23:49:29.820: INFO: Pod "pod-subpath-test-downwardapi-vhqw": Phase="Running", Reason="", readiness=true. Elapsed: 10.019726765s
Sep 16 23:49:31.823: INFO: Pod "pod-subpath-test-downwardapi-vhqw": Phase="Running", Reason="", readiness=true. Elapsed: 12.022990208s
Sep 16 23:49:33.826: INFO: Pod "pod-subpath-test-downwardapi-vhqw": Phase="Running", Reason="", readiness=true. Elapsed: 14.026342083s
Sep 16 23:49:35.829: INFO: Pod "pod-subpath-test-downwardapi-vhqw": Phase="Running", Reason="", readiness=true. Elapsed: 16.02917234s
Sep 16 23:49:37.832: INFO: Pod "pod-subpath-test-downwardapi-vhqw": Phase="Running", Reason="", readiness=true. Elapsed: 18.032493637s
Sep 16 23:49:39.836: INFO: Pod "pod-subpath-test-downwardapi-vhqw": Phase="Running", Reason="", readiness=true. Elapsed: 20.035566538s
Sep 16 23:49:41.839: INFO: Pod "pod-subpath-test-downwardapi-vhqw": Phase="Running", Reason="", readiness=true. Elapsed: 22.03879632s
Sep 16 23:49:43.842: INFO: Pod "pod-subpath-test-downwardapi-vhqw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.042044068s
STEP: Saw pod success
Sep 16 23:49:43.842: INFO: Pod "pod-subpath-test-downwardapi-vhqw" satisfied condition "Succeeded or Failed"
Sep 16 23:49:43.844: INFO: Trying to get logs from node eqx03-flash06 pod pod-subpath-test-downwardapi-vhqw container test-container-subpath-downwardapi-vhqw: <nil>
STEP: delete the pod
Sep 16 23:49:43.914: INFO: Waiting for pod pod-subpath-test-downwardapi-vhqw to disappear
Sep 16 23:49:43.916: INFO: Pod pod-subpath-test-downwardapi-vhqw no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-vhqw
Sep 16 23:49:43.916: INFO: Deleting pod "pod-subpath-test-downwardapi-vhqw" in namespace "subpath-3540"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:49:43.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3540" for this suite.

• [SLOW TEST:24.226 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":277,"completed":145,"skipped":2574,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:49:43.928: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-ccf2e1b1-b471-4277-96a9-7ced5b4270ab
STEP: Creating a pod to test consume configMaps
Sep 16 23:49:44.028: INFO: Waiting up to 5m0s for pod "pod-configmaps-fa74dc32-d269-4fa0-bcd4-3f0bd2246bc1" in namespace "configmap-2166" to be "Succeeded or Failed"
Sep 16 23:49:44.030: INFO: Pod "pod-configmaps-fa74dc32-d269-4fa0-bcd4-3f0bd2246bc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.261744ms
Sep 16 23:49:46.033: INFO: Pod "pod-configmaps-fa74dc32-d269-4fa0-bcd4-3f0bd2246bc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005116287s
Sep 16 23:49:48.035: INFO: Pod "pod-configmaps-fa74dc32-d269-4fa0-bcd4-3f0bd2246bc1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007721185s
STEP: Saw pod success
Sep 16 23:49:48.035: INFO: Pod "pod-configmaps-fa74dc32-d269-4fa0-bcd4-3f0bd2246bc1" satisfied condition "Succeeded or Failed"
Sep 16 23:49:48.037: INFO: Trying to get logs from node eqx03-flash06 pod pod-configmaps-fa74dc32-d269-4fa0-bcd4-3f0bd2246bc1 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 16 23:49:48.073: INFO: Waiting for pod pod-configmaps-fa74dc32-d269-4fa0-bcd4-3f0bd2246bc1 to disappear
Sep 16 23:49:48.074: INFO: Pod pod-configmaps-fa74dc32-d269-4fa0-bcd4-3f0bd2246bc1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:49:48.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2166" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":277,"completed":146,"skipped":2589,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:49:48.084: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Sep 16 23:49:48.127: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 16 23:49:48.146: INFO: Waiting for terminating namespaces to be deleted...
Sep 16 23:49:48.149: INFO: 
Logging pods the kubelet thinks is on node eqx03-flash06 before test
Sep 16 23:49:48.161: INFO: sonobuoy-e2e-job-362cab5d438b4e90 from sonobuoy started at 2020-09-16 23:06:47 +0000 UTC (2 container statuses recorded)
Sep 16 23:49:48.161: INFO: 	Container e2e ready: true, restart count 0
Sep 16 23:49:48.161: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 16 23:49:48.161: INFO: sonobuoy-systemd-logs-daemon-set-71797a053a4d44ec-pp8g4 from sonobuoy started at 2020-09-16 23:06:47 +0000 UTC (2 container statuses recorded)
Sep 16 23:49:48.161: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 16 23:49:48.161: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 16 23:49:48.161: INFO: etcd-eqx03-flash06 from kube-system started at 2020-09-04 20:35:41 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.161: INFO: 	Container etcd ready: true, restart count 0
Sep 16 23:49:48.161: INFO: calico-node-zqtk2 from kube-system started at 2020-09-09 19:40:56 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.161: INFO: 	Container calico-node ready: true, restart count 0
Sep 16 23:49:48.161: INFO: kube-apiserver-eqx03-flash06 from kube-system started at 2020-09-04 20:36:01 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.161: INFO: 	Container kube-apiserver ready: true, restart count 0
Sep 16 23:49:48.161: INFO: csi-nodeplugin-robin-rnhf4 from robinio started at 2020-09-16 23:34:31 +0000 UTC (2 container statuses recorded)
Sep 16 23:49:48.161: INFO: 	Container driver-registrar ready: true, restart count 0
Sep 16 23:49:48.161: INFO: 	Container robin ready: true, restart count 0
Sep 16 23:49:48.161: INFO: kube-controller-manager-eqx03-flash06 from kube-system started at 2020-09-04 20:36:01 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.161: INFO: 	Container kube-controller-manager ready: true, restart count 1
Sep 16 23:49:48.161: INFO: kube-sriov-device-plugin-amd64-29hgs from kube-system started at 2020-09-16 23:34:31 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.161: INFO: 	Container kube-sriovdp ready: true, restart count 0
Sep 16 23:49:48.161: INFO: kube-multus-ds-amd64-wjsv5 from kube-system started at 2020-09-16 23:34:31 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.161: INFO: 	Container kube-multus ready: true, restart count 0
Sep 16 23:49:48.161: INFO: robin-master-hr288 from robinio started at 2020-09-16 23:34:41 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.161: INFO: 	Container robinrcm ready: true, restart count 0
Sep 16 23:49:48.161: INFO: kube-scheduler-eqx03-flash06 from kube-system started at 2020-09-04 20:35:41 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.161: INFO: 	Container kube-scheduler ready: true, restart count 0
Sep 16 23:49:48.161: INFO: kube-proxy-mchgt from kube-system started at 2020-09-04 20:35:41 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.161: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 16 23:49:48.161: INFO: csi-snapshotter-robin-0 from robinio started at 2020-09-16 23:34:41 +0000 UTC (2 container statuses recorded)
Sep 16 23:49:48.161: INFO: 	Container csi-snapshotter ready: true, restart count 0
Sep 16 23:49:48.161: INFO: 	Container robin ready: true, restart count 0
Sep 16 23:49:48.161: INFO: 
Logging pods the kubelet thinks is on node eqx03-flash07 before test
Sep 16 23:49:48.202: INFO: hari-logstash-0 from spk started at 2020-09-05 06:37:24 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container logstash ready: true, restart count 0
Sep 16 23:49:48.202: INFO: test1-kibana-84f6c46bf6-mndsz from t001-u000003 started at 2020-09-05 06:42:07 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container kibana ready: true, restart count 0
Sep 16 23:49:48.202: INFO: test1-logstash-0 from t001-u000003 started at 2020-09-05 06:42:09 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container logstash ready: true, restart count 0
Sep 16 23:49:48.202: INFO: csi-nodeplugin-robin-bcq5b from robinio started at 2020-09-09 19:56:04 +0000 UTC (2 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container driver-registrar ready: true, restart count 0
Sep 16 23:49:48.202: INFO: 	Container robin ready: true, restart count 0
Sep 16 23:49:48.202: INFO: neo-neo4j-core-1 from default started at 2020-09-16 23:34:17 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 16 23:49:48.202: INFO: elkname-elasticsearch-data-0 from t001-u000003 started at 2020-09-05 06:08:19 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:49:48.202: INFO: neo-neo4j-core-2 from ripul started at 2020-09-16 23:33:57 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 16 23:49:48.202: INFO: neo-neo4j-core-2 from default started at 2020-09-16 23:34:00 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 16 23:49:48.202: INFO: kube-scheduler-eqx03-flash07 from kube-system started at 2020-09-04 20:24:11 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container kube-scheduler ready: true, restart count 0
Sep 16 23:49:48.202: INFO: cert-manager-cainjector-5ffff9dd7c-7fv5l from cert-manager started at 2020-09-04 20:38:07 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container cert-manager ready: true, restart count 0
Sep 16 23:49:48.202: INFO: mg-runner-gitlab-runner-97dd45f4-8z4zg from gitlab-runner started at 2020-09-10 19:25:18 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container mg-runner-gitlab-runner ready: false, restart count 1148
Sep 16 23:49:48.202: INFO: neo-neo4j-core-1 from madhura started at 2020-09-16 23:33:58 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 16 23:49:48.202: INFO: neo-neo4j-core-0 from default started at 2020-09-16 23:34:12 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 16 23:49:48.202: INFO: hari-elasticsearch-data-0 from spk started at 2020-09-05 06:37:25 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:49:48.202: INFO: test1-elasticsearch-client-df46b4cd8-2knzt from t001-u000003 started at 2020-09-05 06:42:07 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:49:48.202: INFO: calico-node-8t2kf from kube-system started at 2020-09-09 19:41:23 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container calico-node ready: true, restart count 0
Sep 16 23:49:48.202: INFO: kube-proxy-9zgnq from kube-system started at 2020-09-04 20:24:11 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 16 23:49:48.202: INFO: etcd-eqx03-flash07 from kube-system started at 2020-09-04 20:24:37 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container etcd ready: true, restart count 0
Sep 16 23:49:48.202: INFO: kube-sriov-device-plugin-amd64-rxs7g from kube-system started at 2020-09-04 20:25:01 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container kube-sriovdp ready: true, restart count 0
Sep 16 23:49:48.202: INFO: cert-manager-578cd6d964-sq59x from cert-manager started at 2020-09-04 20:38:07 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container cert-manager ready: true, restart count 0
Sep 16 23:49:48.202: INFO: elkname-kibana-56c986874d-nx4t6 from t001-u000003 started at 2020-09-05 06:08:19 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container kibana ready: true, restart count 0
Sep 16 23:49:48.202: INFO: test1-elasticsearch-data-0 from t001-u000003 started at 2020-09-05 06:42:08 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:49:48.202: INFO: sonobuoy from sonobuoy started at 2020-09-16 23:06:40 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 16 23:49:48.202: INFO: neo-neo4j-core-1 from ripul started at 2020-09-16 23:34:02 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 16 23:49:48.202: INFO: elkname-logstash-0 from t001-u000003 started at 2020-09-05 06:08:19 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container logstash ready: true, restart count 0
Sep 16 23:49:48.202: INFO: hari-kibana-b6f7d64-j96s2 from spk started at 2020-09-05 06:37:23 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container kibana ready: true, restart count 0
Sep 16 23:49:48.202: INFO: csi-resizer-robin-7d566f9df9-sq477 from robinio started at 2020-09-09 19:56:05 +0000 UTC (2 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container csi-resizer ready: true, restart count 0
Sep 16 23:49:48.202: INFO: 	Container robin ready: true, restart count 0
Sep 16 23:49:48.202: INFO: sonobuoy-systemd-logs-daemon-set-71797a053a4d44ec-4zr6m from sonobuoy started at 2020-09-16 23:06:47 +0000 UTC (2 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 16 23:49:48.202: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 16 23:49:48.202: INFO: neo-neo4j-replica-0 from ripul started at 2020-09-16 23:34:01 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container neo4j ready: true, restart count 0
Sep 16 23:49:48.202: INFO: kube-multus-ds-amd64-lf8vr from kube-system started at 2020-09-04 20:24:11 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container kube-multus ready: true, restart count 0
Sep 16 23:49:48.202: INFO: kube-apiserver-eqx03-flash07 from kube-system started at 2020-09-04 20:24:11 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container kube-apiserver ready: true, restart count 0
Sep 16 23:49:48.202: INFO: coredns-6878cb8f64-7rgtp from kube-system started at 2020-09-04 20:25:00 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container coredns ready: true, restart count 0
Sep 16 23:49:48.202: INFO: csi-provisioner-robin-6646bdd46b-kl2ft from robinio started at 2020-09-16 23:31:18 +0000 UTC (2 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container csi-provisioner ready: true, restart count 0
Sep 16 23:49:48.202: INFO: 	Container robin ready: true, restart count 0
Sep 16 23:49:48.202: INFO: robin-master-w2m7f from robinio started at 2020-09-09 19:47:17 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container robinrcm ready: true, restart count 0
Sep 16 23:49:48.202: INFO: cert-manager-webhook-556b9d7dfd-swlll from cert-manager started at 2020-09-04 20:38:07 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container cert-manager ready: true, restart count 0
Sep 16 23:49:48.202: INFO: elkname-elasticsearch-client-6768458985-6sgxz from t001-u000003 started at 2020-09-05 06:08:19 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:49:48.202: INFO: elkname-elasticsearch-master-0 from t001-u000003 started at 2020-09-05 06:08:19 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:49:48.202: INFO: csi-attacher-robin-5d956884cf-nqxfg from robinio started at 2020-09-16 23:31:18 +0000 UTC (2 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container csi-attacher ready: true, restart count 0
Sep 16 23:49:48.202: INFO: 	Container robin ready: true, restart count 0
Sep 16 23:49:48.202: INFO: neo-neo4j-replica-0 from madhura started at 2020-09-16 23:33:58 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container neo4j ready: true, restart count 0
Sep 16 23:49:48.202: INFO: snapshot-controller-0 from robinio started at 2020-09-09 19:56:15 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container snapshot-controller ready: true, restart count 0
Sep 16 23:49:48.202: INFO: kube-controller-manager-eqx03-flash07 from kube-system started at 2020-09-04 20:24:11 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container kube-controller-manager ready: true, restart count 1
Sep 16 23:49:48.202: INFO: hari-elasticsearch-client-c59586c8-k84fl from spk started at 2020-09-05 06:37:23 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:49:48.202: INFO: neo-neo4j-core-0 from ripul started at 2020-09-16 23:34:03 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.202: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 16 23:49:48.202: INFO: 
Logging pods the kubelet thinks is on node eqx04-flash04 before test
Sep 16 23:49:48.222: INFO: sonobuoy-systemd-logs-daemon-set-71797a053a4d44ec-dx2j7 from sonobuoy started at 2020-09-16 23:06:47 +0000 UTC (2 container statuses recorded)
Sep 16 23:49:48.222: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 16 23:49:48.222: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 16 23:49:48.222: INFO: kube-apiserver-eqx04-flash04 from kube-system started at 2020-09-04 20:12:22 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.222: INFO: 	Container kube-apiserver ready: true, restart count 0
Sep 16 23:49:48.222: INFO: robin-master-hkj7r from robinio started at 2020-09-09 19:48:36 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.222: INFO: 	Container robinrcm ready: true, restart count 0
Sep 16 23:49:48.222: INFO: kube-controller-manager-eqx04-flash04 from kube-system started at 2020-09-04 20:12:22 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.222: INFO: 	Container kube-controller-manager ready: true, restart count 1
Sep 16 23:49:48.222: INFO: kube-multus-ds-amd64-mzx44 from kube-system started at 2020-09-04 20:12:27 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.222: INFO: 	Container kube-multus ready: true, restart count 0
Sep 16 23:49:48.222: INFO: coredns-6878cb8f64-n75hx from kube-system started at 2020-09-04 20:12:55 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.222: INFO: 	Container coredns ready: true, restart count 0
Sep 16 23:49:48.222: INFO: neo-neo4j-core-0 from madhura started at 2020-09-16 23:34:02 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.222: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 16 23:49:48.222: INFO: elkname-elasticsearch-master-2 from t001-u000003 started at 2020-09-05 06:09:50 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.222: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:49:48.222: INFO: hari-elasticsearch-master-0 from spk started at 2020-09-05 06:37:24 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.222: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:49:48.222: INFO: csi-nodeplugin-robin-6xzkh from robinio started at 2020-09-09 19:56:04 +0000 UTC (2 container statuses recorded)
Sep 16 23:49:48.222: INFO: 	Container driver-registrar ready: true, restart count 0
Sep 16 23:49:48.222: INFO: 	Container robin ready: true, restart count 0
Sep 16 23:49:48.222: INFO: neo-neo4j-replica-0 from default started at 2020-09-16 23:34:03 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.222: INFO: 	Container neo4j ready: true, restart count 0
Sep 16 23:49:48.222: INFO: kube-proxy-2mmk7 from kube-system started at 2020-09-04 20:12:22 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.222: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 16 23:49:48.222: INFO: etcd-eqx04-flash04 from kube-system started at 2020-09-04 20:12:22 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.222: INFO: 	Container etcd ready: true, restart count 0
Sep 16 23:49:48.222: INFO: kube-sriov-device-plugin-amd64-xlvxl from kube-system started at 2020-09-04 20:12:30 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.222: INFO: 	Container kube-sriovdp ready: true, restart count 0
Sep 16 23:49:48.222: INFO: neo-neo4j-core-2 from madhura started at 2020-09-16 23:34:01 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.222: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 16 23:49:48.222: INFO: test1-elasticsearch-data-1 from t001-u000003 started at 2020-09-05 06:44:40 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.222: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:49:48.222: INFO: calico-node-wpq99 from kube-system started at 2020-09-09 19:41:33 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.222: INFO: 	Container calico-node ready: true, restart count 0
Sep 16 23:49:48.222: INFO: kube-scheduler-eqx04-flash04 from kube-system started at 2020-09-04 20:12:22 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.222: INFO: 	Container kube-scheduler ready: true, restart count 2
Sep 16 23:49:48.222: INFO: calico-kube-controllers-6c49f88586-jzdjm from kube-system started at 2020-09-04 20:12:28 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.222: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep 16 23:49:48.222: INFO: 
Logging pods the kubelet thinks is on node eqx04-flash06 before test
Sep 16 23:49:48.244: INFO: hari-elasticsearch-data-1 from spk started at 2020-09-05 06:40:02 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.244: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:49:48.244: INFO: kube-proxy-n5ckt from kube-system started at 2020-09-04 20:34:34 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.245: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 16 23:49:48.245: INFO: robin-worker-6zwcm from robinio started at 2020-09-09 19:53:16 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.245: INFO: 	Container robinrcm ready: true, restart count 0
Sep 16 23:49:48.245: INFO: kube-sriov-device-plugin-amd64-wwdl2 from kube-system started at 2020-09-04 20:34:39 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.245: INFO: 	Container kube-sriovdp ready: true, restart count 0
Sep 16 23:49:48.245: INFO: calico-node-x8tqn from kube-system started at 2020-09-09 19:41:10 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.245: INFO: 	Container calico-node ready: true, restart count 0
Sep 16 23:49:48.245: INFO: elkname-elasticsearch-master-1 from t001-u000003 started at 2020-09-05 06:09:25 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.245: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:49:48.245: INFO: test1-elasticsearch-master-1 from t001-u000003 started at 2020-09-05 06:43:08 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.245: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:49:48.245: INFO: test1-elasticsearch-master-2 from t001-u000003 started at 2020-09-05 06:43:44 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.245: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:49:48.245: INFO: test1-elasticsearch-master-0 from t001-u000003 started at 2020-09-05 06:42:08 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.245: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:49:48.245: INFO: kube-multus-ds-amd64-j9jmp from kube-system started at 2020-09-04 20:34:34 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.245: INFO: 	Container kube-multus ready: true, restart count 0
Sep 16 23:49:48.245: INFO: elkname-elasticsearch-data-1 from t001-u000003 started at 2020-09-05 06:10:51 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.245: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:49:48.245: INFO: elkname-elasticsearch-client-6768458985-j2fnt from t001-u000003 started at 2020-09-05 06:08:19 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.245: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:49:48.245: INFO: hari-elasticsearch-master-2 from spk started at 2020-09-05 06:38:48 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.245: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:49:48.245: INFO: test1-elasticsearch-client-df46b4cd8-52bhb from t001-u000003 started at 2020-09-05 06:42:07 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.245: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:49:48.245: INFO: csi-nodeplugin-robin-wk7pp from robinio started at 2020-09-09 19:56:04 +0000 UTC (2 container statuses recorded)
Sep 16 23:49:48.245: INFO: 	Container driver-registrar ready: true, restart count 0
Sep 16 23:49:48.245: INFO: 	Container robin ready: true, restart count 0
Sep 16 23:49:48.245: INFO: sonobuoy-systemd-logs-daemon-set-71797a053a4d44ec-w59x2 from sonobuoy started at 2020-09-16 23:06:47 +0000 UTC (2 container statuses recorded)
Sep 16 23:49:48.245: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 16 23:49:48.245: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 16 23:49:48.245: INFO: gitlab-runner-gitlab-runner-69dbb57dd4-sptqz from gitlab-runner started at 2020-09-10 19:12:45 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.245: INFO: 	Container gitlab-runner-gitlab-runner ready: false, restart count 0
Sep 16 23:49:48.245: INFO: hari-elasticsearch-client-c59586c8-xvm2k from spk started at 2020-09-05 06:37:23 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.245: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 16 23:49:48.245: INFO: hari-elasticsearch-master-1 from spk started at 2020-09-05 06:38:21 +0000 UTC (1 container statuses recorded)
Sep 16 23:49:48.245: INFO: 	Container elasticsearch ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1635688ae15b05a7], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1635688ae2446bdd], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:49:49.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8895" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":277,"completed":147,"skipped":2598,"failed":0}

------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:49:49.306: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-e10c3f11-7f52-4d27-a1ae-f1b651310008
STEP: Creating a pod to test consume secrets
Sep 16 23:49:49.390: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-36491ca8-0c8c-4c1a-90ff-f37818ed591b" in namespace "projected-7640" to be "Succeeded or Failed"
Sep 16 23:49:49.397: INFO: Pod "pod-projected-secrets-36491ca8-0c8c-4c1a-90ff-f37818ed591b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.977626ms
Sep 16 23:49:51.400: INFO: Pod "pod-projected-secrets-36491ca8-0c8c-4c1a-90ff-f37818ed591b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010257392s
Sep 16 23:49:53.403: INFO: Pod "pod-projected-secrets-36491ca8-0c8c-4c1a-90ff-f37818ed591b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013607843s
STEP: Saw pod success
Sep 16 23:49:53.403: INFO: Pod "pod-projected-secrets-36491ca8-0c8c-4c1a-90ff-f37818ed591b" satisfied condition "Succeeded or Failed"
Sep 16 23:49:53.405: INFO: Trying to get logs from node eqx03-flash06 pod pod-projected-secrets-36491ca8-0c8c-4c1a-90ff-f37818ed591b container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 16 23:49:53.455: INFO: Waiting for pod pod-projected-secrets-36491ca8-0c8c-4c1a-90ff-f37818ed591b to disappear
Sep 16 23:49:53.457: INFO: Pod pod-projected-secrets-36491ca8-0c8c-4c1a-90ff-f37818ed591b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:49:53.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7640" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":277,"completed":148,"skipped":2598,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:49:53.470: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9481.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9481.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9481.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9481.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9481.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9481.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9481.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9481.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9481.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9481.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9481.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9481.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9481.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 131.76.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.76.131_udp@PTR;check="$$(dig +tcp +noall +answer +search 131.76.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.76.131_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9481.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9481.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9481.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9481.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9481.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9481.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9481.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9481.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9481.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9481.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9481.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9481.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9481.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 131.76.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.76.131_udp@PTR;check="$$(dig +tcp +noall +answer +search 131.76.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.76.131_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 16 23:49:57.589: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9481.svc.cluster.local from pod dns-9481/dns-test-598dedb7-fdac-4aa3-9e68-42a6a1e91a89: the server could not find the requested resource (get pods dns-test-598dedb7-fdac-4aa3-9e68-42a6a1e91a89)
Sep 16 23:49:57.592: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9481.svc.cluster.local from pod dns-9481/dns-test-598dedb7-fdac-4aa3-9e68-42a6a1e91a89: the server could not find the requested resource (get pods dns-test-598dedb7-fdac-4aa3-9e68-42a6a1e91a89)
Sep 16 23:49:57.594: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9481.svc.cluster.local from pod dns-9481/dns-test-598dedb7-fdac-4aa3-9e68-42a6a1e91a89: the server could not find the requested resource (get pods dns-test-598dedb7-fdac-4aa3-9e68-42a6a1e91a89)
Sep 16 23:49:57.606: INFO: Unable to read wheezy_udp@PodARecord from pod dns-9481/dns-test-598dedb7-fdac-4aa3-9e68-42a6a1e91a89: the server could not find the requested resource (get pods dns-test-598dedb7-fdac-4aa3-9e68-42a6a1e91a89)
Sep 16 23:49:57.609: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-9481/dns-test-598dedb7-fdac-4aa3-9e68-42a6a1e91a89: the server could not find the requested resource (get pods dns-test-598dedb7-fdac-4aa3-9e68-42a6a1e91a89)
Sep 16 23:49:57.618: INFO: Unable to read jessie_tcp@dns-test-service.dns-9481.svc.cluster.local from pod dns-9481/dns-test-598dedb7-fdac-4aa3-9e68-42a6a1e91a89: the server could not find the requested resource (get pods dns-test-598dedb7-fdac-4aa3-9e68-42a6a1e91a89)
Sep 16 23:49:57.620: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9481.svc.cluster.local from pod dns-9481/dns-test-598dedb7-fdac-4aa3-9e68-42a6a1e91a89: the server could not find the requested resource (get pods dns-test-598dedb7-fdac-4aa3-9e68-42a6a1e91a89)
Sep 16 23:49:57.622: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9481.svc.cluster.local from pod dns-9481/dns-test-598dedb7-fdac-4aa3-9e68-42a6a1e91a89: the server could not find the requested resource (get pods dns-test-598dedb7-fdac-4aa3-9e68-42a6a1e91a89)
Sep 16 23:49:57.629: INFO: Unable to read jessie_udp@PodARecord from pod dns-9481/dns-test-598dedb7-fdac-4aa3-9e68-42a6a1e91a89: the server could not find the requested resource (get pods dns-test-598dedb7-fdac-4aa3-9e68-42a6a1e91a89)
Sep 16 23:49:57.632: INFO: Unable to read jessie_tcp@PodARecord from pod dns-9481/dns-test-598dedb7-fdac-4aa3-9e68-42a6a1e91a89: the server could not find the requested resource (get pods dns-test-598dedb7-fdac-4aa3-9e68-42a6a1e91a89)
Sep 16 23:49:57.636: INFO: Lookups using dns-9481/dns-test-598dedb7-fdac-4aa3-9e68-42a6a1e91a89 failed for: [wheezy_tcp@dns-test-service.dns-9481.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9481.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9481.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_tcp@dns-test-service.dns-9481.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9481.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9481.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep 16 23:50:02.688: INFO: DNS probes using dns-9481/dns-test-598dedb7-fdac-4aa3-9e68-42a6a1e91a89 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:50:02.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9481" for this suite.

• [SLOW TEST:9.353 seconds]
[sig-network] DNS
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":277,"completed":149,"skipped":2638,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:50:02.823: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:50:09.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8809" for this suite.

• [SLOW TEST:7.102 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":277,"completed":150,"skipped":2664,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:50:09.925: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-7700
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a new StatefulSet
Sep 16 23:50:10.011: INFO: Found 0 stateful pods, waiting for 3
Sep 16 23:50:20.032: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 16 23:50:20.032: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 16 23:50:20.032: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=false
Sep 16 23:50:30.015: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 16 23:50:30.015: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 16 23:50:30.015: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Sep 16 23:50:30.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7700 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 16 23:50:30.570: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 16 23:50:30.570: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 16 23:50:30.570: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Sep 16 23:50:40.601: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Sep 16 23:50:50.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7700 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:50:51.437: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 16 23:50:51.437: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 16 23:50:51.437: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 16 23:51:01.469: INFO: Waiting for StatefulSet statefulset-7700/ss2 to complete update
Sep 16 23:51:01.469: INFO: Waiting for Pod statefulset-7700/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 16 23:51:01.469: INFO: Waiting for Pod statefulset-7700/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 16 23:51:11.475: INFO: Waiting for StatefulSet statefulset-7700/ss2 to complete update
Sep 16 23:51:11.475: INFO: Waiting for Pod statefulset-7700/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 16 23:51:21.475: INFO: Waiting for StatefulSet statefulset-7700/ss2 to complete update
STEP: Rolling back to a previous revision
Sep 16 23:51:31.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7700 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 16 23:51:32.097: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 16 23:51:32.097: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 16 23:51:32.097: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 16 23:51:42.130: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Sep 16 23:51:52.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7700 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:51:53.135: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 16 23:51:53.135: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 16 23:51:53.135: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 16 23:52:13.152: INFO: Waiting for StatefulSet statefulset-7700/ss2 to complete update
Sep 16 23:52:13.152: INFO: Waiting for Pod statefulset-7700/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Sep 16 23:52:23.165: INFO: Waiting for StatefulSet statefulset-7700/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Sep 16 23:52:33.158: INFO: Deleting all statefulset in ns statefulset-7700
Sep 16 23:52:33.160: INFO: Scaling statefulset ss2 to 0
Sep 16 23:53:03.198: INFO: Waiting for statefulset status.replicas updated to 0
Sep 16 23:53:03.200: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:53:03.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7700" for this suite.

• [SLOW TEST:173.307 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":277,"completed":151,"skipped":2697,"failed":0}
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:53:03.232: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 16 23:53:06.329: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:53:06.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7133" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":277,"completed":152,"skipped":2697,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:53:06.367: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Sep 16 23:53:10.450: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1365 PodName:pod-sharedvolume-f0ef6b47-e3d0-40fe-a30e-bb36469d3d08 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 16 23:53:10.450: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
Sep 16 23:53:10.676: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:53:10.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1365" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":277,"completed":153,"skipped":2705,"failed":0}

------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:53:10.689: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Sep 16 23:53:10.751: INFO: Created pod &Pod{ObjectMeta:{dns-1220  dns-1220 /api/v1/namespaces/dns-1220/pods/dns-1220 03fdade9-b76a-4cfe-a45c-33da6c877121 4240819 0 2020-09-16 23:53:10 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2020-09-16 23:53:10 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 114 103 115 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 67 111 110 102 105 103 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 115 101 114 118 101 114 115 34 58 123 125 44 34 102 58 115 101 97 114 99 104 101 115 34 58 123 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zwvvz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zwvvz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zwvvz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 16 23:53:10.753: INFO: The status of Pod dns-1220 is Pending, waiting for it to be Running (with Ready = true)
Sep 16 23:53:12.756: INFO: The status of Pod dns-1220 is Pending, waiting for it to be Running (with Ready = true)
Sep 16 23:53:14.756: INFO: The status of Pod dns-1220 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Sep 16 23:53:14.756: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1220 PodName:dns-1220 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 16 23:53:14.756: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Verifying customized DNS server is configured on pod...
Sep 16 23:53:15.011: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1220 PodName:dns-1220 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 16 23:53:15.011: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
Sep 16 23:53:15.388: INFO: Deleting pod dns-1220...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:53:15.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1220" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":277,"completed":154,"skipped":2705,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:53:15.435: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-7631
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating stateful set ss in namespace statefulset-7631
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7631
Sep 16 23:53:15.540: INFO: Found 0 stateful pods, waiting for 1
Sep 16 23:53:25.544: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Sep 16 23:53:25.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 16 23:53:26.007: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 16 23:53:26.007: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 16 23:53:26.007: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 16 23:53:26.014: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep 16 23:53:36.040: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 16 23:53:36.040: INFO: Waiting for statefulset status.replicas updated to 0
Sep 16 23:53:36.060: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Sep 16 23:53:36.060: INFO: ss-0  eqx03-flash06  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:15 +0000 UTC  }]
Sep 16 23:53:36.060: INFO: 
Sep 16 23:53:36.060: INFO: StatefulSet ss has not reached scale 3, at 1
Sep 16 23:53:37.063: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99741447s
Sep 16 23:53:38.084: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993910584s
Sep 16 23:53:39.088: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.973542901s
Sep 16 23:53:40.091: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.969761853s
Sep 16 23:53:41.095: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.966275814s
Sep 16 23:53:42.099: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.962660309s
Sep 16 23:53:43.102: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.958649949s
Sep 16 23:53:44.106: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.954984596s
Sep 16 23:53:45.110: INFO: Verifying statefulset ss doesn't scale past 3 for another 951.426739ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7631
Sep 16 23:53:46.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:53:46.451: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 16 23:53:46.452: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 16 23:53:46.452: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 16 23:53:46.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:53:47.439: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep 16 23:53:47.439: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 16 23:53:47.439: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 16 23:53:47.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:53:47.697: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep 16 23:53:47.697: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 16 23:53:47.697: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 16 23:53:47.700: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 16 23:53:47.700: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 16 23:53:47.700: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Sep 16 23:53:47.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 16 23:53:48.024: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 16 23:53:48.025: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 16 23:53:48.025: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 16 23:53:48.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 16 23:53:48.583: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 16 23:53:48.583: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 16 23:53:48.583: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 16 23:53:48.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 16 23:53:48.892: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 16 23:53:48.892: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 16 23:53:48.892: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 16 23:53:48.892: INFO: Waiting for statefulset status.replicas updated to 0
Sep 16 23:53:48.895: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Sep 16 23:53:58.912: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 16 23:53:58.912: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep 16 23:53:58.912: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep 16 23:53:58.936: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Sep 16 23:53:58.936: INFO: ss-0  eqx03-flash06  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:15 +0000 UTC  }]
Sep 16 23:53:58.936: INFO: ss-1  eqx03-flash07  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:36 +0000 UTC  }]
Sep 16 23:53:58.937: INFO: ss-2  eqx04-flash04  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:36 +0000 UTC  }]
Sep 16 23:53:58.937: INFO: 
Sep 16 23:53:58.937: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 16 23:53:59.940: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Sep 16 23:53:59.940: INFO: ss-0  eqx03-flash06  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:15 +0000 UTC  }]
Sep 16 23:53:59.940: INFO: ss-1  eqx03-flash07  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:36 +0000 UTC  }]
Sep 16 23:53:59.940: INFO: ss-2  eqx04-flash04  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:36 +0000 UTC  }]
Sep 16 23:53:59.940: INFO: 
Sep 16 23:53:59.940: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 16 23:54:00.946: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Sep 16 23:54:00.946: INFO: ss-0  eqx03-flash06  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:15 +0000 UTC  }]
Sep 16 23:54:00.947: INFO: ss-1  eqx03-flash07  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:36 +0000 UTC  }]
Sep 16 23:54:00.947: INFO: ss-2  eqx04-flash04  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:36 +0000 UTC  }]
Sep 16 23:54:00.947: INFO: 
Sep 16 23:54:00.947: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 16 23:54:01.954: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Sep 16 23:54:01.954: INFO: ss-0  eqx03-flash06  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:15 +0000 UTC  }]
Sep 16 23:54:01.955: INFO: ss-1  eqx03-flash07  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:36 +0000 UTC  }]
Sep 16 23:54:01.955: INFO: ss-2  eqx04-flash04  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:36 +0000 UTC  }]
Sep 16 23:54:01.955: INFO: 
Sep 16 23:54:01.955: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 16 23:54:02.958: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Sep 16 23:54:02.958: INFO: ss-0  eqx03-flash06  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:15 +0000 UTC  }]
Sep 16 23:54:02.958: INFO: ss-1  eqx03-flash07  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:36 +0000 UTC  }]
Sep 16 23:54:02.958: INFO: ss-2  eqx04-flash04  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:36 +0000 UTC  }]
Sep 16 23:54:02.958: INFO: 
Sep 16 23:54:02.958: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 16 23:54:03.962: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Sep 16 23:54:03.962: INFO: ss-0  eqx03-flash06  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:15 +0000 UTC  }]
Sep 16 23:54:03.962: INFO: ss-2  eqx04-flash04  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:36 +0000 UTC  }]
Sep 16 23:54:03.962: INFO: 
Sep 16 23:54:03.962: INFO: StatefulSet ss has not reached scale 0, at 2
Sep 16 23:54:04.965: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Sep 16 23:54:04.965: INFO: ss-0  eqx03-flash06  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:15 +0000 UTC  }]
Sep 16 23:54:04.965: INFO: ss-2  eqx04-flash04  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:36 +0000 UTC  }]
Sep 16 23:54:04.965: INFO: 
Sep 16 23:54:04.965: INFO: StatefulSet ss has not reached scale 0, at 2
Sep 16 23:54:05.968: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Sep 16 23:54:05.968: INFO: ss-0  eqx03-flash06  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:15 +0000 UTC  }]
Sep 16 23:54:05.969: INFO: ss-2  eqx04-flash04  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:36 +0000 UTC  }]
Sep 16 23:54:05.969: INFO: 
Sep 16 23:54:05.969: INFO: StatefulSet ss has not reached scale 0, at 2
Sep 16 23:54:06.972: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Sep 16 23:54:06.972: INFO: ss-0  eqx03-flash06  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:15 +0000 UTC  }]
Sep 16 23:54:06.972: INFO: ss-2  eqx04-flash04  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:36 +0000 UTC  }]
Sep 16 23:54:06.972: INFO: 
Sep 16 23:54:06.972: INFO: StatefulSet ss has not reached scale 0, at 2
Sep 16 23:54:07.982: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Sep 16 23:54:07.982: INFO: ss-0  eqx03-flash06  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:15 +0000 UTC  }]
Sep 16 23:54:07.982: INFO: ss-2  eqx04-flash04  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-16 23:53:36 +0000 UTC  }]
Sep 16 23:54:07.982: INFO: 
Sep 16 23:54:07.982: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7631
Sep 16 23:54:08.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:54:09.257: INFO: rc: 1
Sep 16 23:54:09.257: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Sep 16 23:54:19.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:54:19.354: INFO: rc: 1
Sep 16 23:54:19.354: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 16 23:54:29.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:54:29.451: INFO: rc: 1
Sep 16 23:54:29.451: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 16 23:54:39.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:54:39.554: INFO: rc: 1
Sep 16 23:54:39.554: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 16 23:54:49.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:54:49.659: INFO: rc: 1
Sep 16 23:54:49.659: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 16 23:54:59.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:54:59.757: INFO: rc: 1
Sep 16 23:54:59.757: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 16 23:55:09.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:55:09.858: INFO: rc: 1
Sep 16 23:55:09.858: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 16 23:55:19.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:55:19.955: INFO: rc: 1
Sep 16 23:55:19.955: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 16 23:55:29.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:55:30.051: INFO: rc: 1
Sep 16 23:55:30.051: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 16 23:55:40.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:55:40.151: INFO: rc: 1
Sep 16 23:55:40.151: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 16 23:55:50.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:55:50.250: INFO: rc: 1
Sep 16 23:55:50.250: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 16 23:56:00.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:56:00.347: INFO: rc: 1
Sep 16 23:56:00.347: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 16 23:56:10.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:56:10.459: INFO: rc: 1
Sep 16 23:56:10.459: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 16 23:56:20.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:56:20.557: INFO: rc: 1
Sep 16 23:56:20.557: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 16 23:56:30.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:56:30.654: INFO: rc: 1
Sep 16 23:56:30.655: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 16 23:56:40.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:56:40.762: INFO: rc: 1
Sep 16 23:56:40.762: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 16 23:56:50.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:56:50.860: INFO: rc: 1
Sep 16 23:56:50.860: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 16 23:57:00.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:57:00.970: INFO: rc: 1
Sep 16 23:57:00.970: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 16 23:57:10.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:57:11.066: INFO: rc: 1
Sep 16 23:57:11.066: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 16 23:57:21.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:57:21.165: INFO: rc: 1
Sep 16 23:57:21.165: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 16 23:57:31.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:57:31.339: INFO: rc: 1
Sep 16 23:57:31.339: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 16 23:57:41.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:57:41.436: INFO: rc: 1
Sep 16 23:57:41.436: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 16 23:57:51.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:57:51.534: INFO: rc: 1
Sep 16 23:57:51.534: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 16 23:58:01.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:58:01.649: INFO: rc: 1
Sep 16 23:58:01.649: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 16 23:58:11.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:58:11.756: INFO: rc: 1
Sep 16 23:58:11.756: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 16 23:58:21.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:58:21.853: INFO: rc: 1
Sep 16 23:58:21.853: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 16 23:58:31.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:58:31.955: INFO: rc: 1
Sep 16 23:58:31.955: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 16 23:58:41.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:58:42.065: INFO: rc: 1
Sep 16 23:58:42.065: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 16 23:58:52.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:58:52.168: INFO: rc: 1
Sep 16 23:58:52.168: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 16 23:59:02.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:59:02.281: INFO: rc: 1
Sep 16 23:59:02.281: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 16 23:59:12.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-7631 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 16 23:59:12.377: INFO: rc: 1
Sep 16 23:59:12.377: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: 
Sep 16 23:59:12.377: INFO: Scaling statefulset ss to 0
Sep 16 23:59:12.385: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Sep 16 23:59:12.387: INFO: Deleting all statefulset in ns statefulset-7631
Sep 16 23:59:12.388: INFO: Scaling statefulset ss to 0
Sep 16 23:59:12.394: INFO: Waiting for statefulset status.replicas updated to 0
Sep 16 23:59:12.396: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:59:12.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7631" for this suite.

• [SLOW TEST:356.994 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":277,"completed":155,"skipped":2719,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:59:12.430: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2801.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2801.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2801.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2801.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2801.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2801.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2801.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2801.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2801.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2801.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2801.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2801.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2801.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2801.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2801.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2801.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2801.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2801.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 16 23:59:16.548: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2801.svc.cluster.local from pod dns-2801/dns-test-71b39421-87e9-4085-9f09-5de65babe536: the server could not find the requested resource (get pods dns-test-71b39421-87e9-4085-9f09-5de65babe536)
Sep 16 23:59:16.557: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2801.svc.cluster.local from pod dns-2801/dns-test-71b39421-87e9-4085-9f09-5de65babe536: the server could not find the requested resource (get pods dns-test-71b39421-87e9-4085-9f09-5de65babe536)
Sep 16 23:59:16.559: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2801.svc.cluster.local from pod dns-2801/dns-test-71b39421-87e9-4085-9f09-5de65babe536: the server could not find the requested resource (get pods dns-test-71b39421-87e9-4085-9f09-5de65babe536)
Sep 16 23:59:16.561: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2801.svc.cluster.local from pod dns-2801/dns-test-71b39421-87e9-4085-9f09-5de65babe536: the server could not find the requested resource (get pods dns-test-71b39421-87e9-4085-9f09-5de65babe536)
Sep 16 23:59:16.564: INFO: Unable to read wheezy_udp@PodARecord from pod dns-2801/dns-test-71b39421-87e9-4085-9f09-5de65babe536: the server could not find the requested resource (get pods dns-test-71b39421-87e9-4085-9f09-5de65babe536)
Sep 16 23:59:16.566: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-2801/dns-test-71b39421-87e9-4085-9f09-5de65babe536: the server could not find the requested resource (get pods dns-test-71b39421-87e9-4085-9f09-5de65babe536)
Sep 16 23:59:16.568: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2801.svc.cluster.local from pod dns-2801/dns-test-71b39421-87e9-4085-9f09-5de65babe536: the server could not find the requested resource (get pods dns-test-71b39421-87e9-4085-9f09-5de65babe536)
Sep 16 23:59:16.571: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2801.svc.cluster.local from pod dns-2801/dns-test-71b39421-87e9-4085-9f09-5de65babe536: the server could not find the requested resource (get pods dns-test-71b39421-87e9-4085-9f09-5de65babe536)
Sep 16 23:59:16.573: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2801.svc.cluster.local from pod dns-2801/dns-test-71b39421-87e9-4085-9f09-5de65babe536: the server could not find the requested resource (get pods dns-test-71b39421-87e9-4085-9f09-5de65babe536)
Sep 16 23:59:16.575: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2801.svc.cluster.local from pod dns-2801/dns-test-71b39421-87e9-4085-9f09-5de65babe536: the server could not find the requested resource (get pods dns-test-71b39421-87e9-4085-9f09-5de65babe536)
Sep 16 23:59:16.577: INFO: Unable to read jessie_udp@PodARecord from pod dns-2801/dns-test-71b39421-87e9-4085-9f09-5de65babe536: the server could not find the requested resource (get pods dns-test-71b39421-87e9-4085-9f09-5de65babe536)
Sep 16 23:59:16.580: INFO: Unable to read jessie_tcp@PodARecord from pod dns-2801/dns-test-71b39421-87e9-4085-9f09-5de65babe536: the server could not find the requested resource (get pods dns-test-71b39421-87e9-4085-9f09-5de65babe536)
Sep 16 23:59:16.580: INFO: Lookups using dns-2801/dns-test-71b39421-87e9-4085-9f09-5de65babe536 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2801.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2801.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2801.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2801.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-querier-2.dns-test-service-2.dns-2801.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2801.svc.cluster.local jessie_udp@dns-test-service-2.dns-2801.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2801.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep 16 23:59:21.616: INFO: DNS probes using dns-2801/dns-test-71b39421-87e9-4085-9f09-5de65babe536 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:59:21.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2801" for this suite.

• [SLOW TEST:9.298 seconds]
[sig-network] DNS
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":277,"completed":156,"skipped":2721,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:59:21.729: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 16 23:59:21.812: INFO: Waiting up to 5m0s for pod "downwardapi-volume-504cf93d-700e-459f-bac6-a29228e495e3" in namespace "downward-api-3960" to be "Succeeded or Failed"
Sep 16 23:59:21.814: INFO: Pod "downwardapi-volume-504cf93d-700e-459f-bac6-a29228e495e3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.974256ms
Sep 16 23:59:23.817: INFO: Pod "downwardapi-volume-504cf93d-700e-459f-bac6-a29228e495e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005151819s
Sep 16 23:59:25.820: INFO: Pod "downwardapi-volume-504cf93d-700e-459f-bac6-a29228e495e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008450366s
STEP: Saw pod success
Sep 16 23:59:25.820: INFO: Pod "downwardapi-volume-504cf93d-700e-459f-bac6-a29228e495e3" satisfied condition "Succeeded or Failed"
Sep 16 23:59:25.823: INFO: Trying to get logs from node eqx03-flash06 pod downwardapi-volume-504cf93d-700e-459f-bac6-a29228e495e3 container client-container: <nil>
STEP: delete the pod
Sep 16 23:59:25.889: INFO: Waiting for pod downwardapi-volume-504cf93d-700e-459f-bac6-a29228e495e3 to disappear
Sep 16 23:59:25.891: INFO: Pod downwardapi-volume-504cf93d-700e-459f-bac6-a29228e495e3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:59:25.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3960" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":277,"completed":157,"skipped":2734,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:59:25.906: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-5270bc4d-46a9-4f6b-bd8b-a81489062c9a
STEP: Creating a pod to test consume configMaps
Sep 16 23:59:25.986: INFO: Waiting up to 5m0s for pod "pod-configmaps-ac0b674e-f79a-4651-8444-08b3e9320bdc" in namespace "configmap-1832" to be "Succeeded or Failed"
Sep 16 23:59:25.988: INFO: Pod "pod-configmaps-ac0b674e-f79a-4651-8444-08b3e9320bdc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.961101ms
Sep 16 23:59:27.990: INFO: Pod "pod-configmaps-ac0b674e-f79a-4651-8444-08b3e9320bdc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004562668s
STEP: Saw pod success
Sep 16 23:59:27.990: INFO: Pod "pod-configmaps-ac0b674e-f79a-4651-8444-08b3e9320bdc" satisfied condition "Succeeded or Failed"
Sep 16 23:59:27.992: INFO: Trying to get logs from node eqx03-flash06 pod pod-configmaps-ac0b674e-f79a-4651-8444-08b3e9320bdc container configmap-volume-test: <nil>
STEP: delete the pod
Sep 16 23:59:28.023: INFO: Waiting for pod pod-configmaps-ac0b674e-f79a-4651-8444-08b3e9320bdc to disappear
Sep 16 23:59:28.025: INFO: Pod pod-configmaps-ac0b674e-f79a-4651-8444-08b3e9320bdc no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:59:28.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1832" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":158,"skipped":2734,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:59:28.058: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4544 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4544;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4544 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4544;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4544.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4544.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4544.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4544.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4544.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4544.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4544.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4544.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4544.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4544.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4544.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4544.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4544.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 219.171.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.171.219_udp@PTR;check="$$(dig +tcp +noall +answer +search 219.171.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.171.219_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4544 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4544;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4544 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4544;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4544.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4544.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4544.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4544.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4544.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4544.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4544.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4544.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4544.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4544.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4544.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4544.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4544.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 219.171.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.171.219_udp@PTR;check="$$(dig +tcp +noall +answer +search 219.171.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.171.219_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 16 23:59:32.185: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4544/dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1: the server could not find the requested resource (get pods dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1)
Sep 16 23:59:32.188: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4544/dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1: the server could not find the requested resource (get pods dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1)
Sep 16 23:59:32.190: INFO: Unable to read wheezy_udp@dns-test-service.dns-4544 from pod dns-4544/dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1: the server could not find the requested resource (get pods dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1)
Sep 16 23:59:32.193: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4544 from pod dns-4544/dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1: the server could not find the requested resource (get pods dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1)
Sep 16 23:59:32.196: INFO: Unable to read wheezy_udp@dns-test-service.dns-4544.svc from pod dns-4544/dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1: the server could not find the requested resource (get pods dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1)
Sep 16 23:59:32.199: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4544.svc from pod dns-4544/dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1: the server could not find the requested resource (get pods dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1)
Sep 16 23:59:32.201: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4544.svc from pod dns-4544/dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1: the server could not find the requested resource (get pods dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1)
Sep 16 23:59:32.204: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4544.svc from pod dns-4544/dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1: the server could not find the requested resource (get pods dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1)
Sep 16 23:59:32.212: INFO: Unable to read wheezy_udp@PodARecord from pod dns-4544/dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1: the server could not find the requested resource (get pods dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1)
Sep 16 23:59:32.215: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-4544/dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1: the server could not find the requested resource (get pods dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1)
Sep 16 23:59:32.223: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4544/dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1: the server could not find the requested resource (get pods dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1)
Sep 16 23:59:32.225: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4544/dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1: the server could not find the requested resource (get pods dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1)
Sep 16 23:59:32.228: INFO: Unable to read jessie_udp@dns-test-service.dns-4544 from pod dns-4544/dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1: the server could not find the requested resource (get pods dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1)
Sep 16 23:59:32.230: INFO: Unable to read jessie_tcp@dns-test-service.dns-4544 from pod dns-4544/dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1: the server could not find the requested resource (get pods dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1)
Sep 16 23:59:32.233: INFO: Unable to read jessie_udp@dns-test-service.dns-4544.svc from pod dns-4544/dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1: the server could not find the requested resource (get pods dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1)
Sep 16 23:59:32.235: INFO: Unable to read jessie_tcp@dns-test-service.dns-4544.svc from pod dns-4544/dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1: the server could not find the requested resource (get pods dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1)
Sep 16 23:59:32.238: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4544.svc from pod dns-4544/dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1: the server could not find the requested resource (get pods dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1)
Sep 16 23:59:32.241: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4544.svc from pod dns-4544/dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1: the server could not find the requested resource (get pods dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1)
Sep 16 23:59:32.248: INFO: Unable to read jessie_udp@PodARecord from pod dns-4544/dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1: the server could not find the requested resource (get pods dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1)
Sep 16 23:59:32.251: INFO: Unable to read jessie_tcp@PodARecord from pod dns-4544/dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1: the server could not find the requested resource (get pods dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1)
Sep 16 23:59:32.257: INFO: Lookups using dns-4544/dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4544 wheezy_tcp@dns-test-service.dns-4544 wheezy_udp@dns-test-service.dns-4544.svc wheezy_tcp@dns-test-service.dns-4544.svc wheezy_udp@_http._tcp.dns-test-service.dns-4544.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4544.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4544 jessie_tcp@dns-test-service.dns-4544 jessie_udp@dns-test-service.dns-4544.svc jessie_tcp@dns-test-service.dns-4544.svc jessie_udp@_http._tcp.dns-test-service.dns-4544.svc jessie_tcp@_http._tcp.dns-test-service.dns-4544.svc jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep 16 23:59:37.333: INFO: DNS probes using dns-4544/dns-test-a9245356-4c9d-421e-a77b-e124ed6d12d1 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:59:37.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4544" for this suite.

• [SLOW TEST:9.413 seconds]
[sig-network] DNS
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":277,"completed":159,"skipped":2739,"failed":0}
SSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:59:37.471: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating pod
Sep 16 23:59:41.557: INFO: Pod pod-hostip-0f9047c1-6c6b-471c-b45a-154506a56f9f has hostIP: 10.9.140.106
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:59:41.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5721" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":277,"completed":160,"skipped":2742,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:59:41.568: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name projected-secret-test-3bcc053b-2e5f-498f-9426-4c617f9be8cc
STEP: Creating a pod to test consume secrets
Sep 16 23:59:41.652: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-94236bff-46a9-4ed6-8625-2ee89c0b4e51" in namespace "projected-1215" to be "Succeeded or Failed"
Sep 16 23:59:41.654: INFO: Pod "pod-projected-secrets-94236bff-46a9-4ed6-8625-2ee89c0b4e51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.217939ms
Sep 16 23:59:43.657: INFO: Pod "pod-projected-secrets-94236bff-46a9-4ed6-8625-2ee89c0b4e51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005022074s
STEP: Saw pod success
Sep 16 23:59:43.657: INFO: Pod "pod-projected-secrets-94236bff-46a9-4ed6-8625-2ee89c0b4e51" satisfied condition "Succeeded or Failed"
Sep 16 23:59:43.664: INFO: Trying to get logs from node eqx03-flash06 pod pod-projected-secrets-94236bff-46a9-4ed6-8625-2ee89c0b4e51 container secret-volume-test: <nil>
STEP: delete the pod
Sep 16 23:59:43.703: INFO: Waiting for pod pod-projected-secrets-94236bff-46a9-4ed6-8625-2ee89c0b4e51 to disappear
Sep 16 23:59:43.705: INFO: Pod pod-projected-secrets-94236bff-46a9-4ed6-8625-2ee89c0b4e51 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:59:43.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1215" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":277,"completed":161,"skipped":2783,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:59:43.717: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1418
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 16 23:59:43.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-538'
Sep 16 23:59:43.883: INFO: stderr: ""
Sep 16 23:59:43.883: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1423
Sep 16 23:59:43.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 delete pods e2e-test-httpd-pod --namespace=kubectl-538'
Sep 16 23:59:50.976: INFO: stderr: ""
Sep 16 23:59:50.976: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 16 23:59:50.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-538" for this suite.

• [SLOW TEST:7.271 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":277,"completed":162,"skipped":2787,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 16 23:59:50.990: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod busybox-416a52d4-cc61-4703-86dc-40b56e3cca68 in namespace container-probe-7520
Sep 16 23:59:55.047: INFO: Started pod busybox-416a52d4-cc61-4703-86dc-40b56e3cca68 in namespace container-probe-7520
STEP: checking the pod's current state and verifying that restartCount is present
Sep 16 23:59:55.050: INFO: Initial restart count of pod busybox-416a52d4-cc61-4703-86dc-40b56e3cca68 is 0
Sep 17 00:00:47.148: INFO: Restart count of pod container-probe-7520/busybox-416a52d4-cc61-4703-86dc-40b56e3cca68 is now 1 (52.098613798s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:00:47.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7520" for this suite.

• [SLOW TEST:56.208 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":277,"completed":163,"skipped":2833,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:00:47.198: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 17 00:00:47.243: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:00:48.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6205" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":277,"completed":164,"skipped":2851,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:00:48.293: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-map-34594cdd-678e-41b9-bb15-d13dec27b24c
STEP: Creating a pod to test consume configMaps
Sep 17 00:00:48.370: INFO: Waiting up to 5m0s for pod "pod-configmaps-389be3e3-886d-45fd-a52d-941cc5dbeb23" in namespace "configmap-1392" to be "Succeeded or Failed"
Sep 17 00:00:48.372: INFO: Pod "pod-configmaps-389be3e3-886d-45fd-a52d-941cc5dbeb23": Phase="Pending", Reason="", readiness=false. Elapsed: 1.893143ms
Sep 17 00:00:50.375: INFO: Pod "pod-configmaps-389be3e3-886d-45fd-a52d-941cc5dbeb23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004828523s
Sep 17 00:00:52.377: INFO: Pod "pod-configmaps-389be3e3-886d-45fd-a52d-941cc5dbeb23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00755707s
STEP: Saw pod success
Sep 17 00:00:52.377: INFO: Pod "pod-configmaps-389be3e3-886d-45fd-a52d-941cc5dbeb23" satisfied condition "Succeeded or Failed"
Sep 17 00:00:52.379: INFO: Trying to get logs from node eqx03-flash06 pod pod-configmaps-389be3e3-886d-45fd-a52d-941cc5dbeb23 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 17 00:00:52.420: INFO: Waiting for pod pod-configmaps-389be3e3-886d-45fd-a52d-941cc5dbeb23 to disappear
Sep 17 00:00:52.422: INFO: Pod pod-configmaps-389be3e3-886d-45fd-a52d-941cc5dbeb23 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:00:52.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1392" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":165,"skipped":2870,"failed":0}
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:00:52.433: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Sep 17 00:00:52.493: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:00:58.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-297" for this suite.

• [SLOW TEST:5.944 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":277,"completed":166,"skipped":2875,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:00:58.378: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 17 00:00:58.447: INFO: Waiting up to 5m0s for pod "downwardapi-volume-882b82ab-ab9e-42d5-bd76-ab34344e44bd" in namespace "projected-1562" to be "Succeeded or Failed"
Sep 17 00:00:58.449: INFO: Pod "downwardapi-volume-882b82ab-ab9e-42d5-bd76-ab34344e44bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.863909ms
Sep 17 00:01:00.452: INFO: Pod "downwardapi-volume-882b82ab-ab9e-42d5-bd76-ab34344e44bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004945966s
Sep 17 00:01:02.456: INFO: Pod "downwardapi-volume-882b82ab-ab9e-42d5-bd76-ab34344e44bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008356217s
STEP: Saw pod success
Sep 17 00:01:02.456: INFO: Pod "downwardapi-volume-882b82ab-ab9e-42d5-bd76-ab34344e44bd" satisfied condition "Succeeded or Failed"
Sep 17 00:01:02.458: INFO: Trying to get logs from node eqx03-flash06 pod downwardapi-volume-882b82ab-ab9e-42d5-bd76-ab34344e44bd container client-container: <nil>
STEP: delete the pod
Sep 17 00:01:02.527: INFO: Waiting for pod downwardapi-volume-882b82ab-ab9e-42d5-bd76-ab34344e44bd to disappear
Sep 17 00:01:02.529: INFO: Pod downwardapi-volume-882b82ab-ab9e-42d5-bd76-ab34344e44bd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:01:02.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1562" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":277,"completed":167,"skipped":2877,"failed":0}
SSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:01:02.541: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Sep 17 00:01:02.590: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:01:07.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6607" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":277,"completed":168,"skipped":2883,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:01:07.307: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:01:07.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-353" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":277,"completed":169,"skipped":2905,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:01:07.386: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep 17 00:01:07.456: INFO: Waiting up to 5m0s for pod "pod-7b4c9091-968d-44f5-b5de-cfdb1b6d0501" in namespace "emptydir-7177" to be "Succeeded or Failed"
Sep 17 00:01:07.459: INFO: Pod "pod-7b4c9091-968d-44f5-b5de-cfdb1b6d0501": Phase="Pending", Reason="", readiness=false. Elapsed: 2.126508ms
Sep 17 00:01:09.464: INFO: Pod "pod-7b4c9091-968d-44f5-b5de-cfdb1b6d0501": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007347445s
Sep 17 00:01:11.467: INFO: Pod "pod-7b4c9091-968d-44f5-b5de-cfdb1b6d0501": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010096552s
STEP: Saw pod success
Sep 17 00:01:11.467: INFO: Pod "pod-7b4c9091-968d-44f5-b5de-cfdb1b6d0501" satisfied condition "Succeeded or Failed"
Sep 17 00:01:11.469: INFO: Trying to get logs from node eqx03-flash06 pod pod-7b4c9091-968d-44f5-b5de-cfdb1b6d0501 container test-container: <nil>
STEP: delete the pod
Sep 17 00:01:11.505: INFO: Waiting for pod pod-7b4c9091-968d-44f5-b5de-cfdb1b6d0501 to disappear
Sep 17 00:01:11.507: INFO: Pod pod-7b4c9091-968d-44f5-b5de-cfdb1b6d0501 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:01:11.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7177" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":170,"skipped":2914,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:01:11.518: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-d0bd078a-bff8-4c4b-94d8-b6546c4a12bd
STEP: Creating a pod to test consume configMaps
Sep 17 00:01:11.587: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b1565e03-ec96-4621-9c80-f780b0f8004d" in namespace "projected-261" to be "Succeeded or Failed"
Sep 17 00:01:11.590: INFO: Pod "pod-projected-configmaps-b1565e03-ec96-4621-9c80-f780b0f8004d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.093525ms
Sep 17 00:01:13.593: INFO: Pod "pod-projected-configmaps-b1565e03-ec96-4621-9c80-f780b0f8004d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005191178s
Sep 17 00:01:15.596: INFO: Pod "pod-projected-configmaps-b1565e03-ec96-4621-9c80-f780b0f8004d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008104835s
STEP: Saw pod success
Sep 17 00:01:15.596: INFO: Pod "pod-projected-configmaps-b1565e03-ec96-4621-9c80-f780b0f8004d" satisfied condition "Succeeded or Failed"
Sep 17 00:01:15.598: INFO: Trying to get logs from node eqx03-flash06 pod pod-projected-configmaps-b1565e03-ec96-4621-9c80-f780b0f8004d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 17 00:01:15.677: INFO: Waiting for pod pod-projected-configmaps-b1565e03-ec96-4621-9c80-f780b0f8004d to disappear
Sep 17 00:01:15.679: INFO: Pod pod-projected-configmaps-b1565e03-ec96-4621-9c80-f780b0f8004d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:01:15.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-261" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":277,"completed":171,"skipped":2916,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:01:15.689: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Update Demo
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:271
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a replication controller
Sep 17 00:01:15.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 create -f - --namespace=kubectl-927'
Sep 17 00:01:16.105: INFO: stderr: ""
Sep 17 00:01:16.105: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 17 00:01:16.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-927'
Sep 17 00:01:16.207: INFO: stderr: ""
Sep 17 00:01:16.207: INFO: stdout: "update-demo-nautilus-6jv76 update-demo-nautilus-79c4b "
Sep 17 00:01:16.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pods update-demo-nautilus-6jv76 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-927'
Sep 17 00:01:16.302: INFO: stderr: ""
Sep 17 00:01:16.302: INFO: stdout: ""
Sep 17 00:01:16.302: INFO: update-demo-nautilus-6jv76 is created but not running
Sep 17 00:01:21.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-927'
Sep 17 00:01:21.399: INFO: stderr: ""
Sep 17 00:01:21.399: INFO: stdout: "update-demo-nautilus-6jv76 update-demo-nautilus-79c4b "
Sep 17 00:01:21.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pods update-demo-nautilus-6jv76 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-927'
Sep 17 00:01:21.495: INFO: stderr: ""
Sep 17 00:01:21.495: INFO: stdout: "true"
Sep 17 00:01:21.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pods update-demo-nautilus-6jv76 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-927'
Sep 17 00:01:21.584: INFO: stderr: ""
Sep 17 00:01:21.584: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 17 00:01:21.584: INFO: validating pod update-demo-nautilus-6jv76
Sep 17 00:01:21.589: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 17 00:01:21.589: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 17 00:01:21.589: INFO: update-demo-nautilus-6jv76 is verified up and running
Sep 17 00:01:21.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pods update-demo-nautilus-79c4b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-927'
Sep 17 00:01:21.686: INFO: stderr: ""
Sep 17 00:01:21.686: INFO: stdout: "true"
Sep 17 00:01:21.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pods update-demo-nautilus-79c4b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-927'
Sep 17 00:01:21.781: INFO: stderr: ""
Sep 17 00:01:21.781: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 17 00:01:21.781: INFO: validating pod update-demo-nautilus-79c4b
Sep 17 00:01:21.785: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 17 00:01:21.785: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 17 00:01:21.785: INFO: update-demo-nautilus-79c4b is verified up and running
STEP: using delete to clean up resources
Sep 17 00:01:21.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 delete --grace-period=0 --force -f - --namespace=kubectl-927'
Sep 17 00:01:21.888: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 17 00:01:21.888: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep 17 00:01:21.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-927'
Sep 17 00:01:21.984: INFO: stderr: "No resources found in kubectl-927 namespace.\n"
Sep 17 00:01:21.984: INFO: stdout: ""
Sep 17 00:01:21.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pods -l name=update-demo --namespace=kubectl-927 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 17 00:01:22.088: INFO: stderr: ""
Sep 17 00:01:22.088: INFO: stdout: "update-demo-nautilus-6jv76\nupdate-demo-nautilus-79c4b\n"
Sep 17 00:01:22.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-927'
Sep 17 00:01:22.688: INFO: stderr: "No resources found in kubectl-927 namespace.\n"
Sep 17 00:01:22.688: INFO: stdout: ""
Sep 17 00:01:22.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pods -l name=update-demo --namespace=kubectl-927 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 17 00:01:22.788: INFO: stderr: ""
Sep 17 00:01:22.788: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:01:22.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-927" for this suite.

• [SLOW TEST:7.119 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:269
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":277,"completed":172,"skipped":2925,"failed":0}
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:01:22.809: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Sep 17 00:01:22.873: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 17 00:01:22.883: INFO: Waiting for terminating namespaces to be deleted...
Sep 17 00:01:22.885: INFO: 
Logging pods the kubelet thinks is on node eqx03-flash06 before test
Sep 17 00:01:22.900: INFO: kube-scheduler-eqx03-flash06 from kube-system started at 2020-09-04 20:35:41 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.900: INFO: 	Container kube-scheduler ready: true, restart count 0
Sep 17 00:01:22.900: INFO: kube-proxy-mchgt from kube-system started at 2020-09-04 20:35:41 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.900: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 17 00:01:22.900: INFO: csi-snapshotter-robin-0 from robinio started at 2020-09-16 23:34:41 +0000 UTC (2 container statuses recorded)
Sep 17 00:01:22.900: INFO: 	Container csi-snapshotter ready: true, restart count 0
Sep 17 00:01:22.900: INFO: 	Container robin ready: true, restart count 0
Sep 17 00:01:22.900: INFO: sonobuoy-e2e-job-362cab5d438b4e90 from sonobuoy started at 2020-09-16 23:06:47 +0000 UTC (2 container statuses recorded)
Sep 17 00:01:22.900: INFO: 	Container e2e ready: true, restart count 0
Sep 17 00:01:22.900: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 17 00:01:22.900: INFO: sonobuoy-systemd-logs-daemon-set-71797a053a4d44ec-pp8g4 from sonobuoy started at 2020-09-16 23:06:47 +0000 UTC (2 container statuses recorded)
Sep 17 00:01:22.900: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 17 00:01:22.900: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 17 00:01:22.900: INFO: etcd-eqx03-flash06 from kube-system started at 2020-09-04 20:35:41 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.900: INFO: 	Container etcd ready: true, restart count 0
Sep 17 00:01:22.900: INFO: calico-node-zqtk2 from kube-system started at 2020-09-09 19:40:56 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.900: INFO: 	Container calico-node ready: true, restart count 0
Sep 17 00:01:22.900: INFO: update-demo-nautilus-6jv76 from kubectl-927 started at 2020-09-17 00:01:16 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.900: INFO: 	Container update-demo ready: true, restart count 0
Sep 17 00:01:22.900: INFO: kube-apiserver-eqx03-flash06 from kube-system started at 2020-09-04 20:36:01 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.900: INFO: 	Container kube-apiserver ready: true, restart count 0
Sep 17 00:01:22.900: INFO: csi-nodeplugin-robin-rnhf4 from robinio started at 2020-09-16 23:34:31 +0000 UTC (2 container statuses recorded)
Sep 17 00:01:22.900: INFO: 	Container driver-registrar ready: true, restart count 0
Sep 17 00:01:22.900: INFO: 	Container robin ready: true, restart count 0
Sep 17 00:01:22.900: INFO: kube-controller-manager-eqx03-flash06 from kube-system started at 2020-09-04 20:36:01 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.900: INFO: 	Container kube-controller-manager ready: true, restart count 1
Sep 17 00:01:22.900: INFO: kube-sriov-device-plugin-amd64-29hgs from kube-system started at 2020-09-16 23:34:31 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.900: INFO: 	Container kube-sriovdp ready: true, restart count 0
Sep 17 00:01:22.900: INFO: kube-multus-ds-amd64-wjsv5 from kube-system started at 2020-09-16 23:34:31 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.900: INFO: 	Container kube-multus ready: true, restart count 0
Sep 17 00:01:22.900: INFO: robin-master-hr288 from robinio started at 2020-09-16 23:34:41 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.900: INFO: 	Container robinrcm ready: true, restart count 0
Sep 17 00:01:22.900: INFO: update-demo-nautilus-79c4b from kubectl-927 started at 2020-09-17 00:01:16 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.900: INFO: 	Container update-demo ready: true, restart count 0
Sep 17 00:01:22.900: INFO: 
Logging pods the kubelet thinks is on node eqx03-flash07 before test
Sep 17 00:01:22.942: INFO: hari-elasticsearch-client-c59586c8-k84fl from spk started at 2020-09-05 06:37:23 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:01:22.942: INFO: neo-neo4j-core-0 from ripul started at 2020-09-16 23:34:03 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 17 00:01:22.942: INFO: snapshot-controller-0 from robinio started at 2020-09-09 19:56:15 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container snapshot-controller ready: true, restart count 0
Sep 17 00:01:22.942: INFO: kube-controller-manager-eqx03-flash07 from kube-system started at 2020-09-04 20:24:11 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container kube-controller-manager ready: true, restart count 1
Sep 17 00:01:22.942: INFO: test1-logstash-0 from t001-u000003 started at 2020-09-05 06:42:09 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container logstash ready: true, restart count 0
Sep 17 00:01:22.942: INFO: csi-nodeplugin-robin-bcq5b from robinio started at 2020-09-09 19:56:04 +0000 UTC (2 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container driver-registrar ready: true, restart count 0
Sep 17 00:01:22.942: INFO: 	Container robin ready: true, restart count 0
Sep 17 00:01:22.942: INFO: neo-neo4j-core-1 from default started at 2020-09-16 23:34:17 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 17 00:01:22.942: INFO: hari-logstash-0 from spk started at 2020-09-05 06:37:24 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container logstash ready: true, restart count 0
Sep 17 00:01:22.942: INFO: test1-kibana-84f6c46bf6-mndsz from t001-u000003 started at 2020-09-05 06:42:07 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container kibana ready: true, restart count 0
Sep 17 00:01:22.942: INFO: neo-neo4j-core-2 from default started at 2020-09-16 23:34:00 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 17 00:01:22.942: INFO: elkname-elasticsearch-data-0 from t001-u000003 started at 2020-09-05 06:08:19 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:01:22.942: INFO: neo-neo4j-core-2 from ripul started at 2020-09-16 23:33:57 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 17 00:01:22.942: INFO: mg-runner-gitlab-runner-97dd45f4-8z4zg from gitlab-runner started at 2020-09-10 19:25:18 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container mg-runner-gitlab-runner ready: false, restart count 1150
Sep 17 00:01:22.942: INFO: neo-neo4j-core-1 from madhura started at 2020-09-16 23:33:58 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 17 00:01:22.942: INFO: neo-neo4j-core-0 from default started at 2020-09-16 23:34:12 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 17 00:01:22.942: INFO: kube-scheduler-eqx03-flash07 from kube-system started at 2020-09-04 20:24:11 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container kube-scheduler ready: true, restart count 0
Sep 17 00:01:22.942: INFO: cert-manager-cainjector-5ffff9dd7c-7fv5l from cert-manager started at 2020-09-04 20:38:07 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container cert-manager ready: true, restart count 0
Sep 17 00:01:22.942: INFO: calico-node-8t2kf from kube-system started at 2020-09-09 19:41:23 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container calico-node ready: true, restart count 0
Sep 17 00:01:22.942: INFO: hari-elasticsearch-data-0 from spk started at 2020-09-05 06:37:25 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:01:22.942: INFO: test1-elasticsearch-client-df46b4cd8-2knzt from t001-u000003 started at 2020-09-05 06:42:07 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:01:22.942: INFO: kube-sriov-device-plugin-amd64-rxs7g from kube-system started at 2020-09-04 20:25:01 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container kube-sriovdp ready: true, restart count 0
Sep 17 00:01:22.942: INFO: cert-manager-578cd6d964-sq59x from cert-manager started at 2020-09-04 20:38:07 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container cert-manager ready: true, restart count 0
Sep 17 00:01:22.942: INFO: elkname-kibana-56c986874d-nx4t6 from t001-u000003 started at 2020-09-05 06:08:19 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container kibana ready: true, restart count 0
Sep 17 00:01:22.942: INFO: test1-elasticsearch-data-0 from t001-u000003 started at 2020-09-05 06:42:08 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:01:22.942: INFO: sonobuoy from sonobuoy started at 2020-09-16 23:06:40 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 17 00:01:22.942: INFO: neo-neo4j-core-1 from ripul started at 2020-09-16 23:34:02 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 17 00:01:22.942: INFO: kube-proxy-9zgnq from kube-system started at 2020-09-04 20:24:11 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 17 00:01:22.942: INFO: etcd-eqx03-flash07 from kube-system started at 2020-09-04 20:24:37 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container etcd ready: true, restart count 0
Sep 17 00:01:22.942: INFO: csi-resizer-robin-7d566f9df9-sq477 from robinio started at 2020-09-09 19:56:05 +0000 UTC (2 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container csi-resizer ready: true, restart count 0
Sep 17 00:01:22.942: INFO: 	Container robin ready: true, restart count 0
Sep 17 00:01:22.942: INFO: sonobuoy-systemd-logs-daemon-set-71797a053a4d44ec-4zr6m from sonobuoy started at 2020-09-16 23:06:47 +0000 UTC (2 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 17 00:01:22.942: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 17 00:01:22.942: INFO: neo-neo4j-replica-0 from ripul started at 2020-09-16 23:34:01 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container neo4j ready: true, restart count 0
Sep 17 00:01:22.942: INFO: elkname-logstash-0 from t001-u000003 started at 2020-09-05 06:08:19 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container logstash ready: true, restart count 0
Sep 17 00:01:22.942: INFO: hari-kibana-b6f7d64-j96s2 from spk started at 2020-09-05 06:37:23 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container kibana ready: true, restart count 0
Sep 17 00:01:22.942: INFO: coredns-6878cb8f64-7rgtp from kube-system started at 2020-09-04 20:25:00 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container coredns ready: true, restart count 0
Sep 17 00:01:22.942: INFO: csi-provisioner-robin-6646bdd46b-kl2ft from robinio started at 2020-09-16 23:31:18 +0000 UTC (2 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container csi-provisioner ready: true, restart count 0
Sep 17 00:01:22.942: INFO: 	Container robin ready: true, restart count 0
Sep 17 00:01:22.942: INFO: robin-master-w2m7f from robinio started at 2020-09-09 19:47:17 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.942: INFO: 	Container robinrcm ready: true, restart count 0
Sep 17 00:01:22.942: INFO: cert-manager-webhook-556b9d7dfd-swlll from cert-manager started at 2020-09-04 20:38:07 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.943: INFO: 	Container cert-manager ready: true, restart count 0
Sep 17 00:01:22.943: INFO: elkname-elasticsearch-client-6768458985-6sgxz from t001-u000003 started at 2020-09-05 06:08:19 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.943: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:01:22.943: INFO: elkname-elasticsearch-master-0 from t001-u000003 started at 2020-09-05 06:08:19 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.943: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:01:22.943: INFO: kube-multus-ds-amd64-lf8vr from kube-system started at 2020-09-04 20:24:11 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.943: INFO: 	Container kube-multus ready: true, restart count 0
Sep 17 00:01:22.943: INFO: kube-apiserver-eqx03-flash07 from kube-system started at 2020-09-04 20:24:11 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.943: INFO: 	Container kube-apiserver ready: true, restart count 0
Sep 17 00:01:22.943: INFO: csi-attacher-robin-5d956884cf-nqxfg from robinio started at 2020-09-16 23:31:18 +0000 UTC (2 container statuses recorded)
Sep 17 00:01:22.943: INFO: 	Container csi-attacher ready: true, restart count 0
Sep 17 00:01:22.943: INFO: 	Container robin ready: true, restart count 0
Sep 17 00:01:22.943: INFO: neo-neo4j-replica-0 from madhura started at 2020-09-16 23:33:58 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.943: INFO: 	Container neo4j ready: true, restart count 0
Sep 17 00:01:22.943: INFO: 
Logging pods the kubelet thinks is on node eqx04-flash04 before test
Sep 17 00:01:22.966: INFO: test1-elasticsearch-data-1 from t001-u000003 started at 2020-09-05 06:44:40 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.966: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:01:22.966: INFO: calico-node-wpq99 from kube-system started at 2020-09-09 19:41:33 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.966: INFO: 	Container calico-node ready: true, restart count 0
Sep 17 00:01:22.966: INFO: kube-scheduler-eqx04-flash04 from kube-system started at 2020-09-04 20:12:22 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.966: INFO: 	Container kube-scheduler ready: true, restart count 2
Sep 17 00:01:22.966: INFO: calico-kube-controllers-6c49f88586-jzdjm from kube-system started at 2020-09-04 20:12:28 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.966: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep 17 00:01:22.966: INFO: kube-sriov-device-plugin-amd64-xlvxl from kube-system started at 2020-09-04 20:12:30 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.966: INFO: 	Container kube-sriovdp ready: true, restart count 0
Sep 17 00:01:22.966: INFO: neo-neo4j-core-2 from madhura started at 2020-09-16 23:34:01 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.966: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 17 00:01:22.966: INFO: kube-apiserver-eqx04-flash04 from kube-system started at 2020-09-04 20:12:22 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.966: INFO: 	Container kube-apiserver ready: true, restart count 0
Sep 17 00:01:22.966: INFO: robin-master-hkj7r from robinio started at 2020-09-09 19:48:36 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.966: INFO: 	Container robinrcm ready: true, restart count 0
Sep 17 00:01:22.966: INFO: kube-controller-manager-eqx04-flash04 from kube-system started at 2020-09-04 20:12:22 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.966: INFO: 	Container kube-controller-manager ready: true, restart count 1
Sep 17 00:01:22.966: INFO: kube-multus-ds-amd64-mzx44 from kube-system started at 2020-09-04 20:12:27 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.966: INFO: 	Container kube-multus ready: true, restart count 0
Sep 17 00:01:22.966: INFO: sonobuoy-systemd-logs-daemon-set-71797a053a4d44ec-dx2j7 from sonobuoy started at 2020-09-16 23:06:47 +0000 UTC (2 container statuses recorded)
Sep 17 00:01:22.966: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 17 00:01:22.966: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 17 00:01:22.966: INFO: elkname-elasticsearch-master-2 from t001-u000003 started at 2020-09-05 06:09:50 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.966: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:01:22.966: INFO: hari-elasticsearch-master-0 from spk started at 2020-09-05 06:37:24 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.966: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:01:22.966: INFO: csi-nodeplugin-robin-6xzkh from robinio started at 2020-09-09 19:56:04 +0000 UTC (2 container statuses recorded)
Sep 17 00:01:22.966: INFO: 	Container driver-registrar ready: true, restart count 0
Sep 17 00:01:22.966: INFO: 	Container robin ready: true, restart count 0
Sep 17 00:01:22.966: INFO: neo-neo4j-replica-0 from default started at 2020-09-16 23:34:03 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.966: INFO: 	Container neo4j ready: true, restart count 0
Sep 17 00:01:22.966: INFO: kube-proxy-2mmk7 from kube-system started at 2020-09-04 20:12:22 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.966: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 17 00:01:22.966: INFO: etcd-eqx04-flash04 from kube-system started at 2020-09-04 20:12:22 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.966: INFO: 	Container etcd ready: true, restart count 0
Sep 17 00:01:22.966: INFO: coredns-6878cb8f64-n75hx from kube-system started at 2020-09-04 20:12:55 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.966: INFO: 	Container coredns ready: true, restart count 0
Sep 17 00:01:22.966: INFO: neo-neo4j-core-0 from madhura started at 2020-09-16 23:34:02 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.966: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 17 00:01:22.966: INFO: 
Logging pods the kubelet thinks is on node eqx04-flash06 before test
Sep 17 00:01:22.992: INFO: kube-proxy-n5ckt from kube-system started at 2020-09-04 20:34:34 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.992: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 17 00:01:22.992: INFO: robin-worker-6zwcm from robinio started at 2020-09-09 19:53:16 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.992: INFO: 	Container robinrcm ready: true, restart count 0
Sep 17 00:01:22.992: INFO: kube-sriov-device-plugin-amd64-wwdl2 from kube-system started at 2020-09-04 20:34:39 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.992: INFO: 	Container kube-sriovdp ready: true, restart count 0
Sep 17 00:01:22.992: INFO: hari-elasticsearch-data-1 from spk started at 2020-09-05 06:40:02 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.992: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:01:22.992: INFO: elkname-elasticsearch-master-1 from t001-u000003 started at 2020-09-05 06:09:25 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.992: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:01:22.992: INFO: test1-elasticsearch-master-1 from t001-u000003 started at 2020-09-05 06:43:08 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.992: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:01:22.992: INFO: test1-elasticsearch-master-2 from t001-u000003 started at 2020-09-05 06:43:44 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.992: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:01:22.992: INFO: calico-node-x8tqn from kube-system started at 2020-09-09 19:41:10 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.992: INFO: 	Container calico-node ready: true, restart count 0
Sep 17 00:01:22.992: INFO: kube-multus-ds-amd64-j9jmp from kube-system started at 2020-09-04 20:34:34 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.992: INFO: 	Container kube-multus ready: true, restart count 0
Sep 17 00:01:22.992: INFO: elkname-elasticsearch-data-1 from t001-u000003 started at 2020-09-05 06:10:51 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.992: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:01:22.992: INFO: test1-elasticsearch-master-0 from t001-u000003 started at 2020-09-05 06:42:08 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.992: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:01:22.992: INFO: csi-nodeplugin-robin-wk7pp from robinio started at 2020-09-09 19:56:04 +0000 UTC (2 container statuses recorded)
Sep 17 00:01:22.992: INFO: 	Container driver-registrar ready: true, restart count 0
Sep 17 00:01:22.992: INFO: 	Container robin ready: true, restart count 0
Sep 17 00:01:22.992: INFO: sonobuoy-systemd-logs-daemon-set-71797a053a4d44ec-w59x2 from sonobuoy started at 2020-09-16 23:06:47 +0000 UTC (2 container statuses recorded)
Sep 17 00:01:22.992: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 17 00:01:22.992: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 17 00:01:22.992: INFO: gitlab-runner-gitlab-runner-69dbb57dd4-sptqz from gitlab-runner started at 2020-09-10 19:12:45 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.992: INFO: 	Container gitlab-runner-gitlab-runner ready: false, restart count 0
Sep 17 00:01:22.992: INFO: hari-elasticsearch-client-c59586c8-xvm2k from spk started at 2020-09-05 06:37:23 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.992: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:01:22.992: INFO: hari-elasticsearch-master-1 from spk started at 2020-09-05 06:38:21 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.992: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:01:22.992: INFO: elkname-elasticsearch-client-6768458985-j2fnt from t001-u000003 started at 2020-09-05 06:08:19 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.992: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:01:22.992: INFO: hari-elasticsearch-master-2 from spk started at 2020-09-05 06:38:48 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.992: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:01:22.992: INFO: test1-elasticsearch-client-df46b4cd8-52bhb from t001-u000003 started at 2020-09-05 06:42:07 +0000 UTC (1 container statuses recorded)
Sep 17 00:01:22.992: INFO: 	Container elasticsearch ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-66d2ebd4-9031-41eb-81bc-3a5819977645 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-66d2ebd4-9031-41eb-81bc-3a5819977645 off the node eqx03-flash06
STEP: verifying the node doesn't have the label kubernetes.io/e2e-66d2ebd4-9031-41eb-81bc-3a5819977645
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:01:31.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5142" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

• [SLOW TEST:8.316 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":277,"completed":173,"skipped":2930,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:01:31.125: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:01:31.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9539" for this suite.
STEP: Destroying namespace "nspatchtest-3ab5dbb1-436e-4c10-8a17-63c0e576a8d5-2438" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":277,"completed":174,"skipped":2961,"failed":0}
SSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:01:31.261: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Sep 17 00:01:31.330: INFO: Waiting up to 5m0s for pod "downward-api-d3a895df-efb9-4f39-91b8-66b399fbe626" in namespace "downward-api-8422" to be "Succeeded or Failed"
Sep 17 00:01:31.332: INFO: Pod "downward-api-d3a895df-efb9-4f39-91b8-66b399fbe626": Phase="Pending", Reason="", readiness=false. Elapsed: 2.289479ms
Sep 17 00:01:33.340: INFO: Pod "downward-api-d3a895df-efb9-4f39-91b8-66b399fbe626": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009544791s
Sep 17 00:01:35.343: INFO: Pod "downward-api-d3a895df-efb9-4f39-91b8-66b399fbe626": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012836094s
STEP: Saw pod success
Sep 17 00:01:35.343: INFO: Pod "downward-api-d3a895df-efb9-4f39-91b8-66b399fbe626" satisfied condition "Succeeded or Failed"
Sep 17 00:01:35.345: INFO: Trying to get logs from node eqx03-flash06 pod downward-api-d3a895df-efb9-4f39-91b8-66b399fbe626 container dapi-container: <nil>
STEP: delete the pod
Sep 17 00:01:35.401: INFO: Waiting for pod downward-api-d3a895df-efb9-4f39-91b8-66b399fbe626 to disappear
Sep 17 00:01:35.403: INFO: Pod downward-api-d3a895df-efb9-4f39-91b8-66b399fbe626 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:01:35.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8422" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":277,"completed":175,"skipped":2964,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:01:35.415: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 17 00:01:36.204: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 17 00:01:38.211: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735897696, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735897696, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735897696, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735897696, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 17 00:01:41.231: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Sep 17 00:01:41.254: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:01:41.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3205" for this suite.
STEP: Destroying namespace "webhook-3205-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.947 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":277,"completed":176,"skipped":2982,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:01:41.362: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 17 00:01:41.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 create -f - --namespace=kubectl-8258'
Sep 17 00:01:41.745: INFO: stderr: ""
Sep 17 00:01:41.745: INFO: stdout: "replicationcontroller/agnhost-master created\n"
Sep 17 00:01:41.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 create -f - --namespace=kubectl-8258'
Sep 17 00:01:42.114: INFO: stderr: ""
Sep 17 00:01:42.114: INFO: stdout: "service/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Sep 17 00:01:43.118: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 17 00:01:43.118: INFO: Found 0 / 1
Sep 17 00:01:44.118: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 17 00:01:44.118: INFO: Found 0 / 1
Sep 17 00:01:45.118: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 17 00:01:45.118: INFO: Found 1 / 1
Sep 17 00:01:45.118: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 17 00:01:45.121: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 17 00:01:45.121: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 17 00:01:45.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 describe pod agnhost-master-jfq4h --namespace=kubectl-8258'
Sep 17 00:01:45.242: INFO: stderr: ""
Sep 17 00:01:45.242: INFO: stdout: "Name:         agnhost-master-jfq4h\nNamespace:    kubectl-8258\nPriority:     0\nNode:         eqx03-flash06/10.9.140.106\nStart Time:   Thu, 17 Sep 2020 00:01:41 +0000\nLabels:       app=agnhost\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 172.21.4.78/32\n              cni.projectcalico.org/podIPs: 172.21.4.78/32\n              k8s.v1.cni.cncf.io/network-status:\n                [{\n                    \"name\": \"calico\",\n                    \"ips\": [\n                        \"172.21.4.78\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\n              k8s.v1.cni.cncf.io/networks-status:\n                [{\n                    \"name\": \"calico\",\n                    \"ips\": [\n                        \"172.21.4.78\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\nStatus:       Running\nIP:           172.21.4.78\nIPs:\n  IP:           172.21.4.78\nControlled By:  ReplicationController/agnhost-master\nContainers:\n  agnhost-master:\n    Container ID:   robin://11a36a6e659556139807244337efe0ad5f99fcf306747906116a0de5382facf1\n    Image:          us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12\n    Image ID:       docker-pullable://us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 17 Sep 2020 00:01:43 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-22c99 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-22c99:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-22c99\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From                    Message\n  ----    ------     ----       ----                    -------\n  Normal  Scheduled  <unknown>  default-scheduler       Successfully assigned kubectl-8258/agnhost-master-jfq4h to eqx03-flash06\n  Normal  Pulled     2s         kubelet, eqx03-flash06  Container image \"us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12\" already present on machine\n  Normal  Created    2s         kubelet, eqx03-flash06  Created container agnhost-master\n  Normal  Started    2s         kubelet, eqx03-flash06  Started container agnhost-master\n"
Sep 17 00:01:45.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 describe rc agnhost-master --namespace=kubectl-8258'
Sep 17 00:01:45.360: INFO: stderr: ""
Sep 17 00:01:45.360: INFO: stdout: "Name:         agnhost-master\nNamespace:    kubectl-8258\nSelector:     app=agnhost,role=master\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=master\n  Containers:\n   agnhost-master:\n    Image:        us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-master-jfq4h\n"
Sep 17 00:01:45.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 describe service agnhost-master --namespace=kubectl-8258'
Sep 17 00:01:45.469: INFO: stderr: ""
Sep 17 00:01:45.469: INFO: stdout: "Name:              agnhost-master\nNamespace:         kubectl-8258\nLabels:            app=agnhost\n                   role=master\nAnnotations:       <none>\nSelector:          app=agnhost,role=master\nType:              ClusterIP\nIP:                172.19.205.112\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.21.4.78:6379\nSession Affinity:  None\nEvents:            <none>\n"
Sep 17 00:01:45.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 describe node eqx03-flash06'
Sep 17 00:01:45.622: INFO: stderr: ""
Sep 17 00:01:45.623: INFO: stdout: "Name:               eqx03-flash06\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=eqx03-flash06\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    robin.io/domain=ROBIN\n                    robin.io/nodetype=robin-node\n                    robin.io/rnodetype=robin-master-node\n                    robin.io/robinhost=eqx03-flash06\n                    robin.io/robinrpool=default\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"robin\":\"eqx03-flash06.robinsystems.com\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.9.140.106/16\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.18.64.192\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 04 Sep 2020 20:35:41 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  eqx03-flash06\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 17 Sep 2020 00:01:36 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 09 Sep 2020 19:40:59 +0000   Wed, 09 Sep 2020 19:40:59 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Thu, 17 Sep 2020 00:01:26 +0000   Fri, 04 Sep 2020 20:35:41 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 17 Sep 2020 00:01:26 +0000   Fri, 04 Sep 2020 20:35:41 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 17 Sep 2020 00:01:26 +0000   Fri, 04 Sep 2020 20:35:41 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 17 Sep 2020 00:01:26 +0000   Fri, 04 Sep 2020 20:36:01 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.9.140.106\n  Hostname:    eqx03-flash06\nCapacity:\n  cpu:                40\n  ephemeral-storage:  81880Mi\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  intel.com/dummy:    0\n  memory:             396052848Ki\n  pods:               110\nAllocatable:\n  cpu:                40\n  ephemeral-storage:  77271662465\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  intel.com/dummy:    0\n  memory:             395950448Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 17681d4feb7649d194012328dec49cae\n  System UUID:                00000000-0000-0000-0000-0CC47A45244C\n  Boot ID:                    892b824c-e405-424b-a1f6-67be3b41556f\n  Kernel Version:             3.10.0-1062.el7.x86_64\n  OS Image:                   CentOS Linux 7 (Core)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  robin://19.3.6\n  Kubelet Version:            v1.18.6\n  Kube-Proxy Version:         v1.18.6\nPodCIDR:                      172.18.3.0/24\nPodCIDRs:                     172.18.3.0/24\nNon-terminated Pods:          (14 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-zqtk2                                          250m (0%)     0 (0%)      0 (0%)           0 (0%)         7d4h\n  kube-system                 etcd-eqx03-flash06                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         12d\n  kube-system                 kube-apiserver-eqx03-flash06                               250m (0%)     0 (0%)      0 (0%)           0 (0%)         12d\n  kube-system                 kube-controller-manager-eqx03-flash06                      200m (0%)     0 (0%)      0 (0%)           0 (0%)         12d\n  kube-system                 kube-multus-ds-amd64-wjsv5                                 100m (0%)     100m (0%)   50Mi (0%)        50Mi (0%)      27m\n  kube-system                 kube-proxy-mchgt                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         12d\n  kube-system                 kube-scheduler-eqx03-flash06                               100m (0%)     0 (0%)      0 (0%)           0 (0%)         12d\n  kube-system                 kube-sriov-device-plugin-amd64-29hgs                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         27m\n  kubectl-8258                agnhost-master-jfq4h                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\n  robinio                     csi-nodeplugin-robin-rnhf4                                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         27m\n  robinio                     csi-snapshotter-robin-0                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         27m\n  robinio                     robin-master-hr288                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         27m\n  sonobuoy                    sonobuoy-e2e-job-362cab5d438b4e90                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         54m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-71797a053a4d44ec-pp8g4    0 (0%)        0 (0%)      0 (0%)           0 (0%)         54m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests   Limits\n  --------           --------   ------\n  cpu                900m (2%)  100m (0%)\n  memory             50Mi (0%)  50Mi (0%)\n  ephemeral-storage  0 (0%)     0 (0%)\n  hugepages-1Gi      0 (0%)     0 (0%)\n  hugepages-2Mi      0 (0%)     0 (0%)\n  intel.com/dummy    0          0\nEvents:\n  Type     Reason         Age                   From                    Message\n  ----     ------         ----                  ----                    -------\n  Warning  ImageGCFailed  26s (x3497 over 12d)  kubelet, eqx03-flash06  imageFs information is unavailable\n"
Sep 17 00:01:45.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 describe namespace kubectl-8258'
Sep 17 00:01:45.727: INFO: stderr: ""
Sep 17 00:01:45.727: INFO: stdout: "Name:         kubectl-8258\nLabels:       e2e-framework=kubectl\n              e2e-run=91443dd6-2583-4790-8e10-b1b7123afebe\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:01:45.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8258" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":277,"completed":177,"skipped":2987,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:01:45.741: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 17 00:01:45.817: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:01:46.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7445" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":277,"completed":178,"skipped":2990,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:01:47.003: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test override all
Sep 17 00:01:47.067: INFO: Waiting up to 5m0s for pod "client-containers-7e1a3d28-b3fb-448a-9292-63fedd298064" in namespace "containers-9481" to be "Succeeded or Failed"
Sep 17 00:01:47.069: INFO: Pod "client-containers-7e1a3d28-b3fb-448a-9292-63fedd298064": Phase="Pending", Reason="", readiness=false. Elapsed: 1.90035ms
Sep 17 00:01:49.072: INFO: Pod "client-containers-7e1a3d28-b3fb-448a-9292-63fedd298064": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004785766s
Sep 17 00:01:51.076: INFO: Pod "client-containers-7e1a3d28-b3fb-448a-9292-63fedd298064": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008144958s
STEP: Saw pod success
Sep 17 00:01:51.076: INFO: Pod "client-containers-7e1a3d28-b3fb-448a-9292-63fedd298064" satisfied condition "Succeeded or Failed"
Sep 17 00:01:51.078: INFO: Trying to get logs from node eqx03-flash06 pod client-containers-7e1a3d28-b3fb-448a-9292-63fedd298064 container test-container: <nil>
STEP: delete the pod
Sep 17 00:01:51.122: INFO: Waiting for pod client-containers-7e1a3d28-b3fb-448a-9292-63fedd298064 to disappear
Sep 17 00:01:51.124: INFO: Pod client-containers-7e1a3d28-b3fb-448a-9292-63fedd298064 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:01:51.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9481" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":277,"completed":179,"skipped":3009,"failed":0}
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:01:51.135: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test substitution in container's args
Sep 17 00:01:51.213: INFO: Waiting up to 5m0s for pod "var-expansion-82b9f34a-ee93-4ac0-954e-f6028deb90cf" in namespace "var-expansion-8862" to be "Succeeded or Failed"
Sep 17 00:01:51.215: INFO: Pod "var-expansion-82b9f34a-ee93-4ac0-954e-f6028deb90cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019011ms
Sep 17 00:01:53.217: INFO: Pod "var-expansion-82b9f34a-ee93-4ac0-954e-f6028deb90cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004471193s
STEP: Saw pod success
Sep 17 00:01:53.217: INFO: Pod "var-expansion-82b9f34a-ee93-4ac0-954e-f6028deb90cf" satisfied condition "Succeeded or Failed"
Sep 17 00:01:53.219: INFO: Trying to get logs from node eqx03-flash06 pod var-expansion-82b9f34a-ee93-4ac0-954e-f6028deb90cf container dapi-container: <nil>
STEP: delete the pod
Sep 17 00:01:53.256: INFO: Waiting for pod var-expansion-82b9f34a-ee93-4ac0-954e-f6028deb90cf to disappear
Sep 17 00:01:53.257: INFO: Pod var-expansion-82b9f34a-ee93-4ac0-954e-f6028deb90cf no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:01:53.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8862" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":277,"completed":180,"skipped":3013,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:01:53.267: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 17 00:01:54.133: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 17 00:01:56.140: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735897714, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735897714, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735897714, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735897714, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 17 00:01:59.159: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:02:09.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8494" for this suite.
STEP: Destroying namespace "webhook-8494-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.157 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":277,"completed":181,"skipped":3017,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:02:09.424: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-configmap-ccg9
STEP: Creating a pod to test atomic-volume-subpath
Sep 17 00:02:09.517: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-ccg9" in namespace "subpath-5904" to be "Succeeded or Failed"
Sep 17 00:02:09.519: INFO: Pod "pod-subpath-test-configmap-ccg9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.984672ms
Sep 17 00:02:11.533: INFO: Pod "pod-subpath-test-configmap-ccg9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015579449s
Sep 17 00:02:13.536: INFO: Pod "pod-subpath-test-configmap-ccg9": Phase="Running", Reason="", readiness=true. Elapsed: 4.018861091s
Sep 17 00:02:15.540: INFO: Pod "pod-subpath-test-configmap-ccg9": Phase="Running", Reason="", readiness=true. Elapsed: 6.022139019s
Sep 17 00:02:17.543: INFO: Pod "pod-subpath-test-configmap-ccg9": Phase="Running", Reason="", readiness=true. Elapsed: 8.025498867s
Sep 17 00:02:19.547: INFO: Pod "pod-subpath-test-configmap-ccg9": Phase="Running", Reason="", readiness=true. Elapsed: 10.029296486s
Sep 17 00:02:21.550: INFO: Pod "pod-subpath-test-configmap-ccg9": Phase="Running", Reason="", readiness=true. Elapsed: 12.032315858s
Sep 17 00:02:23.553: INFO: Pod "pod-subpath-test-configmap-ccg9": Phase="Running", Reason="", readiness=true. Elapsed: 14.035646668s
Sep 17 00:02:25.556: INFO: Pod "pod-subpath-test-configmap-ccg9": Phase="Running", Reason="", readiness=true. Elapsed: 16.038855142s
Sep 17 00:02:27.560: INFO: Pod "pod-subpath-test-configmap-ccg9": Phase="Running", Reason="", readiness=true. Elapsed: 18.042440186s
Sep 17 00:02:29.564: INFO: Pod "pod-subpath-test-configmap-ccg9": Phase="Running", Reason="", readiness=true. Elapsed: 20.046079746s
Sep 17 00:02:31.566: INFO: Pod "pod-subpath-test-configmap-ccg9": Phase="Running", Reason="", readiness=true. Elapsed: 22.048837338s
Sep 17 00:02:33.569: INFO: Pod "pod-subpath-test-configmap-ccg9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.05189451s
STEP: Saw pod success
Sep 17 00:02:33.569: INFO: Pod "pod-subpath-test-configmap-ccg9" satisfied condition "Succeeded or Failed"
Sep 17 00:02:33.571: INFO: Trying to get logs from node eqx03-flash06 pod pod-subpath-test-configmap-ccg9 container test-container-subpath-configmap-ccg9: <nil>
STEP: delete the pod
Sep 17 00:02:33.607: INFO: Waiting for pod pod-subpath-test-configmap-ccg9 to disappear
Sep 17 00:02:33.609: INFO: Pod pod-subpath-test-configmap-ccg9 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-ccg9
Sep 17 00:02:33.609: INFO: Deleting pod "pod-subpath-test-configmap-ccg9" in namespace "subpath-5904"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:02:33.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5904" for this suite.

• [SLOW TEST:24.197 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":277,"completed":182,"skipped":3050,"failed":0}
SSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:02:33.621: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 17 00:02:33.684: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:02:35.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1775" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":277,"completed":183,"skipped":3053,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:02:35.940: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating service endpoint-test2 in namespace services-5581
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5581 to expose endpoints map[]
Sep 17 00:02:36.021: INFO: Get endpoints failed (2.01641ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Sep 17 00:02:37.024: INFO: successfully validated that service endpoint-test2 in namespace services-5581 exposes endpoints map[] (1.004720296s elapsed)
STEP: Creating pod pod1 in namespace services-5581
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5581 to expose endpoints map[pod1:[80]]
Sep 17 00:02:40.107: INFO: successfully validated that service endpoint-test2 in namespace services-5581 exposes endpoints map[pod1:[80]] (3.052800434s elapsed)
STEP: Creating pod pod2 in namespace services-5581
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5581 to expose endpoints map[pod1:[80] pod2:[80]]
Sep 17 00:02:43.152: INFO: successfully validated that service endpoint-test2 in namespace services-5581 exposes endpoints map[pod1:[80] pod2:[80]] (3.03426838s elapsed)
STEP: Deleting pod pod1 in namespace services-5581
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5581 to expose endpoints map[pod2:[80]]
Sep 17 00:02:44.193: INFO: successfully validated that service endpoint-test2 in namespace services-5581 exposes endpoints map[pod2:[80]] (1.025639424s elapsed)
STEP: Deleting pod pod2 in namespace services-5581
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5581 to expose endpoints map[]
Sep 17 00:02:45.210: INFO: successfully validated that service endpoint-test2 in namespace services-5581 exposes endpoints map[] (1.005016056s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:02:45.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5581" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:9.321 seconds]
[sig-network] Services
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":277,"completed":184,"skipped":3086,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:02:45.261: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:02:45.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-358" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":277,"completed":185,"skipped":3090,"failed":0}
SSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:02:45.340: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap configmap-5893/configmap-test-1a9da7d9-feb1-4e33-9c41-11c785d892e9
STEP: Creating a pod to test consume configMaps
Sep 17 00:02:45.432: INFO: Waiting up to 5m0s for pod "pod-configmaps-e67bcf9c-6cb1-4b49-9ad6-4a5dd704b7f1" in namespace "configmap-5893" to be "Succeeded or Failed"
Sep 17 00:02:45.434: INFO: Pod "pod-configmaps-e67bcf9c-6cb1-4b49-9ad6-4a5dd704b7f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.111958ms
Sep 17 00:02:47.436: INFO: Pod "pod-configmaps-e67bcf9c-6cb1-4b49-9ad6-4a5dd704b7f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004658822s
Sep 17 00:02:49.439: INFO: Pod "pod-configmaps-e67bcf9c-6cb1-4b49-9ad6-4a5dd704b7f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007617156s
STEP: Saw pod success
Sep 17 00:02:49.439: INFO: Pod "pod-configmaps-e67bcf9c-6cb1-4b49-9ad6-4a5dd704b7f1" satisfied condition "Succeeded or Failed"
Sep 17 00:02:49.441: INFO: Trying to get logs from node eqx03-flash06 pod pod-configmaps-e67bcf9c-6cb1-4b49-9ad6-4a5dd704b7f1 container env-test: <nil>
STEP: delete the pod
Sep 17 00:02:49.489: INFO: Waiting for pod pod-configmaps-e67bcf9c-6cb1-4b49-9ad6-4a5dd704b7f1 to disappear
Sep 17 00:02:49.491: INFO: Pod pod-configmaps-e67bcf9c-6cb1-4b49-9ad6-4a5dd704b7f1 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:02:49.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5893" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":277,"completed":186,"skipped":3098,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:02:49.517: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 17 00:02:51.245: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 17 00:02:53.253: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735897771, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735897771, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735897771, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735897771, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 17 00:02:56.276: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:03:08.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1132" for this suite.
STEP: Destroying namespace "webhook-1132-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:19.111 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":277,"completed":187,"skipped":3100,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:03:08.629: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-3553
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a new StatefulSet
Sep 17 00:03:08.709: INFO: Found 0 stateful pods, waiting for 3
Sep 17 00:03:18.713: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 17 00:03:18.713: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 17 00:03:18.713: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Sep 17 00:03:18.755: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Sep 17 00:03:28.803: INFO: Updating stateful set ss2
Sep 17 00:03:28.807: INFO: Waiting for Pod statefulset-3553/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 17 00:03:38.813: INFO: Waiting for Pod statefulset-3553/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Sep 17 00:03:48.945: INFO: Found 2 stateful pods, waiting for 3
Sep 17 00:03:58.949: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 17 00:03:58.949: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 17 00:03:58.949: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Sep 17 00:03:58.985: INFO: Updating stateful set ss2
Sep 17 00:03:58.999: INFO: Waiting for Pod statefulset-3553/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 17 00:04:09.043: INFO: Updating stateful set ss2
Sep 17 00:04:09.048: INFO: Waiting for StatefulSet statefulset-3553/ss2 to complete update
Sep 17 00:04:09.048: INFO: Waiting for Pod statefulset-3553/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 17 00:04:19.054: INFO: Waiting for StatefulSet statefulset-3553/ss2 to complete update
Sep 17 00:04:19.054: INFO: Waiting for Pod statefulset-3553/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Sep 17 00:04:29.054: INFO: Deleting all statefulset in ns statefulset-3553
Sep 17 00:04:29.057: INFO: Scaling statefulset ss2 to 0
Sep 17 00:05:09.092: INFO: Waiting for statefulset status.replicas updated to 0
Sep 17 00:05:09.095: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:05:09.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3553" for this suite.

• [SLOW TEST:120.502 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":277,"completed":188,"skipped":3121,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:05:09.131: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep 17 00:05:17.293: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 17 00:05:17.295: INFO: Pod pod-with-poststart-http-hook still exists
Sep 17 00:05:19.295: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 17 00:05:19.299: INFO: Pod pod-with-poststart-http-hook still exists
Sep 17 00:05:21.295: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 17 00:05:21.298: INFO: Pod pod-with-poststart-http-hook still exists
Sep 17 00:05:23.295: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 17 00:05:23.475: INFO: Pod pod-with-poststart-http-hook still exists
Sep 17 00:05:25.295: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 17 00:05:25.298: INFO: Pod pod-with-poststart-http-hook still exists
Sep 17 00:05:27.295: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 17 00:05:27.298: INFO: Pod pod-with-poststart-http-hook still exists
Sep 17 00:05:29.295: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 17 00:05:29.300: INFO: Pod pod-with-poststart-http-hook still exists
Sep 17 00:05:31.295: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 17 00:05:31.298: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:05:31.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1652" for this suite.

• [SLOW TEST:22.200 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":277,"completed":189,"skipped":3165,"failed":0}
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:05:31.332: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test env composition
Sep 17 00:05:31.385: INFO: Waiting up to 5m0s for pod "var-expansion-2c54e4dd-0790-4854-a5c4-1511a7ea45ca" in namespace "var-expansion-3098" to be "Succeeded or Failed"
Sep 17 00:05:31.387: INFO: Pod "var-expansion-2c54e4dd-0790-4854-a5c4-1511a7ea45ca": Phase="Pending", Reason="", readiness=false. Elapsed: 1.698303ms
Sep 17 00:05:33.390: INFO: Pod "var-expansion-2c54e4dd-0790-4854-a5c4-1511a7ea45ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004960603s
Sep 17 00:05:35.393: INFO: Pod "var-expansion-2c54e4dd-0790-4854-a5c4-1511a7ea45ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007936264s
STEP: Saw pod success
Sep 17 00:05:35.393: INFO: Pod "var-expansion-2c54e4dd-0790-4854-a5c4-1511a7ea45ca" satisfied condition "Succeeded or Failed"
Sep 17 00:05:35.396: INFO: Trying to get logs from node eqx03-flash06 pod var-expansion-2c54e4dd-0790-4854-a5c4-1511a7ea45ca container dapi-container: <nil>
STEP: delete the pod
Sep 17 00:05:35.435: INFO: Waiting for pod var-expansion-2c54e4dd-0790-4854-a5c4-1511a7ea45ca to disappear
Sep 17 00:05:35.437: INFO: Pod var-expansion-2c54e4dd-0790-4854-a5c4-1511a7ea45ca no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:05:35.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3098" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":277,"completed":190,"skipped":3167,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:05:35.449: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name cm-test-opt-del-ae24abf0-c957-4a0b-b925-d25a524bf9af
STEP: Creating configMap with name cm-test-opt-upd-78500154-16b1-446b-a185-057521f84683
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-ae24abf0-c957-4a0b-b925-d25a524bf9af
STEP: Updating configmap cm-test-opt-upd-78500154-16b1-446b-a185-057521f84683
STEP: Creating configMap with name cm-test-opt-create-cf0ca195-10f6-4525-a28f-3c5d0963e470
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:05:43.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1782" for this suite.

• [SLOW TEST:8.271 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":191,"skipped":3173,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:05:43.720: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 17 00:05:43.765: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Creating first CR 
Sep 17 00:05:44.343: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-09-17T00:05:44Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-09-17T00:05:44Z]] name:name1 resourceVersion:4245768 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:acf08b48-5c70-401e-9f44-35f3bd59fd08] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Sep 17 00:05:54.355: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-09-17T00:05:54Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-09-17T00:05:54Z]] name:name2 resourceVersion:4245827 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:a2f7369c-6b4b-42d2-b2a3-365d29dccad3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Sep 17 00:06:04.367: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-09-17T00:05:44Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-09-17T00:06:04Z]] name:name1 resourceVersion:4245870 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:acf08b48-5c70-401e-9f44-35f3bd59fd08] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Sep 17 00:06:14.377: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-09-17T00:05:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-09-17T00:06:14Z]] name:name2 resourceVersion:4245922 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:a2f7369c-6b4b-42d2-b2a3-365d29dccad3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Sep 17 00:06:24.389: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-09-17T00:05:44Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-09-17T00:06:04Z]] name:name1 resourceVersion:4245963 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:acf08b48-5c70-401e-9f44-35f3bd59fd08] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Sep 17 00:06:34.399: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-09-17T00:05:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-09-17T00:06:14Z]] name:name2 resourceVersion:4246007 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:a2f7369c-6b4b-42d2-b2a3-365d29dccad3] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:06:44.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-9164" for this suite.

• [SLOW TEST:61.217 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":277,"completed":192,"skipped":3181,"failed":0}
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:06:44.937: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-4322
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 17 00:06:44.981: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 17 00:06:45.072: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 17 00:06:47.076: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 17 00:06:49.075: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 17 00:06:51.075: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 17 00:06:53.077: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 17 00:06:55.075: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 17 00:06:57.075: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 17 00:06:59.075: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 17 00:07:01.076: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 17 00:07:03.075: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 17 00:07:05.074: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep 17 00:07:05.079: INFO: The status of Pod netserver-1 is Running (Ready = true)
Sep 17 00:07:05.083: INFO: The status of Pod netserver-2 is Running (Ready = true)
Sep 17 00:07:05.086: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
Sep 17 00:07:09.146: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.21.3.105 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4322 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 17 00:07:09.146: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
Sep 17 00:07:10.412: INFO: Found all expected endpoints: [netserver-0]
Sep 17 00:07:10.414: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.21.13.123 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4322 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 17 00:07:10.414: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
Sep 17 00:07:11.658: INFO: Found all expected endpoints: [netserver-1]
Sep 17 00:07:11.661: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.21.13.133 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4322 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 17 00:07:11.661: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
Sep 17 00:07:12.911: INFO: Found all expected endpoints: [netserver-2]
Sep 17 00:07:12.913: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.21.4.78 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4322 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 17 00:07:12.913: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
Sep 17 00:07:14.261: INFO: Found all expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:07:14.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4322" for this suite.

• [SLOW TEST:29.336 seconds]
[sig-network] Networking
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":193,"skipped":3186,"failed":0}
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:07:14.272: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 17 00:07:14.326: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8a6c0ab5-b0a6-4794-a041-aefc8632c602" in namespace "downward-api-8522" to be "Succeeded or Failed"
Sep 17 00:07:14.340: INFO: Pod "downwardapi-volume-8a6c0ab5-b0a6-4794-a041-aefc8632c602": Phase="Pending", Reason="", readiness=false. Elapsed: 14.354606ms
Sep 17 00:07:16.343: INFO: Pod "downwardapi-volume-8a6c0ab5-b0a6-4794-a041-aefc8632c602": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017805776s
Sep 17 00:07:18.346: INFO: Pod "downwardapi-volume-8a6c0ab5-b0a6-4794-a041-aefc8632c602": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020437473s
STEP: Saw pod success
Sep 17 00:07:18.346: INFO: Pod "downwardapi-volume-8a6c0ab5-b0a6-4794-a041-aefc8632c602" satisfied condition "Succeeded or Failed"
Sep 17 00:07:18.348: INFO: Trying to get logs from node eqx03-flash06 pod downwardapi-volume-8a6c0ab5-b0a6-4794-a041-aefc8632c602 container client-container: <nil>
STEP: delete the pod
Sep 17 00:07:18.388: INFO: Waiting for pod downwardapi-volume-8a6c0ab5-b0a6-4794-a041-aefc8632c602 to disappear
Sep 17 00:07:18.390: INFO: Pod downwardapi-volume-8a6c0ab5-b0a6-4794-a041-aefc8632c602 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:07:18.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8522" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":194,"skipped":3186,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:07:18.416: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-map-593e93fb-79ec-474b-ac3d-255b550032a4
STEP: Creating a pod to test consume configMaps
Sep 17 00:07:18.489: INFO: Waiting up to 5m0s for pod "pod-configmaps-d3f964b8-57fd-4768-819d-ce0b070fb9d4" in namespace "configmap-4586" to be "Succeeded or Failed"
Sep 17 00:07:18.491: INFO: Pod "pod-configmaps-d3f964b8-57fd-4768-819d-ce0b070fb9d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.861554ms
Sep 17 00:07:20.496: INFO: Pod "pod-configmaps-d3f964b8-57fd-4768-819d-ce0b070fb9d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006901317s
STEP: Saw pod success
Sep 17 00:07:20.496: INFO: Pod "pod-configmaps-d3f964b8-57fd-4768-819d-ce0b070fb9d4" satisfied condition "Succeeded or Failed"
Sep 17 00:07:20.498: INFO: Trying to get logs from node eqx03-flash06 pod pod-configmaps-d3f964b8-57fd-4768-819d-ce0b070fb9d4 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 17 00:07:20.540: INFO: Waiting for pod pod-configmaps-d3f964b8-57fd-4768-819d-ce0b070fb9d4 to disappear
Sep 17 00:07:20.542: INFO: Pod pod-configmaps-d3f964b8-57fd-4768-819d-ce0b070fb9d4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:07:20.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4586" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":277,"completed":195,"skipped":3271,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:07:20.559: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-map-f94fb4d8-0416-4f0f-aaaf-b59a4f570752
STEP: Creating a pod to test consume configMaps
Sep 17 00:07:20.628: INFO: Waiting up to 5m0s for pod "pod-configmaps-220a2b25-caba-4982-a31c-f60322c74ca1" in namespace "configmap-6271" to be "Succeeded or Failed"
Sep 17 00:07:20.630: INFO: Pod "pod-configmaps-220a2b25-caba-4982-a31c-f60322c74ca1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.20257ms
Sep 17 00:07:22.641: INFO: Pod "pod-configmaps-220a2b25-caba-4982-a31c-f60322c74ca1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012881795s
Sep 17 00:07:24.644: INFO: Pod "pod-configmaps-220a2b25-caba-4982-a31c-f60322c74ca1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016036373s
STEP: Saw pod success
Sep 17 00:07:24.644: INFO: Pod "pod-configmaps-220a2b25-caba-4982-a31c-f60322c74ca1" satisfied condition "Succeeded or Failed"
Sep 17 00:07:24.646: INFO: Trying to get logs from node eqx03-flash06 pod pod-configmaps-220a2b25-caba-4982-a31c-f60322c74ca1 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 17 00:07:24.684: INFO: Waiting for pod pod-configmaps-220a2b25-caba-4982-a31c-f60322c74ca1 to disappear
Sep 17 00:07:24.686: INFO: Pod pod-configmaps-220a2b25-caba-4982-a31c-f60322c74ca1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:07:24.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6271" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":277,"completed":196,"skipped":3276,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:07:24.696: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Sep 17 00:07:24.760: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Sep 17 00:07:24.770: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep 17 00:07:24.770: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Sep 17 00:07:24.788: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep 17 00:07:24.788: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Sep 17 00:07:24.805: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Sep 17 00:07:24.805: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Sep 17 00:07:31.879: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:07:31.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-9570" for this suite.

• [SLOW TEST:7.238 seconds]
[sig-scheduling] LimitRange
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":277,"completed":197,"skipped":3289,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:07:31.934: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 17 00:07:31.999: INFO: Waiting up to 5m0s for pod "downwardapi-volume-98b27ff2-e457-4b5b-83bd-9e4f83b08b27" in namespace "downward-api-7970" to be "Succeeded or Failed"
Sep 17 00:07:32.001: INFO: Pod "downwardapi-volume-98b27ff2-e457-4b5b-83bd-9e4f83b08b27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.064871ms
Sep 17 00:07:34.004: INFO: Pod "downwardapi-volume-98b27ff2-e457-4b5b-83bd-9e4f83b08b27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005324061s
Sep 17 00:07:36.008: INFO: Pod "downwardapi-volume-98b27ff2-e457-4b5b-83bd-9e4f83b08b27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009008829s
STEP: Saw pod success
Sep 17 00:07:36.008: INFO: Pod "downwardapi-volume-98b27ff2-e457-4b5b-83bd-9e4f83b08b27" satisfied condition "Succeeded or Failed"
Sep 17 00:07:36.010: INFO: Trying to get logs from node eqx03-flash06 pod downwardapi-volume-98b27ff2-e457-4b5b-83bd-9e4f83b08b27 container client-container: <nil>
STEP: delete the pod
Sep 17 00:07:36.044: INFO: Waiting for pod downwardapi-volume-98b27ff2-e457-4b5b-83bd-9e4f83b08b27 to disappear
Sep 17 00:07:36.046: INFO: Pod downwardapi-volume-98b27ff2-e457-4b5b-83bd-9e4f83b08b27 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:07:36.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7970" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":277,"completed":198,"skipped":3305,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:07:36.059: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Sep 17 00:07:36.117: INFO: PodSpec: initContainers in spec.initContainers
Sep 17 00:08:26.736: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-2b3e50df-c678-4a29-876a-4315e13bf958", GenerateName:"", Namespace:"init-container-94", SelfLink:"/api/v1/namespaces/init-container-94/pods/pod-init-2b3e50df-c678-4a29-876a-4315e13bf958", UID:"1df1bfc9-ca01-4ca9-993c-a6d0b1229258", ResourceVersion:"4246818", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63735898056, loc:(*time.Location)(0x7b51220)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"117441368"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"172.21.4.78/32", "cni.projectcalico.org/podIPs":"172.21.4.78/32", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"calico\",\n    \"ips\": [\n        \"172.21.4.78\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"calico\",\n    \"ips\": [\n        \"172.21.4.78\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0041dcd00), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0041dcd20)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0041dcd40), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0041dcd60)}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0041dcd80), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0041dcda0)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0041dcdc0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0041dcde0)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-pvfzp", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001ff9a00), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-pvfzp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-pvfzp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-pvfzp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003ed9978), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"eqx03-flash06", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00060ea80), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003ed9a00)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003ed9a20)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003ed9a28), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc003ed9a2c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898056, loc:(*time.Location)(0x7b51220)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898056, loc:(*time.Location)(0x7b51220)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898056, loc:(*time.Location)(0x7b51220)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898056, loc:(*time.Location)(0x7b51220)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.9.140.106", PodIP:"172.21.4.78", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.21.4.78"}}, StartTime:(*v1.Time)(0xc0041dce00), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00060ec40)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00060ecb0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"robin://cec6b77dc4c862560854e0ae6f25482dd477b9dc0ea3fa54fddfaab49f453d23", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0041dce40), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0041dce20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc003ed9aaf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:08:26.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-94" for this suite.

• [SLOW TEST:50.694 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":277,"completed":199,"skipped":3322,"failed":0}
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:08:26.753: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-2152
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-2152
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2152
Sep 17 00:08:26.876: INFO: Found 0 stateful pods, waiting for 1
Sep 17 00:08:36.879: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Sep 17 00:08:36.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-2152 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 17 00:08:37.356: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 17 00:08:37.356: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 17 00:08:37.356: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 17 00:08:37.365: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep 17 00:08:47.371: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 17 00:08:47.371: INFO: Waiting for statefulset status.replicas updated to 0
Sep 17 00:08:47.388: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999577s
Sep 17 00:08:48.391: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997369538s
Sep 17 00:08:49.394: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.993842473s
Sep 17 00:08:50.398: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.990622324s
Sep 17 00:08:51.402: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.98693629s
Sep 17 00:08:52.404: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.983237s
Sep 17 00:08:53.409: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.980512682s
Sep 17 00:08:54.413: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.975464048s
Sep 17 00:08:55.425: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.971542043s
Sep 17 00:08:56.429: INFO: Verifying statefulset ss doesn't scale past 1 for another 959.782922ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2152
Sep 17 00:08:57.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-2152 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 17 00:08:57.760: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 17 00:08:57.760: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 17 00:08:57.760: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 17 00:08:57.763: INFO: Found 1 stateful pods, waiting for 3
Sep 17 00:09:07.767: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 17 00:09:07.767: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 17 00:09:07.767: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Sep 17 00:09:07.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-2152 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 17 00:09:08.112: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 17 00:09:08.112: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 17 00:09:08.112: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 17 00:09:08.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-2152 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 17 00:09:08.677: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 17 00:09:08.677: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 17 00:09:08.677: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 17 00:09:08.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-2152 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 17 00:09:08.958: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 17 00:09:08.958: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 17 00:09:08.958: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 17 00:09:08.958: INFO: Waiting for statefulset status.replicas updated to 0
Sep 17 00:09:08.960: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Sep 17 00:09:18.967: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 17 00:09:18.967: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep 17 00:09:18.967: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep 17 00:09:18.992: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999571s
Sep 17 00:09:19.995: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996979768s
Sep 17 00:09:20.999: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993375111s
Sep 17 00:09:22.003: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.989978768s
Sep 17 00:09:23.008: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.986193529s
Sep 17 00:09:24.012: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.980984148s
Sep 17 00:09:25.016: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.977190198s
Sep 17 00:09:26.021: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.973057158s
Sep 17 00:09:27.024: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.967692858s
Sep 17 00:09:28.027: INFO: Verifying statefulset ss doesn't scale past 3 for another 964.517308ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2152
Sep 17 00:09:29.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-2152 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 17 00:09:29.467: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 17 00:09:29.467: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 17 00:09:29.467: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 17 00:09:29.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-2152 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 17 00:09:30.017: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 17 00:09:30.017: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 17 00:09:30.017: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 17 00:09:30.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=statefulset-2152 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 17 00:09:30.292: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 17 00:09:30.292: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 17 00:09:30.292: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 17 00:09:30.292: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Sep 17 00:10:00.320: INFO: Deleting all statefulset in ns statefulset-2152
Sep 17 00:10:00.322: INFO: Scaling statefulset ss to 0
Sep 17 00:10:00.328: INFO: Waiting for statefulset status.replicas updated to 0
Sep 17 00:10:00.330: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:10:00.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2152" for this suite.

• [SLOW TEST:93.610 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":277,"completed":200,"skipped":3329,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:10:00.363: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:10:16.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8816" for this suite.

• [SLOW TEST:16.204 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":277,"completed":201,"skipped":3331,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:10:16.567: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:10:16.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-1029" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":277,"completed":202,"skipped":3390,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:10:16.708: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod liveness-6d698fd0-2da1-43a9-81b4-ef39a04bfa08 in namespace container-probe-5111
Sep 17 00:10:20.782: INFO: Started pod liveness-6d698fd0-2da1-43a9-81b4-ef39a04bfa08 in namespace container-probe-5111
STEP: checking the pod's current state and verifying that restartCount is present
Sep 17 00:10:20.784: INFO: Initial restart count of pod liveness-6d698fd0-2da1-43a9-81b4-ef39a04bfa08 is 0
Sep 17 00:10:34.818: INFO: Restart count of pod container-probe-5111/liveness-6d698fd0-2da1-43a9-81b4-ef39a04bfa08 is now 1 (14.034221876s elapsed)
Sep 17 00:10:56.855: INFO: Restart count of pod container-probe-5111/liveness-6d698fd0-2da1-43a9-81b4-ef39a04bfa08 is now 2 (36.070644254s elapsed)
Sep 17 00:11:16.890: INFO: Restart count of pod container-probe-5111/liveness-6d698fd0-2da1-43a9-81b4-ef39a04bfa08 is now 3 (56.106143024s elapsed)
Sep 17 00:11:34.936: INFO: Restart count of pod container-probe-5111/liveness-6d698fd0-2da1-43a9-81b4-ef39a04bfa08 is now 4 (1m14.15207651s elapsed)
Sep 17 00:12:41.045: INFO: Restart count of pod container-probe-5111/liveness-6d698fd0-2da1-43a9-81b4-ef39a04bfa08 is now 5 (2m20.260321197s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:12:41.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5111" for this suite.

• [SLOW TEST:144.374 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":277,"completed":203,"skipped":3411,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:12:41.082: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Sep 17 00:12:45.713: INFO: Successfully updated pod "labelsupdatef771aa81-eee6-4fe9-873e-714e10ac1da5"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:12:47.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6673" for this suite.

• [SLOW TEST:6.686 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":277,"completed":204,"skipped":3434,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:12:47.768: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name s-test-opt-del-eb8e5a5f-2e72-4b71-9c54-fae8ce8781b6
STEP: Creating secret with name s-test-opt-upd-08b13851-9a4d-45c7-8867-41d85c2ebf84
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-eb8e5a5f-2e72-4b71-9c54-fae8ce8781b6
STEP: Updating secret s-test-opt-upd-08b13851-9a4d-45c7-8867-41d85c2ebf84
STEP: Creating secret with name s-test-opt-create-7a07f051-370e-4f59-babb-cbb1e1b06511
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:13:58.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9393" for this suite.

• [SLOW TEST:71.062 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":205,"skipped":3440,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:13:58.832: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 17 00:13:58.904: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cf7d29af-95df-44ab-873c-8fc887c0df59" in namespace "downward-api-6663" to be "Succeeded or Failed"
Sep 17 00:13:58.906: INFO: Pod "downwardapi-volume-cf7d29af-95df-44ab-873c-8fc887c0df59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035026ms
Sep 17 00:14:00.910: INFO: Pod "downwardapi-volume-cf7d29af-95df-44ab-873c-8fc887c0df59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005950696s
Sep 17 00:14:02.913: INFO: Pod "downwardapi-volume-cf7d29af-95df-44ab-873c-8fc887c0df59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009378895s
STEP: Saw pod success
Sep 17 00:14:02.913: INFO: Pod "downwardapi-volume-cf7d29af-95df-44ab-873c-8fc887c0df59" satisfied condition "Succeeded or Failed"
Sep 17 00:14:02.915: INFO: Trying to get logs from node eqx03-flash06 pod downwardapi-volume-cf7d29af-95df-44ab-873c-8fc887c0df59 container client-container: <nil>
STEP: delete the pod
Sep 17 00:14:02.969: INFO: Waiting for pod downwardapi-volume-cf7d29af-95df-44ab-873c-8fc887c0df59 to disappear
Sep 17 00:14:02.971: INFO: Pod downwardapi-volume-cf7d29af-95df-44ab-873c-8fc887c0df59 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:14:02.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6663" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":277,"completed":206,"skipped":3507,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:14:02.982: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-map-cb238d83-4496-461b-a08b-e01242667cd8
STEP: Creating a pod to test consume configMaps
Sep 17 00:14:03.053: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a81396ba-8182-49c4-9fa6-562aac4ac786" in namespace "projected-3738" to be "Succeeded or Failed"
Sep 17 00:14:03.067: INFO: Pod "pod-projected-configmaps-a81396ba-8182-49c4-9fa6-562aac4ac786": Phase="Pending", Reason="", readiness=false. Elapsed: 13.687851ms
Sep 17 00:14:05.080: INFO: Pod "pod-projected-configmaps-a81396ba-8182-49c4-9fa6-562aac4ac786": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026660325s
Sep 17 00:14:07.083: INFO: Pod "pod-projected-configmaps-a81396ba-8182-49c4-9fa6-562aac4ac786": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029716446s
STEP: Saw pod success
Sep 17 00:14:07.083: INFO: Pod "pod-projected-configmaps-a81396ba-8182-49c4-9fa6-562aac4ac786" satisfied condition "Succeeded or Failed"
Sep 17 00:14:07.085: INFO: Trying to get logs from node eqx03-flash06 pod pod-projected-configmaps-a81396ba-8182-49c4-9fa6-562aac4ac786 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 17 00:14:07.123: INFO: Waiting for pod pod-projected-configmaps-a81396ba-8182-49c4-9fa6-562aac4ac786 to disappear
Sep 17 00:14:07.125: INFO: Pod pod-projected-configmaps-a81396ba-8182-49c4-9fa6-562aac4ac786 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:14:07.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3738" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":277,"completed":207,"skipped":3512,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:14:07.136: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name secret-emptykey-test-23a872d4-eca7-4a2c-93e5-d031e8175a52
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:14:07.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3309" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":277,"completed":208,"skipped":3533,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:14:07.215: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Update Demo
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:271
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a replication controller
Sep 17 00:14:07.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 create -f - --namespace=kubectl-9204'
Sep 17 00:14:07.609: INFO: stderr: ""
Sep 17 00:14:07.609: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 17 00:14:07.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9204'
Sep 17 00:14:07.712: INFO: stderr: ""
Sep 17 00:14:07.713: INFO: stdout: "update-demo-nautilus-nhmzq update-demo-nautilus-q5blq "
Sep 17 00:14:07.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pods update-demo-nautilus-nhmzq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9204'
Sep 17 00:14:07.808: INFO: stderr: ""
Sep 17 00:14:07.808: INFO: stdout: ""
Sep 17 00:14:07.808: INFO: update-demo-nautilus-nhmzq is created but not running
Sep 17 00:14:12.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9204'
Sep 17 00:14:12.922: INFO: stderr: ""
Sep 17 00:14:12.922: INFO: stdout: "update-demo-nautilus-nhmzq update-demo-nautilus-q5blq "
Sep 17 00:14:12.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pods update-demo-nautilus-nhmzq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9204'
Sep 17 00:14:13.019: INFO: stderr: ""
Sep 17 00:14:13.019: INFO: stdout: "true"
Sep 17 00:14:13.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pods update-demo-nautilus-nhmzq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9204'
Sep 17 00:14:13.113: INFO: stderr: ""
Sep 17 00:14:13.113: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 17 00:14:13.113: INFO: validating pod update-demo-nautilus-nhmzq
Sep 17 00:14:13.117: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 17 00:14:13.117: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 17 00:14:13.117: INFO: update-demo-nautilus-nhmzq is verified up and running
Sep 17 00:14:13.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pods update-demo-nautilus-q5blq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9204'
Sep 17 00:14:13.210: INFO: stderr: ""
Sep 17 00:14:13.210: INFO: stdout: "true"
Sep 17 00:14:13.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pods update-demo-nautilus-q5blq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9204'
Sep 17 00:14:13.307: INFO: stderr: ""
Sep 17 00:14:13.307: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 17 00:14:13.307: INFO: validating pod update-demo-nautilus-q5blq
Sep 17 00:14:13.311: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 17 00:14:13.311: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 17 00:14:13.311: INFO: update-demo-nautilus-q5blq is verified up and running
STEP: scaling down the replication controller
Sep 17 00:14:13.314: INFO: scanned /root for discovery docs: <nil>
Sep 17 00:14:13.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-9204'
Sep 17 00:14:14.444: INFO: stderr: ""
Sep 17 00:14:14.444: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 17 00:14:14.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9204'
Sep 17 00:14:14.550: INFO: stderr: ""
Sep 17 00:14:14.550: INFO: stdout: "update-demo-nautilus-nhmzq update-demo-nautilus-q5blq "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep 17 00:14:19.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9204'
Sep 17 00:14:19.646: INFO: stderr: ""
Sep 17 00:14:19.646: INFO: stdout: "update-demo-nautilus-nhmzq update-demo-nautilus-q5blq "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep 17 00:14:24.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9204'
Sep 17 00:14:24.746: INFO: stderr: ""
Sep 17 00:14:24.746: INFO: stdout: "update-demo-nautilus-nhmzq "
Sep 17 00:14:24.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pods update-demo-nautilus-nhmzq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9204'
Sep 17 00:14:24.840: INFO: stderr: ""
Sep 17 00:14:24.840: INFO: stdout: "true"
Sep 17 00:14:24.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pods update-demo-nautilus-nhmzq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9204'
Sep 17 00:14:24.938: INFO: stderr: ""
Sep 17 00:14:24.938: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 17 00:14:24.938: INFO: validating pod update-demo-nautilus-nhmzq
Sep 17 00:14:24.941: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 17 00:14:24.941: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 17 00:14:24.941: INFO: update-demo-nautilus-nhmzq is verified up and running
STEP: scaling up the replication controller
Sep 17 00:14:24.944: INFO: scanned /root for discovery docs: <nil>
Sep 17 00:14:24.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-9204'
Sep 17 00:14:26.061: INFO: stderr: ""
Sep 17 00:14:26.061: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 17 00:14:26.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9204'
Sep 17 00:14:26.163: INFO: stderr: ""
Sep 17 00:14:26.163: INFO: stdout: "update-demo-nautilus-nhmzq update-demo-nautilus-pk2j4 "
Sep 17 00:14:26.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pods update-demo-nautilus-nhmzq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9204'
Sep 17 00:14:26.257: INFO: stderr: ""
Sep 17 00:14:26.257: INFO: stdout: "true"
Sep 17 00:14:26.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pods update-demo-nautilus-nhmzq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9204'
Sep 17 00:14:26.354: INFO: stderr: ""
Sep 17 00:14:26.354: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 17 00:14:26.354: INFO: validating pod update-demo-nautilus-nhmzq
Sep 17 00:14:26.357: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 17 00:14:26.357: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 17 00:14:26.357: INFO: update-demo-nautilus-nhmzq is verified up and running
Sep 17 00:14:26.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pods update-demo-nautilus-pk2j4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9204'
Sep 17 00:14:26.454: INFO: stderr: ""
Sep 17 00:14:26.454: INFO: stdout: ""
Sep 17 00:14:26.454: INFO: update-demo-nautilus-pk2j4 is created but not running
Sep 17 00:14:31.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9204'
Sep 17 00:14:31.558: INFO: stderr: ""
Sep 17 00:14:31.558: INFO: stdout: "update-demo-nautilus-nhmzq update-demo-nautilus-pk2j4 "
Sep 17 00:14:31.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pods update-demo-nautilus-nhmzq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9204'
Sep 17 00:14:31.656: INFO: stderr: ""
Sep 17 00:14:31.656: INFO: stdout: "true"
Sep 17 00:14:31.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pods update-demo-nautilus-nhmzq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9204'
Sep 17 00:14:31.747: INFO: stderr: ""
Sep 17 00:14:31.747: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 17 00:14:31.747: INFO: validating pod update-demo-nautilus-nhmzq
Sep 17 00:14:31.751: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 17 00:14:31.751: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 17 00:14:31.751: INFO: update-demo-nautilus-nhmzq is verified up and running
Sep 17 00:14:31.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pods update-demo-nautilus-pk2j4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9204'
Sep 17 00:14:31.842: INFO: stderr: ""
Sep 17 00:14:31.842: INFO: stdout: "true"
Sep 17 00:14:31.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pods update-demo-nautilus-pk2j4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9204'
Sep 17 00:14:31.936: INFO: stderr: ""
Sep 17 00:14:31.936: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 17 00:14:31.936: INFO: validating pod update-demo-nautilus-pk2j4
Sep 17 00:14:31.940: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 17 00:14:31.940: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 17 00:14:31.940: INFO: update-demo-nautilus-pk2j4 is verified up and running
STEP: using delete to clean up resources
Sep 17 00:14:31.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 delete --grace-period=0 --force -f - --namespace=kubectl-9204'
Sep 17 00:14:32.040: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 17 00:14:32.040: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep 17 00:14:32.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9204'
Sep 17 00:14:32.148: INFO: stderr: "No resources found in kubectl-9204 namespace.\n"
Sep 17 00:14:32.148: INFO: stdout: ""
Sep 17 00:14:32.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pods -l name=update-demo --namespace=kubectl-9204 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 17 00:14:32.249: INFO: stderr: ""
Sep 17 00:14:32.249: INFO: stdout: "update-demo-nautilus-nhmzq\nupdate-demo-nautilus-pk2j4\n"
Sep 17 00:14:32.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9204'
Sep 17 00:14:32.850: INFO: stderr: "No resources found in kubectl-9204 namespace.\n"
Sep 17 00:14:32.850: INFO: stdout: ""
Sep 17 00:14:32.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 get pods -l name=update-demo --namespace=kubectl-9204 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 17 00:14:32.957: INFO: stderr: ""
Sep 17 00:14:32.957: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:14:32.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9204" for this suite.

• [SLOW TEST:25.753 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:269
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":277,"completed":209,"skipped":3538,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:14:32.968: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:14:37.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6185" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":277,"completed":210,"skipped":3542,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:14:37.236: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:14:48.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8436" for this suite.

• [SLOW TEST:11.122 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":277,"completed":211,"skipped":3551,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:14:48.358: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Sep 17 00:14:48.404: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:14:52.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4992" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":277,"completed":212,"skipped":3564,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:14:52.994: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 17 00:14:54.154: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 17 00:14:56.163: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898494, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898494, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898494, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898494, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 17 00:14:59.196: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 17 00:14:59.199: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2567-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:15:00.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4103" for this suite.
STEP: Destroying namespace "webhook-4103-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.495 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":277,"completed":213,"skipped":3565,"failed":0}
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:15:00.489: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-projected-5nc2
STEP: Creating a pod to test atomic-volume-subpath
Sep 17 00:15:00.581: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-5nc2" in namespace "subpath-4341" to be "Succeeded or Failed"
Sep 17 00:15:00.583: INFO: Pod "pod-subpath-test-projected-5nc2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.847726ms
Sep 17 00:15:02.586: INFO: Pod "pod-subpath-test-projected-5nc2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004586269s
Sep 17 00:15:04.589: INFO: Pod "pod-subpath-test-projected-5nc2": Phase="Running", Reason="", readiness=true. Elapsed: 4.007415561s
Sep 17 00:15:06.592: INFO: Pod "pod-subpath-test-projected-5nc2": Phase="Running", Reason="", readiness=true. Elapsed: 6.01041684s
Sep 17 00:15:08.595: INFO: Pod "pod-subpath-test-projected-5nc2": Phase="Running", Reason="", readiness=true. Elapsed: 8.014141622s
Sep 17 00:15:10.602: INFO: Pod "pod-subpath-test-projected-5nc2": Phase="Running", Reason="", readiness=true. Elapsed: 10.021062163s
Sep 17 00:15:12.605: INFO: Pod "pod-subpath-test-projected-5nc2": Phase="Running", Reason="", readiness=true. Elapsed: 12.023961047s
Sep 17 00:15:14.609: INFO: Pod "pod-subpath-test-projected-5nc2": Phase="Running", Reason="", readiness=true. Elapsed: 14.027461536s
Sep 17 00:15:16.612: INFO: Pod "pod-subpath-test-projected-5nc2": Phase="Running", Reason="", readiness=true. Elapsed: 16.030453939s
Sep 17 00:15:18.616: INFO: Pod "pod-subpath-test-projected-5nc2": Phase="Running", Reason="", readiness=true. Elapsed: 18.034187789s
Sep 17 00:15:20.619: INFO: Pod "pod-subpath-test-projected-5nc2": Phase="Running", Reason="", readiness=true. Elapsed: 20.037956981s
Sep 17 00:15:22.623: INFO: Pod "pod-subpath-test-projected-5nc2": Phase="Running", Reason="", readiness=true. Elapsed: 22.041603939s
Sep 17 00:15:24.626: INFO: Pod "pod-subpath-test-projected-5nc2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.044841137s
STEP: Saw pod success
Sep 17 00:15:24.626: INFO: Pod "pod-subpath-test-projected-5nc2" satisfied condition "Succeeded or Failed"
Sep 17 00:15:24.629: INFO: Trying to get logs from node eqx03-flash06 pod pod-subpath-test-projected-5nc2 container test-container-subpath-projected-5nc2: <nil>
STEP: delete the pod
Sep 17 00:15:24.681: INFO: Waiting for pod pod-subpath-test-projected-5nc2 to disappear
Sep 17 00:15:24.683: INFO: Pod pod-subpath-test-projected-5nc2 no longer exists
STEP: Deleting pod pod-subpath-test-projected-5nc2
Sep 17 00:15:24.683: INFO: Deleting pod "pod-subpath-test-projected-5nc2" in namespace "subpath-4341"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:15:24.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4341" for this suite.

• [SLOW TEST:24.218 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":277,"completed":214,"skipped":3565,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:15:24.708: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep 17 00:15:29.300: INFO: Successfully updated pod "pod-update-d484bf07-e6b5-4882-964b-21d059fcf9a7"
STEP: verifying the updated pod is in kubernetes
Sep 17 00:15:29.304: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:15:29.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4568" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":277,"completed":215,"skipped":3580,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:15:29.314: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: getting the auto-created API token
Sep 17 00:15:29.886: INFO: created pod pod-service-account-defaultsa
Sep 17 00:15:29.886: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Sep 17 00:15:29.900: INFO: created pod pod-service-account-mountsa
Sep 17 00:15:29.900: INFO: pod pod-service-account-mountsa service account token volume mount: true
Sep 17 00:15:29.937: INFO: created pod pod-service-account-nomountsa
Sep 17 00:15:29.937: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Sep 17 00:15:29.957: INFO: created pod pod-service-account-defaultsa-mountspec
Sep 17 00:15:29.957: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Sep 17 00:15:29.978: INFO: created pod pod-service-account-mountsa-mountspec
Sep 17 00:15:29.978: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Sep 17 00:15:30.010: INFO: created pod pod-service-account-nomountsa-mountspec
Sep 17 00:15:30.010: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Sep 17 00:15:30.029: INFO: created pod pod-service-account-defaultsa-nomountspec
Sep 17 00:15:30.029: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Sep 17 00:15:30.060: INFO: created pod pod-service-account-mountsa-nomountspec
Sep 17 00:15:30.060: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Sep 17 00:15:30.100: INFO: created pod pod-service-account-nomountsa-nomountspec
Sep 17 00:15:30.100: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:15:30.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6737" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":277,"completed":216,"skipped":3600,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:15:30.121: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 17 00:15:30.197: INFO: Waiting up to 5m0s for pod "downwardapi-volume-45f40895-44d0-4c54-9898-a9efdaede254" in namespace "projected-4862" to be "Succeeded or Failed"
Sep 17 00:15:30.200: INFO: Pod "downwardapi-volume-45f40895-44d0-4c54-9898-a9efdaede254": Phase="Pending", Reason="", readiness=false. Elapsed: 2.227591ms
Sep 17 00:15:32.202: INFO: Pod "downwardapi-volume-45f40895-44d0-4c54-9898-a9efdaede254": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004704822s
Sep 17 00:15:34.208: INFO: Pod "downwardapi-volume-45f40895-44d0-4c54-9898-a9efdaede254": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010703745s
Sep 17 00:15:36.211: INFO: Pod "downwardapi-volume-45f40895-44d0-4c54-9898-a9efdaede254": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013656293s
Sep 17 00:15:38.214: INFO: Pod "downwardapi-volume-45f40895-44d0-4c54-9898-a9efdaede254": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.016751977s
STEP: Saw pod success
Sep 17 00:15:38.214: INFO: Pod "downwardapi-volume-45f40895-44d0-4c54-9898-a9efdaede254" satisfied condition "Succeeded or Failed"
Sep 17 00:15:38.216: INFO: Trying to get logs from node eqx03-flash06 pod downwardapi-volume-45f40895-44d0-4c54-9898-a9efdaede254 container client-container: <nil>
STEP: delete the pod
Sep 17 00:15:38.313: INFO: Waiting for pod downwardapi-volume-45f40895-44d0-4c54-9898-a9efdaede254 to disappear
Sep 17 00:15:38.324: INFO: Pod downwardapi-volume-45f40895-44d0-4c54-9898-a9efdaede254 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:15:38.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4862" for this suite.

• [SLOW TEST:8.215 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":277,"completed":217,"skipped":3606,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:15:38.337: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 17 00:15:38.416: INFO: Number of nodes with available pods: 0
Sep 17 00:15:38.416: INFO: Node eqx03-flash06 is running more than one daemon pod
Sep 17 00:15:39.428: INFO: Number of nodes with available pods: 0
Sep 17 00:15:39.428: INFO: Node eqx03-flash06 is running more than one daemon pod
Sep 17 00:15:40.424: INFO: Number of nodes with available pods: 0
Sep 17 00:15:40.424: INFO: Node eqx03-flash06 is running more than one daemon pod
Sep 17 00:15:41.424: INFO: Number of nodes with available pods: 2
Sep 17 00:15:41.424: INFO: Node eqx03-flash06 is running more than one daemon pod
Sep 17 00:15:42.425: INFO: Number of nodes with available pods: 4
Sep 17 00:15:42.425: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Stop a daemon pod, check that the daemon pod is revived.
Sep 17 00:15:42.446: INFO: Number of nodes with available pods: 3
Sep 17 00:15:42.446: INFO: Node eqx04-flash04 is running more than one daemon pod
Sep 17 00:15:43.480: INFO: Number of nodes with available pods: 3
Sep 17 00:15:43.480: INFO: Node eqx04-flash04 is running more than one daemon pod
Sep 17 00:15:44.462: INFO: Number of nodes with available pods: 3
Sep 17 00:15:44.462: INFO: Node eqx04-flash04 is running more than one daemon pod
Sep 17 00:15:45.453: INFO: Number of nodes with available pods: 3
Sep 17 00:15:45.453: INFO: Node eqx04-flash04 is running more than one daemon pod
Sep 17 00:15:46.471: INFO: Number of nodes with available pods: 3
Sep 17 00:15:46.471: INFO: Node eqx04-flash04 is running more than one daemon pod
Sep 17 00:15:47.454: INFO: Number of nodes with available pods: 3
Sep 17 00:15:47.454: INFO: Node eqx04-flash04 is running more than one daemon pod
Sep 17 00:15:48.452: INFO: Number of nodes with available pods: 3
Sep 17 00:15:48.452: INFO: Node eqx04-flash04 is running more than one daemon pod
Sep 17 00:15:49.453: INFO: Number of nodes with available pods: 3
Sep 17 00:15:49.453: INFO: Node eqx04-flash04 is running more than one daemon pod
Sep 17 00:15:50.454: INFO: Number of nodes with available pods: 3
Sep 17 00:15:50.454: INFO: Node eqx04-flash04 is running more than one daemon pod
Sep 17 00:15:51.453: INFO: Number of nodes with available pods: 3
Sep 17 00:15:51.453: INFO: Node eqx04-flash04 is running more than one daemon pod
Sep 17 00:15:52.453: INFO: Number of nodes with available pods: 3
Sep 17 00:15:52.453: INFO: Node eqx04-flash04 is running more than one daemon pod
Sep 17 00:15:53.452: INFO: Number of nodes with available pods: 3
Sep 17 00:15:53.452: INFO: Node eqx04-flash04 is running more than one daemon pod
Sep 17 00:15:54.453: INFO: Number of nodes with available pods: 4
Sep 17 00:15:54.453: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9706, will wait for the garbage collector to delete the pods
Sep 17 00:15:54.517: INFO: Deleting DaemonSet.extensions daemon-set took: 10.729593ms
Sep 17 00:15:55.418: INFO: Terminating DaemonSet.extensions daemon-set pods took: 900.225587ms
Sep 17 00:16:06.721: INFO: Number of nodes with available pods: 0
Sep 17 00:16:06.721: INFO: Number of running nodes: 0, number of available pods: 0
Sep 17 00:16:06.723: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9706/daemonsets","resourceVersion":"4249821"},"items":null}

Sep 17 00:16:06.725: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9706/pods","resourceVersion":"4249821"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:16:06.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9706" for this suite.

• [SLOW TEST:28.416 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":277,"completed":218,"skipped":3652,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:16:06.754: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 17 00:16:06.836: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-9f7dad56-542c-4ca7-90a9-f2fe884c3b6d" in namespace "security-context-test-2624" to be "Succeeded or Failed"
Sep 17 00:16:06.839: INFO: Pod "alpine-nnp-false-9f7dad56-542c-4ca7-90a9-f2fe884c3b6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.672205ms
Sep 17 00:16:08.842: INFO: Pod "alpine-nnp-false-9f7dad56-542c-4ca7-90a9-f2fe884c3b6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005877103s
Sep 17 00:16:10.845: INFO: Pod "alpine-nnp-false-9f7dad56-542c-4ca7-90a9-f2fe884c3b6d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00877287s
Sep 17 00:16:12.848: INFO: Pod "alpine-nnp-false-9f7dad56-542c-4ca7-90a9-f2fe884c3b6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012170225s
Sep 17 00:16:12.849: INFO: Pod "alpine-nnp-false-9f7dad56-542c-4ca7-90a9-f2fe884c3b6d" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:16:12.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2624" for this suite.

• [SLOW TEST:6.121 seconds]
[k8s.io] Security Context
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:291
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":219,"skipped":3686,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:16:12.875: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:16:17.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3728" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":277,"completed":220,"skipped":3701,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:16:17.040: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1612.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1612.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 17 00:16:21.131: INFO: Unable to read wheezy_udp@PodARecord from pod dns-1612/dns-test-f0569aa2-1e1c-4fee-b65c-9406f4737a91: the server could not find the requested resource (get pods dns-test-f0569aa2-1e1c-4fee-b65c-9406f4737a91)
Sep 17 00:16:21.134: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-1612/dns-test-f0569aa2-1e1c-4fee-b65c-9406f4737a91: the server could not find the requested resource (get pods dns-test-f0569aa2-1e1c-4fee-b65c-9406f4737a91)
Sep 17 00:16:21.141: INFO: Unable to read jessie_udp@PodARecord from pod dns-1612/dns-test-f0569aa2-1e1c-4fee-b65c-9406f4737a91: the server could not find the requested resource (get pods dns-test-f0569aa2-1e1c-4fee-b65c-9406f4737a91)
Sep 17 00:16:21.144: INFO: Unable to read jessie_tcp@PodARecord from pod dns-1612/dns-test-f0569aa2-1e1c-4fee-b65c-9406f4737a91: the server could not find the requested resource (get pods dns-test-f0569aa2-1e1c-4fee-b65c-9406f4737a91)
Sep 17 00:16:21.144: INFO: Lookups using dns-1612/dns-test-f0569aa2-1e1c-4fee-b65c-9406f4737a91 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep 17 00:16:26.174: INFO: DNS probes using dns-1612/dns-test-f0569aa2-1e1c-4fee-b65c-9406f4737a91 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:16:26.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1612" for this suite.

• [SLOW TEST:9.192 seconds]
[sig-network] DNS
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":277,"completed":221,"skipped":3705,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:16:26.233: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-c428a223-1072-4375-accb-60e9b8b36dd9
STEP: Creating a pod to test consume configMaps
Sep 17 00:16:26.320: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0028a211-238b-4164-b0b3-b124029be0d4" in namespace "projected-8082" to be "Succeeded or Failed"
Sep 17 00:16:26.322: INFO: Pod "pod-projected-configmaps-0028a211-238b-4164-b0b3-b124029be0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.16993ms
Sep 17 00:16:28.325: INFO: Pod "pod-projected-configmaps-0028a211-238b-4164-b0b3-b124029be0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005461085s
Sep 17 00:16:30.328: INFO: Pod "pod-projected-configmaps-0028a211-238b-4164-b0b3-b124029be0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008003261s
STEP: Saw pod success
Sep 17 00:16:30.328: INFO: Pod "pod-projected-configmaps-0028a211-238b-4164-b0b3-b124029be0d4" satisfied condition "Succeeded or Failed"
Sep 17 00:16:30.330: INFO: Trying to get logs from node eqx03-flash06 pod pod-projected-configmaps-0028a211-238b-4164-b0b3-b124029be0d4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 17 00:16:30.378: INFO: Waiting for pod pod-projected-configmaps-0028a211-238b-4164-b0b3-b124029be0d4 to disappear
Sep 17 00:16:30.379: INFO: Pod pod-projected-configmaps-0028a211-238b-4164-b0b3-b124029be0d4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:16:30.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8082" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":222,"skipped":3756,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:16:30.393: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Sep 17 00:16:35.014: INFO: Successfully updated pod "labelsupdatef1007c0e-044d-44e8-b900-e399996b7ec7"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:16:37.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-40" for this suite.

• [SLOW TEST:6.755 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":277,"completed":223,"skipped":3799,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:16:37.148: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test override command
Sep 17 00:16:37.231: INFO: Waiting up to 5m0s for pod "client-containers-ef12d65c-091d-4822-8eda-27c710fcce5a" in namespace "containers-2895" to be "Succeeded or Failed"
Sep 17 00:16:37.233: INFO: Pod "client-containers-ef12d65c-091d-4822-8eda-27c710fcce5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027388ms
Sep 17 00:16:39.237: INFO: Pod "client-containers-ef12d65c-091d-4822-8eda-27c710fcce5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005389052s
Sep 17 00:16:41.240: INFO: Pod "client-containers-ef12d65c-091d-4822-8eda-27c710fcce5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008782363s
STEP: Saw pod success
Sep 17 00:16:41.240: INFO: Pod "client-containers-ef12d65c-091d-4822-8eda-27c710fcce5a" satisfied condition "Succeeded or Failed"
Sep 17 00:16:41.243: INFO: Trying to get logs from node eqx03-flash06 pod client-containers-ef12d65c-091d-4822-8eda-27c710fcce5a container test-container: <nil>
STEP: delete the pod
Sep 17 00:16:41.278: INFO: Waiting for pod client-containers-ef12d65c-091d-4822-8eda-27c710fcce5a to disappear
Sep 17 00:16:41.280: INFO: Pod client-containers-ef12d65c-091d-4822-8eda-27c710fcce5a no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:16:41.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2895" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":277,"completed":224,"skipped":3810,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:16:41.312: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 17 00:16:41.387: INFO: Create a RollingUpdate DaemonSet
Sep 17 00:16:41.394: INFO: Check that daemon pods launch on every node of the cluster
Sep 17 00:16:41.410: INFO: Number of nodes with available pods: 0
Sep 17 00:16:41.410: INFO: Node eqx03-flash06 is running more than one daemon pod
Sep 17 00:16:42.417: INFO: Number of nodes with available pods: 0
Sep 17 00:16:42.417: INFO: Node eqx03-flash06 is running more than one daemon pod
Sep 17 00:16:43.422: INFO: Number of nodes with available pods: 0
Sep 17 00:16:43.422: INFO: Node eqx03-flash06 is running more than one daemon pod
Sep 17 00:16:44.419: INFO: Number of nodes with available pods: 2
Sep 17 00:16:44.419: INFO: Node eqx03-flash06 is running more than one daemon pod
Sep 17 00:16:45.418: INFO: Number of nodes with available pods: 4
Sep 17 00:16:45.418: INFO: Number of running nodes: 4, number of available pods: 4
Sep 17 00:16:45.418: INFO: Update the DaemonSet to trigger a rollout
Sep 17 00:16:45.435: INFO: Updating DaemonSet daemon-set
Sep 17 00:16:48.447: INFO: Roll back the DaemonSet before rollout is complete
Sep 17 00:16:48.455: INFO: Updating DaemonSet daemon-set
Sep 17 00:16:48.455: INFO: Make sure DaemonSet rollback is complete
Sep 17 00:16:48.458: INFO: Wrong image for pod: daemon-set-lgjld. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 17 00:16:48.458: INFO: Pod daemon-set-lgjld is not available
Sep 17 00:16:49.474: INFO: Wrong image for pod: daemon-set-lgjld. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 17 00:16:49.474: INFO: Pod daemon-set-lgjld is not available
Sep 17 00:16:50.474: INFO: Wrong image for pod: daemon-set-lgjld. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 17 00:16:50.474: INFO: Pod daemon-set-lgjld is not available
Sep 17 00:16:51.473: INFO: Pod daemon-set-lkr42 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3665, will wait for the garbage collector to delete the pods
Sep 17 00:16:51.542: INFO: Deleting DaemonSet.extensions daemon-set took: 8.383346ms
Sep 17 00:16:52.442: INFO: Terminating DaemonSet.extensions daemon-set pods took: 900.201656ms
Sep 17 00:18:11.845: INFO: Number of nodes with available pods: 0
Sep 17 00:18:11.845: INFO: Number of running nodes: 0, number of available pods: 0
Sep 17 00:18:11.847: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3665/daemonsets","resourceVersion":"4250683"},"items":null}

Sep 17 00:18:11.848: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3665/pods","resourceVersion":"4250683"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:18:11.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3665" for this suite.

• [SLOW TEST:90.565 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":277,"completed":225,"skipped":3819,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:18:11.877: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-map-5cdc0eaf-9d87-4a89-bc47-bafaf46ec3a2
STEP: Creating a pod to test consume configMaps
Sep 17 00:18:11.956: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bb12cfca-56db-4672-81de-8b44025818df" in namespace "projected-9208" to be "Succeeded or Failed"
Sep 17 00:18:11.958: INFO: Pod "pod-projected-configmaps-bb12cfca-56db-4672-81de-8b44025818df": Phase="Pending", Reason="", readiness=false. Elapsed: 1.97956ms
Sep 17 00:18:13.967: INFO: Pod "pod-projected-configmaps-bb12cfca-56db-4672-81de-8b44025818df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011688954s
Sep 17 00:18:15.970: INFO: Pod "pod-projected-configmaps-bb12cfca-56db-4672-81de-8b44025818df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014288282s
STEP: Saw pod success
Sep 17 00:18:15.970: INFO: Pod "pod-projected-configmaps-bb12cfca-56db-4672-81de-8b44025818df" satisfied condition "Succeeded or Failed"
Sep 17 00:18:15.975: INFO: Trying to get logs from node eqx03-flash06 pod pod-projected-configmaps-bb12cfca-56db-4672-81de-8b44025818df container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 17 00:18:16.030: INFO: Waiting for pod pod-projected-configmaps-bb12cfca-56db-4672-81de-8b44025818df to disappear
Sep 17 00:18:16.031: INFO: Pod pod-projected-configmaps-bb12cfca-56db-4672-81de-8b44025818df no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:18:16.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9208" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":226,"skipped":3820,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:18:16.046: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: starting the proxy server
Sep 17 00:18:16.095: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-480724876 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:18:16.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6403" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":277,"completed":227,"skipped":3844,"failed":0}
S
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:18:16.210: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Sep 17 00:18:22.300: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6453 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 17 00:18:22.300: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
Sep 17 00:18:22.538: INFO: Exec stderr: ""
Sep 17 00:18:22.538: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6453 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 17 00:18:22.538: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
Sep 17 00:18:22.776: INFO: Exec stderr: ""
Sep 17 00:18:22.776: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6453 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 17 00:18:22.776: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
Sep 17 00:18:23.180: INFO: Exec stderr: ""
Sep 17 00:18:23.180: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6453 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 17 00:18:23.180: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
Sep 17 00:18:23.477: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Sep 17 00:18:23.477: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6453 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 17 00:18:23.477: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
Sep 17 00:18:23.718: INFO: Exec stderr: ""
Sep 17 00:18:23.718: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6453 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 17 00:18:23.718: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
Sep 17 00:18:23.954: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Sep 17 00:18:23.954: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6453 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 17 00:18:23.955: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
Sep 17 00:18:24.233: INFO: Exec stderr: ""
Sep 17 00:18:24.233: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6453 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 17 00:18:24.233: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
Sep 17 00:18:24.468: INFO: Exec stderr: ""
Sep 17 00:18:24.468: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6453 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 17 00:18:24.468: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
Sep 17 00:18:24.799: INFO: Exec stderr: ""
Sep 17 00:18:24.799: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6453 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 17 00:18:24.799: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
Sep 17 00:18:25.089: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:18:25.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-6453" for this suite.

• [SLOW TEST:8.906 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":228,"skipped":3845,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:18:25.117: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 17 00:18:25.257: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"45606ef8-79ad-40b6-8bc1-1db95c7bace0", Controller:(*bool)(0xc002b5e9aa), BlockOwnerDeletion:(*bool)(0xc002b5e9ab)}}
Sep 17 00:18:25.270: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"69424c63-21b7-4dc5-a0b8-47c237b3fcd4", Controller:(*bool)(0xc0045fda42), BlockOwnerDeletion:(*bool)(0xc0045fda43)}}
Sep 17 00:18:25.288: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"5bd098ac-0b8a-4e8d-a3bc-0d28d5e854a2", Controller:(*bool)(0xc002b5ebba), BlockOwnerDeletion:(*bool)(0xc002b5ebbb)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:18:30.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7885" for this suite.

• [SLOW TEST:5.213 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":277,"completed":229,"skipped":3851,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:18:30.330: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Sep 17 00:18:30.829: INFO: Pod name wrapped-volume-race-2830e463-452c-413c-b90c-391f255133ec: Found 0 pods out of 5
Sep 17 00:18:35.835: INFO: Pod name wrapped-volume-race-2830e463-452c-413c-b90c-391f255133ec: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-2830e463-452c-413c-b90c-391f255133ec in namespace emptydir-wrapper-5708, will wait for the garbage collector to delete the pods
Sep 17 00:18:45.921: INFO: Deleting ReplicationController wrapped-volume-race-2830e463-452c-413c-b90c-391f255133ec took: 15.4531ms
Sep 17 00:18:46.821: INFO: Terminating ReplicationController wrapped-volume-race-2830e463-452c-413c-b90c-391f255133ec pods took: 900.179611ms
STEP: Creating RC which spawns configmap-volume pods
Sep 17 00:18:58.445: INFO: Pod name wrapped-volume-race-c48a1cb2-34e9-45c1-82d6-e6b2083de286: Found 0 pods out of 5
Sep 17 00:19:03.450: INFO: Pod name wrapped-volume-race-c48a1cb2-34e9-45c1-82d6-e6b2083de286: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-c48a1cb2-34e9-45c1-82d6-e6b2083de286 in namespace emptydir-wrapper-5708, will wait for the garbage collector to delete the pods
Sep 17 00:19:15.541: INFO: Deleting ReplicationController wrapped-volume-race-c48a1cb2-34e9-45c1-82d6-e6b2083de286 took: 10.606753ms
Sep 17 00:19:16.441: INFO: Terminating ReplicationController wrapped-volume-race-c48a1cb2-34e9-45c1-82d6-e6b2083de286 pods took: 900.182871ms
STEP: Creating RC which spawns configmap-volume pods
Sep 17 00:19:20.467: INFO: Pod name wrapped-volume-race-692cf84f-0562-4e2a-8f99-336b0db6196e: Found 0 pods out of 5
Sep 17 00:19:25.473: INFO: Pod name wrapped-volume-race-692cf84f-0562-4e2a-8f99-336b0db6196e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-692cf84f-0562-4e2a-8f99-336b0db6196e in namespace emptydir-wrapper-5708, will wait for the garbage collector to delete the pods
Sep 17 00:19:37.593: INFO: Deleting ReplicationController wrapped-volume-race-692cf84f-0562-4e2a-8f99-336b0db6196e took: 44.238574ms
Sep 17 00:19:37.693: INFO: Terminating ReplicationController wrapped-volume-race-692cf84f-0562-4e2a-8f99-336b0db6196e pods took: 100.169283ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:19:52.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5708" for this suite.

• [SLOW TEST:82.221 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":277,"completed":230,"skipped":3884,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:19:52.551: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Sep 17 00:19:52.631: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Sep 17 00:20:12.977: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
Sep 17 00:20:18.106: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:20:40.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7879" for this suite.

• [SLOW TEST:47.720 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":277,"completed":231,"skipped":3896,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:20:40.271: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-2f925c30-8857-4389-be65-3190298963b8
STEP: Creating a pod to test consume secrets
Sep 17 00:20:40.368: INFO: Waiting up to 5m0s for pod "pod-secrets-44df1292-9834-42b0-99c3-822685fdab3f" in namespace "secrets-2715" to be "Succeeded or Failed"
Sep 17 00:20:40.370: INFO: Pod "pod-secrets-44df1292-9834-42b0-99c3-822685fdab3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.239991ms
Sep 17 00:20:42.374: INFO: Pod "pod-secrets-44df1292-9834-42b0-99c3-822685fdab3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005926937s
Sep 17 00:20:44.377: INFO: Pod "pod-secrets-44df1292-9834-42b0-99c3-822685fdab3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008989906s
STEP: Saw pod success
Sep 17 00:20:44.377: INFO: Pod "pod-secrets-44df1292-9834-42b0-99c3-822685fdab3f" satisfied condition "Succeeded or Failed"
Sep 17 00:20:44.379: INFO: Trying to get logs from node eqx03-flash06 pod pod-secrets-44df1292-9834-42b0-99c3-822685fdab3f container secret-env-test: <nil>
STEP: delete the pod
Sep 17 00:20:44.445: INFO: Waiting for pod pod-secrets-44df1292-9834-42b0-99c3-822685fdab3f to disappear
Sep 17 00:20:44.447: INFO: Pod pod-secrets-44df1292-9834-42b0-99c3-822685fdab3f no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:20:44.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2715" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":277,"completed":232,"skipped":3906,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:20:44.460: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating Agnhost RC
Sep 17 00:20:44.508: INFO: namespace kubectl-5903
Sep 17 00:20:44.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 create -f - --namespace=kubectl-5903'
Sep 17 00:20:44.909: INFO: stderr: ""
Sep 17 00:20:44.909: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Sep 17 00:20:45.912: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 17 00:20:45.912: INFO: Found 0 / 1
Sep 17 00:20:46.913: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 17 00:20:46.913: INFO: Found 0 / 1
Sep 17 00:20:47.912: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 17 00:20:47.912: INFO: Found 1 / 1
Sep 17 00:20:47.912: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 17 00:20:47.914: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 17 00:20:47.914: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 17 00:20:47.914: INFO: wait on agnhost-master startup in kubectl-5903 
Sep 17 00:20:47.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 logs agnhost-master-hcz2z agnhost-master --namespace=kubectl-5903'
Sep 17 00:20:48.036: INFO: stderr: ""
Sep 17 00:20:48.036: INFO: stdout: "Paused\n"
STEP: exposing RC
Sep 17 00:20:48.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 expose rc agnhost-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-5903'
Sep 17 00:20:48.165: INFO: stderr: ""
Sep 17 00:20:48.165: INFO: stdout: "service/rm2 exposed\n"
Sep 17 00:20:48.170: INFO: Service rm2 in namespace kubectl-5903 found.
STEP: exposing service
Sep 17 00:20:50.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-5903'
Sep 17 00:20:50.300: INFO: stderr: ""
Sep 17 00:20:50.300: INFO: stdout: "service/rm3 exposed\n"
Sep 17 00:20:50.322: INFO: Service rm3 in namespace kubectl-5903 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:20:52.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5903" for this suite.

• [SLOW TEST:7.877 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1119
    should create services for rc  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":277,"completed":233,"skipped":3932,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:20:52.337: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Sep 17 00:20:52.390: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the sample API server.
Sep 17 00:20:53.052: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Sep 17 00:20:55.127: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898853, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898853, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898853, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898853, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 17 00:20:57.131: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898853, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898853, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898853, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898853, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 17 00:20:59.130: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898853, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898853, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898853, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898853, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 17 00:21:01.978: INFO: Waited 822.112797ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:21:02.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-1882" for this suite.

• [SLOW TEST:10.733 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":277,"completed":234,"skipped":3939,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:21:03.071: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep 17 00:21:03.127: INFO: Waiting up to 5m0s for pod "pod-4a2984d0-218a-48a9-ae19-0fd53693caf6" in namespace "emptydir-3247" to be "Succeeded or Failed"
Sep 17 00:21:03.129: INFO: Pod "pod-4a2984d0-218a-48a9-ae19-0fd53693caf6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.970411ms
Sep 17 00:21:05.133: INFO: Pod "pod-4a2984d0-218a-48a9-ae19-0fd53693caf6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005701732s
Sep 17 00:21:07.136: INFO: Pod "pod-4a2984d0-218a-48a9-ae19-0fd53693caf6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008970364s
STEP: Saw pod success
Sep 17 00:21:07.136: INFO: Pod "pod-4a2984d0-218a-48a9-ae19-0fd53693caf6" satisfied condition "Succeeded or Failed"
Sep 17 00:21:07.138: INFO: Trying to get logs from node eqx03-flash06 pod pod-4a2984d0-218a-48a9-ae19-0fd53693caf6 container test-container: <nil>
STEP: delete the pod
Sep 17 00:21:07.227: INFO: Waiting for pod pod-4a2984d0-218a-48a9-ae19-0fd53693caf6 to disappear
Sep 17 00:21:07.229: INFO: Pod pod-4a2984d0-218a-48a9-ae19-0fd53693caf6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:21:07.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3247" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":235,"skipped":3992,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:21:07.242: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-d64d1d67-9503-488b-b158-1781b7ad3243
STEP: Creating a pod to test consume configMaps
Sep 17 00:21:07.323: INFO: Waiting up to 5m0s for pod "pod-configmaps-8b791a0b-8d66-43dc-aa7b-a4d1cf1f738f" in namespace "configmap-5388" to be "Succeeded or Failed"
Sep 17 00:21:07.330: INFO: Pod "pod-configmaps-8b791a0b-8d66-43dc-aa7b-a4d1cf1f738f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.22564ms
Sep 17 00:21:09.332: INFO: Pod "pod-configmaps-8b791a0b-8d66-43dc-aa7b-a4d1cf1f738f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009960068s
STEP: Saw pod success
Sep 17 00:21:09.333: INFO: Pod "pod-configmaps-8b791a0b-8d66-43dc-aa7b-a4d1cf1f738f" satisfied condition "Succeeded or Failed"
Sep 17 00:21:09.334: INFO: Trying to get logs from node eqx03-flash06 pod pod-configmaps-8b791a0b-8d66-43dc-aa7b-a4d1cf1f738f container configmap-volume-test: <nil>
STEP: delete the pod
Sep 17 00:21:09.396: INFO: Waiting for pod pod-configmaps-8b791a0b-8d66-43dc-aa7b-a4d1cf1f738f to disappear
Sep 17 00:21:09.398: INFO: Pod pod-configmaps-8b791a0b-8d66-43dc-aa7b-a4d1cf1f738f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:21:09.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5388" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":277,"completed":236,"skipped":4003,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:21:09.417: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Sep 17 00:21:10.314: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Sep 17 00:21:12.321: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898870, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898870, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898870, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898870, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-65c6cd5fdf\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 17 00:21:15.348: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 17 00:21:15.351: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:21:16.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2083" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.221 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":277,"completed":237,"skipped":4065,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:21:16.638: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Sep 17 00:21:21.229: INFO: Successfully updated pod "adopt-release-r7g6s"
STEP: Checking that the Job readopts the Pod
Sep 17 00:21:21.229: INFO: Waiting up to 15m0s for pod "adopt-release-r7g6s" in namespace "job-1295" to be "adopted"
Sep 17 00:21:21.231: INFO: Pod "adopt-release-r7g6s": Phase="Running", Reason="", readiness=true. Elapsed: 2.085012ms
Sep 17 00:21:23.234: INFO: Pod "adopt-release-r7g6s": Phase="Running", Reason="", readiness=true. Elapsed: 2.005061323s
Sep 17 00:21:23.234: INFO: Pod "adopt-release-r7g6s" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Sep 17 00:21:23.747: INFO: Successfully updated pod "adopt-release-r7g6s"
STEP: Checking that the Job releases the Pod
Sep 17 00:21:23.747: INFO: Waiting up to 15m0s for pod "adopt-release-r7g6s" in namespace "job-1295" to be "released"
Sep 17 00:21:23.749: INFO: Pod "adopt-release-r7g6s": Phase="Running", Reason="", readiness=true. Elapsed: 2.048053ms
Sep 17 00:21:25.752: INFO: Pod "adopt-release-r7g6s": Phase="Running", Reason="", readiness=true. Elapsed: 2.005038149s
Sep 17 00:21:25.752: INFO: Pod "adopt-release-r7g6s" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:21:25.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1295" for this suite.

• [SLOW TEST:9.125 seconds]
[sig-apps] Job
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":277,"completed":238,"skipped":4124,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:21:25.763: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3024.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3024.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3024.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3024.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 17 00:21:29.868: INFO: DNS probes using dns-test-cf44c625-664a-434d-a3d6-3841c3a219ad succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3024.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3024.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3024.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3024.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 17 00:21:33.929: INFO: DNS probes using dns-test-47976ce0-8660-4a17-abe1-60259a3a3cde succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3024.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3024.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3024.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3024.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 17 00:21:38.031: INFO: DNS probes using dns-test-1178d8d0-2990-4369-b5ea-ee6765bddf97 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:21:38.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3024" for this suite.

• [SLOW TEST:12.367 seconds]
[sig-network] DNS
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":277,"completed":239,"skipped":4125,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:21:38.130: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 17 00:21:38.469: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 17 00:21:40.477: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898898, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898898, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898898, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898898, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 17 00:21:43.499: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:21:43.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3933" for this suite.
STEP: Destroying namespace "webhook-3933-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.527 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":277,"completed":240,"skipped":4126,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:21:43.658: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep 17 00:21:43.715: INFO: Waiting up to 5m0s for pod "pod-bd904500-a65c-43d3-b884-998bf1e9e941" in namespace "emptydir-4194" to be "Succeeded or Failed"
Sep 17 00:21:43.728: INFO: Pod "pod-bd904500-a65c-43d3-b884-998bf1e9e941": Phase="Pending", Reason="", readiness=false. Elapsed: 12.985954ms
Sep 17 00:21:45.731: INFO: Pod "pod-bd904500-a65c-43d3-b884-998bf1e9e941": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016060586s
Sep 17 00:21:47.734: INFO: Pod "pod-bd904500-a65c-43d3-b884-998bf1e9e941": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01917021s
STEP: Saw pod success
Sep 17 00:21:47.734: INFO: Pod "pod-bd904500-a65c-43d3-b884-998bf1e9e941" satisfied condition "Succeeded or Failed"
Sep 17 00:21:47.736: INFO: Trying to get logs from node eqx03-flash06 pod pod-bd904500-a65c-43d3-b884-998bf1e9e941 container test-container: <nil>
STEP: delete the pod
Sep 17 00:21:47.767: INFO: Waiting for pod pod-bd904500-a65c-43d3-b884-998bf1e9e941 to disappear
Sep 17 00:21:47.769: INFO: Pod pod-bd904500-a65c-43d3-b884-998bf1e9e941 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:21:47.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4194" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":241,"skipped":4141,"failed":0}
SSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:21:47.793: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Sep 17 00:21:51.868: INFO: &Pod{ObjectMeta:{send-events-d887b3c4-2062-4aa4-a7c9-518da6d56398  events-2565 /api/v1/namespaces/events-2565/pods/send-events-d887b3c4-2062-4aa4-a7c9-518da6d56398 a00cc576-08d6-471a-9a15-75134f618e6f 4253254 0 2020-09-17 00:21:47 +0000 UTC <nil> <nil> map[name:foo time:844481843] map[cni.projectcalico.org/podIP:172.21.13.123/32 cni.projectcalico.org/podIPs:172.21.13.123/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "calico",
    "ips": [
        "172.21.13.123"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "calico",
    "ips": [
        "172.21.13.123"
    ],
    "default": true,
    "dns": {}
}]] [] []  [{e2e.test Update v1 2020-09-17 00:21:47 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 116 105 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 112 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 114 103 115 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 114 116 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 99 111 110 116 97 105 110 101 114 80 111 114 116 92 34 58 56 48 44 92 34 112 114 111 116 111 99 111 108 92 34 58 92 34 84 67 80 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 99 111 110 116 97 105 110 101 114 80 111 114 116 34 58 123 125 44 34 102 58 112 114 111 116 111 99 111 108 34 58 123 125 125 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-09-17 00:21:49 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-09-17 00:21:49 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 50 49 46 49 51 46 49 50 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}} {multus Update v1 2020-09-17 00:21:49 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 107 56 115 46 118 49 46 99 110 105 46 99 110 99 102 46 105 111 47 110 101 116 119 111 114 107 45 115 116 97 116 117 115 34 58 123 125 44 34 102 58 107 56 115 46 118 49 46 99 110 105 46 99 110 99 102 46 105 111 47 110 101 116 119 111 114 107 115 45 115 116 97 116 117 115 34 58 123 125 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sx4q9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sx4q9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sx4q9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:21:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:21:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:21:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:21:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.140.106,PodIP:172.21.13.123,StartTime:2020-09-17 00:21:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-17 00:21:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,ImageID:docker-pullable://us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c,ContainerID:robin://d23497b56f9c4de7bd492b4d4749247b98f685e723dbaeafa7cdfb7fd650ba58,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.13.123,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Sep 17 00:21:53.871: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Sep 17 00:21:55.875: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:21:55.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2565" for this suite.

• [SLOW TEST:8.113 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":277,"completed":242,"skipped":4145,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:21:55.907: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:21:59.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6665" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":277,"completed":243,"skipped":4170,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:22:00.006: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 17 00:22:00.110: INFO: Pod name rollover-pod: Found 0 pods out of 1
Sep 17 00:22:05.114: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 17 00:22:05.114: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Sep 17 00:22:07.117: INFO: Creating deployment "test-rollover-deployment"
Sep 17 00:22:07.135: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Sep 17 00:22:09.171: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Sep 17 00:22:09.179: INFO: Ensure that both replica sets have 1 created replica
Sep 17 00:22:09.183: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Sep 17 00:22:09.194: INFO: Updating deployment test-rollover-deployment
Sep 17 00:22:09.194: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Sep 17 00:22:11.199: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Sep 17 00:22:11.204: INFO: Make sure deployment "test-rollover-deployment" is complete
Sep 17 00:22:11.209: INFO: all replica sets need to contain the pod-template-hash label
Sep 17 00:22:11.209: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898927, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898927, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898929, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898927, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 17 00:22:13.215: INFO: all replica sets need to contain the pod-template-hash label
Sep 17 00:22:13.215: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898927, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898927, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898932, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898927, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 17 00:22:15.216: INFO: all replica sets need to contain the pod-template-hash label
Sep 17 00:22:15.216: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898927, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898927, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898932, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898927, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 17 00:22:17.215: INFO: all replica sets need to contain the pod-template-hash label
Sep 17 00:22:17.215: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898927, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898927, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898932, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898927, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 17 00:22:19.215: INFO: all replica sets need to contain the pod-template-hash label
Sep 17 00:22:19.215: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898927, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898927, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898932, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898927, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 17 00:22:21.216: INFO: all replica sets need to contain the pod-template-hash label
Sep 17 00:22:21.216: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898927, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898927, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898932, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898927, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 17 00:22:23.215: INFO: 
Sep 17 00:22:23.215: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Sep 17 00:22:23.223: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-6344 /apis/apps/v1/namespaces/deployment-6344/deployments/test-rollover-deployment c34a4a71-c76c-41e6-aad0-fce25f64a644 4253517 2 2020-09-17 00:22:07 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-09-17 00:22:09 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 109 105 110 82 101 97 100 121 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-09-17 00:22:22 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0030a4588 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-09-17 00:22:07 +0000 UTC,LastTransitionTime:2020-09-17 00:22:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-84f7f6f64b" has successfully progressed.,LastUpdateTime:2020-09-17 00:22:22 +0000 UTC,LastTransitionTime:2020-09-17 00:22:07 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 17 00:22:23.226: INFO: New ReplicaSet "test-rollover-deployment-84f7f6f64b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-84f7f6f64b  deployment-6344 /apis/apps/v1/namespaces/deployment-6344/replicasets/test-rollover-deployment-84f7f6f64b d1626983-1065-485f-9ea3-d76dd610f66c 4253507 2 2020-09-17 00:22:09 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:84f7f6f64b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment c34a4a71-c76c-41e6-aad0-fce25f64a644 0xc0030a5f87 0xc0030a5f88}] []  [{kube-controller-manager Update apps/v1 2020-09-17 00:22:22 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 99 51 52 97 52 97 55 49 45 99 55 54 99 45 52 49 101 54 45 97 97 100 48 45 102 99 101 50 53 102 54 52 97 54 52 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 109 105 110 82 101 97 100 121 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 84f7f6f64b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:84f7f6f64b] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0006da0d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 17 00:22:23.226: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Sep 17 00:22:23.226: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6344 /apis/apps/v1/namespaces/deployment-6344/replicasets/test-rollover-controller 2de667cc-f7b0-4551-bc82-3a53bc1f0ccb 4253516 2 2020-09-17 00:22:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment c34a4a71-c76c-41e6-aad0-fce25f64a644 0xc0030a5377 0xc0030a5378}] []  [{e2e.test Update apps/v1 2020-09-17 00:22:00 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-09-17 00:22:22 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 99 51 52 97 52 97 55 49 45 99 55 54 99 45 52 49 101 54 45 97 97 100 48 45 102 99 101 50 53 102 54 52 97 54 52 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0030a5658 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 17 00:22:23.226: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-5686c4cfd5  deployment-6344 /apis/apps/v1/namespaces/deployment-6344/replicasets/test-rollover-deployment-5686c4cfd5 9f71a0cc-f930-48fe-89e7-6c85bb0307df 4253432 2 2020-09-17 00:22:07 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5686c4cfd5] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment c34a4a71-c76c-41e6-aad0-fce25f64a644 0xc0030a58b7 0xc0030a58b8}] []  [{kube-controller-manager Update apps/v1 2020-09-17 00:22:09 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 99 51 52 97 52 97 55 49 45 99 55 54 99 45 52 49 101 54 45 97 97 100 48 45 102 99 101 50 53 102 54 52 97 54 52 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 109 105 110 82 101 97 100 121 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 114 101 100 105 115 45 115 108 97 118 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5686c4cfd5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5686c4cfd5] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0030a5e68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 17 00:22:23.229: INFO: Pod "test-rollover-deployment-84f7f6f64b-xxblq" is available:
&Pod{ObjectMeta:{test-rollover-deployment-84f7f6f64b-xxblq test-rollover-deployment-84f7f6f64b- deployment-6344 /api/v1/namespaces/deployment-6344/pods/test-rollover-deployment-84f7f6f64b-xxblq d83ba25d-cd05-4c6c-82e4-559f4792a274 4253464 0 2020-09-17 00:22:09 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:84f7f6f64b] map[cni.projectcalico.org/podIP:172.21.4.78/32 cni.projectcalico.org/podIPs:172.21.4.78/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "calico",
    "ips": [
        "172.21.4.78"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "calico",
    "ips": [
        "172.21.4.78"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-rollover-deployment-84f7f6f64b d1626983-1065-485f-9ea3-d76dd610f66c 0xc0006daba7 0xc0006daba8}] []  [{kube-controller-manager Update v1 2020-09-17 00:22:09 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 100 49 54 50 54 57 56 51 45 49 48 54 53 45 52 56 53 102 45 57 101 97 51 45 100 55 54 100 100 54 49 48 102 54 54 99 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-09-17 00:22:10 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {multus Update v1 2020-09-17 00:22:10 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 107 56 115 46 118 49 46 99 110 105 46 99 110 99 102 46 105 111 47 110 101 116 119 111 114 107 45 115 116 97 116 117 115 34 58 123 125 44 34 102 58 107 56 115 46 118 49 46 99 110 105 46 99 110 99 102 46 105 111 47 110 101 116 119 111 114 107 115 45 115 116 97 116 117 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-09-17 00:22:12 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 50 49 46 52 46 55 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-r42jw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-r42jw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-r42jw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:22:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:22:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:22:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:22:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.140.106,PodIP:172.21.4.78,StartTime:2020-09-17 00:22:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-17 00:22:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,ImageID:docker-pullable://us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c,ContainerID:robin://dd719afdfc3aa253423e9782414ddbc04c028cba21dc1d368159da840c467101,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.4.78,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:22:23.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6344" for this suite.

• [SLOW TEST:23.238 seconds]
[sig-apps] Deployment
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":277,"completed":244,"skipped":4173,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:22:23.244: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:22:23.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8587" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":277,"completed":245,"skipped":4186,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:22:23.452: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep 17 00:22:31.565: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 17 00:22:31.568: INFO: Pod pod-with-prestop-http-hook still exists
Sep 17 00:22:33.568: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 17 00:22:33.572: INFO: Pod pod-with-prestop-http-hook still exists
Sep 17 00:22:35.568: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 17 00:22:35.571: INFO: Pod pod-with-prestop-http-hook still exists
Sep 17 00:22:37.568: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 17 00:22:37.572: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:22:37.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9345" for this suite.

• [SLOW TEST:14.156 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":277,"completed":246,"skipped":4197,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:22:37.608: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 17 00:22:37.682: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 17 00:22:43.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 --namespace=crd-publish-openapi-8326 create -f -'
Sep 17 00:22:43.755: INFO: stderr: ""
Sep 17 00:22:43.755: INFO: stdout: "e2e-test-crd-publish-openapi-8390-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep 17 00:22:43.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 --namespace=crd-publish-openapi-8326 delete e2e-test-crd-publish-openapi-8390-crds test-cr'
Sep 17 00:22:43.890: INFO: stderr: ""
Sep 17 00:22:43.890: INFO: stdout: "e2e-test-crd-publish-openapi-8390-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Sep 17 00:22:43.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 --namespace=crd-publish-openapi-8326 apply -f -'
Sep 17 00:22:44.252: INFO: stderr: ""
Sep 17 00:22:44.252: INFO: stdout: "e2e-test-crd-publish-openapi-8390-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep 17 00:22:44.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 --namespace=crd-publish-openapi-8326 delete e2e-test-crd-publish-openapi-8390-crds test-cr'
Sep 17 00:22:44.344: INFO: stderr: ""
Sep 17 00:22:44.344: INFO: stdout: "e2e-test-crd-publish-openapi-8390-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Sep 17 00:22:44.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 explain e2e-test-crd-publish-openapi-8390-crds'
Sep 17 00:22:44.640: INFO: stderr: ""
Sep 17 00:22:44.640: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8390-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:22:49.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8326" for this suite.

• [SLOW TEST:11.827 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":277,"completed":247,"skipped":4218,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:22:49.435: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 17 00:22:50.174: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 17 00:22:52.182: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898970, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898970, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898970, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735898970, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 17 00:22:55.209: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:22:55.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9163" for this suite.
STEP: Destroying namespace "webhook-9163-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.980 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":277,"completed":248,"skipped":4219,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:22:55.415: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap configmap-2716/configmap-test-7cb79cde-f419-43c9-b7d9-7c1d3ef7e594
STEP: Creating a pod to test consume configMaps
Sep 17 00:22:55.491: INFO: Waiting up to 5m0s for pod "pod-configmaps-e0c29811-0e4e-4607-aff2-8b892e26b63d" in namespace "configmap-2716" to be "Succeeded or Failed"
Sep 17 00:22:55.493: INFO: Pod "pod-configmaps-e0c29811-0e4e-4607-aff2-8b892e26b63d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.950782ms
Sep 17 00:22:57.496: INFO: Pod "pod-configmaps-e0c29811-0e4e-4607-aff2-8b892e26b63d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004655667s
Sep 17 00:22:59.505: INFO: Pod "pod-configmaps-e0c29811-0e4e-4607-aff2-8b892e26b63d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013908733s
STEP: Saw pod success
Sep 17 00:22:59.505: INFO: Pod "pod-configmaps-e0c29811-0e4e-4607-aff2-8b892e26b63d" satisfied condition "Succeeded or Failed"
Sep 17 00:22:59.508: INFO: Trying to get logs from node eqx03-flash06 pod pod-configmaps-e0c29811-0e4e-4607-aff2-8b892e26b63d container env-test: <nil>
STEP: delete the pod
Sep 17 00:22:59.548: INFO: Waiting for pod pod-configmaps-e0c29811-0e4e-4607-aff2-8b892e26b63d to disappear
Sep 17 00:22:59.550: INFO: Pod pod-configmaps-e0c29811-0e4e-4607-aff2-8b892e26b63d no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:22:59.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2716" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":277,"completed":249,"skipped":4253,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:22:59.564: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-4942
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4942
STEP: creating replication controller externalsvc in namespace services-4942
I0917 00:22:59.688902      23 runners.go:190] Created replication controller with name: externalsvc, namespace: services-4942, replica count: 2
I0917 00:23:02.739361      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Sep 17 00:23:02.764: INFO: Creating new exec pod
Sep 17 00:23:04.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 exec --namespace=services-4942 execpodc6ddj -- /bin/sh -x -c nslookup clusterip-service'
Sep 17 00:23:05.181: INFO: stderr: "+ nslookup clusterip-service\n"
Sep 17 00:23:05.181: INFO: stdout: "Server:\t\t172.19.0.10\nAddress:\t172.19.0.10#53\n\nclusterip-service.services-4942.svc.cluster.local\tcanonical name = externalsvc.services-4942.svc.cluster.local.\nName:\texternalsvc.services-4942.svc.cluster.local\nAddress: 172.19.61.135\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4942, will wait for the garbage collector to delete the pods
Sep 17 00:23:05.254: INFO: Deleting ReplicationController externalsvc took: 20.433975ms
Sep 17 00:23:05.354: INFO: Terminating ReplicationController externalsvc pods took: 100.234251ms
Sep 17 00:23:21.086: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:23:21.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4942" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:21.550 seconds]
[sig-network] Services
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":277,"completed":250,"skipped":4264,"failed":0}
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:23:21.114: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-configmap-wlmp
STEP: Creating a pod to test atomic-volume-subpath
Sep 17 00:23:21.197: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-wlmp" in namespace "subpath-4529" to be "Succeeded or Failed"
Sep 17 00:23:21.199: INFO: Pod "pod-subpath-test-configmap-wlmp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035662ms
Sep 17 00:23:23.206: INFO: Pod "pod-subpath-test-configmap-wlmp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009079449s
Sep 17 00:23:25.209: INFO: Pod "pod-subpath-test-configmap-wlmp": Phase="Running", Reason="", readiness=true. Elapsed: 4.012211482s
Sep 17 00:23:27.212: INFO: Pod "pod-subpath-test-configmap-wlmp": Phase="Running", Reason="", readiness=true. Elapsed: 6.015084176s
Sep 17 00:23:29.215: INFO: Pod "pod-subpath-test-configmap-wlmp": Phase="Running", Reason="", readiness=true. Elapsed: 8.018474672s
Sep 17 00:23:31.219: INFO: Pod "pod-subpath-test-configmap-wlmp": Phase="Running", Reason="", readiness=true. Elapsed: 10.022196781s
Sep 17 00:23:33.222: INFO: Pod "pod-subpath-test-configmap-wlmp": Phase="Running", Reason="", readiness=true. Elapsed: 12.024832385s
Sep 17 00:23:35.225: INFO: Pod "pod-subpath-test-configmap-wlmp": Phase="Running", Reason="", readiness=true. Elapsed: 14.028141107s
Sep 17 00:23:37.228: INFO: Pod "pod-subpath-test-configmap-wlmp": Phase="Running", Reason="", readiness=true. Elapsed: 16.03111951s
Sep 17 00:23:39.231: INFO: Pod "pod-subpath-test-configmap-wlmp": Phase="Running", Reason="", readiness=true. Elapsed: 18.034102976s
Sep 17 00:23:41.234: INFO: Pod "pod-subpath-test-configmap-wlmp": Phase="Running", Reason="", readiness=true. Elapsed: 20.037329642s
Sep 17 00:23:43.237: INFO: Pod "pod-subpath-test-configmap-wlmp": Phase="Running", Reason="", readiness=true. Elapsed: 22.040457487s
Sep 17 00:23:45.241: INFO: Pod "pod-subpath-test-configmap-wlmp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.043952665s
STEP: Saw pod success
Sep 17 00:23:45.241: INFO: Pod "pod-subpath-test-configmap-wlmp" satisfied condition "Succeeded or Failed"
Sep 17 00:23:45.243: INFO: Trying to get logs from node eqx03-flash06 pod pod-subpath-test-configmap-wlmp container test-container-subpath-configmap-wlmp: <nil>
STEP: delete the pod
Sep 17 00:23:45.328: INFO: Waiting for pod pod-subpath-test-configmap-wlmp to disappear
Sep 17 00:23:45.330: INFO: Pod pod-subpath-test-configmap-wlmp no longer exists
STEP: Deleting pod pod-subpath-test-configmap-wlmp
Sep 17 00:23:45.330: INFO: Deleting pod "pod-subpath-test-configmap-wlmp" in namespace "subpath-4529"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:23:45.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4529" for this suite.

• [SLOW TEST:24.236 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":277,"completed":251,"skipped":4265,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:23:45.351: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating all guestbook components
Sep 17 00:23:45.400: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-slave
  labels:
    app: agnhost
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: slave
    tier: backend

Sep 17 00:23:45.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 create -f - --namespace=kubectl-3283'
Sep 17 00:23:45.736: INFO: stderr: ""
Sep 17 00:23:45.736: INFO: stdout: "service/agnhost-slave created\n"
Sep 17 00:23:45.737: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-master
  labels:
    app: agnhost
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: master
    tier: backend

Sep 17 00:23:45.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 create -f - --namespace=kubectl-3283'
Sep 17 00:23:46.104: INFO: stderr: ""
Sep 17 00:23:46.104: INFO: stdout: "service/agnhost-master created\n"
Sep 17 00:23:46.104: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Sep 17 00:23:46.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 create -f - --namespace=kubectl-3283'
Sep 17 00:23:46.442: INFO: stderr: ""
Sep 17 00:23:46.442: INFO: stdout: "service/frontend created\n"
Sep 17 00:23:46.442: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Sep 17 00:23:46.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 create -f - --namespace=kubectl-3283'
Sep 17 00:23:46.809: INFO: stderr: ""
Sep 17 00:23:46.809: INFO: stdout: "deployment.apps/frontend created\n"
Sep 17 00:23:46.809: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep 17 00:23:46.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 create -f - --namespace=kubectl-3283'
Sep 17 00:23:47.155: INFO: stderr: ""
Sep 17 00:23:47.155: INFO: stdout: "deployment.apps/agnhost-master created\n"
Sep 17 00:23:47.155: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12
        args: [ "guestbook", "--slaveof", "agnhost-master", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep 17 00:23:47.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 create -f - --namespace=kubectl-3283'
Sep 17 00:23:47.522: INFO: stderr: ""
Sep 17 00:23:47.522: INFO: stdout: "deployment.apps/agnhost-slave created\n"
STEP: validating guestbook app
Sep 17 00:23:47.522: INFO: Waiting for all frontend pods to be Running.
Sep 17 00:23:52.573: INFO: Waiting for frontend to serve content.
Sep 17 00:23:52.581: INFO: Trying to add a new entry to the guestbook.
Sep 17 00:23:52.588: INFO: Verifying that added entry can be retrieved.
Sep 17 00:23:52.594: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Sep 17 00:23:57.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 delete --grace-period=0 --force -f - --namespace=kubectl-3283'
Sep 17 00:23:57.731: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 17 00:23:57.731: INFO: stdout: "service \"agnhost-slave\" force deleted\n"
STEP: using delete to clean up resources
Sep 17 00:23:57.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 delete --grace-period=0 --force -f - --namespace=kubectl-3283'
Sep 17 00:23:57.856: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 17 00:23:57.856: INFO: stdout: "service \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Sep 17 00:23:57.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 delete --grace-period=0 --force -f - --namespace=kubectl-3283'
Sep 17 00:23:57.961: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 17 00:23:57.961: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep 17 00:23:57.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 delete --grace-period=0 --force -f - --namespace=kubectl-3283'
Sep 17 00:23:58.059: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 17 00:23:58.059: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep 17 00:23:58.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 delete --grace-period=0 --force -f - --namespace=kubectl-3283'
Sep 17 00:23:58.165: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 17 00:23:58.165: INFO: stdout: "deployment.apps \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Sep 17 00:23:58.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 delete --grace-period=0 --force -f - --namespace=kubectl-3283'
Sep 17 00:23:58.267: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 17 00:23:58.267: INFO: stdout: "deployment.apps \"agnhost-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:23:58.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3283" for this suite.

• [SLOW TEST:12.937 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:310
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":277,"completed":252,"skipped":4292,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:23:58.288: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir volume type on tmpfs
Sep 17 00:23:58.348: INFO: Waiting up to 5m0s for pod "pod-9e38ac95-de03-4600-9283-b290d04370bd" in namespace "emptydir-8653" to be "Succeeded or Failed"
Sep 17 00:23:58.349: INFO: Pod "pod-9e38ac95-de03-4600-9283-b290d04370bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.897141ms
Sep 17 00:24:00.353: INFO: Pod "pod-9e38ac95-de03-4600-9283-b290d04370bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005389574s
Sep 17 00:24:02.356: INFO: Pod "pod-9e38ac95-de03-4600-9283-b290d04370bd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008388852s
Sep 17 00:24:04.359: INFO: Pod "pod-9e38ac95-de03-4600-9283-b290d04370bd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011685711s
Sep 17 00:24:06.363: INFO: Pod "pod-9e38ac95-de03-4600-9283-b290d04370bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.015019349s
STEP: Saw pod success
Sep 17 00:24:06.363: INFO: Pod "pod-9e38ac95-de03-4600-9283-b290d04370bd" satisfied condition "Succeeded or Failed"
Sep 17 00:24:06.365: INFO: Trying to get logs from node eqx03-flash06 pod pod-9e38ac95-de03-4600-9283-b290d04370bd container test-container: <nil>
STEP: delete the pod
Sep 17 00:24:06.425: INFO: Waiting for pod pod-9e38ac95-de03-4600-9283-b290d04370bd to disappear
Sep 17 00:24:06.427: INFO: Pod pod-9e38ac95-de03-4600-9283-b290d04370bd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:24:06.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8653" for this suite.

• [SLOW TEST:8.159 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":253,"skipped":4308,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:24:06.448: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 17 00:24:07.218: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 17 00:24:09.231: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735899047, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735899047, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735899047, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735899047, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 17 00:24:11.235: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735899047, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735899047, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63735899047, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63735899047, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 17 00:24:14.265: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:24:14.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4261" for this suite.
STEP: Destroying namespace "webhook-4261-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.005 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":277,"completed":254,"skipped":4335,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:24:14.454: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:24:14.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-2453" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":277,"completed":255,"skipped":4359,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:24:14.509: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:24:45.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1030" for this suite.
STEP: Destroying namespace "nsdeletetest-524" for this suite.
Sep 17 00:24:45.706: INFO: Namespace nsdeletetest-524 was already deleted
STEP: Destroying namespace "nsdeletetest-7863" for this suite.

• [SLOW TEST:31.203 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":277,"completed":256,"skipped":4369,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:24:45.713: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:24:45.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2796" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":277,"completed":257,"skipped":4394,"failed":0}
SSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:24:45.823: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Sep 17 00:24:45.892: INFO: Waiting up to 5m0s for pod "downward-api-5a41501d-7868-45c5-9dee-373d8ab20029" in namespace "downward-api-8413" to be "Succeeded or Failed"
Sep 17 00:24:45.894: INFO: Pod "downward-api-5a41501d-7868-45c5-9dee-373d8ab20029": Phase="Pending", Reason="", readiness=false. Elapsed: 2.000941ms
Sep 17 00:24:47.897: INFO: Pod "downward-api-5a41501d-7868-45c5-9dee-373d8ab20029": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00525104s
Sep 17 00:24:49.901: INFO: Pod "downward-api-5a41501d-7868-45c5-9dee-373d8ab20029": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008490441s
STEP: Saw pod success
Sep 17 00:24:49.901: INFO: Pod "downward-api-5a41501d-7868-45c5-9dee-373d8ab20029" satisfied condition "Succeeded or Failed"
Sep 17 00:24:49.903: INFO: Trying to get logs from node eqx03-flash06 pod downward-api-5a41501d-7868-45c5-9dee-373d8ab20029 container dapi-container: <nil>
STEP: delete the pod
Sep 17 00:24:49.946: INFO: Waiting for pod downward-api-5a41501d-7868-45c5-9dee-373d8ab20029 to disappear
Sep 17 00:24:49.948: INFO: Pod downward-api-5a41501d-7868-45c5-9dee-373d8ab20029 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:24:49.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8413" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":277,"completed":258,"skipped":4399,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:24:49.960: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Sep 17 00:24:50.045: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2666 /api/v1/namespaces/watch-2666/configmaps/e2e-watch-test-resource-version 5392dcff-bfbf-4d8d-aa6c-6935fc068b9d 4254885 0 2020-09-17 00:24:50 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-09-17 00:24:50 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 17 00:24:50.045: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2666 /api/v1/namespaces/watch-2666/configmaps/e2e-watch-test-resource-version 5392dcff-bfbf-4d8d-aa6c-6935fc068b9d 4254886 0 2020-09-17 00:24:50 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-09-17 00:24:50 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:24:50.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2666" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":277,"completed":259,"skipped":4410,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:24:50.055: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 17 00:24:50.131: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f2071aa1-33a0-4ba0-8a77-ff5b8aa03577" in namespace "downward-api-8834" to be "Succeeded or Failed"
Sep 17 00:24:50.133: INFO: Pod "downwardapi-volume-f2071aa1-33a0-4ba0-8a77-ff5b8aa03577": Phase="Pending", Reason="", readiness=false. Elapsed: 1.973077ms
Sep 17 00:24:52.137: INFO: Pod "downwardapi-volume-f2071aa1-33a0-4ba0-8a77-ff5b8aa03577": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005296336s
Sep 17 00:24:54.140: INFO: Pod "downwardapi-volume-f2071aa1-33a0-4ba0-8a77-ff5b8aa03577": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008626838s
STEP: Saw pod success
Sep 17 00:24:54.140: INFO: Pod "downwardapi-volume-f2071aa1-33a0-4ba0-8a77-ff5b8aa03577" satisfied condition "Succeeded or Failed"
Sep 17 00:24:54.142: INFO: Trying to get logs from node eqx03-flash06 pod downwardapi-volume-f2071aa1-33a0-4ba0-8a77-ff5b8aa03577 container client-container: <nil>
STEP: delete the pod
Sep 17 00:24:54.191: INFO: Waiting for pod downwardapi-volume-f2071aa1-33a0-4ba0-8a77-ff5b8aa03577 to disappear
Sep 17 00:24:54.193: INFO: Pod downwardapi-volume-f2071aa1-33a0-4ba0-8a77-ff5b8aa03577 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:24:54.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8834" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":277,"completed":260,"skipped":4411,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:24:54.205: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-map-255233db-f487-4ff2-a8b0-b3181101ce34
STEP: Creating a pod to test consume secrets
Sep 17 00:24:54.268: INFO: Waiting up to 5m0s for pod "pod-secrets-20dc67c2-be6a-4589-a1c5-88cd86616e4a" in namespace "secrets-875" to be "Succeeded or Failed"
Sep 17 00:24:54.271: INFO: Pod "pod-secrets-20dc67c2-be6a-4589-a1c5-88cd86616e4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.181033ms
Sep 17 00:24:56.274: INFO: Pod "pod-secrets-20dc67c2-be6a-4589-a1c5-88cd86616e4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005752316s
Sep 17 00:24:58.277: INFO: Pod "pod-secrets-20dc67c2-be6a-4589-a1c5-88cd86616e4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008794315s
STEP: Saw pod success
Sep 17 00:24:58.277: INFO: Pod "pod-secrets-20dc67c2-be6a-4589-a1c5-88cd86616e4a" satisfied condition "Succeeded or Failed"
Sep 17 00:24:58.280: INFO: Trying to get logs from node eqx03-flash06 pod pod-secrets-20dc67c2-be6a-4589-a1c5-88cd86616e4a container secret-volume-test: <nil>
STEP: delete the pod
Sep 17 00:24:58.328: INFO: Waiting for pod pod-secrets-20dc67c2-be6a-4589-a1c5-88cd86616e4a to disappear
Sep 17 00:24:58.335: INFO: Pod pod-secrets-20dc67c2-be6a-4589-a1c5-88cd86616e4a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:24:58.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-875" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":277,"completed":261,"skipped":4421,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:24:58.349: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test substitution in container's command
Sep 17 00:24:58.422: INFO: Waiting up to 5m0s for pod "var-expansion-2d9ca042-1652-4a8d-aba5-82c5a82d4c71" in namespace "var-expansion-3566" to be "Succeeded or Failed"
Sep 17 00:24:58.424: INFO: Pod "var-expansion-2d9ca042-1652-4a8d-aba5-82c5a82d4c71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076594ms
Sep 17 00:25:00.433: INFO: Pod "var-expansion-2d9ca042-1652-4a8d-aba5-82c5a82d4c71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010672225s
Sep 17 00:25:02.436: INFO: Pod "var-expansion-2d9ca042-1652-4a8d-aba5-82c5a82d4c71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013856246s
STEP: Saw pod success
Sep 17 00:25:02.436: INFO: Pod "var-expansion-2d9ca042-1652-4a8d-aba5-82c5a82d4c71" satisfied condition "Succeeded or Failed"
Sep 17 00:25:02.438: INFO: Trying to get logs from node eqx03-flash06 pod var-expansion-2d9ca042-1652-4a8d-aba5-82c5a82d4c71 container dapi-container: <nil>
STEP: delete the pod
Sep 17 00:25:02.497: INFO: Waiting for pod var-expansion-2d9ca042-1652-4a8d-aba5-82c5a82d4c71 to disappear
Sep 17 00:25:02.499: INFO: Pod var-expansion-2d9ca042-1652-4a8d-aba5-82c5a82d4c71 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:25:02.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3566" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":277,"completed":262,"skipped":4430,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:25:02.510: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 17 00:25:05.591: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:25:05.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3874" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":277,"completed":263,"skipped":4445,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] version v1
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:25:05.628: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 17 00:25:05.702: INFO: (0) /api/v1/nodes/eqx03-flash07/proxy/logs/: <pre>
<a href=".messages.swp">.messages.swp</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/... (200; 15.061827ms)
Sep 17 00:25:05.706: INFO: (1) /api/v1/nodes/eqx03-flash07/proxy/logs/: <pre>
<a href=".messages.swp">.messages.swp</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/... (200; 3.433458ms)
Sep 17 00:25:05.709: INFO: (2) /api/v1/nodes/eqx03-flash07/proxy/logs/: <pre>
<a href=".messages.swp">.messages.swp</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/... (200; 3.266235ms)
Sep 17 00:25:05.712: INFO: (3) /api/v1/nodes/eqx03-flash07/proxy/logs/: <pre>
<a href=".messages.swp">.messages.swp</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/... (200; 3.283392ms)
Sep 17 00:25:05.715: INFO: (4) /api/v1/nodes/eqx03-flash07/proxy/logs/: <pre>
<a href=".messages.swp">.messages.swp</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/... (200; 3.069377ms)
Sep 17 00:25:05.718: INFO: (5) /api/v1/nodes/eqx03-flash07/proxy/logs/: <pre>
<a href=".messages.swp">.messages.swp</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/... (200; 2.930624ms)
Sep 17 00:25:05.721: INFO: (6) /api/v1/nodes/eqx03-flash07/proxy/logs/: <pre>
<a href=".messages.swp">.messages.swp</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/... (200; 2.968347ms)
Sep 17 00:25:05.724: INFO: (7) /api/v1/nodes/eqx03-flash07/proxy/logs/: <pre>
<a href=".messages.swp">.messages.swp</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/... (200; 2.874657ms)
Sep 17 00:25:05.727: INFO: (8) /api/v1/nodes/eqx03-flash07/proxy/logs/: <pre>
<a href=".messages.swp">.messages.swp</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/... (200; 3.021615ms)
Sep 17 00:25:05.730: INFO: (9) /api/v1/nodes/eqx03-flash07/proxy/logs/: <pre>
<a href=".messages.swp">.messages.swp</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/... (200; 2.968864ms)
Sep 17 00:25:05.733: INFO: (10) /api/v1/nodes/eqx03-flash07/proxy/logs/: <pre>
<a href=".messages.swp">.messages.swp</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/... (200; 2.938228ms)
Sep 17 00:25:05.736: INFO: (11) /api/v1/nodes/eqx03-flash07/proxy/logs/: <pre>
<a href=".messages.swp">.messages.swp</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/... (200; 2.878387ms)
Sep 17 00:25:05.739: INFO: (12) /api/v1/nodes/eqx03-flash07/proxy/logs/: <pre>
<a href=".messages.swp">.messages.swp</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/... (200; 3.184129ms)
Sep 17 00:25:05.742: INFO: (13) /api/v1/nodes/eqx03-flash07/proxy/logs/: <pre>
<a href=".messages.swp">.messages.swp</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/... (200; 3.071853ms)
Sep 17 00:25:05.745: INFO: (14) /api/v1/nodes/eqx03-flash07/proxy/logs/: <pre>
<a href=".messages.swp">.messages.swp</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/... (200; 2.951639ms)
Sep 17 00:25:05.748: INFO: (15) /api/v1/nodes/eqx03-flash07/proxy/logs/: <pre>
<a href=".messages.swp">.messages.swp</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/... (200; 2.98688ms)
Sep 17 00:25:05.752: INFO: (16) /api/v1/nodes/eqx03-flash07/proxy/logs/: <pre>
<a href=".messages.swp">.messages.swp</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/... (200; 3.14651ms)
Sep 17 00:25:05.755: INFO: (17) /api/v1/nodes/eqx03-flash07/proxy/logs/: <pre>
<a href=".messages.swp">.messages.swp</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/... (200; 3.128337ms)
Sep 17 00:25:05.758: INFO: (18) /api/v1/nodes/eqx03-flash07/proxy/logs/: <pre>
<a href=".messages.swp">.messages.swp</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/... (200; 2.963484ms)
Sep 17 00:25:05.761: INFO: (19) /api/v1/nodes/eqx03-flash07/proxy/logs/: <pre>
<a href=".messages.swp">.messages.swp</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/... (200; 3.035325ms)
[AfterEach] version v1
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:25:05.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-146" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]","total":277,"completed":264,"skipped":4478,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:25:05.772: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:26:05.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3378" for this suite.

• [SLOW TEST:60.087 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":277,"completed":265,"skipped":4526,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:26:05.859: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Sep 17 00:26:05.924: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 17 00:26:05.934: INFO: Waiting for terminating namespaces to be deleted...
Sep 17 00:26:05.937: INFO: 
Logging pods the kubelet thinks is on node eqx03-flash06 before test
Sep 17 00:26:05.951: INFO: kube-scheduler-eqx03-flash06 from kube-system started at 2020-09-04 20:35:41 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.951: INFO: 	Container kube-scheduler ready: true, restart count 0
Sep 17 00:26:05.951: INFO: csi-snapshotter-robin-0 from robinio started at 2020-09-16 23:34:41 +0000 UTC (2 container statuses recorded)
Sep 17 00:26:05.951: INFO: 	Container csi-snapshotter ready: true, restart count 0
Sep 17 00:26:05.951: INFO: 	Container robin ready: true, restart count 0
Sep 17 00:26:05.951: INFO: test-webserver-73c14964-7e84-45d4-a473-4a64228b3f93 from container-probe-3378 started at 2020-09-17 00:25:05 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.951: INFO: 	Container test-webserver ready: false, restart count 0
Sep 17 00:26:05.951: INFO: kube-proxy-mchgt from kube-system started at 2020-09-04 20:35:41 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.951: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 17 00:26:05.951: INFO: sonobuoy-e2e-job-362cab5d438b4e90 from sonobuoy started at 2020-09-16 23:06:47 +0000 UTC (2 container statuses recorded)
Sep 17 00:26:05.951: INFO: 	Container e2e ready: true, restart count 0
Sep 17 00:26:05.951: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 17 00:26:05.951: INFO: sonobuoy-systemd-logs-daemon-set-71797a053a4d44ec-pp8g4 from sonobuoy started at 2020-09-16 23:06:47 +0000 UTC (2 container statuses recorded)
Sep 17 00:26:05.951: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 17 00:26:05.951: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 17 00:26:05.951: INFO: etcd-eqx03-flash06 from kube-system started at 2020-09-04 20:35:41 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.951: INFO: 	Container etcd ready: true, restart count 0
Sep 17 00:26:05.951: INFO: calico-node-zqtk2 from kube-system started at 2020-09-09 19:40:56 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.951: INFO: 	Container calico-node ready: true, restart count 0
Sep 17 00:26:05.951: INFO: csi-nodeplugin-robin-rnhf4 from robinio started at 2020-09-16 23:34:31 +0000 UTC (2 container statuses recorded)
Sep 17 00:26:05.951: INFO: 	Container driver-registrar ready: true, restart count 0
Sep 17 00:26:05.951: INFO: 	Container robin ready: true, restart count 0
Sep 17 00:26:05.951: INFO: kube-apiserver-eqx03-flash06 from kube-system started at 2020-09-04 20:36:01 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.951: INFO: 	Container kube-apiserver ready: true, restart count 0
Sep 17 00:26:05.951: INFO: kube-sriov-device-plugin-amd64-29hgs from kube-system started at 2020-09-16 23:34:31 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.951: INFO: 	Container kube-sriovdp ready: true, restart count 0
Sep 17 00:26:05.951: INFO: kube-multus-ds-amd64-wjsv5 from kube-system started at 2020-09-16 23:34:31 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.951: INFO: 	Container kube-multus ready: true, restart count 0
Sep 17 00:26:05.951: INFO: robin-master-hr288 from robinio started at 2020-09-16 23:34:41 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.951: INFO: 	Container robinrcm ready: true, restart count 0
Sep 17 00:26:05.951: INFO: kube-controller-manager-eqx03-flash06 from kube-system started at 2020-09-04 20:36:01 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.951: INFO: 	Container kube-controller-manager ready: true, restart count 1
Sep 17 00:26:05.951: INFO: 
Logging pods the kubelet thinks is on node eqx03-flash07 before test
Sep 17 00:26:05.985: INFO: calico-node-8t2kf from kube-system started at 2020-09-09 19:41:23 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container calico-node ready: true, restart count 0
Sep 17 00:26:05.985: INFO: hari-elasticsearch-data-0 from spk started at 2020-09-05 06:37:25 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:26:05.985: INFO: test1-elasticsearch-client-df46b4cd8-2knzt from t001-u000003 started at 2020-09-05 06:42:07 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:26:05.985: INFO: kube-sriov-device-plugin-amd64-rxs7g from kube-system started at 2020-09-04 20:25:01 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container kube-sriovdp ready: true, restart count 0
Sep 17 00:26:05.985: INFO: cert-manager-578cd6d964-sq59x from cert-manager started at 2020-09-04 20:38:07 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container cert-manager ready: true, restart count 0
Sep 17 00:26:05.985: INFO: elkname-kibana-56c986874d-nx4t6 from t001-u000003 started at 2020-09-05 06:08:19 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container kibana ready: true, restart count 0
Sep 17 00:26:05.985: INFO: test1-elasticsearch-data-0 from t001-u000003 started at 2020-09-05 06:42:08 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:26:05.985: INFO: sonobuoy from sonobuoy started at 2020-09-16 23:06:40 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 17 00:26:05.985: INFO: neo-neo4j-core-1 from ripul started at 2020-09-16 23:34:02 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 17 00:26:05.985: INFO: kube-proxy-9zgnq from kube-system started at 2020-09-04 20:24:11 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 17 00:26:05.985: INFO: etcd-eqx03-flash07 from kube-system started at 2020-09-04 20:24:37 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container etcd ready: true, restart count 0
Sep 17 00:26:05.985: INFO: csi-resizer-robin-7d566f9df9-sq477 from robinio started at 2020-09-09 19:56:05 +0000 UTC (2 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container csi-resizer ready: true, restart count 0
Sep 17 00:26:05.985: INFO: 	Container robin ready: true, restart count 0
Sep 17 00:26:05.985: INFO: sonobuoy-systemd-logs-daemon-set-71797a053a4d44ec-4zr6m from sonobuoy started at 2020-09-16 23:06:47 +0000 UTC (2 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 17 00:26:05.985: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 17 00:26:05.985: INFO: neo-neo4j-replica-0 from ripul started at 2020-09-16 23:34:01 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container neo4j ready: true, restart count 0
Sep 17 00:26:05.985: INFO: elkname-logstash-0 from t001-u000003 started at 2020-09-05 06:08:19 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container logstash ready: true, restart count 0
Sep 17 00:26:05.985: INFO: hari-kibana-b6f7d64-j96s2 from spk started at 2020-09-05 06:37:23 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container kibana ready: true, restart count 0
Sep 17 00:26:05.985: INFO: coredns-6878cb8f64-7rgtp from kube-system started at 2020-09-04 20:25:00 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container coredns ready: true, restart count 0
Sep 17 00:26:05.985: INFO: csi-provisioner-robin-6646bdd46b-kl2ft from robinio started at 2020-09-16 23:31:18 +0000 UTC (2 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container csi-provisioner ready: true, restart count 0
Sep 17 00:26:05.985: INFO: 	Container robin ready: true, restart count 0
Sep 17 00:26:05.985: INFO: robin-master-w2m7f from robinio started at 2020-09-09 19:47:17 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container robinrcm ready: true, restart count 0
Sep 17 00:26:05.985: INFO: cert-manager-webhook-556b9d7dfd-swlll from cert-manager started at 2020-09-04 20:38:07 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container cert-manager ready: true, restart count 0
Sep 17 00:26:05.985: INFO: elkname-elasticsearch-client-6768458985-6sgxz from t001-u000003 started at 2020-09-05 06:08:19 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:26:05.985: INFO: elkname-elasticsearch-master-0 from t001-u000003 started at 2020-09-05 06:08:19 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:26:05.985: INFO: kube-multus-ds-amd64-lf8vr from kube-system started at 2020-09-04 20:24:11 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container kube-multus ready: true, restart count 0
Sep 17 00:26:05.985: INFO: kube-apiserver-eqx03-flash07 from kube-system started at 2020-09-04 20:24:11 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container kube-apiserver ready: true, restart count 0
Sep 17 00:26:05.985: INFO: csi-attacher-robin-5d956884cf-nqxfg from robinio started at 2020-09-16 23:31:18 +0000 UTC (2 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container csi-attacher ready: true, restart count 0
Sep 17 00:26:05.985: INFO: 	Container robin ready: true, restart count 0
Sep 17 00:26:05.985: INFO: neo-neo4j-replica-0 from madhura started at 2020-09-16 23:33:58 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container neo4j ready: true, restart count 0
Sep 17 00:26:05.985: INFO: hari-elasticsearch-client-c59586c8-k84fl from spk started at 2020-09-05 06:37:23 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:26:05.985: INFO: neo-neo4j-core-0 from ripul started at 2020-09-16 23:34:03 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 17 00:26:05.985: INFO: snapshot-controller-0 from robinio started at 2020-09-09 19:56:15 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container snapshot-controller ready: true, restart count 0
Sep 17 00:26:05.985: INFO: kube-controller-manager-eqx03-flash07 from kube-system started at 2020-09-04 20:24:11 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container kube-controller-manager ready: true, restart count 1
Sep 17 00:26:05.985: INFO: test1-logstash-0 from t001-u000003 started at 2020-09-05 06:42:09 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container logstash ready: true, restart count 0
Sep 17 00:26:05.985: INFO: csi-nodeplugin-robin-bcq5b from robinio started at 2020-09-09 19:56:04 +0000 UTC (2 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container driver-registrar ready: true, restart count 0
Sep 17 00:26:05.985: INFO: 	Container robin ready: true, restart count 0
Sep 17 00:26:05.985: INFO: neo-neo4j-core-1 from default started at 2020-09-16 23:34:17 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 17 00:26:05.985: INFO: hari-logstash-0 from spk started at 2020-09-05 06:37:24 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container logstash ready: true, restart count 0
Sep 17 00:26:05.985: INFO: test1-kibana-84f6c46bf6-mndsz from t001-u000003 started at 2020-09-05 06:42:07 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container kibana ready: true, restart count 0
Sep 17 00:26:05.985: INFO: neo-neo4j-core-2 from default started at 2020-09-16 23:34:00 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 17 00:26:05.985: INFO: elkname-elasticsearch-data-0 from t001-u000003 started at 2020-09-05 06:08:19 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.985: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:26:05.985: INFO: neo-neo4j-core-2 from ripul started at 2020-09-16 23:33:57 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.986: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 17 00:26:05.986: INFO: mg-runner-gitlab-runner-97dd45f4-8z4zg from gitlab-runner started at 2020-09-10 19:25:18 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.986: INFO: 	Container mg-runner-gitlab-runner ready: false, restart count 1153
Sep 17 00:26:05.986: INFO: neo-neo4j-core-1 from madhura started at 2020-09-16 23:33:58 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.986: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 17 00:26:05.986: INFO: neo-neo4j-core-0 from default started at 2020-09-16 23:34:12 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.986: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 17 00:26:05.986: INFO: kube-scheduler-eqx03-flash07 from kube-system started at 2020-09-04 20:24:11 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.986: INFO: 	Container kube-scheduler ready: true, restart count 0
Sep 17 00:26:05.986: INFO: cert-manager-cainjector-5ffff9dd7c-7fv5l from cert-manager started at 2020-09-04 20:38:07 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:05.986: INFO: 	Container cert-manager ready: true, restart count 0
Sep 17 00:26:05.986: INFO: 
Logging pods the kubelet thinks is on node eqx04-flash04 before test
Sep 17 00:26:06.006: INFO: neo-neo4j-core-0 from madhura started at 2020-09-16 23:34:02 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:06.006: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 17 00:26:06.006: INFO: elkname-elasticsearch-master-2 from t001-u000003 started at 2020-09-05 06:09:50 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:06.006: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:26:06.006: INFO: hari-elasticsearch-master-0 from spk started at 2020-09-05 06:37:24 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:06.006: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:26:06.006: INFO: csi-nodeplugin-robin-6xzkh from robinio started at 2020-09-09 19:56:04 +0000 UTC (2 container statuses recorded)
Sep 17 00:26:06.006: INFO: 	Container driver-registrar ready: true, restart count 0
Sep 17 00:26:06.006: INFO: 	Container robin ready: true, restart count 0
Sep 17 00:26:06.006: INFO: neo-neo4j-replica-0 from default started at 2020-09-16 23:34:03 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:06.006: INFO: 	Container neo4j ready: true, restart count 0
Sep 17 00:26:06.006: INFO: kube-proxy-2mmk7 from kube-system started at 2020-09-04 20:12:22 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:06.006: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 17 00:26:06.006: INFO: etcd-eqx04-flash04 from kube-system started at 2020-09-04 20:12:22 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:06.006: INFO: 	Container etcd ready: true, restart count 0
Sep 17 00:26:06.006: INFO: coredns-6878cb8f64-n75hx from kube-system started at 2020-09-04 20:12:55 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:06.006: INFO: 	Container coredns ready: true, restart count 0
Sep 17 00:26:06.006: INFO: neo-neo4j-core-2 from madhura started at 2020-09-16 23:34:01 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:06.006: INFO: 	Container neo-neo4j ready: true, restart count 0
Sep 17 00:26:06.006: INFO: test1-elasticsearch-data-1 from t001-u000003 started at 2020-09-05 06:44:40 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:06.006: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:26:06.006: INFO: calico-node-wpq99 from kube-system started at 2020-09-09 19:41:33 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:06.006: INFO: 	Container calico-node ready: true, restart count 0
Sep 17 00:26:06.006: INFO: kube-scheduler-eqx04-flash04 from kube-system started at 2020-09-04 20:12:22 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:06.006: INFO: 	Container kube-scheduler ready: true, restart count 2
Sep 17 00:26:06.006: INFO: calico-kube-controllers-6c49f88586-jzdjm from kube-system started at 2020-09-04 20:12:28 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:06.006: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep 17 00:26:06.006: INFO: kube-sriov-device-plugin-amd64-xlvxl from kube-system started at 2020-09-04 20:12:30 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:06.006: INFO: 	Container kube-sriovdp ready: true, restart count 0
Sep 17 00:26:06.006: INFO: kube-apiserver-eqx04-flash04 from kube-system started at 2020-09-04 20:12:22 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:06.006: INFO: 	Container kube-apiserver ready: true, restart count 0
Sep 17 00:26:06.006: INFO: robin-master-hkj7r from robinio started at 2020-09-09 19:48:36 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:06.006: INFO: 	Container robinrcm ready: true, restart count 0
Sep 17 00:26:06.006: INFO: kube-controller-manager-eqx04-flash04 from kube-system started at 2020-09-04 20:12:22 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:06.006: INFO: 	Container kube-controller-manager ready: true, restart count 1
Sep 17 00:26:06.006: INFO: kube-multus-ds-amd64-mzx44 from kube-system started at 2020-09-04 20:12:27 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:06.006: INFO: 	Container kube-multus ready: true, restart count 0
Sep 17 00:26:06.006: INFO: sonobuoy-systemd-logs-daemon-set-71797a053a4d44ec-dx2j7 from sonobuoy started at 2020-09-16 23:06:47 +0000 UTC (2 container statuses recorded)
Sep 17 00:26:06.006: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 17 00:26:06.006: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 17 00:26:06.006: INFO: 
Logging pods the kubelet thinks is on node eqx04-flash06 before test
Sep 17 00:26:06.031: INFO: hari-elasticsearch-data-1 from spk started at 2020-09-05 06:40:02 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:06.031: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:26:06.031: INFO: kube-proxy-n5ckt from kube-system started at 2020-09-04 20:34:34 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:06.031: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 17 00:26:06.031: INFO: robin-worker-6zwcm from robinio started at 2020-09-09 19:53:16 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:06.031: INFO: 	Container robinrcm ready: true, restart count 0
Sep 17 00:26:06.031: INFO: kube-sriov-device-plugin-amd64-wwdl2 from kube-system started at 2020-09-04 20:34:39 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:06.031: INFO: 	Container kube-sriovdp ready: true, restart count 0
Sep 17 00:26:06.031: INFO: calico-node-x8tqn from kube-system started at 2020-09-09 19:41:10 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:06.031: INFO: 	Container calico-node ready: true, restart count 0
Sep 17 00:26:06.031: INFO: elkname-elasticsearch-master-1 from t001-u000003 started at 2020-09-05 06:09:25 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:06.031: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:26:06.031: INFO: test1-elasticsearch-master-1 from t001-u000003 started at 2020-09-05 06:43:08 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:06.031: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:26:06.031: INFO: test1-elasticsearch-master-2 from t001-u000003 started at 2020-09-05 06:43:44 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:06.031: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:26:06.031: INFO: test1-elasticsearch-master-0 from t001-u000003 started at 2020-09-05 06:42:08 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:06.031: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:26:06.031: INFO: kube-multus-ds-amd64-j9jmp from kube-system started at 2020-09-04 20:34:34 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:06.031: INFO: 	Container kube-multus ready: true, restart count 0
Sep 17 00:26:06.031: INFO: elkname-elasticsearch-data-1 from t001-u000003 started at 2020-09-05 06:10:51 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:06.031: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:26:06.031: INFO: elkname-elasticsearch-client-6768458985-j2fnt from t001-u000003 started at 2020-09-05 06:08:19 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:06.031: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:26:06.031: INFO: hari-elasticsearch-master-2 from spk started at 2020-09-05 06:38:48 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:06.031: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:26:06.031: INFO: test1-elasticsearch-client-df46b4cd8-52bhb from t001-u000003 started at 2020-09-05 06:42:07 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:06.031: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:26:06.031: INFO: csi-nodeplugin-robin-wk7pp from robinio started at 2020-09-09 19:56:04 +0000 UTC (2 container statuses recorded)
Sep 17 00:26:06.031: INFO: 	Container driver-registrar ready: true, restart count 0
Sep 17 00:26:06.031: INFO: 	Container robin ready: true, restart count 0
Sep 17 00:26:06.031: INFO: sonobuoy-systemd-logs-daemon-set-71797a053a4d44ec-w59x2 from sonobuoy started at 2020-09-16 23:06:47 +0000 UTC (2 container statuses recorded)
Sep 17 00:26:06.031: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 17 00:26:06.031: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 17 00:26:06.031: INFO: gitlab-runner-gitlab-runner-69dbb57dd4-sptqz from gitlab-runner started at 2020-09-10 19:12:45 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:06.031: INFO: 	Container gitlab-runner-gitlab-runner ready: false, restart count 0
Sep 17 00:26:06.031: INFO: hari-elasticsearch-client-c59586c8-xvm2k from spk started at 2020-09-05 06:37:23 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:06.031: INFO: 	Container elasticsearch ready: true, restart count 0
Sep 17 00:26:06.031: INFO: hari-elasticsearch-master-1 from spk started at 2020-09-05 06:38:21 +0000 UTC (1 container statuses recorded)
Sep 17 00:26:06.031: INFO: 	Container elasticsearch ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: verifying the node has the label node eqx03-flash06
STEP: verifying the node has the label node eqx03-flash07
STEP: verifying the node has the label node eqx04-flash04
STEP: verifying the node has the label node eqx04-flash06
Sep 17 00:26:06.145: INFO: Pod cert-manager-578cd6d964-sq59x requesting resource cpu=0m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod cert-manager-cainjector-5ffff9dd7c-7fv5l requesting resource cpu=0m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod cert-manager-webhook-556b9d7dfd-swlll requesting resource cpu=0m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod test-webserver-73c14964-7e84-45d4-a473-4a64228b3f93 requesting resource cpu=0m on Node eqx03-flash06
Sep 17 00:26:06.145: INFO: Pod neo-neo4j-core-0 requesting resource cpu=0m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod neo-neo4j-core-1 requesting resource cpu=0m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod neo-neo4j-core-2 requesting resource cpu=0m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod neo-neo4j-replica-0 requesting resource cpu=0m on Node eqx04-flash04
Sep 17 00:26:06.145: INFO: Pod gitlab-runner-gitlab-runner-69dbb57dd4-sptqz requesting resource cpu=100m on Node eqx04-flash06
Sep 17 00:26:06.145: INFO: Pod mg-runner-gitlab-runner-97dd45f4-8z4zg requesting resource cpu=0m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod calico-kube-controllers-6c49f88586-jzdjm requesting resource cpu=0m on Node eqx04-flash04
Sep 17 00:26:06.145: INFO: Pod calico-node-8t2kf requesting resource cpu=250m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod calico-node-wpq99 requesting resource cpu=250m on Node eqx04-flash04
Sep 17 00:26:06.145: INFO: Pod calico-node-x8tqn requesting resource cpu=250m on Node eqx04-flash06
Sep 17 00:26:06.145: INFO: Pod calico-node-zqtk2 requesting resource cpu=250m on Node eqx03-flash06
Sep 17 00:26:06.145: INFO: Pod coredns-6878cb8f64-7rgtp requesting resource cpu=100m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod coredns-6878cb8f64-n75hx requesting resource cpu=100m on Node eqx04-flash04
Sep 17 00:26:06.145: INFO: Pod etcd-eqx03-flash06 requesting resource cpu=0m on Node eqx03-flash06
Sep 17 00:26:06.145: INFO: Pod etcd-eqx03-flash07 requesting resource cpu=0m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod etcd-eqx04-flash04 requesting resource cpu=0m on Node eqx04-flash04
Sep 17 00:26:06.145: INFO: Pod kube-apiserver-eqx03-flash06 requesting resource cpu=250m on Node eqx03-flash06
Sep 17 00:26:06.145: INFO: Pod kube-apiserver-eqx03-flash07 requesting resource cpu=250m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod kube-apiserver-eqx04-flash04 requesting resource cpu=250m on Node eqx04-flash04
Sep 17 00:26:06.145: INFO: Pod kube-controller-manager-eqx03-flash06 requesting resource cpu=200m on Node eqx03-flash06
Sep 17 00:26:06.145: INFO: Pod kube-controller-manager-eqx03-flash07 requesting resource cpu=200m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod kube-controller-manager-eqx04-flash04 requesting resource cpu=200m on Node eqx04-flash04
Sep 17 00:26:06.145: INFO: Pod kube-multus-ds-amd64-j9jmp requesting resource cpu=100m on Node eqx04-flash06
Sep 17 00:26:06.145: INFO: Pod kube-multus-ds-amd64-lf8vr requesting resource cpu=100m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod kube-multus-ds-amd64-mzx44 requesting resource cpu=100m on Node eqx04-flash04
Sep 17 00:26:06.145: INFO: Pod kube-multus-ds-amd64-wjsv5 requesting resource cpu=100m on Node eqx03-flash06
Sep 17 00:26:06.145: INFO: Pod kube-proxy-2mmk7 requesting resource cpu=0m on Node eqx04-flash04
Sep 17 00:26:06.145: INFO: Pod kube-proxy-9zgnq requesting resource cpu=0m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod kube-proxy-mchgt requesting resource cpu=0m on Node eqx03-flash06
Sep 17 00:26:06.145: INFO: Pod kube-proxy-n5ckt requesting resource cpu=0m on Node eqx04-flash06
Sep 17 00:26:06.145: INFO: Pod kube-scheduler-eqx03-flash06 requesting resource cpu=100m on Node eqx03-flash06
Sep 17 00:26:06.145: INFO: Pod kube-scheduler-eqx03-flash07 requesting resource cpu=100m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod kube-scheduler-eqx04-flash04 requesting resource cpu=100m on Node eqx04-flash04
Sep 17 00:26:06.145: INFO: Pod kube-sriov-device-plugin-amd64-29hgs requesting resource cpu=0m on Node eqx03-flash06
Sep 17 00:26:06.145: INFO: Pod kube-sriov-device-plugin-amd64-rxs7g requesting resource cpu=0m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod kube-sriov-device-plugin-amd64-wwdl2 requesting resource cpu=0m on Node eqx04-flash06
Sep 17 00:26:06.145: INFO: Pod kube-sriov-device-plugin-amd64-xlvxl requesting resource cpu=0m on Node eqx04-flash04
Sep 17 00:26:06.145: INFO: Pod neo-neo4j-core-0 requesting resource cpu=0m on Node eqx04-flash04
Sep 17 00:26:06.145: INFO: Pod neo-neo4j-core-1 requesting resource cpu=0m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod neo-neo4j-core-2 requesting resource cpu=0m on Node eqx04-flash04
Sep 17 00:26:06.145: INFO: Pod neo-neo4j-replica-0 requesting resource cpu=0m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod neo-neo4j-core-0 requesting resource cpu=0m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod neo-neo4j-core-1 requesting resource cpu=0m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod neo-neo4j-core-2 requesting resource cpu=0m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod neo-neo4j-replica-0 requesting resource cpu=0m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod csi-attacher-robin-5d956884cf-nqxfg requesting resource cpu=0m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod csi-nodeplugin-robin-6xzkh requesting resource cpu=0m on Node eqx04-flash04
Sep 17 00:26:06.145: INFO: Pod csi-nodeplugin-robin-bcq5b requesting resource cpu=0m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod csi-nodeplugin-robin-rnhf4 requesting resource cpu=0m on Node eqx03-flash06
Sep 17 00:26:06.145: INFO: Pod csi-nodeplugin-robin-wk7pp requesting resource cpu=0m on Node eqx04-flash06
Sep 17 00:26:06.145: INFO: Pod csi-provisioner-robin-6646bdd46b-kl2ft requesting resource cpu=0m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod csi-resizer-robin-7d566f9df9-sq477 requesting resource cpu=0m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod csi-snapshotter-robin-0 requesting resource cpu=0m on Node eqx03-flash06
Sep 17 00:26:06.145: INFO: Pod robin-master-hkj7r requesting resource cpu=0m on Node eqx04-flash04
Sep 17 00:26:06.145: INFO: Pod robin-master-hr288 requesting resource cpu=0m on Node eqx03-flash06
Sep 17 00:26:06.145: INFO: Pod robin-master-w2m7f requesting resource cpu=0m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod robin-worker-6zwcm requesting resource cpu=0m on Node eqx04-flash06
Sep 17 00:26:06.145: INFO: Pod snapshot-controller-0 requesting resource cpu=0m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod sonobuoy requesting resource cpu=0m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod sonobuoy-e2e-job-362cab5d438b4e90 requesting resource cpu=0m on Node eqx03-flash06
Sep 17 00:26:06.145: INFO: Pod sonobuoy-systemd-logs-daemon-set-71797a053a4d44ec-4zr6m requesting resource cpu=0m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod sonobuoy-systemd-logs-daemon-set-71797a053a4d44ec-dx2j7 requesting resource cpu=0m on Node eqx04-flash04
Sep 17 00:26:06.145: INFO: Pod sonobuoy-systemd-logs-daemon-set-71797a053a4d44ec-pp8g4 requesting resource cpu=0m on Node eqx03-flash06
Sep 17 00:26:06.145: INFO: Pod sonobuoy-systemd-logs-daemon-set-71797a053a4d44ec-w59x2 requesting resource cpu=0m on Node eqx04-flash06
Sep 17 00:26:06.145: INFO: Pod hari-elasticsearch-client-c59586c8-k84fl requesting resource cpu=25m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod hari-elasticsearch-client-c59586c8-xvm2k requesting resource cpu=25m on Node eqx04-flash06
Sep 17 00:26:06.145: INFO: Pod hari-elasticsearch-data-0 requesting resource cpu=25m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod hari-elasticsearch-data-1 requesting resource cpu=25m on Node eqx04-flash06
Sep 17 00:26:06.145: INFO: Pod hari-elasticsearch-master-0 requesting resource cpu=25m on Node eqx04-flash04
Sep 17 00:26:06.145: INFO: Pod hari-elasticsearch-master-1 requesting resource cpu=25m on Node eqx04-flash06
Sep 17 00:26:06.145: INFO: Pod hari-elasticsearch-master-2 requesting resource cpu=25m on Node eqx04-flash06
Sep 17 00:26:06.145: INFO: Pod hari-kibana-b6f7d64-j96s2 requesting resource cpu=0m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod hari-logstash-0 requesting resource cpu=0m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod elkname-elasticsearch-client-6768458985-6sgxz requesting resource cpu=25m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod elkname-elasticsearch-client-6768458985-j2fnt requesting resource cpu=25m on Node eqx04-flash06
Sep 17 00:26:06.145: INFO: Pod elkname-elasticsearch-data-0 requesting resource cpu=25m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod elkname-elasticsearch-data-1 requesting resource cpu=25m on Node eqx04-flash06
Sep 17 00:26:06.145: INFO: Pod elkname-elasticsearch-master-0 requesting resource cpu=25m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod elkname-elasticsearch-master-1 requesting resource cpu=25m on Node eqx04-flash06
Sep 17 00:26:06.145: INFO: Pod elkname-elasticsearch-master-2 requesting resource cpu=25m on Node eqx04-flash04
Sep 17 00:26:06.145: INFO: Pod elkname-kibana-56c986874d-nx4t6 requesting resource cpu=0m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod elkname-logstash-0 requesting resource cpu=0m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod test1-elasticsearch-client-df46b4cd8-2knzt requesting resource cpu=25m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod test1-elasticsearch-client-df46b4cd8-52bhb requesting resource cpu=25m on Node eqx04-flash06
Sep 17 00:26:06.145: INFO: Pod test1-elasticsearch-data-0 requesting resource cpu=25m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod test1-elasticsearch-data-1 requesting resource cpu=25m on Node eqx04-flash04
Sep 17 00:26:06.145: INFO: Pod test1-elasticsearch-master-0 requesting resource cpu=25m on Node eqx04-flash06
Sep 17 00:26:06.145: INFO: Pod test1-elasticsearch-master-1 requesting resource cpu=25m on Node eqx04-flash06
Sep 17 00:26:06.145: INFO: Pod test1-elasticsearch-master-2 requesting resource cpu=25m on Node eqx04-flash06
Sep 17 00:26:06.145: INFO: Pod test1-kibana-84f6c46bf6-mndsz requesting resource cpu=0m on Node eqx03-flash07
Sep 17 00:26:06.145: INFO: Pod test1-logstash-0 requesting resource cpu=0m on Node eqx03-flash07
STEP: Starting Pods to consume most of the cluster CPU.
Sep 17 00:26:06.145: INFO: Creating a pod which consumes cpu=10692m on Node eqx04-flash06
Sep 17 00:26:06.175: INFO: Creating a pod which consumes cpu=27370m on Node eqx03-flash06
Sep 17 00:26:06.198: INFO: Creating a pod which consumes cpu=27177m on Node eqx03-flash07
Sep 17 00:26:06.224: INFO: Creating a pod which consumes cpu=10447m on Node eqx04-flash04
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-25ec02c1-bd47-42b1-a8d1-5026c2ea138f.16356a85f9ff4857], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3376/filler-pod-25ec02c1-bd47-42b1-a8d1-5026c2ea138f to eqx03-flash07]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-25ec02c1-bd47-42b1-a8d1-5026c2ea138f.16356a86531039a1], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-25ec02c1-bd47-42b1-a8d1-5026c2ea138f.16356a86594a4ad3], Reason = [Created], Message = [Created container filler-pod-25ec02c1-bd47-42b1-a8d1-5026c2ea138f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-25ec02c1-bd47-42b1-a8d1-5026c2ea138f.16356a8662851194], Reason = [Started], Message = [Started container filler-pod-25ec02c1-bd47-42b1-a8d1-5026c2ea138f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-42c707ab-0652-489e-8472-15e2cd36a6eb.16356a85f6045bd6], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3376/filler-pod-42c707ab-0652-489e-8472-15e2cd36a6eb to eqx04-flash06]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-42c707ab-0652-489e-8472-15e2cd36a6eb.16356a864198c186], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-42c707ab-0652-489e-8472-15e2cd36a6eb.16356a8649d2485f], Reason = [Created], Message = [Created container filler-pod-42c707ab-0652-489e-8472-15e2cd36a6eb]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-42c707ab-0652-489e-8472-15e2cd36a6eb.16356a865263df17], Reason = [Started], Message = [Started container filler-pod-42c707ab-0652-489e-8472-15e2cd36a6eb]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-94da455b-e8e4-4d3e-8042-0e31f48111dd.16356a85fb898be9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3376/filler-pod-94da455b-e8e4-4d3e-8042-0e31f48111dd to eqx04-flash04]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-94da455b-e8e4-4d3e-8042-0e31f48111dd.16356a864b7d4e61], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-94da455b-e8e4-4d3e-8042-0e31f48111dd.16356a8654a6b505], Reason = [Created], Message = [Created container filler-pod-94da455b-e8e4-4d3e-8042-0e31f48111dd]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-94da455b-e8e4-4d3e-8042-0e31f48111dd.16356a865ff12cf6], Reason = [Started], Message = [Started container filler-pod-94da455b-e8e4-4d3e-8042-0e31f48111dd]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f3c38258-a29f-48b6-a33e-c66772071ff9.16356a85f79614be], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3376/filler-pod-f3c38258-a29f-48b6-a33e-c66772071ff9 to eqx03-flash06]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f3c38258-a29f-48b6-a33e-c66772071ff9.16356a864e1d4d3f], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f3c38258-a29f-48b6-a33e-c66772071ff9.16356a8652e365c0], Reason = [Created], Message = [Created container filler-pod-f3c38258-a29f-48b6-a33e-c66772071ff9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f3c38258-a29f-48b6-a33e-c66772071ff9.16356a865ade898c], Reason = [Started], Message = [Started container filler-pod-f3c38258-a29f-48b6-a33e-c66772071ff9]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16356a86eb3c5037], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16356a86ec3ecbb5], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 Insufficient cpu.]
STEP: removing the label node off the node eqx03-flash06
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node eqx03-flash07
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node eqx04-flash04
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node eqx04-flash06
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:26:11.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3376" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

• [SLOW TEST:5.544 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":277,"completed":266,"skipped":4541,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:26:11.403: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:26:22.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-162" for this suite.

• [SLOW TEST:11.172 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":277,"completed":267,"skipped":4542,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:26:22.575: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Sep 17 00:26:22.658: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6a31b1c4-5e5d-420b-9c5e-9437aba147f8" in namespace "downward-api-4712" to be "Succeeded or Failed"
Sep 17 00:26:22.661: INFO: Pod "downwardapi-volume-6a31b1c4-5e5d-420b-9c5e-9437aba147f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.166912ms
Sep 17 00:26:24.664: INFO: Pod "downwardapi-volume-6a31b1c4-5e5d-420b-9c5e-9437aba147f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005315486s
Sep 17 00:26:26.667: INFO: Pod "downwardapi-volume-6a31b1c4-5e5d-420b-9c5e-9437aba147f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00842491s
STEP: Saw pod success
Sep 17 00:26:26.667: INFO: Pod "downwardapi-volume-6a31b1c4-5e5d-420b-9c5e-9437aba147f8" satisfied condition "Succeeded or Failed"
Sep 17 00:26:26.669: INFO: Trying to get logs from node eqx03-flash06 pod downwardapi-volume-6a31b1c4-5e5d-420b-9c5e-9437aba147f8 container client-container: <nil>
STEP: delete the pod
Sep 17 00:26:26.724: INFO: Waiting for pod downwardapi-volume-6a31b1c4-5e5d-420b-9c5e-9437aba147f8 to disappear
Sep 17 00:26:26.726: INFO: Pod downwardapi-volume-6a31b1c4-5e5d-420b-9c5e-9437aba147f8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:26:26.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4712" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":268,"skipped":4583,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:26:26.737: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 17 00:26:26.783: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 17 00:26:32.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 --namespace=crd-publish-openapi-4109 create -f -'
Sep 17 00:26:32.886: INFO: stderr: ""
Sep 17 00:26:32.886: INFO: stdout: "e2e-test-crd-publish-openapi-158-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep 17 00:26:32.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 --namespace=crd-publish-openapi-4109 delete e2e-test-crd-publish-openapi-158-crds test-cr'
Sep 17 00:26:32.993: INFO: stderr: ""
Sep 17 00:26:32.993: INFO: stdout: "e2e-test-crd-publish-openapi-158-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Sep 17 00:26:32.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 --namespace=crd-publish-openapi-4109 apply -f -'
Sep 17 00:26:33.357: INFO: stderr: ""
Sep 17 00:26:33.357: INFO: stdout: "e2e-test-crd-publish-openapi-158-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep 17 00:26:33.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 --namespace=crd-publish-openapi-4109 delete e2e-test-crd-publish-openapi-158-crds test-cr'
Sep 17 00:26:33.468: INFO: stderr: ""
Sep 17 00:26:33.468: INFO: stdout: "e2e-test-crd-publish-openapi-158-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Sep 17 00:26:33.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-480724876 explain e2e-test-crd-publish-openapi-158-crds'
Sep 17 00:26:33.806: INFO: stderr: ""
Sep 17 00:26:33.806: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-158-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:26:39.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4109" for this suite.

• [SLOW TEST:12.705 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":277,"completed":269,"skipped":4614,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:26:39.443: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep 17 00:26:39.513: INFO: Waiting up to 5m0s for pod "pod-4a4fe7cd-cacc-45ff-8413-7bd744eed1fb" in namespace "emptydir-1330" to be "Succeeded or Failed"
Sep 17 00:26:39.515: INFO: Pod "pod-4a4fe7cd-cacc-45ff-8413-7bd744eed1fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.321271ms
Sep 17 00:26:41.519: INFO: Pod "pod-4a4fe7cd-cacc-45ff-8413-7bd744eed1fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005824446s
Sep 17 00:26:43.535: INFO: Pod "pod-4a4fe7cd-cacc-45ff-8413-7bd744eed1fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021948083s
STEP: Saw pod success
Sep 17 00:26:43.535: INFO: Pod "pod-4a4fe7cd-cacc-45ff-8413-7bd744eed1fb" satisfied condition "Succeeded or Failed"
Sep 17 00:26:43.546: INFO: Trying to get logs from node eqx03-flash06 pod pod-4a4fe7cd-cacc-45ff-8413-7bd744eed1fb container test-container: <nil>
STEP: delete the pod
Sep 17 00:26:43.591: INFO: Waiting for pod pod-4a4fe7cd-cacc-45ff-8413-7bd744eed1fb to disappear
Sep 17 00:26:43.593: INFO: Pod pod-4a4fe7cd-cacc-45ff-8413-7bd744eed1fb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:26:43.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1330" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":270,"skipped":4648,"failed":0}
S
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:26:43.610: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-projected-all-test-volume-3730145f-733e-4fef-84d5-0b448902add2
STEP: Creating secret with name secret-projected-all-test-volume-6e7c3537-eef5-4d5b-b1d7-dbaa81559a3b
STEP: Creating a pod to test Check all projections for projected volume plugin
Sep 17 00:26:43.696: INFO: Waiting up to 5m0s for pod "projected-volume-4833e3b0-2294-4ea5-ba36-ca5cf9137d1b" in namespace "projected-1790" to be "Succeeded or Failed"
Sep 17 00:26:43.698: INFO: Pod "projected-volume-4833e3b0-2294-4ea5-ba36-ca5cf9137d1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057048ms
Sep 17 00:26:45.701: INFO: Pod "projected-volume-4833e3b0-2294-4ea5-ba36-ca5cf9137d1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005505261s
Sep 17 00:26:47.705: INFO: Pod "projected-volume-4833e3b0-2294-4ea5-ba36-ca5cf9137d1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00870975s
STEP: Saw pod success
Sep 17 00:26:47.705: INFO: Pod "projected-volume-4833e3b0-2294-4ea5-ba36-ca5cf9137d1b" satisfied condition "Succeeded or Failed"
Sep 17 00:26:47.707: INFO: Trying to get logs from node eqx03-flash06 pod projected-volume-4833e3b0-2294-4ea5-ba36-ca5cf9137d1b container projected-all-volume-test: <nil>
STEP: delete the pod
Sep 17 00:26:47.756: INFO: Waiting for pod projected-volume-4833e3b0-2294-4ea5-ba36-ca5cf9137d1b to disappear
Sep 17 00:26:47.758: INFO: Pod projected-volume-4833e3b0-2294-4ea5-ba36-ca5cf9137d1b no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:26:47.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1790" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":277,"completed":271,"skipped":4649,"failed":0}

------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:26:47.768: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:27:00.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6425" for this suite.

• [SLOW TEST:13.205 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":277,"completed":272,"skipped":4649,"failed":0}
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:27:00.973: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Sep 17 00:27:01.059: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5178 /api/v1/namespaces/watch-5178/configmaps/e2e-watch-test-label-changed 8d3ec46b-c6e1-4033-93c2-83d8b8925255 4255847 0 2020-09-17 00:27:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-09-17 00:27:01 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 17 00:27:01.059: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5178 /api/v1/namespaces/watch-5178/configmaps/e2e-watch-test-label-changed 8d3ec46b-c6e1-4033-93c2-83d8b8925255 4255848 0 2020-09-17 00:27:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-09-17 00:27:01 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 17 00:27:01.059: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5178 /api/v1/namespaces/watch-5178/configmaps/e2e-watch-test-label-changed 8d3ec46b-c6e1-4033-93c2-83d8b8925255 4255849 0 2020-09-17 00:27:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-09-17 00:27:01 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Sep 17 00:27:11.103: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5178 /api/v1/namespaces/watch-5178/configmaps/e2e-watch-test-label-changed 8d3ec46b-c6e1-4033-93c2-83d8b8925255 4255900 0 2020-09-17 00:27:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-09-17 00:27:11 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 17 00:27:11.104: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5178 /api/v1/namespaces/watch-5178/configmaps/e2e-watch-test-label-changed 8d3ec46b-c6e1-4033-93c2-83d8b8925255 4255901 0 2020-09-17 00:27:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-09-17 00:27:11 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 17 00:27:11.104: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5178 /api/v1/namespaces/watch-5178/configmaps/e2e-watch-test-label-changed 8d3ec46b-c6e1-4033-93c2-83d8b8925255 4255902 0 2020-09-17 00:27:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-09-17 00:27:11 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:27:11.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5178" for this suite.

• [SLOW TEST:10.144 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":277,"completed":273,"skipped":4649,"failed":0}
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:27:11.117: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-1132
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-1132
STEP: Creating statefulset with conflicting port in namespace statefulset-1132
STEP: Waiting until pod test-pod will start running in namespace statefulset-1132
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1132
Sep 17 00:27:15.230: INFO: Observed stateful pod in namespace: statefulset-1132, name: ss-0, uid: 12e7e6ef-9e70-45ff-9f49-171ab90adc73, status phase: Pending. Waiting for statefulset controller to delete.
Sep 17 00:27:15.422: INFO: Observed stateful pod in namespace: statefulset-1132, name: ss-0, uid: 12e7e6ef-9e70-45ff-9f49-171ab90adc73, status phase: Failed. Waiting for statefulset controller to delete.
Sep 17 00:27:15.432: INFO: Observed stateful pod in namespace: statefulset-1132, name: ss-0, uid: 12e7e6ef-9e70-45ff-9f49-171ab90adc73, status phase: Failed. Waiting for statefulset controller to delete.
Sep 17 00:27:15.443: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1132
STEP: Removing pod with conflicting port in namespace statefulset-1132
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1132 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Sep 17 00:27:19.474: INFO: Deleting all statefulset in ns statefulset-1132
Sep 17 00:27:19.476: INFO: Scaling statefulset ss to 0
Sep 17 00:27:29.526: INFO: Waiting for statefulset status.replicas updated to 0
Sep 17 00:27:29.529: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:27:29.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1132" for this suite.

• [SLOW TEST:18.452 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":277,"completed":274,"skipped":4649,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:27:29.569: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-37db3845-be1a-490b-99b2-5b0850bcfa42
STEP: Creating a pod to test consume secrets
Sep 17 00:27:29.654: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-145d6f03-4cf8-4a30-8888-c6dae6abd604" in namespace "projected-2959" to be "Succeeded or Failed"
Sep 17 00:27:29.656: INFO: Pod "pod-projected-secrets-145d6f03-4cf8-4a30-8888-c6dae6abd604": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057325ms
Sep 17 00:27:31.659: INFO: Pod "pod-projected-secrets-145d6f03-4cf8-4a30-8888-c6dae6abd604": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005049995s
Sep 17 00:27:33.662: INFO: Pod "pod-projected-secrets-145d6f03-4cf8-4a30-8888-c6dae6abd604": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008161825s
STEP: Saw pod success
Sep 17 00:27:33.662: INFO: Pod "pod-projected-secrets-145d6f03-4cf8-4a30-8888-c6dae6abd604" satisfied condition "Succeeded or Failed"
Sep 17 00:27:33.664: INFO: Trying to get logs from node eqx03-flash06 pod pod-projected-secrets-145d6f03-4cf8-4a30-8888-c6dae6abd604 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 17 00:27:33.716: INFO: Waiting for pod pod-projected-secrets-145d6f03-4cf8-4a30-8888-c6dae6abd604 to disappear
Sep 17 00:27:33.718: INFO: Pod pod-projected-secrets-145d6f03-4cf8-4a30-8888-c6dae6abd604 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:27:33.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2959" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":275,"skipped":4652,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:27:33.730: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod test-webserver-52369970-125e-4b6e-9b15-42f75ccf614e in namespace container-probe-3847
Sep 17 00:27:35.828: INFO: Started pod test-webserver-52369970-125e-4b6e-9b15-42f75ccf614e in namespace container-probe-3847
STEP: checking the pod's current state and verifying that restartCount is present
Sep 17 00:27:35.830: INFO: Initial restart count of pod test-webserver-52369970-125e-4b6e-9b15-42f75ccf614e is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:31:36.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3847" for this suite.

• [SLOW TEST:242.606 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":277,"completed":276,"skipped":4695,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Sep 17 00:31:36.336: INFO: >>> kubeConfig: /tmp/kubeconfig-480724876
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Sep 17 00:31:36.379: INFO: Creating deployment "webserver-deployment"
Sep 17 00:31:36.394: INFO: Waiting for observed generation 1
Sep 17 00:31:38.408: INFO: Waiting for all required pods to come up
Sep 17 00:31:38.411: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Sep 17 00:31:44.418: INFO: Waiting for deployment "webserver-deployment" to complete
Sep 17 00:31:44.422: INFO: Updating deployment "webserver-deployment" with a non-existent image
Sep 17 00:31:44.432: INFO: Updating deployment webserver-deployment
Sep 17 00:31:44.432: INFO: Waiting for observed generation 2
Sep 17 00:31:46.437: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Sep 17 00:31:46.439: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Sep 17 00:31:46.442: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep 17 00:31:46.449: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Sep 17 00:31:46.449: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Sep 17 00:31:46.451: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep 17 00:31:46.455: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Sep 17 00:31:46.455: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Sep 17 00:31:46.490: INFO: Updating deployment webserver-deployment
Sep 17 00:31:46.490: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Sep 17 00:31:46.517: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Sep 17 00:31:46.545: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Sep 17 00:31:46.642: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8939 /apis/apps/v1/namespaces/deployment-8939/deployments/webserver-deployment 033faf67-2c38-4d58-ad03-7401b5c28680 4257421 3 2020-09-17 00:31:36 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-09-17 00:31:46 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-09-17 00:31:46 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 110 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003e23738 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-6676bcd6d4" is progressing.,LastUpdateTime:2020-09-17 00:31:44 +0000 UTC,LastTransitionTime:2020-09-17 00:31:36 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-09-17 00:31:46 +0000 UTC,LastTransitionTime:2020-09-17 00:31:46 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Sep 17 00:31:46.684: INFO: New ReplicaSet "webserver-deployment-6676bcd6d4" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-6676bcd6d4  deployment-8939 /apis/apps/v1/namespaces/deployment-8939/replicasets/webserver-deployment-6676bcd6d4 e03a91b9-0dfc-4558-91a5-752aa4e64630 4257418 3 2020-09-17 00:31:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 033faf67-2c38-4d58-ad03-7401b5c28680 0xc003e23be7 0xc003e23be8}] []  [{kube-controller-manager Update apps/v1 2020-09-17 00:31:46 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 48 51 51 102 97 102 54 55 45 50 99 51 56 45 52 100 53 56 45 97 100 48 51 45 55 52 48 49 98 53 99 50 56 54 56 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 6676bcd6d4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003e23c68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 17 00:31:46.684: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Sep 17 00:31:46.684: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-84855cf797  deployment-8939 /apis/apps/v1/namespaces/deployment-8939/replicasets/webserver-deployment-84855cf797 f3d93b23-0242-4f0e-b892-88fb2fbe5e5b 4257416 3 2020-09-17 00:31:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 033faf67-2c38-4d58-ad03-7401b5c28680 0xc003e23cc7 0xc003e23cc8}] []  [{kube-controller-manager Update apps/v1 2020-09-17 00:31:46 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 48 51 51 102 97 102 54 55 45 50 99 51 56 45 52 100 53 56 45 97 100 48 51 45 55 52 48 49 98 53 99 50 56 54 56 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 84855cf797,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003e23d38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Sep 17 00:31:46.747: INFO: Pod "webserver-deployment-6676bcd6d4-4gr5k" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-4gr5k webserver-deployment-6676bcd6d4- deployment-8939 /api/v1/namespaces/deployment-8939/pods/webserver-deployment-6676bcd6d4-4gr5k c8f770c8-07ee-49f0-baf1-cd528c8af3f1 4257450 0 2020-09-17 00:31:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e03a91b9-0dfc-4558-91a5-752aa4e64630 0xc0078a5757 0xc0078a5758}] []  [{kube-controller-manager Update v1 2020-09-17 00:31:46 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 48 51 97 57 49 98 57 45 48 100 102 99 45 52 53 53 56 45 57 49 97 53 45 55 53 50 97 97 52 101 54 52 54 51 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rcgm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rcgm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rcgm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 17 00:31:46.748: INFO: Pod "webserver-deployment-6676bcd6d4-8khn6" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-8khn6 webserver-deployment-6676bcd6d4- deployment-8939 /api/v1/namespaces/deployment-8939/pods/webserver-deployment-6676bcd6d4-8khn6 d67ff62b-aa26-4e93-850f-fa9fbc4dec9c 4257435 0 2020-09-17 00:31:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e03a91b9-0dfc-4558-91a5-752aa4e64630 0xc0078a5870 0xc0078a5871}] []  [{kube-controller-manager Update v1 2020-09-17 00:31:46 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 48 51 97 57 49 98 57 45 48 100 102 99 45 52 53 53 56 45 57 49 97 53 45 55 53 50 97 97 52 101 54 52 54 51 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rcgm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rcgm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rcgm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 17 00:31:46.748: INFO: Pod "webserver-deployment-6676bcd6d4-9pjqx" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-9pjqx webserver-deployment-6676bcd6d4- deployment-8939 /api/v1/namespaces/deployment-8939/pods/webserver-deployment-6676bcd6d4-9pjqx eee5f32c-da21-4271-94e8-c7da9184fb73 4257401 0 2020-09-17 00:31:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e03a91b9-0dfc-4558-91a5-752aa4e64630 0xc0078a5990 0xc0078a5991}] []  [{kube-controller-manager Update v1 2020-09-17 00:31:44 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 48 51 97 57 49 98 57 45 48 100 102 99 45 52 53 53 56 45 57 49 97 53 45 55 53 50 97 97 52 101 54 52 54 51 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-09-17 00:31:44 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rcgm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rcgm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rcgm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.140.106,PodIP:,StartTime:2020-09-17 00:31:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 17 00:31:46.748: INFO: Pod "webserver-deployment-6676bcd6d4-bjq7r" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-bjq7r webserver-deployment-6676bcd6d4- deployment-8939 /api/v1/namespaces/deployment-8939/pods/webserver-deployment-6676bcd6d4-bjq7r a20b3610-42d2-4fbc-ba68-7b996df92f6b 4257460 0 2020-09-17 00:31:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e03a91b9-0dfc-4558-91a5-752aa4e64630 0xc0078a5b37 0xc0078a5b38}] []  [{kube-controller-manager Update v1 2020-09-17 00:31:46 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 48 51 97 57 49 98 57 45 48 100 102 99 45 52 53 53 56 45 57 49 97 53 45 55 53 50 97 97 52 101 54 52 54 51 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rcgm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rcgm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rcgm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 17 00:31:46.748: INFO: Pod "webserver-deployment-6676bcd6d4-cxsvz" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-cxsvz webserver-deployment-6676bcd6d4- deployment-8939 /api/v1/namespaces/deployment-8939/pods/webserver-deployment-6676bcd6d4-cxsvz d41a633f-013d-4a92-ac97-1571f476e116 4257461 0 2020-09-17 00:31:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e03a91b9-0dfc-4558-91a5-752aa4e64630 0xc0078a5c77 0xc0078a5c78}] []  [{kube-controller-manager Update v1 2020-09-17 00:31:46 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 48 51 97 57 49 98 57 45 48 100 102 99 45 52 53 53 56 45 57 49 97 53 45 55 53 50 97 97 52 101 54 52 54 51 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-09-17 00:31:46 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rcgm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rcgm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rcgm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.140.106,PodIP:,StartTime:2020-09-17 00:31:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 17 00:31:46.749: INFO: Pod "webserver-deployment-6676bcd6d4-d449m" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-d449m webserver-deployment-6676bcd6d4- deployment-8939 /api/v1/namespaces/deployment-8939/pods/webserver-deployment-6676bcd6d4-d449m 0833447c-b05d-4d82-bdae-9c7babea6719 4257406 0 2020-09-17 00:31:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e03a91b9-0dfc-4558-91a5-752aa4e64630 0xc0078a5e27 0xc0078a5e28}] []  [{kube-controller-manager Update v1 2020-09-17 00:31:44 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 48 51 97 57 49 98 57 45 48 100 102 99 45 52 53 53 56 45 57 49 97 53 45 55 53 50 97 97 52 101 54 52 54 51 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-09-17 00:31:44 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rcgm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rcgm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rcgm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.140.106,PodIP:,StartTime:2020-09-17 00:31:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 17 00:31:46.749: INFO: Pod "webserver-deployment-6676bcd6d4-hzgmz" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-hzgmz webserver-deployment-6676bcd6d4- deployment-8939 /api/v1/namespaces/deployment-8939/pods/webserver-deployment-6676bcd6d4-hzgmz 49dbab1c-15c6-4952-8359-ec14d13b0a97 4257442 0 2020-09-17 00:31:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e03a91b9-0dfc-4558-91a5-752aa4e64630 0xc0078a5fd7 0xc0078a5fd8}] []  [{kube-controller-manager Update v1 2020-09-17 00:31:46 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 48 51 97 57 49 98 57 45 48 100 102 99 45 52 53 53 56 45 57 49 97 53 45 55 53 50 97 97 52 101 54 52 54 51 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rcgm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rcgm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rcgm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 17 00:31:46.749: INFO: Pod "webserver-deployment-6676bcd6d4-l95ds" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-l95ds webserver-deployment-6676bcd6d4- deployment-8939 /api/v1/namespaces/deployment-8939/pods/webserver-deployment-6676bcd6d4-l95ds 7fb39589-e261-45e5-83ca-d4da69e205e5 4257453 0 2020-09-17 00:31:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e03a91b9-0dfc-4558-91a5-752aa4e64630 0xc003ed80f0 0xc003ed80f1}] []  [{kube-controller-manager Update v1 2020-09-17 00:31:46 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 48 51 97 57 49 98 57 45 48 100 102 99 45 52 53 53 56 45 57 49 97 53 45 55 53 50 97 97 52 101 54 52 54 51 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rcgm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rcgm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rcgm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 17 00:31:46.749: INFO: Pod "webserver-deployment-6676bcd6d4-p6fnp" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-p6fnp webserver-deployment-6676bcd6d4- deployment-8939 /api/v1/namespaces/deployment-8939/pods/webserver-deployment-6676bcd6d4-p6fnp b2fb0a89-1a74-47fb-9caa-b49c85ecf91c 4257408 0 2020-09-17 00:31:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e03a91b9-0dfc-4558-91a5-752aa4e64630 0xc003ed8237 0xc003ed8238}] []  [{kube-controller-manager Update v1 2020-09-17 00:31:44 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 48 51 97 57 49 98 57 45 48 100 102 99 45 52 53 53 56 45 57 49 97 53 45 55 53 50 97 97 52 101 54 52 54 51 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-09-17 00:31:45 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rcgm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rcgm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rcgm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.140.106,PodIP:,StartTime:2020-09-17 00:31:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 17 00:31:46.749: INFO: Pod "webserver-deployment-6676bcd6d4-pr88v" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-pr88v webserver-deployment-6676bcd6d4- deployment-8939 /api/v1/namespaces/deployment-8939/pods/webserver-deployment-6676bcd6d4-pr88v b03adc65-8c62-4253-bc10-374274a05654 4257439 0 2020-09-17 00:31:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e03a91b9-0dfc-4558-91a5-752aa4e64630 0xc003ed83e7 0xc003ed83e8}] []  [{kube-controller-manager Update v1 2020-09-17 00:31:46 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 48 51 97 57 49 98 57 45 48 100 102 99 45 52 53 53 56 45 57 49 97 53 45 55 53 50 97 97 52 101 54 52 54 51 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rcgm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rcgm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rcgm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 17 00:31:46.750: INFO: Pod "webserver-deployment-6676bcd6d4-pzm2p" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-pzm2p webserver-deployment-6676bcd6d4- deployment-8939 /api/v1/namespaces/deployment-8939/pods/webserver-deployment-6676bcd6d4-pzm2p 54b52bb6-27f8-442b-bde6-b664b639165f 4257470 0 2020-09-17 00:31:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e03a91b9-0dfc-4558-91a5-752aa4e64630 0xc003ed8500 0xc003ed8501}] []  [{kube-controller-manager Update v1 2020-09-17 00:31:46 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 48 51 97 57 49 98 57 45 48 100 102 99 45 52 53 53 56 45 57 49 97 53 45 55 53 50 97 97 52 101 54 52 54 51 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rcgm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rcgm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rcgm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 17 00:31:46.750: INFO: Pod "webserver-deployment-6676bcd6d4-w22w4" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-w22w4 webserver-deployment-6676bcd6d4- deployment-8939 /api/v1/namespaces/deployment-8939/pods/webserver-deployment-6676bcd6d4-w22w4 4f373fa4-658f-4e7c-a5e0-e36e5b4b1aa2 4257407 0 2020-09-17 00:31:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e03a91b9-0dfc-4558-91a5-752aa4e64630 0xc003ed8647 0xc003ed8648}] []  [{kube-controller-manager Update v1 2020-09-17 00:31:44 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 48 51 97 57 49 98 57 45 48 100 102 99 45 52 53 53 56 45 57 49 97 53 45 55 53 50 97 97 52 101 54 52 54 51 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-09-17 00:31:44 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rcgm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rcgm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rcgm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.140.106,PodIP:,StartTime:2020-09-17 00:31:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 17 00:31:46.750: INFO: Pod "webserver-deployment-6676bcd6d4-w64pd" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-w64pd webserver-deployment-6676bcd6d4- deployment-8939 /api/v1/namespaces/deployment-8939/pods/webserver-deployment-6676bcd6d4-w64pd 684674b1-3c9c-4fee-bec5-f8ec822716ab 4257463 0 2020-09-17 00:31:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[cni.projectcalico.org/podIP:172.21.4.78/32 cni.projectcalico.org/podIPs:172.21.4.78/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "calico",
    "ips": [
        "172.21.4.78"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "calico",
    "ips": [
        "172.21.4.78"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e03a91b9-0dfc-4558-91a5-752aa4e64630 0xc003ed8817 0xc003ed8818}] []  [{kube-controller-manager Update v1 2020-09-17 00:31:44 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 48 51 97 57 49 98 57 45 48 100 102 99 45 52 53 53 56 45 57 49 97 53 45 55 53 50 97 97 52 101 54 52 54 51 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-09-17 00:31:44 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}} {calico Update v1 2020-09-17 00:31:46 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {multus Update v1 2020-09-17 00:31:46 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 107 56 115 46 118 49 46 99 110 105 46 99 110 99 102 46 105 111 47 110 101 116 119 111 114 107 45 115 116 97 116 117 115 34 58 123 125 44 34 102 58 107 56 115 46 118 49 46 99 110 105 46 99 110 99 102 46 105 111 47 110 101 116 119 111 114 107 115 45 115 116 97 116 117 115 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rcgm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rcgm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rcgm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.140.106,PodIP:,StartTime:2020-09-17 00:31:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 17 00:31:46.750: INFO: Pod "webserver-deployment-84855cf797-5hxpc" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-5hxpc webserver-deployment-84855cf797- deployment-8939 /api/v1/namespaces/deployment-8939/pods/webserver-deployment-84855cf797-5hxpc ff693f69-5c53-4d6a-8cb3-07156f2a0561 4257454 0 2020-09-17 00:31:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 f3d93b23-0242-4f0e-b892-88fb2fbe5e5b 0xc003ed89f7 0xc003ed89f8}] []  [{kube-controller-manager Update v1 2020-09-17 00:31:46 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 51 100 57 51 98 50 51 45 48 50 52 50 45 52 102 48 101 45 98 56 57 50 45 56 56 102 98 50 102 98 101 53 101 53 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rcgm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rcgm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rcgm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 17 00:31:46.751: INFO: Pod "webserver-deployment-84855cf797-6bc9v" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-6bc9v webserver-deployment-84855cf797- deployment-8939 /api/v1/namespaces/deployment-8939/pods/webserver-deployment-84855cf797-6bc9v b5df19be-bb12-4c3e-9abc-74f20b3306a4 4257457 0 2020-09-17 00:31:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 f3d93b23-0242-4f0e-b892-88fb2fbe5e5b 0xc003ed8b00 0xc003ed8b01}] []  [{kube-controller-manager Update v1 2020-09-17 00:31:46 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 51 100 57 51 98 50 51 45 48 50 52 50 45 52 102 48 101 45 98 56 57 50 45 56 56 102 98 50 102 98 101 53 101 53 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rcgm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rcgm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rcgm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 17 00:31:46.751: INFO: Pod "webserver-deployment-84855cf797-79ql8" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-79ql8 webserver-deployment-84855cf797- deployment-8939 /api/v1/namespaces/deployment-8939/pods/webserver-deployment-84855cf797-79ql8 07c9e426-18ab-4ea2-af55-85fb571fb852 4257321 0 2020-09-17 00:31:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:172.21.0.115/32 cni.projectcalico.org/podIPs:172.21.0.115/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "calico",
    "ips": [
        "172.21.0.115"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "calico",
    "ips": [
        "172.21.0.115"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 f3d93b23-0242-4f0e-b892-88fb2fbe5e5b 0xc003ed8c30 0xc003ed8c31}] []  [{kube-controller-manager Update v1 2020-09-17 00:31:36 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 51 100 57 51 98 50 51 45 48 50 52 50 45 52 102 48 101 45 98 56 57 50 45 56 56 102 98 50 102 98 101 53 101 53 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-09-17 00:31:38 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {multus Update v1 2020-09-17 00:31:38 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 107 56 115 46 118 49 46 99 110 105 46 99 110 99 102 46 105 111 47 110 101 116 119 111 114 107 45 115 116 97 116 117 115 34 58 123 125 44 34 102 58 107 56 115 46 118 49 46 99 110 105 46 99 110 99 102 46 105 111 47 110 101 116 119 111 114 107 115 45 115 116 97 116 117 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-09-17 00:31:41 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 50 49 46 48 46 49 49 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rcgm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rcgm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rcgm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.140.106,PodIP:172.21.0.115,StartTime:2020-09-17 00:31:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-17 00:31:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:robin://6568319ca9a681d7af65d0d364f561fc745300dbadc19e54041c0a0d218f9939,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.0.115,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 17 00:31:46.751: INFO: Pod "webserver-deployment-84855cf797-9hjg8" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-9hjg8 webserver-deployment-84855cf797- deployment-8939 /api/v1/namespaces/deployment-8939/pods/webserver-deployment-84855cf797-9hjg8 323c531c-ae2d-4b61-a3ec-7eee2d16a83b 4257330 0 2020-09-17 00:31:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:172.21.3.105/32 cni.projectcalico.org/podIPs:172.21.3.105/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "calico",
    "ips": [
        "172.21.3.105"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "calico",
    "ips": [
        "172.21.3.105"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 f3d93b23-0242-4f0e-b892-88fb2fbe5e5b 0xc003ed8e27 0xc003ed8e28}] []  [{kube-controller-manager Update v1 2020-09-17 00:31:36 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 51 100 57 51 98 50 51 45 48 50 52 50 45 52 102 48 101 45 98 56 57 50 45 56 56 102 98 50 102 98 101 53 101 53 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-09-17 00:31:38 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {multus Update v1 2020-09-17 00:31:38 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 107 56 115 46 118 49 46 99 110 105 46 99 110 99 102 46 105 111 47 110 101 116 119 111 114 107 45 115 116 97 116 117 115 34 58 123 125 44 34 102 58 107 56 115 46 118 49 46 99 110 105 46 99 110 99 102 46 105 111 47 110 101 116 119 111 114 107 115 45 115 116 97 116 117 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-09-17 00:31:41 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 50 49 46 51 46 49 48 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rcgm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rcgm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rcgm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.140.106,PodIP:172.21.3.105,StartTime:2020-09-17 00:31:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-17 00:31:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:robin://409f1b5f72fcb90a8996ef4bd991111b90f4e53a4633293b021f7fd924d10769,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.3.105,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 17 00:31:46.751: INFO: Pod "webserver-deployment-84855cf797-brqdn" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-brqdn webserver-deployment-84855cf797- deployment-8939 /api/v1/namespaces/deployment-8939/pods/webserver-deployment-84855cf797-brqdn c96a0bb2-46a2-4770-95c2-fc994a2b2d2a 4257465 0 2020-09-17 00:31:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 f3d93b23-0242-4f0e-b892-88fb2fbe5e5b 0xc003ed9017 0xc003ed9018}] []  [{kube-controller-manager Update v1 2020-09-17 00:31:46 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 51 100 57 51 98 50 51 45 48 50 52 50 45 52 102 48 101 45 98 56 57 50 45 56 56 102 98 50 102 98 101 53 101 53 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rcgm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rcgm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rcgm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 17 00:31:46.752: INFO: Pod "webserver-deployment-84855cf797-bssm4" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-bssm4 webserver-deployment-84855cf797- deployment-8939 /api/v1/namespaces/deployment-8939/pods/webserver-deployment-84855cf797-bssm4 6cc9a6c1-57ec-4dd1-b4de-2dca06352f02 4257455 0 2020-09-17 00:31:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 f3d93b23-0242-4f0e-b892-88fb2fbe5e5b 0xc003ed9147 0xc003ed9148}] []  [{kube-controller-manager Update v1 2020-09-17 00:31:46 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 51 100 57 51 98 50 51 45 48 50 52 50 45 52 102 48 101 45 98 56 57 50 45 56 56 102 98 50 102 98 101 53 101 53 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rcgm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rcgm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rcgm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 17 00:31:46.752: INFO: Pod "webserver-deployment-84855cf797-d9jxc" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-d9jxc webserver-deployment-84855cf797- deployment-8939 /api/v1/namespaces/deployment-8939/pods/webserver-deployment-84855cf797-d9jxc 3fff0d5c-3abc-40a0-89ea-ec3f89c08950 4257342 0 2020-09-17 00:31:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:172.21.4.56/32 cni.projectcalico.org/podIPs:172.21.4.56/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "calico",
    "ips": [
        "172.21.4.56"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "calico",
    "ips": [
        "172.21.4.56"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 f3d93b23-0242-4f0e-b892-88fb2fbe5e5b 0xc003ed9270 0xc003ed9271}] []  [{kube-controller-manager Update v1 2020-09-17 00:31:36 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 51 100 57 51 98 50 51 45 48 50 52 50 45 52 102 48 101 45 98 56 57 50 45 56 56 102 98 50 102 98 101 53 101 53 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-09-17 00:31:38 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {multus Update v1 2020-09-17 00:31:38 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 107 56 115 46 118 49 46 99 110 105 46 99 110 99 102 46 105 111 47 110 101 116 119 111 114 107 45 115 116 97 116 117 115 34 58 123 125 44 34 102 58 107 56 115 46 118 49 46 99 110 105 46 99 110 99 102 46 105 111 47 110 101 116 119 111 114 107 115 45 115 116 97 116 117 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-09-17 00:31:42 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 50 49 46 52 46 53 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rcgm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rcgm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rcgm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.140.106,PodIP:172.21.4.56,StartTime:2020-09-17 00:31:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-17 00:31:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:robin://9d8ee59db356191d48979111821e536317c254eb6cc52b07a167f52c9a70e22d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.4.56,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 17 00:31:46.752: INFO: Pod "webserver-deployment-84855cf797-gr88w" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-gr88w webserver-deployment-84855cf797- deployment-8939 /api/v1/namespaces/deployment-8939/pods/webserver-deployment-84855cf797-gr88w 5241a429-3eb9-4822-acad-d0adc69fe5b8 4257464 0 2020-09-17 00:31:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 f3d93b23-0242-4f0e-b892-88fb2fbe5e5b 0xc003ed9447 0xc003ed9448}] []  [{kube-controller-manager Update v1 2020-09-17 00:31:46 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 51 100 57 51 98 50 51 45 48 50 52 50 45 52 102 48 101 45 98 56 57 50 45 56 56 102 98 50 102 98 101 53 101 53 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rcgm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rcgm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rcgm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 17 00:31:46.752: INFO: Pod "webserver-deployment-84855cf797-hgq4k" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-hgq4k webserver-deployment-84855cf797- deployment-8939 /api/v1/namespaces/deployment-8939/pods/webserver-deployment-84855cf797-hgq4k d901e211-2db3-45da-b8fd-f34c656e8106 4257302 0 2020-09-17 00:31:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:172.21.4.18/32 cni.projectcalico.org/podIPs:172.21.4.18/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "calico",
    "ips": [
        "172.21.4.18"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "calico",
    "ips": [
        "172.21.4.18"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 f3d93b23-0242-4f0e-b892-88fb2fbe5e5b 0xc003ed9570 0xc003ed9571}] []  [{kube-controller-manager Update v1 2020-09-17 00:31:36 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 51 100 57 51 98 50 51 45 48 50 52 50 45 52 102 48 101 45 98 56 57 50 45 56 56 102 98 50 102 98 101 53 101 53 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-09-17 00:31:38 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {multus Update v1 2020-09-17 00:31:38 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 107 56 115 46 118 49 46 99 110 105 46 99 110 99 102 46 105 111 47 110 101 116 119 111 114 107 45 115 116 97 116 117 115 34 58 123 125 44 34 102 58 107 56 115 46 118 49 46 99 110 105 46 99 110 99 102 46 105 111 47 110 101 116 119 111 114 107 115 45 115 116 97 116 117 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-09-17 00:31:40 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 50 49 46 52 46 49 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rcgm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rcgm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rcgm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.140.106,PodIP:172.21.4.18,StartTime:2020-09-17 00:31:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-17 00:31:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:robin://048691ab97b65dcf995c8fd54742d2429c790838846383d9314ece0a5a0ff77c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.4.18,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 17 00:31:46.753: INFO: Pod "webserver-deployment-84855cf797-j8cgw" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-j8cgw webserver-deployment-84855cf797- deployment-8939 /api/v1/namespaces/deployment-8939/pods/webserver-deployment-84855cf797-j8cgw 6d1c2454-74c9-4925-8ed4-cc95aa999f77 4257445 0 2020-09-17 00:31:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 f3d93b23-0242-4f0e-b892-88fb2fbe5e5b 0xc003ed9747 0xc003ed9748}] []  [{kube-controller-manager Update v1 2020-09-17 00:31:46 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 51 100 57 51 98 50 51 45 48 50 52 50 45 52 102 48 101 45 98 56 57 50 45 56 56 102 98 50 102 98 101 53 101 53 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rcgm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rcgm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rcgm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 17 00:31:46.753: INFO: Pod "webserver-deployment-84855cf797-km8qh" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-km8qh webserver-deployment-84855cf797- deployment-8939 /api/v1/namespaces/deployment-8939/pods/webserver-deployment-84855cf797-km8qh 175bcb86-decb-43ff-a140-5c1d6fab5495 4257448 0 2020-09-17 00:31:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 f3d93b23-0242-4f0e-b892-88fb2fbe5e5b 0xc003ed9850 0xc003ed9851}] []  [{kube-controller-manager Update v1 2020-09-17 00:31:46 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 51 100 57 51 98 50 51 45 48 50 52 50 45 52 102 48 101 45 98 56 57 50 45 56 56 102 98 50 102 98 101 53 101 53 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rcgm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rcgm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rcgm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 17 00:31:46.753: INFO: Pod "webserver-deployment-84855cf797-l4fc2" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-l4fc2 webserver-deployment-84855cf797- deployment-8939 /api/v1/namespaces/deployment-8939/pods/webserver-deployment-84855cf797-l4fc2 59ebddb5-b35f-46cf-a798-7b2367052418 4257440 0 2020-09-17 00:31:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 f3d93b23-0242-4f0e-b892-88fb2fbe5e5b 0xc003ed9960 0xc003ed9961}] []  [{kube-controller-manager Update v1 2020-09-17 00:31:46 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 51 100 57 51 98 50 51 45 48 50 52 50 45 52 102 48 101 45 98 56 57 50 45 56 56 102 98 50 102 98 101 53 101 53 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rcgm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rcgm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rcgm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 17 00:31:46.753: INFO: Pod "webserver-deployment-84855cf797-n552g" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-n552g webserver-deployment-84855cf797- deployment-8939 /api/v1/namespaces/deployment-8939/pods/webserver-deployment-84855cf797-n552g 4b7e8122-3c4f-4f32-994b-c121954701a1 4257291 0 2020-09-17 00:31:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:172.21.11.106/32 cni.projectcalico.org/podIPs:172.21.11.106/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "calico",
    "ips": [
        "172.21.11.106"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "calico",
    "ips": [
        "172.21.11.106"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 f3d93b23-0242-4f0e-b892-88fb2fbe5e5b 0xc003ed9a90 0xc003ed9a91}] []  [{kube-controller-manager Update v1 2020-09-17 00:31:36 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 51 100 57 51 98 50 51 45 48 50 52 50 45 52 102 48 101 45 98 56 57 50 45 56 56 102 98 50 102 98 101 53 101 53 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-09-17 00:31:38 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {multus Update v1 2020-09-17 00:31:38 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 107 56 115 46 118 49 46 99 110 105 46 99 110 99 102 46 105 111 47 110 101 116 119 111 114 107 45 115 116 97 116 117 115 34 58 123 125 44 34 102 58 107 56 115 46 118 49 46 99 110 105 46 99 110 99 102 46 105 111 47 110 101 116 119 111 114 107 115 45 115 116 97 116 117 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-09-17 00:31:39 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 50 49 46 49 49 46 49 48 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rcgm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rcgm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rcgm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.140.106,PodIP:172.21.11.106,StartTime:2020-09-17 00:31:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-17 00:31:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:robin://91cdaba5e2674085bf9460dbe20db54e4b622ac9592ada6da8a64301ef487948,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.11.106,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 17 00:31:46.753: INFO: Pod "webserver-deployment-84855cf797-n9jm9" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-n9jm9 webserver-deployment-84855cf797- deployment-8939 /api/v1/namespaces/deployment-8939/pods/webserver-deployment-84855cf797-n9jm9 617b38ee-225f-49ca-b3f4-8708e5956dbb 4257462 0 2020-09-17 00:31:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 f3d93b23-0242-4f0e-b892-88fb2fbe5e5b 0xc003ed9c67 0xc003ed9c68}] []  [{kube-controller-manager Update v1 2020-09-17 00:31:46 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 51 100 57 51 98 50 51 45 48 50 52 50 45 52 102 48 101 45 98 56 57 50 45 56 56 102 98 50 102 98 101 53 101 53 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rcgm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rcgm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rcgm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 17 00:31:46.754: INFO: Pod "webserver-deployment-84855cf797-rdx8x" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-rdx8x webserver-deployment-84855cf797- deployment-8939 /api/v1/namespaces/deployment-8939/pods/webserver-deployment-84855cf797-rdx8x a0fcd74f-978d-458a-ad42-0ccaaa5fc5f1 4257467 0 2020-09-17 00:31:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 f3d93b23-0242-4f0e-b892-88fb2fbe5e5b 0xc003ed9d70 0xc003ed9d71}] []  [{kube-controller-manager Update v1 2020-09-17 00:31:46 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 51 100 57 51 98 50 51 45 48 50 52 50 45 52 102 48 101 45 98 56 57 50 45 56 56 102 98 50 102 98 101 53 101 53 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rcgm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rcgm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rcgm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 17 00:31:46.754: INFO: Pod "webserver-deployment-84855cf797-s4kgc" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-s4kgc webserver-deployment-84855cf797- deployment-8939 /api/v1/namespaces/deployment-8939/pods/webserver-deployment-84855cf797-s4kgc 1ec82d20-7f22-4d35-8061-a671696328d4 4257452 0 2020-09-17 00:31:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 f3d93b23-0242-4f0e-b892-88fb2fbe5e5b 0xc003ed9ea7 0xc003ed9ea8}] []  [{kube-controller-manager Update v1 2020-09-17 00:31:46 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 51 100 57 51 98 50 51 45 48 50 52 50 45 52 102 48 101 45 98 56 57 50 45 56 56 102 98 50 102 98 101 53 101 53 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rcgm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rcgm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rcgm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 17 00:31:46.754: INFO: Pod "webserver-deployment-84855cf797-v7zlv" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-v7zlv webserver-deployment-84855cf797- deployment-8939 /api/v1/namespaces/deployment-8939/pods/webserver-deployment-84855cf797-v7zlv 99134f40-eb43-4c91-a970-cd6110c86cd8 4257338 0 2020-09-17 00:31:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:172.21.8.247/32 cni.projectcalico.org/podIPs:172.21.8.247/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "calico",
    "ips": [
        "172.21.8.247"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "calico",
    "ips": [
        "172.21.8.247"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 f3d93b23-0242-4f0e-b892-88fb2fbe5e5b 0xc003ed9ff7 0xc003ed9ff8}] []  [{kube-controller-manager Update v1 2020-09-17 00:31:36 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 51 100 57 51 98 50 51 45 48 50 52 50 45 52 102 48 101 45 98 56 57 50 45 56 56 102 98 50 102 98 101 53 101 53 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-09-17 00:31:38 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {multus Update v1 2020-09-17 00:31:38 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 107 56 115 46 118 49 46 99 110 105 46 99 110 99 102 46 105 111 47 110 101 116 119 111 114 107 45 115 116 97 116 117 115 34 58 123 125 44 34 102 58 107 56 115 46 118 49 46 99 110 105 46 99 110 99 102 46 105 111 47 110 101 116 119 111 114 107 115 45 115 116 97 116 117 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-09-17 00:31:42 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 50 49 46 56 46 50 52 55 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rcgm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rcgm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rcgm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.140.106,PodIP:172.21.8.247,StartTime:2020-09-17 00:31:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-17 00:31:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:robin://12dbe8e3eb79d4a785f1695f7005794b335ccab1022cd5bad2d0730f5f299897,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.8.247,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 17 00:31:46.754: INFO: Pod "webserver-deployment-84855cf797-vx5s2" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-vx5s2 webserver-deployment-84855cf797- deployment-8939 /api/v1/namespaces/deployment-8939/pods/webserver-deployment-84855cf797-vx5s2 e3a5de54-3983-4f76-a1a0-29f2528ddbef 4257447 0 2020-09-17 00:31:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 f3d93b23-0242-4f0e-b892-88fb2fbe5e5b 0xc002d383e7 0xc002d383e8}] []  [{kube-controller-manager Update v1 2020-09-17 00:31:46 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 51 100 57 51 98 50 51 45 48 50 52 50 45 52 102 48 101 45 98 56 57 50 45 56 56 102 98 50 102 98 101 53 101 53 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rcgm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rcgm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rcgm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 17 00:31:46.754: INFO: Pod "webserver-deployment-84855cf797-wxblv" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-wxblv webserver-deployment-84855cf797- deployment-8939 /api/v1/namespaces/deployment-8939/pods/webserver-deployment-84855cf797-wxblv cdbacd05-5e7a-47e0-9766-4a6ab5cfaad9 4257282 0 2020-09-17 00:31:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:172.21.13.133/32 cni.projectcalico.org/podIPs:172.21.13.133/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "calico",
    "ips": [
        "172.21.13.133"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "calico",
    "ips": [
        "172.21.13.133"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 f3d93b23-0242-4f0e-b892-88fb2fbe5e5b 0xc002d38510 0xc002d38511}] []  [{kube-controller-manager Update v1 2020-09-17 00:31:36 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 51 100 57 51 98 50 51 45 48 50 52 50 45 52 102 48 101 45 98 56 57 50 45 56 56 102 98 50 102 98 101 53 101 53 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-09-17 00:31:38 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {multus Update v1 2020-09-17 00:31:38 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 107 56 115 46 118 49 46 99 110 105 46 99 110 99 102 46 105 111 47 110 101 116 119 111 114 107 45 115 116 97 116 117 115 34 58 123 125 44 34 102 58 107 56 115 46 118 49 46 99 110 105 46 99 110 99 102 46 105 111 47 110 101 116 119 111 114 107 115 45 115 116 97 116 117 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-09-17 00:31:39 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 50 49 46 49 51 46 49 51 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rcgm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rcgm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rcgm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.140.106,PodIP:172.21.13.133,StartTime:2020-09-17 00:31:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-17 00:31:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:robin://79f5d39f0296c0768680b1f6589a6904d19fa917ba94d81514c01271c65bd648,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.13.133,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 17 00:31:46.755: INFO: Pod "webserver-deployment-84855cf797-xj4cn" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-xj4cn webserver-deployment-84855cf797- deployment-8939 /api/v1/namespaces/deployment-8939/pods/webserver-deployment-84855cf797-xj4cn 7990c166-2c3d-4aa0-8be0-2e96680370a6 4257279 0 2020-09-17 00:31:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:172.21.13.123/32 cni.projectcalico.org/podIPs:172.21.13.123/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "calico",
    "ips": [
        "172.21.13.123"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "calico",
    "ips": [
        "172.21.13.123"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 f3d93b23-0242-4f0e-b892-88fb2fbe5e5b 0xc002d38707 0xc002d38708}] []  [{kube-controller-manager Update v1 2020-09-17 00:31:36 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 51 100 57 51 98 50 51 45 48 50 52 50 45 52 102 48 101 45 98 56 57 50 45 56 56 102 98 50 102 98 101 53 101 53 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-09-17 00:31:38 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {multus Update v1 2020-09-17 00:31:38 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 107 56 115 46 118 49 46 99 110 105 46 99 110 99 102 46 105 111 47 110 101 116 119 111 114 107 45 115 116 97 116 117 115 34 58 123 125 44 34 102 58 107 56 115 46 118 49 46 99 110 105 46 99 110 99 102 46 105 111 47 110 101 116 119 111 114 107 115 45 115 116 97 116 117 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-09-17 00:31:39 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 55 50 46 50 49 46 49 51 46 49 50 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rcgm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rcgm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rcgm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-17 00:31:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.140.106,PodIP:172.21.13.123,StartTime:2020-09-17 00:31:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-17 00:31:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:robin://1ce26c98366ead680701b5a56eb3dd921b356b99045c754135d5aac7ffb9ba3c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.13.123,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Sep 17 00:31:46.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8939" for this suite.

• [SLOW TEST:10.499 seconds]
[sig-apps] Deployment
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":277,"completed":277,"skipped":4696,"failed":0}
SSSSSSSSSSSSSSSSSSSSep 17 00:31:46.835: INFO: Running AfterSuite actions on all nodes
Sep 17 00:31:46.835: INFO: Running AfterSuite actions on node 1
Sep 17 00:31:46.835: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":277,"completed":277,"skipped":4715,"failed":0}

Ran 277 of 4992 Specs in 5070.561 seconds
SUCCESS! -- 277 Passed | 0 Failed | 0 Pending | 4715 Skipped
PASS

Ginkgo ran 1 suite in 1h24m32.306457697s
Test Suite Passed
