I1224 02:18:05.500299      24 test_context.go:416] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-303221030
I1224 02:18:05.500317      24 test_context.go:429] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I1224 02:18:05.500443      24 e2e.go:129] Starting e2e run "b09050e8-3c82-4a73-ae58-9703fde3db88" on Ginkgo node 1
{"msg":"Test Suite starting","total":305,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1608776283 - Will randomize all specs
Will run 305 of 5234 specs

Dec 24 02:18:05.514: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
Dec 24 02:18:05.518: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Dec 24 02:18:05.544: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec 24 02:18:05.586: INFO: 15 / 15 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec 24 02:18:05.586: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Dec 24 02:18:05.586: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec 24 02:18:05.596: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Dec 24 02:18:05.596: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Dec 24 02:18:05.596: INFO: e2e test version: v1.19.4
Dec 24 02:18:05.597: INFO: kube-apiserver version: v1.19.4
Dec 24 02:18:05.597: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
Dec 24 02:18:05.601: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:18:05.601: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename downward-api
Dec 24 02:18:05.648: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Dec 24 02:18:05.654: INFO: No PSP annotation exists on dry run pod; assuming PodSecurityPolicy is disabled
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Dec 24 02:18:05.660: INFO: Waiting up to 5m0s for pod "downward-api-996db02e-6544-4ae1-9ea2-1dd2db79a164" in namespace "downward-api-4278" to be "Succeeded or Failed"
Dec 24 02:18:05.666: INFO: Pod "downward-api-996db02e-6544-4ae1-9ea2-1dd2db79a164": Phase="Pending", Reason="", readiness=false. Elapsed: 5.519857ms
Dec 24 02:18:07.669: INFO: Pod "downward-api-996db02e-6544-4ae1-9ea2-1dd2db79a164": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008593417s
Dec 24 02:18:09.672: INFO: Pod "downward-api-996db02e-6544-4ae1-9ea2-1dd2db79a164": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011555336s
STEP: Saw pod success
Dec 24 02:18:09.672: INFO: Pod "downward-api-996db02e-6544-4ae1-9ea2-1dd2db79a164" satisfied condition "Succeeded or Failed"
Dec 24 02:18:09.674: INFO: Trying to get logs from node c1-4 pod downward-api-996db02e-6544-4ae1-9ea2-1dd2db79a164 container dapi-container: <nil>
STEP: delete the pod
Dec 24 02:18:09.707: INFO: Waiting for pod downward-api-996db02e-6544-4ae1-9ea2-1dd2db79a164 to disappear
Dec 24 02:18:09.711: INFO: Pod downward-api-996db02e-6544-4ae1-9ea2-1dd2db79a164 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:18:09.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4278" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":305,"completed":1,"skipped":44,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:18:09.719: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 24 02:18:11.788: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:18:11.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5471" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":2,"skipped":80,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:18:11.858: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Dec 24 02:18:13.920: INFO: &Pod{ObjectMeta:{send-events-8aa68ce5-d8f5-40fe-aacc-4366bf96ffc0  events-6089 /api/v1/namespaces/events-6089/pods/send-events-8aa68ce5-d8f5-40fe-aacc-4366bf96ffc0 5cca8ccb-29c3-413c-bd1e-4b98dfed717b 3427757 0 2020-12-24 02:18:11 +0000 UTC <nil> <nil> map[name:foo time:896737432] map[cni.projectcalico.org/podIP:10.244.113.106/32 cni.projectcalico.org/podIPs:10.244.113.106/32] [] []  [{e2e.test Update v1 2020-12-24 02:18:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-24 02:18:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-24 02:18:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.113.106\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xt4f6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xt4f6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xt4f6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 02:18:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 02:18:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 02:18:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 02:18:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.3.5,PodIP:10.244.113.106,StartTime:2020-12-24 02:18:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-24 02:18:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:cri-o://a7196d975b626bf1bf84aae7a6ee0bd63a3f2cd1bc11a45959e352a26b9a50f7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.113.106,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Dec 24 02:18:15.923: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Dec 24 02:18:17.927: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:18:17.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6089" for this suite.

• [SLOW TEST:6.101 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":305,"completed":3,"skipped":107,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:18:17.959: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1224 02:18:28.148771      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Dec 24 02:19:30.179: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Dec 24 02:19:30.180: INFO: Deleting pod "simpletest-rc-to-be-deleted-59h54" in namespace "gc-8277"
Dec 24 02:19:30.209: INFO: Deleting pod "simpletest-rc-to-be-deleted-8lgvp" in namespace "gc-8277"
Dec 24 02:19:30.241: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvwgs" in namespace "gc-8277"
Dec 24 02:19:30.282: INFO: Deleting pod "simpletest-rc-to-be-deleted-fchcq" in namespace "gc-8277"
Dec 24 02:19:30.307: INFO: Deleting pod "simpletest-rc-to-be-deleted-hbn5b" in namespace "gc-8277"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:19:30.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8277" for this suite.

• [SLOW TEST:72.397 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":305,"completed":4,"skipped":116,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:19:30.357: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5480.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5480.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5480.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5480.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5480.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5480.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 24 02:19:34.496: INFO: DNS probes using dns-5480/dns-test-0cbd92e7-2a1d-4ca9-af2e-a17fe2e398fe succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:19:34.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5480" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":305,"completed":5,"skipped":135,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:19:34.534: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 02:19:34.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 version'
Dec 24 02:19:34.747: INFO: stderr: ""
Dec 24 02:19:34.747: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.4\", GitCommit:\"d360454c9bcd1634cf4cc52d1867af5491dc9c5f\", GitTreeState:\"clean\", BuildDate:\"2020-11-11T13:17:17Z\", GoVersion:\"go1.15.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.4\", GitCommit:\"d360454c9bcd1634cf4cc52d1867af5491dc9c5f\", GitTreeState:\"clean\", BuildDate:\"2020-11-11T13:09:17Z\", GoVersion:\"go1.15.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:19:34.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4400" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":305,"completed":6,"skipped":143,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Ingress API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:19:34.755: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Dec 24 02:19:34.857: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Dec 24 02:19:34.865: INFO: starting watch
STEP: patching
STEP: updating
Dec 24 02:19:34.877: INFO: waiting for watch events with expected annotations
Dec 24 02:19:34.877: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:19:34.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-6665" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":305,"completed":7,"skipped":160,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:19:34.986: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:19:51.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5853" for this suite.

• [SLOW TEST:16.201 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":305,"completed":8,"skipped":162,"failed":0}
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:19:51.187: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Dec 24 02:19:51.235: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 24 02:19:51.243: INFO: Waiting for terminating namespaces to be deleted...
Dec 24 02:19:51.245: INFO: 
Logging pods the apiserver thinks is on node c1-4 before test
Dec 24 02:19:51.256: INFO: calico-node-gf9xk from kube-system started at 2020-12-17 09:20:20 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.256: INFO: 	Container calico-node ready: true, restart count 0
Dec 24 02:19:51.256: INFO: kube-proxy-nnl7j from kube-system started at 2020-12-17 09:18:43 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.256: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 24 02:19:51.256: INFO: virt-handler-5nkqw from kubevirt started at 2020-12-23 10:54:33 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.256: INFO: 	Container virt-handler ready: true, restart count 0
Dec 24 02:19:51.256: INFO: csi-cephfsplugin-ghqns from rook-ceph started at 2020-12-23 10:54:33 +0000 UTC (3 container statuses recorded)
Dec 24 02:19:51.256: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 24 02:19:51.256: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 24 02:19:51.256: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 02:19:51.256: INFO: csi-rbdplugin-z74d4 from rook-ceph started at 2020-12-23 10:54:33 +0000 UTC (3 container statuses recorded)
Dec 24 02:19:51.256: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 24 02:19:51.256: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 24 02:19:51.257: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 02:19:51.257: INFO: rook-ceph-crashcollector-c1-4-b5fd6cbf-r86bh from rook-ceph started at 2020-12-23 10:29:44 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.257: INFO: 	Container ceph-crash ready: true, restart count 0
Dec 24 02:19:51.257: INFO: rook-ceph-mon-b-6c84dd66-xj8wb from rook-ceph started at 2020-12-23 10:29:43 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.257: INFO: 	Container mon ready: true, restart count 0
Dec 24 02:19:51.257: INFO: rook-ceph-osd-1-59b659d744-k6c5s from rook-ceph started at 2020-12-23 10:29:43 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.257: INFO: 	Container osd ready: true, restart count 0
Dec 24 02:19:51.257: INFO: rook-discover-svzsw from rook-ceph started at 2020-12-23 10:54:33 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.257: INFO: 	Container rook-discover ready: true, restart count 0
Dec 24 02:19:51.257: INFO: sonobuoy from sonobuoy started at 2020-12-24 02:17:41 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.257: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 24 02:19:51.257: INFO: sonobuoy-e2e-job-683f472080954d4f from sonobuoy started at 2020-12-24 02:17:43 +0000 UTC (2 container statuses recorded)
Dec 24 02:19:51.257: INFO: 	Container e2e ready: true, restart count 0
Dec 24 02:19:51.257: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 24 02:19:51.257: INFO: sonobuoy-systemd-logs-daemon-set-eb313291d8024436-cqjvw from sonobuoy started at 2020-12-24 02:17:43 +0000 UTC (2 container statuses recorded)
Dec 24 02:19:51.257: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 24 02:19:51.257: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 24 02:19:51.257: INFO: 
Logging pods the apiserver thinks is on node c1-5 before test
Dec 24 02:19:51.272: INFO: cdi-apiserver-79dbd664dd-jvbpv from cdi started at 2020-12-23 09:46:37 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.272: INFO: 	Container cdi-apiserver ready: true, restart count 0
Dec 24 02:19:51.272: INFO: snapshot-controller-0 from default started at 2020-12-18 04:06:22 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.272: INFO: 	Container snapshot-controller ready: true, restart count 0
Dec 24 02:19:51.272: INFO: calico-node-tt5dp from kube-system started at 2020-12-17 09:20:20 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.272: INFO: 	Container calico-node ready: true, restart count 0
Dec 24 02:19:51.272: INFO: kube-proxy-2lcn2 from kube-system started at 2020-12-17 09:18:32 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.272: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 24 02:19:51.272: INFO: virt-api-76b44ffc65-kcmtd from kubevirt started at 2020-12-23 09:46:37 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.272: INFO: 	Container virt-api ready: true, restart count 0
Dec 24 02:19:51.272: INFO: virt-controller-84b5d4b779-lbv8p from kubevirt started at 2020-12-23 09:46:37 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.272: INFO: 	Container virt-controller ready: true, restart count 0
Dec 24 02:19:51.272: INFO: virt-handler-scjdh from kubevirt started at 2020-12-17 09:24:03 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.272: INFO: 	Container virt-handler ready: true, restart count 7
Dec 24 02:19:51.272: INFO: virt-operator-867fcd786d-hqk4k from kubevirt started at 2020-12-23 09:46:37 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.272: INFO: 	Container virt-operator ready: true, restart count 0
Dec 24 02:19:51.272: INFO: csi-cephfsplugin-provisioner-d878cdf8b-6g8td from rook-ceph started at 2020-12-18 04:06:36 +0000 UTC (6 container statuses recorded)
Dec 24 02:19:51.272: INFO: 	Container csi-attacher ready: true, restart count 1
Dec 24 02:19:51.272: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 24 02:19:51.272: INFO: 	Container csi-provisioner ready: true, restart count 1
Dec 24 02:19:51.272: INFO: 	Container csi-resizer ready: true, restart count 1
Dec 24 02:19:51.272: INFO: 	Container csi-snapshotter ready: true, restart count 1
Dec 24 02:19:51.272: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 02:19:51.272: INFO: csi-cephfsplugin-xd5rd from rook-ceph started at 2020-12-18 04:06:35 +0000 UTC (3 container statuses recorded)
Dec 24 02:19:51.272: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 24 02:19:51.272: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 24 02:19:51.272: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 02:19:51.272: INFO: csi-rbdplugin-drpwm from rook-ceph started at 2020-12-18 04:06:35 +0000 UTC (3 container statuses recorded)
Dec 24 02:19:51.272: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 24 02:19:51.272: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 24 02:19:51.272: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 02:19:51.272: INFO: csi-rbdplugin-provisioner-7588bc4-dxkjr from rook-ceph started at 2020-12-18 04:06:35 +0000 UTC (6 container statuses recorded)
Dec 24 02:19:51.272: INFO: 	Container csi-attacher ready: true, restart count 1
Dec 24 02:19:51.272: INFO: 	Container csi-provisioner ready: true, restart count 2
Dec 24 02:19:51.272: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 24 02:19:51.272: INFO: 	Container csi-resizer ready: true, restart count 1
Dec 24 02:19:51.272: INFO: 	Container csi-snapshotter ready: true, restart count 1
Dec 24 02:19:51.272: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 02:19:51.272: INFO: rook-ceph-crashcollector-c1-5-7fdd9898c6-mm2r6 from rook-ceph started at 2020-12-18 04:07:17 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.272: INFO: 	Container ceph-crash ready: true, restart count 0
Dec 24 02:19:51.272: INFO: rook-ceph-mon-c-bcf7d945c-mvssc from rook-ceph started at 2020-12-18 04:07:17 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.272: INFO: 	Container mon ready: true, restart count 0
Dec 24 02:19:51.272: INFO: rook-ceph-operator-775d4b6c5f-khccc from rook-ceph started at 2020-12-18 04:06:25 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.272: INFO: 	Container rook-ceph-operator ready: true, restart count 0
Dec 24 02:19:51.272: INFO: rook-ceph-osd-2-5478c54cc5-s282q from rook-ceph started at 2020-12-18 04:07:41 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.272: INFO: 	Container osd ready: true, restart count 0
Dec 24 02:19:51.272: INFO: rook-ceph-osd-prepare-c1-5-l2wd8 from rook-ceph started at 2020-12-18 05:07:16 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.272: INFO: 	Container provision ready: false, restart count 0
Dec 24 02:19:51.272: INFO: rook-ceph-tools-6967fc698d-n2nzp from rook-ceph started at 2020-12-23 09:46:37 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.272: INFO: 	Container rook-ceph-tools ready: true, restart count 0
Dec 24 02:19:51.272: INFO: rook-discover-ds9s8 from rook-ceph started at 2020-12-18 04:06:28 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.272: INFO: 	Container rook-discover ready: true, restart count 0
Dec 24 02:19:51.272: INFO: sonobuoy-systemd-logs-daemon-set-eb313291d8024436-82n6r from sonobuoy started at 2020-12-24 02:17:43 +0000 UTC (2 container statuses recorded)
Dec 24 02:19:51.272: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 24 02:19:51.272: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 24 02:19:51.272: INFO: 
Logging pods the apiserver thinks is on node c1-6 before test
Dec 24 02:19:51.287: INFO: cdi-deployment-6c8975f4b-6bk46 from cdi started at 2020-12-21 09:29:47 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.287: INFO: 	Container cdi-controller ready: true, restart count 1
Dec 24 02:19:51.287: INFO: cdi-operator-5d5654db65-5hdrv from cdi started at 2020-12-21 09:29:21 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.287: INFO: 	Container cdi-operator ready: true, restart count 2
Dec 24 02:19:51.287: INFO: cdi-uploadproxy-5765998fff-qh2cb from cdi started at 2020-12-23 09:46:37 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.287: INFO: 	Container cdi-uploadproxy ready: true, restart count 0
Dec 24 02:19:51.287: INFO: calico-node-5rrck from kube-system started at 2020-12-17 09:20:20 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.287: INFO: 	Container calico-node ready: true, restart count 0
Dec 24 02:19:51.287: INFO: kube-proxy-jmmvs from kube-system started at 2020-12-17 09:19:08 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.287: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 24 02:19:51.287: INFO: virt-api-76b44ffc65-b4xmf from kubevirt started at 2020-12-17 09:23:40 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.287: INFO: 	Container virt-api ready: true, restart count 0
Dec 24 02:19:51.287: INFO: virt-controller-84b5d4b779-krqk6 from kubevirt started at 2020-12-17 09:24:03 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.287: INFO: 	Container virt-controller ready: true, restart count 5
Dec 24 02:19:51.287: INFO: virt-handler-jk5bd from kubevirt started at 2020-12-17 09:24:03 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.287: INFO: 	Container virt-handler ready: true, restart count 0
Dec 24 02:19:51.287: INFO: virt-operator-867fcd786d-9xnf6 from kubevirt started at 2020-12-17 09:23:28 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.287: INFO: 	Container virt-operator ready: true, restart count 2
Dec 24 02:19:51.287: INFO: csi-cephfsplugin-provisioner-d878cdf8b-9v92z from rook-ceph started at 2020-12-23 09:46:37 +0000 UTC (6 container statuses recorded)
Dec 24 02:19:51.287: INFO: 	Container csi-attacher ready: true, restart count 0
Dec 24 02:19:51.287: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 24 02:19:51.287: INFO: 	Container csi-provisioner ready: true, restart count 0
Dec 24 02:19:51.287: INFO: 	Container csi-resizer ready: true, restart count 0
Dec 24 02:19:51.287: INFO: 	Container csi-snapshotter ready: true, restart count 0
Dec 24 02:19:51.287: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 02:19:51.287: INFO: csi-cephfsplugin-q5hm7 from rook-ceph started at 2020-12-18 04:06:35 +0000 UTC (3 container statuses recorded)
Dec 24 02:19:51.287: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 24 02:19:51.287: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 24 02:19:51.287: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 02:19:51.287: INFO: csi-rbdplugin-provisioner-7588bc4-r297x from rook-ceph started at 2020-12-18 04:06:35 +0000 UTC (6 container statuses recorded)
Dec 24 02:19:51.287: INFO: 	Container csi-attacher ready: true, restart count 0
Dec 24 02:19:51.287: INFO: 	Container csi-provisioner ready: true, restart count 0
Dec 24 02:19:51.287: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 24 02:19:51.287: INFO: 	Container csi-resizer ready: true, restart count 0
Dec 24 02:19:51.287: INFO: 	Container csi-snapshotter ready: true, restart count 0
Dec 24 02:19:51.287: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 02:19:51.287: INFO: csi-rbdplugin-xfdqp from rook-ceph started at 2020-12-18 04:06:35 +0000 UTC (3 container statuses recorded)
Dec 24 02:19:51.287: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 24 02:19:51.287: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 24 02:19:51.287: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 02:19:51.287: INFO: rook-ceph-crashcollector-c1-6-5868d4fd57-6c5gp from rook-ceph started at 2020-12-18 04:08:07 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.287: INFO: 	Container ceph-crash ready: true, restart count 0
Dec 24 02:19:51.287: INFO: rook-ceph-mds-myfs-a-59cfdfc57f-zb7rk from rook-ceph started at 2020-12-23 09:46:37 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.287: INFO: 	Container mds ready: true, restart count 0
Dec 24 02:19:51.287: INFO: rook-ceph-mds-myfs-b-6d859f5b5c-r2rqz from rook-ceph started at 2020-12-18 04:08:07 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.287: INFO: 	Container mds ready: true, restart count 0
Dec 24 02:19:51.287: INFO: rook-ceph-mgr-a-8b957cf9b-9nkkq from rook-ceph started at 2020-12-18 04:07:30 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.287: INFO: 	Container mgr ready: true, restart count 0
Dec 24 02:19:51.287: INFO: rook-ceph-mon-a-77f88c5ff8-hlrmz from rook-ceph started at 2020-12-18 04:06:59 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.287: INFO: 	Container mon ready: true, restart count 0
Dec 24 02:19:51.287: INFO: rook-ceph-osd-0-5897494d6d-k6ml7 from rook-ceph started at 2020-12-18 04:07:39 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.287: INFO: 	Container osd ready: true, restart count 0
Dec 24 02:19:51.287: INFO: rook-ceph-osd-prepare-c1-6-jmhbg from rook-ceph started at 2020-12-18 05:07:18 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.287: INFO: 	Container provision ready: false, restart count 0
Dec 24 02:19:51.287: INFO: rook-discover-4kd85 from rook-ceph started at 2020-12-18 04:06:28 +0000 UTC (1 container statuses recorded)
Dec 24 02:19:51.287: INFO: 	Container rook-discover ready: true, restart count 0
Dec 24 02:19:51.287: INFO: sonobuoy-systemd-logs-daemon-set-eb313291d8024436-79g22 from sonobuoy started at 2020-12-24 02:17:43 +0000 UTC (2 container statuses recorded)
Dec 24 02:19:51.287: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 24 02:19:51.287: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-a1f0d120-ffbd-4897-af22-bba4100d7515 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-a1f0d120-ffbd-4897-af22-bba4100d7515 off the node c1-4
STEP: verifying the node doesn't have the label kubernetes.io/e2e-a1f0d120-ffbd-4897-af22-bba4100d7515
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:19:55.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4251" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":305,"completed":9,"skipped":167,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:19:55.407: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating pod
Dec 24 02:19:57.475: INFO: Pod pod-hostip-b1c3f7f9-98a9-401b-86dd-3120e25a7a47 has hostIP: 172.21.3.5
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:19:57.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2775" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":305,"completed":10,"skipped":179,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:19:57.484: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:171
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating server pod server in namespace prestop-9965
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-9965
STEP: Deleting pre-stop pod
Dec 24 02:20:06.638: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:20:06.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-9965" for this suite.

• [SLOW TEST:9.185 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":305,"completed":11,"skipped":199,"failed":0}
S
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:20:06.669: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod test-webserver-f620421d-a691-485f-8d3c-c281d42c32e8 in namespace container-probe-7238
Dec 24 02:20:08.759: INFO: Started pod test-webserver-f620421d-a691-485f-8d3c-c281d42c32e8 in namespace container-probe-7238
STEP: checking the pod's current state and verifying that restartCount is present
Dec 24 02:20:08.762: INFO: Initial restart count of pod test-webserver-f620421d-a691-485f-8d3c-c281d42c32e8 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:24:09.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7238" for this suite.

• [SLOW TEST:242.649 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":12,"skipped":200,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:24:09.318: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 24 02:24:09.369: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dbcca832-22ed-442d-801b-1c4a48628ac7" in namespace "projected-4758" to be "Succeeded or Failed"
Dec 24 02:24:09.385: INFO: Pod "downwardapi-volume-dbcca832-22ed-442d-801b-1c4a48628ac7": Phase="Pending", Reason="", readiness=false. Elapsed: 15.898844ms
Dec 24 02:24:11.420: INFO: Pod "downwardapi-volume-dbcca832-22ed-442d-801b-1c4a48628ac7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.050934487s
STEP: Saw pod success
Dec 24 02:24:11.420: INFO: Pod "downwardapi-volume-dbcca832-22ed-442d-801b-1c4a48628ac7" satisfied condition "Succeeded or Failed"
Dec 24 02:24:11.422: INFO: Trying to get logs from node c1-4 pod downwardapi-volume-dbcca832-22ed-442d-801b-1c4a48628ac7 container client-container: <nil>
STEP: delete the pod
Dec 24 02:24:11.455: INFO: Waiting for pod downwardapi-volume-dbcca832-22ed-442d-801b-1c4a48628ac7 to disappear
Dec 24 02:24:11.471: INFO: Pod downwardapi-volume-dbcca832-22ed-442d-801b-1c4a48628ac7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:24:11.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4758" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":13,"skipped":205,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:24:11.482: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Dec 24 02:24:11.532: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:24:37.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-838" for this suite.

• [SLOW TEST:26.483 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":305,"completed":14,"skipped":255,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:24:37.966: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating replication controller my-hostname-basic-6a15170e-6e0d-4a01-9a5a-baeae0383895
Dec 24 02:24:38.020: INFO: Pod name my-hostname-basic-6a15170e-6e0d-4a01-9a5a-baeae0383895: Found 0 pods out of 1
Dec 24 02:24:43.023: INFO: Pod name my-hostname-basic-6a15170e-6e0d-4a01-9a5a-baeae0383895: Found 1 pods out of 1
Dec 24 02:24:43.023: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-6a15170e-6e0d-4a01-9a5a-baeae0383895" are running
Dec 24 02:24:43.025: INFO: Pod "my-hostname-basic-6a15170e-6e0d-4a01-9a5a-baeae0383895-j4xph" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-24 02:24:38 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-24 02:24:39 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-24 02:24:39 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-24 02:24:38 +0000 UTC Reason: Message:}])
Dec 24 02:24:43.025: INFO: Trying to dial the pod
Dec 24 02:24:48.039: INFO: Controller my-hostname-basic-6a15170e-6e0d-4a01-9a5a-baeae0383895: Got expected result from replica 1 [my-hostname-basic-6a15170e-6e0d-4a01-9a5a-baeae0383895-j4xph]: "my-hostname-basic-6a15170e-6e0d-4a01-9a5a-baeae0383895-j4xph", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:24:48.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7732" for this suite.

• [SLOW TEST:10.081 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":15,"skipped":266,"failed":0}
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:24:48.047: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-5063
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 24 02:24:48.087: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 24 02:24:48.163: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 24 02:24:50.167: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 24 02:24:52.166: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 24 02:24:54.166: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 24 02:24:56.167: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 24 02:24:58.166: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 24 02:25:00.166: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 24 02:25:02.166: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 24 02:25:04.173: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec 24 02:25:04.177: INFO: The status of Pod netserver-1 is Running (Ready = true)
Dec 24 02:25:04.182: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Dec 24 02:25:06.222: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.113.125 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5063 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 24 02:25:06.222: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
Dec 24 02:25:07.324: INFO: Found all expected endpoints: [netserver-0]
Dec 24 02:25:07.327: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.26.115 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5063 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 24 02:25:07.327: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
Dec 24 02:25:08.428: INFO: Found all expected endpoints: [netserver-1]
Dec 24 02:25:08.436: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.92.245 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5063 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 24 02:25:08.436: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
Dec 24 02:25:09.532: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:25:09.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5063" for this suite.

• [SLOW TEST:21.493 seconds]
[sig-network] Networking
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":16,"skipped":271,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:25:09.541: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Dec 24 02:25:12.136: INFO: Successfully updated pod "annotationupdatebcf26eec-ddca-45c9-bcb3-ac8a0acfe3df"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:25:16.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8559" for this suite.

• [SLOW TEST:6.674 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":17,"skipped":272,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:25:16.215: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 24 02:25:16.281: INFO: Waiting up to 5m0s for pod "pod-f2cd9e8d-c6ec-47e2-a406-fd9ea1ec2cae" in namespace "emptydir-92" to be "Succeeded or Failed"
Dec 24 02:25:16.295: INFO: Pod "pod-f2cd9e8d-c6ec-47e2-a406-fd9ea1ec2cae": Phase="Pending", Reason="", readiness=false. Elapsed: 14.218604ms
Dec 24 02:25:18.298: INFO: Pod "pod-f2cd9e8d-c6ec-47e2-a406-fd9ea1ec2cae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017243415s
STEP: Saw pod success
Dec 24 02:25:18.299: INFO: Pod "pod-f2cd9e8d-c6ec-47e2-a406-fd9ea1ec2cae" satisfied condition "Succeeded or Failed"
Dec 24 02:25:18.301: INFO: Trying to get logs from node c1-4 pod pod-f2cd9e8d-c6ec-47e2-a406-fd9ea1ec2cae container test-container: <nil>
STEP: delete the pod
Dec 24 02:25:18.328: INFO: Waiting for pod pod-f2cd9e8d-c6ec-47e2-a406-fd9ea1ec2cae to disappear
Dec 24 02:25:18.337: INFO: Pod pod-f2cd9e8d-c6ec-47e2-a406-fd9ea1ec2cae no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:25:18.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-92" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":18,"skipped":301,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:25:18.344: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 24 02:25:18.407: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8d515d70-5e85-4c71-a25f-890558840bc9" in namespace "projected-3494" to be "Succeeded or Failed"
Dec 24 02:25:18.421: INFO: Pod "downwardapi-volume-8d515d70-5e85-4c71-a25f-890558840bc9": Phase="Pending", Reason="", readiness=false. Elapsed: 14.455435ms
Dec 24 02:25:20.429: INFO: Pod "downwardapi-volume-8d515d70-5e85-4c71-a25f-890558840bc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022444894s
STEP: Saw pod success
Dec 24 02:25:20.429: INFO: Pod "downwardapi-volume-8d515d70-5e85-4c71-a25f-890558840bc9" satisfied condition "Succeeded or Failed"
Dec 24 02:25:20.431: INFO: Trying to get logs from node c1-4 pod downwardapi-volume-8d515d70-5e85-4c71-a25f-890558840bc9 container client-container: <nil>
STEP: delete the pod
Dec 24 02:25:20.445: INFO: Waiting for pod downwardapi-volume-8d515d70-5e85-4c71-a25f-890558840bc9 to disappear
Dec 24 02:25:20.451: INFO: Pod downwardapi-volume-8d515d70-5e85-4c71-a25f-890558840bc9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:25:20.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3494" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":19,"skipped":324,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:25:20.462: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service nodeport-test with type=NodePort in namespace services-6013
STEP: creating replication controller nodeport-test in namespace services-6013
I1224 02:25:20.561088      24 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-6013, replica count: 2
I1224 02:25:23.611528      24 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 24 02:25:23.611: INFO: Creating new exec pod
Dec 24 02:25:26.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-6013 execpodz27gt -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Dec 24 02:25:27.988: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Dec 24 02:25:27.988: INFO: stdout: ""
Dec 24 02:25:27.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-6013 execpodz27gt -- /bin/sh -x -c nc -zv -t -w 2 10.96.129.131 80'
Dec 24 02:25:28.180: INFO: stderr: "+ nc -zv -t -w 2 10.96.129.131 80\nConnection to 10.96.129.131 80 port [tcp/http] succeeded!\n"
Dec 24 02:25:28.180: INFO: stdout: ""
Dec 24 02:25:28.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-6013 execpodz27gt -- /bin/sh -x -c nc -zv -t -w 2 172.21.3.5 30453'
Dec 24 02:25:28.341: INFO: stderr: "+ nc -zv -t -w 2 172.21.3.5 30453\nConnection to 172.21.3.5 30453 port [tcp/30453] succeeded!\n"
Dec 24 02:25:28.341: INFO: stdout: ""
Dec 24 02:25:28.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-6013 execpodz27gt -- /bin/sh -x -c nc -zv -t -w 2 172.21.3.6 30453'
Dec 24 02:25:28.500: INFO: stderr: "+ nc -zv -t -w 2 172.21.3.6 30453\nConnection to 172.21.3.6 30453 port [tcp/30453] succeeded!\n"
Dec 24 02:25:28.500: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:25:28.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6013" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:8.046 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":305,"completed":20,"skipped":330,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:25:28.507: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Dec 24 02:25:30.323: INFO: starting watch
STEP: patching
STEP: updating
Dec 24 02:25:30.342: INFO: waiting for watch events with expected annotations
Dec 24 02:25:30.342: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:25:30.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-6635" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":305,"completed":21,"skipped":344,"failed":0}
SSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:25:30.454: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap that has name configmap-test-emptyKey-5bb86fac-bc40-4174-89b4-568c9052448a
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:25:30.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5968" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":305,"completed":22,"skipped":349,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:25:30.514: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:25:46.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9975" for this suite.

• [SLOW TEST:16.185 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":305,"completed":23,"skipped":366,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:25:46.699: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Dec 24 02:25:46.739: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 24 02:26:46.812: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 02:26:46.814: INFO: Starting informer...
STEP: Starting pod...
Dec 24 02:26:47.026: INFO: Pod is running on c1-4. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Dec 24 02:26:47.052: INFO: Pod wasn't evicted. Proceeding
Dec 24 02:26:47.052: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Dec 24 02:28:02.099: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:28:02.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-876" for this suite.

• [SLOW TEST:135.410 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":305,"completed":24,"skipped":378,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:28:02.109: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-30e2fd50-30cc-471a-aa38-ac53f19e3da3 in namespace container-probe-4361
Dec 24 02:30:12.182: INFO: Started pod busybox-30e2fd50-30cc-471a-aa38-ac53f19e3da3 in namespace container-probe-4361
STEP: checking the pod's current state and verifying that restartCount is present
Dec 24 02:30:12.184: INFO: Initial restart count of pod busybox-30e2fd50-30cc-471a-aa38-ac53f19e3da3 is 0
Dec 24 02:31:02.262: INFO: Restart count of pod container-probe-4361/busybox-30e2fd50-30cc-471a-aa38-ac53f19e3da3 is now 1 (50.077882726s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:31:02.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4361" for this suite.

• [SLOW TEST:180.217 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":25,"skipped":398,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:31:02.326: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8501
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8501
STEP: creating replication controller externalsvc in namespace services-8501
I1224 02:31:02.439266      24 runners.go:190] Created replication controller with name: externalsvc, namespace: services-8501, replica count: 2
I1224 02:31:05.489620      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Dec 24 02:31:05.508: INFO: Creating new exec pod
Dec 24 02:31:07.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-8501 execpodf7dgr -- /bin/sh -x -c nslookup clusterip-service.services-8501.svc.cluster.local'
Dec 24 02:31:07.738: INFO: stderr: "+ nslookup clusterip-service.services-8501.svc.cluster.local\n"
Dec 24 02:31:07.738: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-8501.svc.cluster.local\tcanonical name = externalsvc.services-8501.svc.cluster.local.\nName:\texternalsvc.services-8501.svc.cluster.local\nAddress: 10.96.199.161\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8501, will wait for the garbage collector to delete the pods
Dec 24 02:31:07.795: INFO: Deleting ReplicationController externalsvc took: 4.58408ms
Dec 24 02:31:08.796: INFO: Terminating ReplicationController externalsvc pods took: 1.000186079s
Dec 24 02:31:18.927: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:31:18.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8501" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:16.665 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":305,"completed":26,"skipped":420,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:31:18.991: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:31:19.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1712" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":305,"completed":27,"skipped":432,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:31:19.093: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-58e0bd54-ebbd-4fe0-9ddb-1d8c979aed9a
STEP: Creating a pod to test consume configMaps
Dec 24 02:31:19.145: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-546eca47-44a6-45b7-b930-51129e76c4bf" in namespace "projected-6547" to be "Succeeded or Failed"
Dec 24 02:31:19.150: INFO: Pod "pod-projected-configmaps-546eca47-44a6-45b7-b930-51129e76c4bf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.320233ms
Dec 24 02:31:21.153: INFO: Pod "pod-projected-configmaps-546eca47-44a6-45b7-b930-51129e76c4bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007862257s
STEP: Saw pod success
Dec 24 02:31:21.153: INFO: Pod "pod-projected-configmaps-546eca47-44a6-45b7-b930-51129e76c4bf" satisfied condition "Succeeded or Failed"
Dec 24 02:31:21.156: INFO: Trying to get logs from node c1-4 pod pod-projected-configmaps-546eca47-44a6-45b7-b930-51129e76c4bf container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 24 02:31:21.214: INFO: Waiting for pod pod-projected-configmaps-546eca47-44a6-45b7-b930-51129e76c4bf to disappear
Dec 24 02:31:21.228: INFO: Pod pod-projected-configmaps-546eca47-44a6-45b7-b930-51129e76c4bf no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:31:21.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6547" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":28,"skipped":438,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:31:21.236: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 02:31:21.276: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:31:21.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4964" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":305,"completed":29,"skipped":446,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:31:21.848: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-d7c221c1-80d5-4f22-bec3-77686084d679
STEP: Creating a pod to test consume configMaps
Dec 24 02:31:21.929: INFO: Waiting up to 5m0s for pod "pod-configmaps-cb670adb-e19a-43ca-96f7-cce1d6af2f10" in namespace "configmap-5504" to be "Succeeded or Failed"
Dec 24 02:31:21.942: INFO: Pod "pod-configmaps-cb670adb-e19a-43ca-96f7-cce1d6af2f10": Phase="Pending", Reason="", readiness=false. Elapsed: 13.619003ms
Dec 24 02:31:23.945: INFO: Pod "pod-configmaps-cb670adb-e19a-43ca-96f7-cce1d6af2f10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016411279s
STEP: Saw pod success
Dec 24 02:31:23.945: INFO: Pod "pod-configmaps-cb670adb-e19a-43ca-96f7-cce1d6af2f10" satisfied condition "Succeeded or Failed"
Dec 24 02:31:23.947: INFO: Trying to get logs from node c1-4 pod pod-configmaps-cb670adb-e19a-43ca-96f7-cce1d6af2f10 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 24 02:31:23.973: INFO: Waiting for pod pod-configmaps-cb670adb-e19a-43ca-96f7-cce1d6af2f10 to disappear
Dec 24 02:31:23.979: INFO: Pod pod-configmaps-cb670adb-e19a-43ca-96f7-cce1d6af2f10 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:31:23.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5504" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":30,"skipped":459,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:31:23.987: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 24 02:31:24.417: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 24 02:31:26.423: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744373884, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744373884, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744373884, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744373884, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 24 02:31:29.446: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:31:39.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6961" for this suite.
STEP: Destroying namespace "webhook-6961-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:15.789 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":305,"completed":31,"skipped":473,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:31:39.776: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in volume subpath
Dec 24 02:31:39.862: INFO: Waiting up to 5m0s for pod "var-expansion-948ea8bf-3791-4d77-952a-9aa8bdc38f78" in namespace "var-expansion-4970" to be "Succeeded or Failed"
Dec 24 02:31:39.866: INFO: Pod "var-expansion-948ea8bf-3791-4d77-952a-9aa8bdc38f78": Phase="Pending", Reason="", readiness=false. Elapsed: 4.094912ms
Dec 24 02:31:41.868: INFO: Pod "var-expansion-948ea8bf-3791-4d77-952a-9aa8bdc38f78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006392755s
STEP: Saw pod success
Dec 24 02:31:41.868: INFO: Pod "var-expansion-948ea8bf-3791-4d77-952a-9aa8bdc38f78" satisfied condition "Succeeded or Failed"
Dec 24 02:31:41.870: INFO: Trying to get logs from node c1-4 pod var-expansion-948ea8bf-3791-4d77-952a-9aa8bdc38f78 container dapi-container: <nil>
STEP: delete the pod
Dec 24 02:31:41.893: INFO: Waiting for pod var-expansion-948ea8bf-3791-4d77-952a-9aa8bdc38f78 to disappear
Dec 24 02:31:41.900: INFO: Pod var-expansion-948ea8bf-3791-4d77-952a-9aa8bdc38f78 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:31:41.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4970" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":305,"completed":32,"skipped":486,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:31:41.926: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-454.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-454.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-454.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-454.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-454.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-454.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-454.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-454.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-454.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-454.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-454.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-454.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-454.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 21.8.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.8.21_udp@PTR;check="$$(dig +tcp +noall +answer +search 21.8.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.8.21_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-454.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-454.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-454.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-454.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-454.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-454.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-454.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-454.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-454.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-454.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-454.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-454.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-454.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 21.8.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.8.21_udp@PTR;check="$$(dig +tcp +noall +answer +search 21.8.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.8.21_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 24 02:31:46.077: INFO: Unable to read wheezy_udp@dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:31:46.080: INFO: Unable to read wheezy_tcp@dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:31:46.084: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:31:46.086: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:31:46.106: INFO: Unable to read jessie_udp@dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:31:46.109: INFO: Unable to read jessie_tcp@dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:31:46.112: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:31:46.115: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:31:46.167: INFO: Lookups using dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539 failed for: [wheezy_udp@dns-test-service.dns-454.svc.cluster.local wheezy_tcp@dns-test-service.dns-454.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-454.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-454.svc.cluster.local jessie_udp@dns-test-service.dns-454.svc.cluster.local jessie_tcp@dns-test-service.dns-454.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-454.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-454.svc.cluster.local]

Dec 24 02:31:51.171: INFO: Unable to read wheezy_udp@dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:31:51.174: INFO: Unable to read wheezy_tcp@dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:31:51.177: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:31:51.181: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:31:51.206: INFO: Unable to read jessie_udp@dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:31:51.209: INFO: Unable to read jessie_tcp@dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:31:51.212: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:31:51.214: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:31:51.231: INFO: Lookups using dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539 failed for: [wheezy_udp@dns-test-service.dns-454.svc.cluster.local wheezy_tcp@dns-test-service.dns-454.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-454.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-454.svc.cluster.local jessie_udp@dns-test-service.dns-454.svc.cluster.local jessie_tcp@dns-test-service.dns-454.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-454.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-454.svc.cluster.local]

Dec 24 02:31:56.171: INFO: Unable to read wheezy_udp@dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:31:56.174: INFO: Unable to read wheezy_tcp@dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:31:56.178: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:31:56.180: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:31:56.201: INFO: Unable to read jessie_udp@dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:31:56.204: INFO: Unable to read jessie_tcp@dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:31:56.207: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:31:56.210: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:31:56.233: INFO: Lookups using dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539 failed for: [wheezy_udp@dns-test-service.dns-454.svc.cluster.local wheezy_tcp@dns-test-service.dns-454.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-454.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-454.svc.cluster.local jessie_udp@dns-test-service.dns-454.svc.cluster.local jessie_tcp@dns-test-service.dns-454.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-454.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-454.svc.cluster.local]

Dec 24 02:32:01.171: INFO: Unable to read wheezy_udp@dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:32:01.174: INFO: Unable to read wheezy_tcp@dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:32:01.177: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:32:01.180: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:32:01.205: INFO: Unable to read jessie_udp@dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:32:01.208: INFO: Unable to read jessie_tcp@dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:32:01.210: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:32:01.213: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:32:01.229: INFO: Lookups using dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539 failed for: [wheezy_udp@dns-test-service.dns-454.svc.cluster.local wheezy_tcp@dns-test-service.dns-454.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-454.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-454.svc.cluster.local jessie_udp@dns-test-service.dns-454.svc.cluster.local jessie_tcp@dns-test-service.dns-454.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-454.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-454.svc.cluster.local]

Dec 24 02:32:06.170: INFO: Unable to read wheezy_udp@dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:32:06.174: INFO: Unable to read wheezy_tcp@dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:32:06.177: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:32:06.180: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:32:06.222: INFO: Unable to read jessie_udp@dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:32:06.224: INFO: Unable to read jessie_tcp@dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:32:06.227: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:32:06.230: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:32:06.246: INFO: Lookups using dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539 failed for: [wheezy_udp@dns-test-service.dns-454.svc.cluster.local wheezy_tcp@dns-test-service.dns-454.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-454.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-454.svc.cluster.local jessie_udp@dns-test-service.dns-454.svc.cluster.local jessie_tcp@dns-test-service.dns-454.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-454.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-454.svc.cluster.local]

Dec 24 02:32:11.171: INFO: Unable to read wheezy_udp@dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:32:11.174: INFO: Unable to read wheezy_tcp@dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:32:11.177: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:32:11.180: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:32:11.205: INFO: Unable to read jessie_udp@dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:32:11.208: INFO: Unable to read jessie_tcp@dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:32:11.211: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:32:11.214: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-454.svc.cluster.local from pod dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539: the server could not find the requested resource (get pods dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539)
Dec 24 02:32:11.230: INFO: Lookups using dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539 failed for: [wheezy_udp@dns-test-service.dns-454.svc.cluster.local wheezy_tcp@dns-test-service.dns-454.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-454.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-454.svc.cluster.local jessie_udp@dns-test-service.dns-454.svc.cluster.local jessie_tcp@dns-test-service.dns-454.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-454.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-454.svc.cluster.local]

Dec 24 02:32:16.236: INFO: DNS probes using dns-454/dns-test-2e940d91-55ab-41c3-a8cc-b50be7820539 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:32:16.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-454" for this suite.

• [SLOW TEST:34.473 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":305,"completed":33,"skipped":487,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:32:16.400: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-efbcd5ab-3415-4592-a002-3bbe31eb9b9f
STEP: Creating a pod to test consume secrets
Dec 24 02:32:16.458: INFO: Waiting up to 5m0s for pod "pod-secrets-79031ff8-abbd-4347-950d-c606fd2661bc" in namespace "secrets-4661" to be "Succeeded or Failed"
Dec 24 02:32:16.462: INFO: Pod "pod-secrets-79031ff8-abbd-4347-950d-c606fd2661bc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031523ms
Dec 24 02:32:18.464: INFO: Pod "pod-secrets-79031ff8-abbd-4347-950d-c606fd2661bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006790541s
STEP: Saw pod success
Dec 24 02:32:18.464: INFO: Pod "pod-secrets-79031ff8-abbd-4347-950d-c606fd2661bc" satisfied condition "Succeeded or Failed"
Dec 24 02:32:18.467: INFO: Trying to get logs from node c1-4 pod pod-secrets-79031ff8-abbd-4347-950d-c606fd2661bc container secret-volume-test: <nil>
STEP: delete the pod
Dec 24 02:32:18.480: INFO: Waiting for pod pod-secrets-79031ff8-abbd-4347-950d-c606fd2661bc to disappear
Dec 24 02:32:18.498: INFO: Pod pod-secrets-79031ff8-abbd-4347-950d-c606fd2661bc no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:32:18.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4661" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":34,"skipped":491,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:32:18.506: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 24 02:32:18.575: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9314e4c4-db8b-426e-a4df-66a298ba5052" in namespace "projected-7419" to be "Succeeded or Failed"
Dec 24 02:32:18.582: INFO: Pod "downwardapi-volume-9314e4c4-db8b-426e-a4df-66a298ba5052": Phase="Pending", Reason="", readiness=false. Elapsed: 7.802515ms
Dec 24 02:32:20.585: INFO: Pod "downwardapi-volume-9314e4c4-db8b-426e-a4df-66a298ba5052": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010764233s
Dec 24 02:32:22.589: INFO: Pod "downwardapi-volume-9314e4c4-db8b-426e-a4df-66a298ba5052": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014134154s
STEP: Saw pod success
Dec 24 02:32:22.589: INFO: Pod "downwardapi-volume-9314e4c4-db8b-426e-a4df-66a298ba5052" satisfied condition "Succeeded or Failed"
Dec 24 02:32:22.591: INFO: Trying to get logs from node c1-4 pod downwardapi-volume-9314e4c4-db8b-426e-a4df-66a298ba5052 container client-container: <nil>
STEP: delete the pod
Dec 24 02:32:22.625: INFO: Waiting for pod downwardapi-volume-9314e4c4-db8b-426e-a4df-66a298ba5052 to disappear
Dec 24 02:32:22.628: INFO: Pod downwardapi-volume-9314e4c4-db8b-426e-a4df-66a298ba5052 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:32:22.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7419" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":35,"skipped":510,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:32:22.636: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:32:22.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3077" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":36,"skipped":528,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:32:22.712: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl label
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1333
STEP: creating the pod
Dec 24 02:32:22.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 create -f - --namespace=kubectl-708'
Dec 24 02:32:23.203: INFO: stderr: ""
Dec 24 02:32:23.203: INFO: stdout: "pod/pause created\n"
Dec 24 02:32:23.203: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec 24 02:32:23.203: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-708" to be "running and ready"
Dec 24 02:32:23.225: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 21.186814ms
Dec 24 02:32:25.228: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.024074409s
Dec 24 02:32:25.228: INFO: Pod "pause" satisfied condition "running and ready"
Dec 24 02:32:25.228: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: adding the label testing-label with value testing-label-value to a pod
Dec 24 02:32:25.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 label pods pause testing-label=testing-label-value --namespace=kubectl-708'
Dec 24 02:32:25.335: INFO: stderr: ""
Dec 24 02:32:25.335: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Dec 24 02:32:25.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pod pause -L testing-label --namespace=kubectl-708'
Dec 24 02:32:25.416: INFO: stderr: ""
Dec 24 02:32:25.416: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Dec 24 02:32:25.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 label pods pause testing-label- --namespace=kubectl-708'
Dec 24 02:32:25.506: INFO: stderr: ""
Dec 24 02:32:25.506: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Dec 24 02:32:25.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pod pause -L testing-label --namespace=kubectl-708'
Dec 24 02:32:25.585: INFO: stderr: ""
Dec 24 02:32:25.585: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1340
STEP: using delete to clean up resources
Dec 24 02:32:25.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 delete --grace-period=0 --force -f - --namespace=kubectl-708'
Dec 24 02:32:25.676: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 24 02:32:25.676: INFO: stdout: "pod \"pause\" force deleted\n"
Dec 24 02:32:25.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get rc,svc -l name=pause --no-headers --namespace=kubectl-708'
Dec 24 02:32:25.759: INFO: stderr: "No resources found in kubectl-708 namespace.\n"
Dec 24 02:32:25.759: INFO: stdout: ""
Dec 24 02:32:25.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods -l name=pause --namespace=kubectl-708 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 24 02:32:25.840: INFO: stderr: ""
Dec 24 02:32:25.840: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:32:25.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-708" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":305,"completed":37,"skipped":533,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:32:25.859: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl logs
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1415
STEP: creating an pod
Dec 24 02:32:25.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.20 --namespace=kubectl-7652 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Dec 24 02:32:25.995: INFO: stderr: ""
Dec 24 02:32:25.995: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Waiting for log generator to start.
Dec 24 02:32:25.995: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Dec 24 02:32:25.995: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-7652" to be "running and ready, or succeeded"
Dec 24 02:32:26.013: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 17.935786ms
Dec 24 02:32:28.015: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.020250402s
Dec 24 02:32:28.015: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Dec 24 02:32:28.015: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Dec 24 02:32:28.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 logs logs-generator logs-generator --namespace=kubectl-7652'
Dec 24 02:32:28.131: INFO: stderr: ""
Dec 24 02:32:28.131: INFO: stdout: "I1224 02:32:27.189157       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/csb8 407\nI1224 02:32:27.389252       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/wzfv 243\nI1224 02:32:27.589280       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/4xs 276\nI1224 02:32:27.789351       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/dkkv 536\nI1224 02:32:27.989276       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/dpr 424\n"
STEP: limiting log lines
Dec 24 02:32:28.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 logs logs-generator logs-generator --namespace=kubectl-7652 --tail=1'
Dec 24 02:32:28.238: INFO: stderr: ""
Dec 24 02:32:28.238: INFO: stdout: "I1224 02:32:28.189175       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/68n5 251\n"
Dec 24 02:32:28.238: INFO: got output "I1224 02:32:28.189175       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/68n5 251\n"
STEP: limiting log bytes
Dec 24 02:32:28.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 logs logs-generator logs-generator --namespace=kubectl-7652 --limit-bytes=1'
Dec 24 02:32:28.359: INFO: stderr: ""
Dec 24 02:32:28.359: INFO: stdout: "I"
Dec 24 02:32:28.359: INFO: got output "I"
STEP: exposing timestamps
Dec 24 02:32:28.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 logs logs-generator logs-generator --namespace=kubectl-7652 --tail=1 --timestamps'
Dec 24 02:32:28.477: INFO: stderr: ""
Dec 24 02:32:28.477: INFO: stdout: "2020-12-24T11:32:28.389343563+09:00 I1224 02:32:28.389302       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/4tf 442\n"
Dec 24 02:32:28.477: INFO: got output "2020-12-24T11:32:28.389343563+09:00 I1224 02:32:28.389302       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/4tf 442\n"
STEP: restricting to a time range
Dec 24 02:32:30.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 logs logs-generator logs-generator --namespace=kubectl-7652 --since=1s'
Dec 24 02:32:31.115: INFO: stderr: ""
Dec 24 02:32:31.115: INFO: stdout: "I1224 02:32:30.189265       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/4xw 571\nI1224 02:32:30.389332       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/ppfx 486\nI1224 02:32:30.589184       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/76m7 538\nI1224 02:32:30.789340       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/tggx 554\nI1224 02:32:30.989303       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/pqs 540\n"
Dec 24 02:32:31.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 logs logs-generator logs-generator --namespace=kubectl-7652 --since=24h'
Dec 24 02:32:31.231: INFO: stderr: ""
Dec 24 02:32:31.231: INFO: stdout: "I1224 02:32:27.189157       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/csb8 407\nI1224 02:32:27.389252       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/wzfv 243\nI1224 02:32:27.589280       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/4xs 276\nI1224 02:32:27.789351       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/dkkv 536\nI1224 02:32:27.989276       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/dpr 424\nI1224 02:32:28.189175       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/68n5 251\nI1224 02:32:28.389302       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/4tf 442\nI1224 02:32:28.589349       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/mc4 306\nI1224 02:32:28.789172       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/jdmd 463\nI1224 02:32:28.989318       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/n4b 268\nI1224 02:32:29.189329       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/qq4 584\nI1224 02:32:29.389202       1 logs_generator.go:76] 11 GET /api/v1/namespaces/default/pods/szl 270\nI1224 02:32:29.589345       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/fx4d 217\nI1224 02:32:29.789282       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/zjqq 412\nI1224 02:32:29.989283       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/j59l 472\nI1224 02:32:30.189265       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/4xw 571\nI1224 02:32:30.389332       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/ppfx 486\nI1224 02:32:30.589184       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/76m7 538\nI1224 02:32:30.789340       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/tggx 554\nI1224 02:32:30.989303       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/pqs 540\nI1224 02:32:31.189341       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/jgm 328\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
Dec 24 02:32:31.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 delete pod logs-generator --namespace=kubectl-7652'
Dec 24 02:32:33.674: INFO: stderr: ""
Dec 24 02:32:33.674: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:32:33.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7652" for this suite.

• [SLOW TEST:7.823 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1411
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":305,"completed":38,"skipped":533,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:32:33.682: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 02:32:33.728: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-93d682ec-18f7-4618-882d-7578c3b562b7" in namespace "security-context-test-8354" to be "Succeeded or Failed"
Dec 24 02:32:33.745: INFO: Pod "busybox-readonly-false-93d682ec-18f7-4618-882d-7578c3b562b7": Phase="Pending", Reason="", readiness=false. Elapsed: 16.934257ms
Dec 24 02:32:35.762: INFO: Pod "busybox-readonly-false-93d682ec-18f7-4618-882d-7578c3b562b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033602177s
Dec 24 02:32:35.762: INFO: Pod "busybox-readonly-false-93d682ec-18f7-4618-882d-7578c3b562b7" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:32:35.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8354" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":305,"completed":39,"skipped":563,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:32:35.769: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Starting the proxy
Dec 24 02:32:35.804: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-303221030 proxy --unix-socket=/tmp/kubectl-proxy-unix255901649/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:32:35.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4502" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":305,"completed":40,"skipped":570,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:32:35.882: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 24 02:32:36.471: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 24 02:32:39.505: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 02:32:39.508: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9949-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:32:40.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3543" for this suite.
STEP: Destroying namespace "webhook-3543-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":305,"completed":41,"skipped":578,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] server version
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:32:40.717: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Request ServerVersion
STEP: Confirm major version
Dec 24 02:32:40.765: INFO: Major version: 1
STEP: Confirm minor version
Dec 24 02:32:40.765: INFO: cleanMinorVersion: 19
Dec 24 02:32:40.765: INFO: Minor version: 19
[AfterEach] [sig-api-machinery] server version
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:32:40.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-3613" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":305,"completed":42,"skipped":612,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:32:40.784: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Dec 24 02:32:40.815: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the sample API server.
Dec 24 02:32:41.414: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Dec 24 02:32:43.477: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744373961, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744373961, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744373961, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744373961, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 24 02:32:46.806: INFO: Waited 1.321171236s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:32:47.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-5275" for this suite.

• [SLOW TEST:7.278 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":305,"completed":43,"skipped":621,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:32:48.062: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Dec 24 02:32:48.131: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-467 /api/v1/namespaces/watch-467/configmaps/e2e-watch-test-resource-version 77a3c16b-4503-4cff-9c40-1a4e887ad179 3435257 0 2020-12-24 02:32:48 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-12-24 02:32:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 24 02:32:48.131: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-467 /api/v1/namespaces/watch-467/configmaps/e2e-watch-test-resource-version 77a3c16b-4503-4cff-9c40-1a4e887ad179 3435258 0 2020-12-24 02:32:48 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-12-24 02:32:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:32:48.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-467" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":305,"completed":44,"skipped":645,"failed":0}

------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:32:48.138: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name projected-secret-test-127a30c4-30a7-4e7a-bfc2-71cb03d48ff7
STEP: Creating a pod to test consume secrets
Dec 24 02:32:48.181: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-afe65ce0-3b05-4ede-ac69-e09ea56fd9bd" in namespace "projected-9548" to be "Succeeded or Failed"
Dec 24 02:32:48.197: INFO: Pod "pod-projected-secrets-afe65ce0-3b05-4ede-ac69-e09ea56fd9bd": Phase="Pending", Reason="", readiness=false. Elapsed: 15.671619ms
Dec 24 02:32:50.200: INFO: Pod "pod-projected-secrets-afe65ce0-3b05-4ede-ac69-e09ea56fd9bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018781104s
STEP: Saw pod success
Dec 24 02:32:50.200: INFO: Pod "pod-projected-secrets-afe65ce0-3b05-4ede-ac69-e09ea56fd9bd" satisfied condition "Succeeded or Failed"
Dec 24 02:32:50.202: INFO: Trying to get logs from node c1-4 pod pod-projected-secrets-afe65ce0-3b05-4ede-ac69-e09ea56fd9bd container secret-volume-test: <nil>
STEP: delete the pod
Dec 24 02:32:50.252: INFO: Waiting for pod pod-projected-secrets-afe65ce0-3b05-4ede-ac69-e09ea56fd9bd to disappear
Dec 24 02:32:50.260: INFO: Pod pod-projected-secrets-afe65ce0-3b05-4ede-ac69-e09ea56fd9bd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:32:50.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9548" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":45,"skipped":645,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:32:50.267: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 02:32:50.302: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:32:56.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2153" for this suite.

• [SLOW TEST:6.428 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":305,"completed":46,"skipped":651,"failed":0}
SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:32:56.695: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec 24 02:33:00.809: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 24 02:33:00.821: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 24 02:33:02.821: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 24 02:33:02.824: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 24 02:33:04.821: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 24 02:33:04.824: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 24 02:33:06.821: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 24 02:33:06.824: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 24 02:33:08.821: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 24 02:33:08.824: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 24 02:33:10.821: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 24 02:33:10.824: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 24 02:33:12.821: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 24 02:33:12.824: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 24 02:33:14.821: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 24 02:33:14.824: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:33:14.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2993" for this suite.

• [SLOW TEST:18.156 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":305,"completed":47,"skipped":657,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:33:14.852: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 02:33:14.892: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec 24 02:33:14.912: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 24 02:33:19.915: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 24 02:33:19.915: INFO: Creating deployment "test-rolling-update-deployment"
Dec 24 02:33:19.918: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec 24 02:33:19.927: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Dec 24 02:33:21.941: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec 24 02:33:21.949: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Dec 24 02:33:21.955: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-9488 /apis/apps/v1/namespaces/deployment-9488/deployments/test-rolling-update-deployment 933e85bf-7292-4182-b8f5-26c4158aee55 3435696 1 2020-12-24 02:33:19 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2020-12-24 02:33:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-12-24 02:33:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0017c9308 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-12-24 02:33:19 +0000 UTC,LastTransitionTime:2020-12-24 02:33:19 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" has successfully progressed.,LastUpdateTime:2020-12-24 02:33:21 +0000 UTC,LastTransitionTime:2020-12-24 02:33:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 24 02:33:21.957: INFO: New ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9  deployment-9488 /apis/apps/v1/namespaces/deployment-9488/replicasets/test-rolling-update-deployment-c4cb8d6d9 eb93c421-80b5-429d-bf2b-e41b40ecda8b 3435684 1 2020-12-24 02:33:19 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 933e85bf-7292-4182-b8f5-26c4158aee55 0xc0017c98a0 0xc0017c98a1}] []  [{kube-controller-manager Update apps/v1 2020-12-24 02:33:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"933e85bf-7292-4182-b8f5-26c4158aee55\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: c4cb8d6d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0017c9918 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 24 02:33:21.957: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec 24 02:33:21.957: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-9488 /apis/apps/v1/namespaces/deployment-9488/replicasets/test-rolling-update-controller 08d5c811-c90b-4bae-9eae-a1f435e6a60e 3435695 2 2020-12-24 02:33:14 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 933e85bf-7292-4182-b8f5-26c4158aee55 0xc0017c9797 0xc0017c9798}] []  [{e2e.test Update apps/v1 2020-12-24 02:33:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-12-24 02:33:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"933e85bf-7292-4182-b8f5-26c4158aee55\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0017c9838 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 24 02:33:21.960: INFO: Pod "test-rolling-update-deployment-c4cb8d6d9-lrfps" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9-lrfps test-rolling-update-deployment-c4cb8d6d9- deployment-9488 /api/v1/namespaces/deployment-9488/pods/test-rolling-update-deployment-c4cb8d6d9-lrfps 5d7fad4f-d561-4523-977e-090d6308f702 3435683 0 2020-12-24 02:33:19 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[cni.projectcalico.org/podIP:10.244.113.102/32 cni.projectcalico.org/podIPs:10.244.113.102/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-c4cb8d6d9 eb93c421-80b5-429d-bf2b-e41b40ecda8b 0xc0017c9df0 0xc0017c9df1}] []  [{kube-controller-manager Update v1 2020-12-24 02:33:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eb93c421-80b5-429d-bf2b-e41b40ecda8b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-24 02:33:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-24 02:33:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.113.102\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-s6b6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-s6b6w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-s6b6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 02:33:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 02:33:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 02:33:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 02:33:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.3.5,PodIP:10.244.113.102,StartTime:2020-12-24 02:33:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-24 02:33:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:cri-o://af613a0794a3a3e6a431179bf97099ee9c5bb3d00db89f5137e00ca88ee083dd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.113.102,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:33:21.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9488" for this suite.

• [SLOW TEST:7.114 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":48,"skipped":658,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:33:21.967: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting the proxy server
Dec 24 02:33:22.008: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-303221030 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:33:22.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3795" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":305,"completed":49,"skipped":680,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:33:22.095: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 24 02:33:22.155: INFO: Waiting up to 5m0s for pod "downwardapi-volume-20d07915-c7c5-4845-9db4-2b1acd0b1862" in namespace "downward-api-8424" to be "Succeeded or Failed"
Dec 24 02:33:22.160: INFO: Pod "downwardapi-volume-20d07915-c7c5-4845-9db4-2b1acd0b1862": Phase="Pending", Reason="", readiness=false. Elapsed: 5.284205ms
Dec 24 02:33:24.163: INFO: Pod "downwardapi-volume-20d07915-c7c5-4845-9db4-2b1acd0b1862": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008100143s
STEP: Saw pod success
Dec 24 02:33:24.163: INFO: Pod "downwardapi-volume-20d07915-c7c5-4845-9db4-2b1acd0b1862" satisfied condition "Succeeded or Failed"
Dec 24 02:33:24.165: INFO: Trying to get logs from node c1-4 pod downwardapi-volume-20d07915-c7c5-4845-9db4-2b1acd0b1862 container client-container: <nil>
STEP: delete the pod
Dec 24 02:33:24.183: INFO: Waiting for pod downwardapi-volume-20d07915-c7c5-4845-9db4-2b1acd0b1862 to disappear
Dec 24 02:33:24.189: INFO: Pod downwardapi-volume-20d07915-c7c5-4845-9db4-2b1acd0b1862 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:33:24.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8424" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":50,"skipped":726,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:33:24.196: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Dec 24 02:33:24.248: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 24 02:33:24.257: INFO: Waiting for terminating namespaces to be deleted...
Dec 24 02:33:24.259: INFO: 
Logging pods the apiserver thinks is on node c1-4 before test
Dec 24 02:33:24.273: INFO: pod-handle-http-request from container-lifecycle-hook-2993 started at 2020-12-24 02:32:56 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.273: INFO: 	Container pod-handle-http-request ready: false, restart count 0
Dec 24 02:33:24.273: INFO: test-rolling-update-deployment-c4cb8d6d9-lrfps from deployment-9488 started at 2020-12-24 02:33:20 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.273: INFO: 	Container agnhost ready: true, restart count 0
Dec 24 02:33:24.273: INFO: calico-node-gf9xk from kube-system started at 2020-12-17 09:20:20 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.273: INFO: 	Container calico-node ready: true, restart count 0
Dec 24 02:33:24.273: INFO: kube-proxy-nnl7j from kube-system started at 2020-12-17 09:18:43 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.273: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 24 02:33:24.273: INFO: virt-handler-5nkqw from kubevirt started at 2020-12-23 10:54:33 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.273: INFO: 	Container virt-handler ready: false, restart count 0
Dec 24 02:33:24.273: INFO: csi-cephfsplugin-ghqns from rook-ceph started at 2020-12-23 10:54:33 +0000 UTC (3 container statuses recorded)
Dec 24 02:33:24.273: INFO: 	Container csi-cephfsplugin ready: false, restart count 0
Dec 24 02:33:24.273: INFO: 	Container driver-registrar ready: false, restart count 0
Dec 24 02:33:24.273: INFO: 	Container liveness-prometheus ready: false, restart count 0
Dec 24 02:33:24.273: INFO: csi-rbdplugin-z74d4 from rook-ceph started at 2020-12-23 10:54:33 +0000 UTC (3 container statuses recorded)
Dec 24 02:33:24.273: INFO: 	Container csi-rbdplugin ready: false, restart count 0
Dec 24 02:33:24.273: INFO: 	Container driver-registrar ready: false, restart count 0
Dec 24 02:33:24.273: INFO: 	Container liveness-prometheus ready: false, restart count 0
Dec 24 02:33:24.273: INFO: rook-ceph-crashcollector-c1-4-b5fd6cbf-9lskd from rook-ceph started at 2020-12-24 02:26:47 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.273: INFO: 	Container ceph-crash ready: true, restart count 0
Dec 24 02:33:24.273: INFO: rook-ceph-crashcollector-c1-4-b5fd6cbf-r86bh from rook-ceph started at 2020-12-23 10:29:44 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.273: INFO: 	Container ceph-crash ready: false, restart count 0
Dec 24 02:33:24.273: INFO: rook-ceph-mon-b-6c84dd66-jsvmm from rook-ceph started at 2020-12-24 02:26:47 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.273: INFO: 	Container mon ready: true, restart count 0
Dec 24 02:33:24.273: INFO: rook-ceph-mon-b-6c84dd66-xj8wb from rook-ceph started at 2020-12-23 10:29:43 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.273: INFO: 	Container mon ready: false, restart count 0
Dec 24 02:33:24.273: INFO: rook-ceph-osd-1-59b659d744-k6c5s from rook-ceph started at 2020-12-23 10:29:43 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.273: INFO: 	Container osd ready: false, restart count 0
Dec 24 02:33:24.273: INFO: rook-ceph-osd-1-59b659d744-rls27 from rook-ceph started at 2020-12-24 02:26:47 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.273: INFO: 	Container osd ready: true, restart count 0
Dec 24 02:33:24.273: INFO: rook-discover-svzsw from rook-ceph started at 2020-12-23 10:54:33 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.273: INFO: 	Container rook-discover ready: false, restart count 0
Dec 24 02:33:24.273: INFO: sonobuoy from sonobuoy started at 2020-12-24 02:17:41 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.273: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 24 02:33:24.273: INFO: sonobuoy-e2e-job-683f472080954d4f from sonobuoy started at 2020-12-24 02:17:43 +0000 UTC (2 container statuses recorded)
Dec 24 02:33:24.273: INFO: 	Container e2e ready: true, restart count 0
Dec 24 02:33:24.273: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 24 02:33:24.273: INFO: sonobuoy-systemd-logs-daemon-set-eb313291d8024436-cqjvw from sonobuoy started at 2020-12-24 02:17:43 +0000 UTC (2 container statuses recorded)
Dec 24 02:33:24.273: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 24 02:33:24.273: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 24 02:33:24.273: INFO: 
Logging pods the apiserver thinks is on node c1-5 before test
Dec 24 02:33:24.287: INFO: cdi-apiserver-79dbd664dd-jvbpv from cdi started at 2020-12-23 09:46:37 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.287: INFO: 	Container cdi-apiserver ready: true, restart count 0
Dec 24 02:33:24.287: INFO: snapshot-controller-0 from default started at 2020-12-18 04:06:22 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.287: INFO: 	Container snapshot-controller ready: true, restart count 0
Dec 24 02:33:24.287: INFO: calico-node-tt5dp from kube-system started at 2020-12-17 09:20:20 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.287: INFO: 	Container calico-node ready: true, restart count 0
Dec 24 02:33:24.287: INFO: kube-proxy-2lcn2 from kube-system started at 2020-12-17 09:18:32 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.287: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 24 02:33:24.287: INFO: virt-api-76b44ffc65-kcmtd from kubevirt started at 2020-12-23 09:46:37 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.287: INFO: 	Container virt-api ready: true, restart count 0
Dec 24 02:33:24.287: INFO: virt-controller-84b5d4b779-lbv8p from kubevirt started at 2020-12-23 09:46:37 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.287: INFO: 	Container virt-controller ready: true, restart count 0
Dec 24 02:33:24.287: INFO: virt-handler-scjdh from kubevirt started at 2020-12-17 09:24:03 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.287: INFO: 	Container virt-handler ready: true, restart count 7
Dec 24 02:33:24.287: INFO: virt-operator-867fcd786d-hqk4k from kubevirt started at 2020-12-23 09:46:37 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.287: INFO: 	Container virt-operator ready: true, restart count 0
Dec 24 02:33:24.287: INFO: csi-cephfsplugin-provisioner-d878cdf8b-6g8td from rook-ceph started at 2020-12-18 04:06:36 +0000 UTC (6 container statuses recorded)
Dec 24 02:33:24.287: INFO: 	Container csi-attacher ready: true, restart count 1
Dec 24 02:33:24.287: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 24 02:33:24.287: INFO: 	Container csi-provisioner ready: true, restart count 1
Dec 24 02:33:24.287: INFO: 	Container csi-resizer ready: true, restart count 1
Dec 24 02:33:24.287: INFO: 	Container csi-snapshotter ready: true, restart count 1
Dec 24 02:33:24.287: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 02:33:24.287: INFO: csi-cephfsplugin-xd5rd from rook-ceph started at 2020-12-18 04:06:35 +0000 UTC (3 container statuses recorded)
Dec 24 02:33:24.287: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 24 02:33:24.287: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 24 02:33:24.287: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 02:33:24.287: INFO: csi-rbdplugin-drpwm from rook-ceph started at 2020-12-18 04:06:35 +0000 UTC (3 container statuses recorded)
Dec 24 02:33:24.287: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 24 02:33:24.287: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 24 02:33:24.287: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 02:33:24.287: INFO: csi-rbdplugin-provisioner-7588bc4-dxkjr from rook-ceph started at 2020-12-18 04:06:35 +0000 UTC (6 container statuses recorded)
Dec 24 02:33:24.287: INFO: 	Container csi-attacher ready: true, restart count 1
Dec 24 02:33:24.287: INFO: 	Container csi-provisioner ready: true, restart count 2
Dec 24 02:33:24.287: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 24 02:33:24.287: INFO: 	Container csi-resizer ready: true, restart count 1
Dec 24 02:33:24.287: INFO: 	Container csi-snapshotter ready: true, restart count 1
Dec 24 02:33:24.287: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 02:33:24.287: INFO: rook-ceph-crashcollector-c1-5-7fdd9898c6-mm2r6 from rook-ceph started at 2020-12-18 04:07:17 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.287: INFO: 	Container ceph-crash ready: true, restart count 0
Dec 24 02:33:24.287: INFO: rook-ceph-mon-c-bcf7d945c-mvssc from rook-ceph started at 2020-12-18 04:07:17 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.287: INFO: 	Container mon ready: true, restart count 0
Dec 24 02:33:24.287: INFO: rook-ceph-operator-775d4b6c5f-khccc from rook-ceph started at 2020-12-18 04:06:25 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.287: INFO: 	Container rook-ceph-operator ready: true, restart count 0
Dec 24 02:33:24.287: INFO: rook-ceph-osd-2-5478c54cc5-s282q from rook-ceph started at 2020-12-18 04:07:41 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.287: INFO: 	Container osd ready: true, restart count 0
Dec 24 02:33:24.287: INFO: rook-ceph-osd-prepare-c1-5-l2wd8 from rook-ceph started at 2020-12-18 05:07:16 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.287: INFO: 	Container provision ready: false, restart count 0
Dec 24 02:33:24.287: INFO: rook-ceph-tools-6967fc698d-n2nzp from rook-ceph started at 2020-12-23 09:46:37 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.287: INFO: 	Container rook-ceph-tools ready: true, restart count 0
Dec 24 02:33:24.287: INFO: rook-discover-ds9s8 from rook-ceph started at 2020-12-18 04:06:28 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.287: INFO: 	Container rook-discover ready: true, restart count 0
Dec 24 02:33:24.288: INFO: sonobuoy-systemd-logs-daemon-set-eb313291d8024436-82n6r from sonobuoy started at 2020-12-24 02:17:43 +0000 UTC (2 container statuses recorded)
Dec 24 02:33:24.288: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 24 02:33:24.288: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 24 02:33:24.288: INFO: 
Logging pods the apiserver thinks is on node c1-6 before test
Dec 24 02:33:24.302: INFO: cdi-deployment-6c8975f4b-6bk46 from cdi started at 2020-12-21 09:29:47 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.302: INFO: 	Container cdi-controller ready: true, restart count 1
Dec 24 02:33:24.302: INFO: cdi-operator-5d5654db65-5hdrv from cdi started at 2020-12-21 09:29:21 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.302: INFO: 	Container cdi-operator ready: true, restart count 2
Dec 24 02:33:24.302: INFO: cdi-uploadproxy-5765998fff-qh2cb from cdi started at 2020-12-23 09:46:37 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.302: INFO: 	Container cdi-uploadproxy ready: true, restart count 0
Dec 24 02:33:24.302: INFO: calico-node-5rrck from kube-system started at 2020-12-17 09:20:20 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.302: INFO: 	Container calico-node ready: true, restart count 0
Dec 24 02:33:24.302: INFO: kube-proxy-jmmvs from kube-system started at 2020-12-17 09:19:08 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.302: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 24 02:33:24.302: INFO: virt-api-76b44ffc65-b4xmf from kubevirt started at 2020-12-17 09:23:40 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.302: INFO: 	Container virt-api ready: true, restart count 0
Dec 24 02:33:24.302: INFO: virt-controller-84b5d4b779-krqk6 from kubevirt started at 2020-12-17 09:24:03 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.302: INFO: 	Container virt-controller ready: true, restart count 5
Dec 24 02:33:24.302: INFO: virt-handler-jk5bd from kubevirt started at 2020-12-17 09:24:03 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.302: INFO: 	Container virt-handler ready: true, restart count 0
Dec 24 02:33:24.302: INFO: virt-operator-867fcd786d-9xnf6 from kubevirt started at 2020-12-17 09:23:28 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.302: INFO: 	Container virt-operator ready: true, restart count 2
Dec 24 02:33:24.302: INFO: csi-cephfsplugin-provisioner-d878cdf8b-9v92z from rook-ceph started at 2020-12-23 09:46:37 +0000 UTC (6 container statuses recorded)
Dec 24 02:33:24.302: INFO: 	Container csi-attacher ready: true, restart count 0
Dec 24 02:33:24.302: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 24 02:33:24.302: INFO: 	Container csi-provisioner ready: true, restart count 0
Dec 24 02:33:24.302: INFO: 	Container csi-resizer ready: true, restart count 0
Dec 24 02:33:24.302: INFO: 	Container csi-snapshotter ready: true, restart count 0
Dec 24 02:33:24.302: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 02:33:24.302: INFO: csi-cephfsplugin-q5hm7 from rook-ceph started at 2020-12-18 04:06:35 +0000 UTC (3 container statuses recorded)
Dec 24 02:33:24.302: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 24 02:33:24.302: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 24 02:33:24.302: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 02:33:24.302: INFO: csi-rbdplugin-provisioner-7588bc4-r297x from rook-ceph started at 2020-12-18 04:06:35 +0000 UTC (6 container statuses recorded)
Dec 24 02:33:24.302: INFO: 	Container csi-attacher ready: true, restart count 0
Dec 24 02:33:24.302: INFO: 	Container csi-provisioner ready: true, restart count 0
Dec 24 02:33:24.302: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 24 02:33:24.302: INFO: 	Container csi-resizer ready: true, restart count 0
Dec 24 02:33:24.302: INFO: 	Container csi-snapshotter ready: true, restart count 0
Dec 24 02:33:24.302: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 02:33:24.302: INFO: csi-rbdplugin-xfdqp from rook-ceph started at 2020-12-18 04:06:35 +0000 UTC (3 container statuses recorded)
Dec 24 02:33:24.302: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 24 02:33:24.302: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 24 02:33:24.302: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 02:33:24.302: INFO: rook-ceph-crashcollector-c1-6-5868d4fd57-6c5gp from rook-ceph started at 2020-12-18 04:08:07 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.302: INFO: 	Container ceph-crash ready: true, restart count 0
Dec 24 02:33:24.302: INFO: rook-ceph-mds-myfs-a-59cfdfc57f-zb7rk from rook-ceph started at 2020-12-23 09:46:37 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.302: INFO: 	Container mds ready: true, restart count 0
Dec 24 02:33:24.302: INFO: rook-ceph-mds-myfs-b-6d859f5b5c-r2rqz from rook-ceph started at 2020-12-18 04:08:07 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.302: INFO: 	Container mds ready: true, restart count 0
Dec 24 02:33:24.302: INFO: rook-ceph-mgr-a-8b957cf9b-9nkkq from rook-ceph started at 2020-12-18 04:07:30 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.302: INFO: 	Container mgr ready: true, restart count 0
Dec 24 02:33:24.302: INFO: rook-ceph-mon-a-77f88c5ff8-hlrmz from rook-ceph started at 2020-12-18 04:06:59 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.302: INFO: 	Container mon ready: true, restart count 0
Dec 24 02:33:24.302: INFO: rook-ceph-osd-0-5897494d6d-k6ml7 from rook-ceph started at 2020-12-18 04:07:39 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.302: INFO: 	Container osd ready: true, restart count 0
Dec 24 02:33:24.302: INFO: rook-ceph-osd-prepare-c1-6-jmhbg from rook-ceph started at 2020-12-18 05:07:18 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.302: INFO: 	Container provision ready: false, restart count 0
Dec 24 02:33:24.302: INFO: rook-discover-4kd85 from rook-ceph started at 2020-12-18 04:06:28 +0000 UTC (1 container statuses recorded)
Dec 24 02:33:24.302: INFO: 	Container rook-discover ready: true, restart count 0
Dec 24 02:33:24.302: INFO: sonobuoy-systemd-logs-daemon-set-eb313291d8024436-79g22 from sonobuoy started at 2020-12-24 02:17:43 +0000 UTC (2 container statuses recorded)
Dec 24 02:33:24.302: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 24 02:33:24.302: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16538657fc924893], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:33:25.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5265" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":305,"completed":51,"skipped":753,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:33:25.358: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:33:36.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1605" for this suite.

• [SLOW TEST:11.157 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":305,"completed":52,"skipped":766,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:33:36.518: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 24 02:33:37.017: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 24 02:33:39.032: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744374017, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744374017, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744374017, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744374016, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 24 02:33:42.074: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Dec 24 02:33:44.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 attach --namespace=webhook-9499 to-be-attached-pod -i -c=container1'
Dec 24 02:33:44.250: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:33:44.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9499" for this suite.
STEP: Destroying namespace "webhook-9499-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.838 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":305,"completed":53,"skipped":824,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:33:44.355: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on tmpfs
Dec 24 02:33:44.400: INFO: Waiting up to 5m0s for pod "pod-da66bcfa-c0af-4d21-b2fe-4b29b1d2c5ec" in namespace "emptydir-4478" to be "Succeeded or Failed"
Dec 24 02:33:44.402: INFO: Pod "pod-da66bcfa-c0af-4d21-b2fe-4b29b1d2c5ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.459278ms
Dec 24 02:33:46.405: INFO: Pod "pod-da66bcfa-c0af-4d21-b2fe-4b29b1d2c5ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005111825s
STEP: Saw pod success
Dec 24 02:33:46.405: INFO: Pod "pod-da66bcfa-c0af-4d21-b2fe-4b29b1d2c5ec" satisfied condition "Succeeded or Failed"
Dec 24 02:33:46.407: INFO: Trying to get logs from node c1-4 pod pod-da66bcfa-c0af-4d21-b2fe-4b29b1d2c5ec container test-container: <nil>
STEP: delete the pod
Dec 24 02:33:46.429: INFO: Waiting for pod pod-da66bcfa-c0af-4d21-b2fe-4b29b1d2c5ec to disappear
Dec 24 02:33:46.437: INFO: Pod pod-da66bcfa-c0af-4d21-b2fe-4b29b1d2c5ec no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:33:46.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4478" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":54,"skipped":826,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:33:46.445: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-6234
STEP: creating service affinity-nodeport in namespace services-6234
STEP: creating replication controller affinity-nodeport in namespace services-6234
I1224 02:33:46.542898      24 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-6234, replica count: 3
I1224 02:33:49.593120      24 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 24 02:33:49.607: INFO: Creating new exec pod
Dec 24 02:33:52.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-6234 execpod-affinitybgxlg -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Dec 24 02:33:52.847: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Dec 24 02:33:52.847: INFO: stdout: ""
Dec 24 02:33:52.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-6234 execpod-affinitybgxlg -- /bin/sh -x -c nc -zv -t -w 2 10.96.170.140 80'
Dec 24 02:33:53.017: INFO: stderr: "+ nc -zv -t -w 2 10.96.170.140 80\nConnection to 10.96.170.140 80 port [tcp/http] succeeded!\n"
Dec 24 02:33:53.017: INFO: stdout: ""
Dec 24 02:33:53.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-6234 execpod-affinitybgxlg -- /bin/sh -x -c nc -zv -t -w 2 172.21.3.7 32700'
Dec 24 02:33:53.193: INFO: stderr: "+ nc -zv -t -w 2 172.21.3.7 32700\nConnection to 172.21.3.7 32700 port [tcp/32700] succeeded!\n"
Dec 24 02:33:53.193: INFO: stdout: ""
Dec 24 02:33:53.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-6234 execpod-affinitybgxlg -- /bin/sh -x -c nc -zv -t -w 2 172.21.3.6 32700'
Dec 24 02:33:53.371: INFO: stderr: "+ nc -zv -t -w 2 172.21.3.6 32700\nConnection to 172.21.3.6 32700 port [tcp/32700] succeeded!\n"
Dec 24 02:33:53.371: INFO: stdout: ""
Dec 24 02:33:53.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-6234 execpod-affinitybgxlg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.3.5:32700/ ; done'
Dec 24 02:33:53.619: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:32700/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:32700/\n"
Dec 24 02:33:53.619: INFO: stdout: "\naffinity-nodeport-q4s26\naffinity-nodeport-q4s26\naffinity-nodeport-q4s26\naffinity-nodeport-q4s26\naffinity-nodeport-q4s26\naffinity-nodeport-q4s26\naffinity-nodeport-q4s26\naffinity-nodeport-q4s26\naffinity-nodeport-q4s26\naffinity-nodeport-q4s26\naffinity-nodeport-q4s26\naffinity-nodeport-q4s26\naffinity-nodeport-q4s26\naffinity-nodeport-q4s26\naffinity-nodeport-q4s26\naffinity-nodeport-q4s26"
Dec 24 02:33:53.619: INFO: Received response from host: affinity-nodeport-q4s26
Dec 24 02:33:53.619: INFO: Received response from host: affinity-nodeport-q4s26
Dec 24 02:33:53.619: INFO: Received response from host: affinity-nodeport-q4s26
Dec 24 02:33:53.619: INFO: Received response from host: affinity-nodeport-q4s26
Dec 24 02:33:53.619: INFO: Received response from host: affinity-nodeport-q4s26
Dec 24 02:33:53.619: INFO: Received response from host: affinity-nodeport-q4s26
Dec 24 02:33:53.619: INFO: Received response from host: affinity-nodeport-q4s26
Dec 24 02:33:53.619: INFO: Received response from host: affinity-nodeport-q4s26
Dec 24 02:33:53.619: INFO: Received response from host: affinity-nodeport-q4s26
Dec 24 02:33:53.619: INFO: Received response from host: affinity-nodeport-q4s26
Dec 24 02:33:53.619: INFO: Received response from host: affinity-nodeport-q4s26
Dec 24 02:33:53.619: INFO: Received response from host: affinity-nodeport-q4s26
Dec 24 02:33:53.619: INFO: Received response from host: affinity-nodeport-q4s26
Dec 24 02:33:53.619: INFO: Received response from host: affinity-nodeport-q4s26
Dec 24 02:33:53.619: INFO: Received response from host: affinity-nodeport-q4s26
Dec 24 02:33:53.619: INFO: Received response from host: affinity-nodeport-q4s26
Dec 24 02:33:53.620: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-6234, will wait for the garbage collector to delete the pods
Dec 24 02:33:53.699: INFO: Deleting ReplicationController affinity-nodeport took: 4.714174ms
Dec 24 02:33:53.799: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.15943ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:34:08.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6234" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:22.513 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":55,"skipped":831,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:34:08.958: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Dec 24 02:34:11.037: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8984 PodName:var-expansion-57929e0b-ee17-4ad9-ae36-5ac55f913ab5 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 24 02:34:11.037: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: test for file in mounted path
Dec 24 02:34:11.139: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8984 PodName:var-expansion-57929e0b-ee17-4ad9-ae36-5ac55f913ab5 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 24 02:34:11.139: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: updating the annotation value
Dec 24 02:34:11.735: INFO: Successfully updated pod "var-expansion-57929e0b-ee17-4ad9-ae36-5ac55f913ab5"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Dec 24 02:34:11.772: INFO: Deleting pod "var-expansion-57929e0b-ee17-4ad9-ae36-5ac55f913ab5" in namespace "var-expansion-8984"
Dec 24 02:34:11.776: INFO: Wait up to 5m0s for pod "var-expansion-57929e0b-ee17-4ad9-ae36-5ac55f913ab5" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:34:53.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8984" for this suite.

• [SLOW TEST:44.835 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":305,"completed":56,"skipped":859,"failed":0}
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:34:53.793: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:34:57.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7975" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":305,"completed":57,"skipped":859,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:34:57.876: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-3de564d1-2e89-4291-9299-0a3740340d3f
STEP: Creating configMap with name cm-test-opt-upd-be901229-4d09-43ab-b54d-11d921b5a1a0
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-3de564d1-2e89-4291-9299-0a3740340d3f
STEP: Updating configmap cm-test-opt-upd-be901229-4d09-43ab-b54d-11d921b5a1a0
STEP: Creating configMap with name cm-test-opt-create-d40c85cf-2a93-4109-8e25-03c844bb2176
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:36:26.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1101" for this suite.

• [SLOW TEST:88.455 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":58,"skipped":868,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:36:26.332: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Dec 24 02:36:26.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 create -f - --namespace=kubectl-2545'
Dec 24 02:36:28.150: INFO: stderr: ""
Dec 24 02:36:28.150: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 24 02:36:28.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2545'
Dec 24 02:36:28.256: INFO: stderr: ""
Dec 24 02:36:28.256: INFO: stdout: "update-demo-nautilus-4tkfp update-demo-nautilus-c2vl4 "
Dec 24 02:36:28.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods update-demo-nautilus-4tkfp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2545'
Dec 24 02:36:28.335: INFO: stderr: ""
Dec 24 02:36:28.335: INFO: stdout: ""
Dec 24 02:36:28.335: INFO: update-demo-nautilus-4tkfp is created but not running
Dec 24 02:36:33.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2545'
Dec 24 02:36:33.440: INFO: stderr: ""
Dec 24 02:36:33.440: INFO: stdout: "update-demo-nautilus-4tkfp update-demo-nautilus-c2vl4 "
Dec 24 02:36:33.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods update-demo-nautilus-4tkfp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2545'
Dec 24 02:36:33.533: INFO: stderr: ""
Dec 24 02:36:33.533: INFO: stdout: "true"
Dec 24 02:36:33.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods update-demo-nautilus-4tkfp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2545'
Dec 24 02:36:33.615: INFO: stderr: ""
Dec 24 02:36:33.615: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 24 02:36:33.615: INFO: validating pod update-demo-nautilus-4tkfp
Dec 24 02:36:33.619: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 24 02:36:33.619: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 24 02:36:33.619: INFO: update-demo-nautilus-4tkfp is verified up and running
Dec 24 02:36:33.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods update-demo-nautilus-c2vl4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2545'
Dec 24 02:36:33.694: INFO: stderr: ""
Dec 24 02:36:33.694: INFO: stdout: "true"
Dec 24 02:36:33.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods update-demo-nautilus-c2vl4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2545'
Dec 24 02:36:33.767: INFO: stderr: ""
Dec 24 02:36:33.767: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 24 02:36:33.768: INFO: validating pod update-demo-nautilus-c2vl4
Dec 24 02:36:33.772: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 24 02:36:33.772: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 24 02:36:33.772: INFO: update-demo-nautilus-c2vl4 is verified up and running
STEP: scaling down the replication controller
Dec 24 02:36:33.775: INFO: scanned /root for discovery docs: <nil>
Dec 24 02:36:33.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-2545'
Dec 24 02:36:34.885: INFO: stderr: ""
Dec 24 02:36:34.885: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 24 02:36:34.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2545'
Dec 24 02:36:34.992: INFO: stderr: ""
Dec 24 02:36:34.992: INFO: stdout: "update-demo-nautilus-4tkfp update-demo-nautilus-c2vl4 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec 24 02:36:39.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2545'
Dec 24 02:36:40.101: INFO: stderr: ""
Dec 24 02:36:40.101: INFO: stdout: "update-demo-nautilus-4tkfp update-demo-nautilus-c2vl4 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec 24 02:36:45.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2545'
Dec 24 02:36:45.207: INFO: stderr: ""
Dec 24 02:36:45.208: INFO: stdout: "update-demo-nautilus-c2vl4 "
Dec 24 02:36:45.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods update-demo-nautilus-c2vl4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2545'
Dec 24 02:36:45.291: INFO: stderr: ""
Dec 24 02:36:45.291: INFO: stdout: "true"
Dec 24 02:36:45.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods update-demo-nautilus-c2vl4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2545'
Dec 24 02:36:45.370: INFO: stderr: ""
Dec 24 02:36:45.370: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 24 02:36:45.370: INFO: validating pod update-demo-nautilus-c2vl4
Dec 24 02:36:45.373: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 24 02:36:45.373: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 24 02:36:45.373: INFO: update-demo-nautilus-c2vl4 is verified up and running
STEP: scaling up the replication controller
Dec 24 02:36:45.375: INFO: scanned /root for discovery docs: <nil>
Dec 24 02:36:45.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-2545'
Dec 24 02:36:46.497: INFO: stderr: ""
Dec 24 02:36:46.497: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 24 02:36:46.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2545'
Dec 24 02:36:46.589: INFO: stderr: ""
Dec 24 02:36:46.589: INFO: stdout: "update-demo-nautilus-8nfz7 update-demo-nautilus-c2vl4 "
Dec 24 02:36:46.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods update-demo-nautilus-8nfz7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2545'
Dec 24 02:36:46.669: INFO: stderr: ""
Dec 24 02:36:46.669: INFO: stdout: ""
Dec 24 02:36:46.669: INFO: update-demo-nautilus-8nfz7 is created but not running
Dec 24 02:36:51.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2545'
Dec 24 02:36:51.780: INFO: stderr: ""
Dec 24 02:36:51.780: INFO: stdout: "update-demo-nautilus-8nfz7 update-demo-nautilus-c2vl4 "
Dec 24 02:36:51.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods update-demo-nautilus-8nfz7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2545'
Dec 24 02:36:51.859: INFO: stderr: ""
Dec 24 02:36:51.859: INFO: stdout: "true"
Dec 24 02:36:51.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods update-demo-nautilus-8nfz7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2545'
Dec 24 02:36:51.938: INFO: stderr: ""
Dec 24 02:36:51.938: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 24 02:36:51.938: INFO: validating pod update-demo-nautilus-8nfz7
Dec 24 02:36:51.942: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 24 02:36:51.942: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 24 02:36:51.942: INFO: update-demo-nautilus-8nfz7 is verified up and running
Dec 24 02:36:51.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods update-demo-nautilus-c2vl4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2545'
Dec 24 02:36:52.023: INFO: stderr: ""
Dec 24 02:36:52.023: INFO: stdout: "true"
Dec 24 02:36:52.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods update-demo-nautilus-c2vl4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2545'
Dec 24 02:36:52.104: INFO: stderr: ""
Dec 24 02:36:52.104: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 24 02:36:52.104: INFO: validating pod update-demo-nautilus-c2vl4
Dec 24 02:36:52.107: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 24 02:36:52.107: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 24 02:36:52.107: INFO: update-demo-nautilus-c2vl4 is verified up and running
STEP: using delete to clean up resources
Dec 24 02:36:52.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 delete --grace-period=0 --force -f - --namespace=kubectl-2545'
Dec 24 02:36:52.188: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 24 02:36:52.188: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 24 02:36:52.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2545'
Dec 24 02:36:52.273: INFO: stderr: "No resources found in kubectl-2545 namespace.\n"
Dec 24 02:36:52.273: INFO: stdout: ""
Dec 24 02:36:52.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods -l name=update-demo --namespace=kubectl-2545 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 24 02:36:52.351: INFO: stderr: ""
Dec 24 02:36:52.351: INFO: stdout: "update-demo-nautilus-8nfz7\nupdate-demo-nautilus-c2vl4\n"
Dec 24 02:36:52.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2545'
Dec 24 02:36:52.959: INFO: stderr: "No resources found in kubectl-2545 namespace.\n"
Dec 24 02:36:52.959: INFO: stdout: ""
Dec 24 02:36:52.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods -l name=update-demo --namespace=kubectl-2545 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 24 02:36:53.044: INFO: stderr: ""
Dec 24 02:36:53.044: INFO: stdout: "update-demo-nautilus-8nfz7\nupdate-demo-nautilus-c2vl4\n"
Dec 24 02:36:53.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2545'
Dec 24 02:36:53.467: INFO: stderr: "No resources found in kubectl-2545 namespace.\n"
Dec 24 02:36:53.467: INFO: stdout: ""
Dec 24 02:36:53.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods -l name=update-demo --namespace=kubectl-2545 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 24 02:36:53.595: INFO: stderr: ""
Dec 24 02:36:53.595: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:36:53.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2545" for this suite.

• [SLOW TEST:27.283 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":305,"completed":59,"skipped":899,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:36:53.616: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W1224 02:37:03.702762      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Dec 24 02:38:05.731: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:38:05.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5666" for this suite.

• [SLOW TEST:72.124 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":305,"completed":60,"skipped":907,"failed":0}
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:38:05.740: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Dec 24 02:38:05.789: INFO: Waiting up to 5m0s for pod "downward-api-c9253266-4e3e-42c5-8a6e-a35ed492ba62" in namespace "downward-api-3723" to be "Succeeded or Failed"
Dec 24 02:38:05.804: INFO: Pod "downward-api-c9253266-4e3e-42c5-8a6e-a35ed492ba62": Phase="Pending", Reason="", readiness=false. Elapsed: 15.023805ms
Dec 24 02:38:07.825: INFO: Pod "downward-api-c9253266-4e3e-42c5-8a6e-a35ed492ba62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035819752s
Dec 24 02:38:09.828: INFO: Pod "downward-api-c9253266-4e3e-42c5-8a6e-a35ed492ba62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038667336s
STEP: Saw pod success
Dec 24 02:38:09.828: INFO: Pod "downward-api-c9253266-4e3e-42c5-8a6e-a35ed492ba62" satisfied condition "Succeeded or Failed"
Dec 24 02:38:09.830: INFO: Trying to get logs from node c1-4 pod downward-api-c9253266-4e3e-42c5-8a6e-a35ed492ba62 container dapi-container: <nil>
STEP: delete the pod
Dec 24 02:38:09.863: INFO: Waiting for pod downward-api-c9253266-4e3e-42c5-8a6e-a35ed492ba62 to disappear
Dec 24 02:38:09.870: INFO: Pod downward-api-c9253266-4e3e-42c5-8a6e-a35ed492ba62 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:38:09.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3723" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":305,"completed":61,"skipped":907,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:38:09.879: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 24 02:38:10.268: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 24 02:38:13.291: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:38:13.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8638" for this suite.
STEP: Destroying namespace "webhook-8638-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":305,"completed":62,"skipped":916,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:38:13.512: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-0838c68a-83de-4c53-be94-71a5a9895371
STEP: Creating a pod to test consume secrets
Dec 24 02:38:13.550: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-09d39f8f-f651-4045-adbc-5c4b6242e8db" in namespace "projected-5728" to be "Succeeded or Failed"
Dec 24 02:38:13.554: INFO: Pod "pod-projected-secrets-09d39f8f-f651-4045-adbc-5c4b6242e8db": Phase="Pending", Reason="", readiness=false. Elapsed: 4.370798ms
Dec 24 02:38:15.591: INFO: Pod "pod-projected-secrets-09d39f8f-f651-4045-adbc-5c4b6242e8db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.040642173s
STEP: Saw pod success
Dec 24 02:38:15.591: INFO: Pod "pod-projected-secrets-09d39f8f-f651-4045-adbc-5c4b6242e8db" satisfied condition "Succeeded or Failed"
Dec 24 02:38:15.593: INFO: Trying to get logs from node c1-4 pod pod-projected-secrets-09d39f8f-f651-4045-adbc-5c4b6242e8db container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 24 02:38:15.629: INFO: Waiting for pod pod-projected-secrets-09d39f8f-f651-4045-adbc-5c4b6242e8db to disappear
Dec 24 02:38:15.635: INFO: Pod pod-projected-secrets-09d39f8f-f651-4045-adbc-5c4b6242e8db no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:38:15.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5728" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":63,"skipped":919,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:38:15.646: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 02:38:15.682: INFO: Creating ReplicaSet my-hostname-basic-3e175c50-b27e-4ad9-bb9f-ec219345dcab
Dec 24 02:38:15.710: INFO: Pod name my-hostname-basic-3e175c50-b27e-4ad9-bb9f-ec219345dcab: Found 0 pods out of 1
Dec 24 02:38:20.722: INFO: Pod name my-hostname-basic-3e175c50-b27e-4ad9-bb9f-ec219345dcab: Found 1 pods out of 1
Dec 24 02:38:20.722: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-3e175c50-b27e-4ad9-bb9f-ec219345dcab" is running
Dec 24 02:38:20.724: INFO: Pod "my-hostname-basic-3e175c50-b27e-4ad9-bb9f-ec219345dcab-9fcl4" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-24 02:38:15 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-24 02:38:17 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-24 02:38:17 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-24 02:38:15 +0000 UTC Reason: Message:}])
Dec 24 02:38:20.724: INFO: Trying to dial the pod
Dec 24 02:38:25.733: INFO: Controller my-hostname-basic-3e175c50-b27e-4ad9-bb9f-ec219345dcab: Got expected result from replica 1 [my-hostname-basic-3e175c50-b27e-4ad9-bb9f-ec219345dcab-9fcl4]: "my-hostname-basic-3e175c50-b27e-4ad9-bb9f-ec219345dcab-9fcl4", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:38:25.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1797" for this suite.

• [SLOW TEST:10.104 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":64,"skipped":942,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:38:25.750: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Dec 24 02:38:25.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 create -f - --namespace=kubectl-6644'
Dec 24 02:38:26.173: INFO: stderr: ""
Dec 24 02:38:26.173: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Dec 24 02:38:27.176: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 24 02:38:27.176: INFO: Found 0 / 1
Dec 24 02:38:28.176: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 24 02:38:28.176: INFO: Found 1 / 1
Dec 24 02:38:28.176: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Dec 24 02:38:28.178: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 24 02:38:28.178: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 24 02:38:28.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 patch pod agnhost-primary-kg4xt --namespace=kubectl-6644 -p {"metadata":{"annotations":{"x":"y"}}}'
Dec 24 02:38:28.288: INFO: stderr: ""
Dec 24 02:38:28.288: INFO: stdout: "pod/agnhost-primary-kg4xt patched\n"
STEP: checking annotations
Dec 24 02:38:28.311: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 24 02:38:28.311: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:38:28.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6644" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":305,"completed":65,"skipped":959,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:38:28.318: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 24 02:38:28.829: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 24 02:38:30.847: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744374308, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744374308, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744374308, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744374308, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 24 02:38:33.869: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:38:34.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1947" for this suite.
STEP: Destroying namespace "webhook-1947-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.952 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":305,"completed":66,"skipped":977,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:38:34.271: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating all guestbook components
Dec 24 02:38:34.346: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Dec 24 02:38:34.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 create -f - --namespace=kubectl-140'
Dec 24 02:38:34.706: INFO: stderr: ""
Dec 24 02:38:34.706: INFO: stdout: "service/agnhost-replica created\n"
Dec 24 02:38:34.706: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Dec 24 02:38:34.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 create -f - --namespace=kubectl-140'
Dec 24 02:38:35.014: INFO: stderr: ""
Dec 24 02:38:35.014: INFO: stdout: "service/agnhost-primary created\n"
Dec 24 02:38:35.014: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec 24 02:38:35.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 create -f - --namespace=kubectl-140'
Dec 24 02:38:35.375: INFO: stderr: ""
Dec 24 02:38:35.375: INFO: stdout: "service/frontend created\n"
Dec 24 02:38:35.375: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Dec 24 02:38:35.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 create -f - --namespace=kubectl-140'
Dec 24 02:38:35.690: INFO: stderr: ""
Dec 24 02:38:35.690: INFO: stdout: "deployment.apps/frontend created\n"
Dec 24 02:38:35.690: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 24 02:38:35.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 create -f - --namespace=kubectl-140'
Dec 24 02:38:36.021: INFO: stderr: ""
Dec 24 02:38:36.021: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Dec 24 02:38:36.021: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 24 02:38:36.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 create -f - --namespace=kubectl-140'
Dec 24 02:38:36.375: INFO: stderr: ""
Dec 24 02:38:36.375: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Dec 24 02:38:36.375: INFO: Waiting for all frontend pods to be Running.
Dec 24 02:38:41.426: INFO: Waiting for frontend to serve content.
Dec 24 02:38:41.435: INFO: Trying to add a new entry to the guestbook.
Dec 24 02:38:41.447: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Dec 24 02:38:41.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 delete --grace-period=0 --force -f - --namespace=kubectl-140'
Dec 24 02:38:41.603: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 24 02:38:41.603: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Dec 24 02:38:41.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 delete --grace-period=0 --force -f - --namespace=kubectl-140'
Dec 24 02:38:41.749: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 24 02:38:41.749: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Dec 24 02:38:41.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 delete --grace-period=0 --force -f - --namespace=kubectl-140'
Dec 24 02:38:41.911: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 24 02:38:41.911: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 24 02:38:41.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 delete --grace-period=0 --force -f - --namespace=kubectl-140'
Dec 24 02:38:42.018: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 24 02:38:42.018: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 24 02:38:42.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 delete --grace-period=0 --force -f - --namespace=kubectl-140'
Dec 24 02:38:42.134: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 24 02:38:42.134: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Dec 24 02:38:42.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 delete --grace-period=0 --force -f - --namespace=kubectl-140'
Dec 24 02:38:42.237: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 24 02:38:42.237: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:38:42.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-140" for this suite.

• [SLOW TEST:7.974 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:351
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":305,"completed":67,"skipped":1010,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:38:42.245: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Dec 24 02:38:42.291: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:38:46.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2793" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":305,"completed":68,"skipped":1028,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:38:46.522: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Dec 24 02:38:46.601: INFO: Pod name pod-release: Found 0 pods out of 1
Dec 24 02:38:51.603: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:38:51.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6421" for this suite.

• [SLOW TEST:5.149 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":305,"completed":69,"skipped":1050,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:38:51.671: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Dec 24 02:38:51.738: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 24 02:39:51.824: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:39:51.826: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:487
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Dec 24 02:39:53.930: INFO: found a healthy node: c1-4
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 02:40:00.059: INFO: pods created so far: [1 1 1]
Dec 24 02:40:00.059: INFO: length of pods created so far: 3
Dec 24 02:40:20.066: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:40:27.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-5071" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:461
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:40:27.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4169" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:95.519 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:450
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":305,"completed":70,"skipped":1051,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:40:27.190: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 24 02:40:27.724: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 24 02:40:29.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744374427, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744374427, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744374427, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744374427, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 24 02:40:32.776: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:40:33.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-547" for this suite.
STEP: Destroying namespace "webhook-547-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.029 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":305,"completed":71,"skipped":1058,"failed":0}
S
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:40:33.218: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Dec 24 02:40:33.297: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:40:33.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6376" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":305,"completed":72,"skipped":1059,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:40:33.327: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Dec 24 02:40:33.366: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Dec 24 02:40:51.427: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
Dec 24 02:40:56.103: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:41:14.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-952" for this suite.

• [SLOW TEST:41.073 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":305,"completed":73,"skipped":1179,"failed":0}
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:41:14.400: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's command
Dec 24 02:41:14.449: INFO: Waiting up to 5m0s for pod "var-expansion-8a41c229-40eb-4360-bc5f-7e22b5c3644d" in namespace "var-expansion-8617" to be "Succeeded or Failed"
Dec 24 02:41:14.453: INFO: Pod "var-expansion-8a41c229-40eb-4360-bc5f-7e22b5c3644d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.987858ms
Dec 24 02:41:16.456: INFO: Pod "var-expansion-8a41c229-40eb-4360-bc5f-7e22b5c3644d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006720896s
STEP: Saw pod success
Dec 24 02:41:16.456: INFO: Pod "var-expansion-8a41c229-40eb-4360-bc5f-7e22b5c3644d" satisfied condition "Succeeded or Failed"
Dec 24 02:41:16.458: INFO: Trying to get logs from node c1-4 pod var-expansion-8a41c229-40eb-4360-bc5f-7e22b5c3644d container dapi-container: <nil>
STEP: delete the pod
Dec 24 02:41:16.495: INFO: Waiting for pod var-expansion-8a41c229-40eb-4360-bc5f-7e22b5c3644d to disappear
Dec 24 02:41:16.508: INFO: Pod var-expansion-8a41c229-40eb-4360-bc5f-7e22b5c3644d no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:41:16.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8617" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":305,"completed":74,"skipped":1179,"failed":0}
SSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:41:16.515: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Dec 24 02:41:16.556: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 24 02:42:16.631: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 02:42:16.633: INFO: Starting informer...
STEP: Starting pods...
Dec 24 02:42:16.845: INFO: Pod1 is running on c1-4. Tainting Node
Dec 24 02:42:19.059: INFO: Pod2 is running on c1-4. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Dec 24 02:42:25.996: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Dec 24 02:42:46.008: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:42:46.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-6282" for this suite.

• [SLOW TEST:89.561 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":305,"completed":75,"skipped":1184,"failed":0}
SS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:42:46.076: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-9897/configmap-test-6cb78fb6-80c2-4724-84e4-d30094cc8a20
STEP: Creating a pod to test consume configMaps
Dec 24 02:42:46.139: INFO: Waiting up to 5m0s for pod "pod-configmaps-3b24898c-6c54-4415-af08-12214d4c0053" in namespace "configmap-9897" to be "Succeeded or Failed"
Dec 24 02:42:46.172: INFO: Pod "pod-configmaps-3b24898c-6c54-4415-af08-12214d4c0053": Phase="Pending", Reason="", readiness=false. Elapsed: 32.293693ms
Dec 24 02:42:48.188: INFO: Pod "pod-configmaps-3b24898c-6c54-4415-af08-12214d4c0053": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.049211494s
STEP: Saw pod success
Dec 24 02:42:48.189: INFO: Pod "pod-configmaps-3b24898c-6c54-4415-af08-12214d4c0053" satisfied condition "Succeeded or Failed"
Dec 24 02:42:48.190: INFO: Trying to get logs from node c1-4 pod pod-configmaps-3b24898c-6c54-4415-af08-12214d4c0053 container env-test: <nil>
STEP: delete the pod
Dec 24 02:42:48.219: INFO: Waiting for pod pod-configmaps-3b24898c-6c54-4415-af08-12214d4c0053 to disappear
Dec 24 02:42:48.224: INFO: Pod pod-configmaps-3b24898c-6c54-4415-af08-12214d4c0053 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:42:48.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9897" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":76,"skipped":1186,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:42:48.231: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 24 02:42:48.271: INFO: Waiting up to 5m0s for pod "pod-14973814-a0e7-48f7-af7b-fc5a2b893d45" in namespace "emptydir-6381" to be "Succeeded or Failed"
Dec 24 02:42:48.275: INFO: Pod "pod-14973814-a0e7-48f7-af7b-fc5a2b893d45": Phase="Pending", Reason="", readiness=false. Elapsed: 4.371452ms
Dec 24 02:42:50.278: INFO: Pod "pod-14973814-a0e7-48f7-af7b-fc5a2b893d45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006960058s
STEP: Saw pod success
Dec 24 02:42:50.278: INFO: Pod "pod-14973814-a0e7-48f7-af7b-fc5a2b893d45" satisfied condition "Succeeded or Failed"
Dec 24 02:42:50.280: INFO: Trying to get logs from node c1-4 pod pod-14973814-a0e7-48f7-af7b-fc5a2b893d45 container test-container: <nil>
STEP: delete the pod
Dec 24 02:42:50.300: INFO: Waiting for pod pod-14973814-a0e7-48f7-af7b-fc5a2b893d45 to disappear
Dec 24 02:42:50.304: INFO: Pod pod-14973814-a0e7-48f7-af7b-fc5a2b893d45 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:42:50.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6381" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":77,"skipped":1201,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:42:50.314: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-4d7783cb-4a88-4eb2-a184-ba02ee66d978
STEP: Creating secret with name s-test-opt-upd-e206e0a5-4390-49a4-b21d-3ed7095ba924
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-4d7783cb-4a88-4eb2-a184-ba02ee66d978
STEP: Updating secret s-test-opt-upd-e206e0a5-4390-49a4-b21d-3ed7095ba924
STEP: Creating secret with name s-test-opt-create-f7f08150-f1a9-49da-b9d0-d35963ae531f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:44:22.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1450" for this suite.

• [SLOW TEST:92.502 seconds]
[sig-storage] Secrets
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":78,"skipped":1211,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:44:22.816: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:44:22.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1187" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":305,"completed":79,"skipped":1219,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:44:22.869: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-0b8aac17-16e8-4b8b-a1e0-028083d62036
STEP: Creating a pod to test consume secrets
Dec 24 02:44:22.920: INFO: Waiting up to 5m0s for pod "pod-secrets-c92a003e-f609-4f27-a124-bd8d5c7273dc" in namespace "secrets-513" to be "Succeeded or Failed"
Dec 24 02:44:22.934: INFO: Pod "pod-secrets-c92a003e-f609-4f27-a124-bd8d5c7273dc": Phase="Pending", Reason="", readiness=false. Elapsed: 13.723918ms
Dec 24 02:44:24.937: INFO: Pod "pod-secrets-c92a003e-f609-4f27-a124-bd8d5c7273dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016510726s
Dec 24 02:44:26.940: INFO: Pod "pod-secrets-c92a003e-f609-4f27-a124-bd8d5c7273dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019596306s
STEP: Saw pod success
Dec 24 02:44:26.940: INFO: Pod "pod-secrets-c92a003e-f609-4f27-a124-bd8d5c7273dc" satisfied condition "Succeeded or Failed"
Dec 24 02:44:26.942: INFO: Trying to get logs from node c1-4 pod pod-secrets-c92a003e-f609-4f27-a124-bd8d5c7273dc container secret-env-test: <nil>
STEP: delete the pod
Dec 24 02:44:26.961: INFO: Waiting for pod pod-secrets-c92a003e-f609-4f27-a124-bd8d5c7273dc to disappear
Dec 24 02:44:26.965: INFO: Pod pod-secrets-c92a003e-f609-4f27-a124-bd8d5c7273dc no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:44:26.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-513" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":305,"completed":80,"skipped":1243,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:44:26.972: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-5e25d16f-43b9-4507-92cf-61a3f65ae9c6 in namespace container-probe-4564
Dec 24 02:44:31.054: INFO: Started pod liveness-5e25d16f-43b9-4507-92cf-61a3f65ae9c6 in namespace container-probe-4564
STEP: checking the pod's current state and verifying that restartCount is present
Dec 24 02:44:31.057: INFO: Initial restart count of pod liveness-5e25d16f-43b9-4507-92cf-61a3f65ae9c6 is 0
Dec 24 02:44:45.081: INFO: Restart count of pod container-probe-4564/liveness-5e25d16f-43b9-4507-92cf-61a3f65ae9c6 is now 1 (14.023980178s elapsed)
Dec 24 02:45:05.112: INFO: Restart count of pod container-probe-4564/liveness-5e25d16f-43b9-4507-92cf-61a3f65ae9c6 is now 2 (34.055259294s elapsed)
Dec 24 02:45:25.142: INFO: Restart count of pod container-probe-4564/liveness-5e25d16f-43b9-4507-92cf-61a3f65ae9c6 is now 3 (54.085493916s elapsed)
Dec 24 02:45:45.174: INFO: Restart count of pod container-probe-4564/liveness-5e25d16f-43b9-4507-92cf-61a3f65ae9c6 is now 4 (1m14.117081734s elapsed)
Dec 24 02:46:57.290: INFO: Restart count of pod container-probe-4564/liveness-5e25d16f-43b9-4507-92cf-61a3f65ae9c6 is now 5 (2m26.232803028s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:46:57.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4564" for this suite.

• [SLOW TEST:150.343 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":305,"completed":81,"skipped":1250,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:46:57.315: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Dec 24 02:46:57.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 create -f - --namespace=kubectl-8136'
Dec 24 02:46:58.982: INFO: stderr: ""
Dec 24 02:46:58.982: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 24 02:46:58.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8136'
Dec 24 02:46:59.092: INFO: stderr: ""
Dec 24 02:46:59.092: INFO: stdout: "update-demo-nautilus-6t52f update-demo-nautilus-rnvvd "
Dec 24 02:46:59.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods update-demo-nautilus-6t52f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8136'
Dec 24 02:46:59.185: INFO: stderr: ""
Dec 24 02:46:59.185: INFO: stdout: ""
Dec 24 02:46:59.185: INFO: update-demo-nautilus-6t52f is created but not running
Dec 24 02:47:04.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8136'
Dec 24 02:47:04.298: INFO: stderr: ""
Dec 24 02:47:04.298: INFO: stdout: "update-demo-nautilus-6t52f update-demo-nautilus-rnvvd "
Dec 24 02:47:04.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods update-demo-nautilus-6t52f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8136'
Dec 24 02:47:04.381: INFO: stderr: ""
Dec 24 02:47:04.381: INFO: stdout: "true"
Dec 24 02:47:04.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods update-demo-nautilus-6t52f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8136'
Dec 24 02:47:04.459: INFO: stderr: ""
Dec 24 02:47:04.460: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 24 02:47:04.460: INFO: validating pod update-demo-nautilus-6t52f
Dec 24 02:47:04.464: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 24 02:47:04.464: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 24 02:47:04.464: INFO: update-demo-nautilus-6t52f is verified up and running
Dec 24 02:47:04.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods update-demo-nautilus-rnvvd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8136'
Dec 24 02:47:04.543: INFO: stderr: ""
Dec 24 02:47:04.543: INFO: stdout: "true"
Dec 24 02:47:04.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods update-demo-nautilus-rnvvd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8136'
Dec 24 02:47:04.621: INFO: stderr: ""
Dec 24 02:47:04.621: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 24 02:47:04.621: INFO: validating pod update-demo-nautilus-rnvvd
Dec 24 02:47:04.624: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 24 02:47:04.624: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 24 02:47:04.624: INFO: update-demo-nautilus-rnvvd is verified up and running
STEP: using delete to clean up resources
Dec 24 02:47:04.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 delete --grace-period=0 --force -f - --namespace=kubectl-8136'
Dec 24 02:47:04.712: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 24 02:47:04.712: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 24 02:47:04.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8136'
Dec 24 02:47:04.791: INFO: stderr: "No resources found in kubectl-8136 namespace.\n"
Dec 24 02:47:04.791: INFO: stdout: ""
Dec 24 02:47:04.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods -l name=update-demo --namespace=kubectl-8136 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 24 02:47:04.880: INFO: stderr: ""
Dec 24 02:47:04.880: INFO: stdout: "update-demo-nautilus-6t52f\nupdate-demo-nautilus-rnvvd\n"
Dec 24 02:47:05.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8136'
Dec 24 02:47:05.482: INFO: stderr: "No resources found in kubectl-8136 namespace.\n"
Dec 24 02:47:05.482: INFO: stdout: ""
Dec 24 02:47:05.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods -l name=update-demo --namespace=kubectl-8136 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 24 02:47:05.567: INFO: stderr: ""
Dec 24 02:47:05.567: INFO: stdout: "update-demo-nautilus-6t52f\nupdate-demo-nautilus-rnvvd\n"
Dec 24 02:47:05.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8136'
Dec 24 02:47:05.993: INFO: stderr: "No resources found in kubectl-8136 namespace.\n"
Dec 24 02:47:05.993: INFO: stdout: ""
Dec 24 02:47:05.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods -l name=update-demo --namespace=kubectl-8136 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 24 02:47:06.107: INFO: stderr: ""
Dec 24 02:47:06.107: INFO: stdout: "update-demo-nautilus-6t52f\nupdate-demo-nautilus-rnvvd\n"
Dec 24 02:47:06.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8136'
Dec 24 02:47:06.477: INFO: stderr: "No resources found in kubectl-8136 namespace.\n"
Dec 24 02:47:06.477: INFO: stdout: ""
Dec 24 02:47:06.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods -l name=update-demo --namespace=kubectl-8136 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 24 02:47:06.562: INFO: stderr: ""
Dec 24 02:47:06.562: INFO: stdout: "update-demo-nautilus-6t52f\nupdate-demo-nautilus-rnvvd\n"
Dec 24 02:47:06.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8136'
Dec 24 02:47:06.988: INFO: stderr: "No resources found in kubectl-8136 namespace.\n"
Dec 24 02:47:06.988: INFO: stdout: ""
Dec 24 02:47:06.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods -l name=update-demo --namespace=kubectl-8136 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 24 02:47:07.074: INFO: stderr: ""
Dec 24 02:47:07.074: INFO: stdout: "update-demo-nautilus-6t52f\nupdate-demo-nautilus-rnvvd\n"
Dec 24 02:47:07.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8136'
Dec 24 02:47:07.494: INFO: stderr: "No resources found in kubectl-8136 namespace.\n"
Dec 24 02:47:07.494: INFO: stdout: ""
Dec 24 02:47:07.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods -l name=update-demo --namespace=kubectl-8136 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 24 02:47:07.590: INFO: stderr: ""
Dec 24 02:47:07.590: INFO: stdout: "update-demo-nautilus-6t52f\nupdate-demo-nautilus-rnvvd\n"
Dec 24 02:47:07.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8136'
Dec 24 02:47:07.982: INFO: stderr: "No resources found in kubectl-8136 namespace.\n"
Dec 24 02:47:07.982: INFO: stdout: ""
Dec 24 02:47:07.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods -l name=update-demo --namespace=kubectl-8136 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 24 02:47:08.067: INFO: stderr: ""
Dec 24 02:47:08.067: INFO: stdout: "update-demo-nautilus-6t52f\nupdate-demo-nautilus-rnvvd\n"
Dec 24 02:47:08.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8136'
Dec 24 02:47:08.490: INFO: stderr: "No resources found in kubectl-8136 namespace.\n"
Dec 24 02:47:08.490: INFO: stdout: ""
Dec 24 02:47:08.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pods -l name=update-demo --namespace=kubectl-8136 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 24 02:47:08.580: INFO: stderr: ""
Dec 24 02:47:08.580: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:47:08.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8136" for this suite.

• [SLOW TEST:11.282 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":305,"completed":82,"skipped":1274,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:47:08.597: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 02:47:08.661: INFO: Waiting up to 5m0s for pod "busybox-user-65534-f833a2d6-bdf3-4eae-bb8a-8da3c502f2a6" in namespace "security-context-test-8001" to be "Succeeded or Failed"
Dec 24 02:47:08.674: INFO: Pod "busybox-user-65534-f833a2d6-bdf3-4eae-bb8a-8da3c502f2a6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.913648ms
Dec 24 02:47:10.680: INFO: Pod "busybox-user-65534-f833a2d6-bdf3-4eae-bb8a-8da3c502f2a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019227696s
Dec 24 02:47:10.680: INFO: Pod "busybox-user-65534-f833a2d6-bdf3-4eae-bb8a-8da3c502f2a6" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:47:10.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8001" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":83,"skipped":1287,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:47:10.695: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-a4f7afe4-54e5-42c7-b9e6-6d700719b318
STEP: Creating a pod to test consume secrets
Dec 24 02:47:10.741: INFO: Waiting up to 5m0s for pod "pod-secrets-9bb5ee82-3d7b-42b0-8874-c02cd31da2b2" in namespace "secrets-6819" to be "Succeeded or Failed"
Dec 24 02:47:10.772: INFO: Pod "pod-secrets-9bb5ee82-3d7b-42b0-8874-c02cd31da2b2": Phase="Pending", Reason="", readiness=false. Elapsed: 30.513018ms
Dec 24 02:47:12.774: INFO: Pod "pod-secrets-9bb5ee82-3d7b-42b0-8874-c02cd31da2b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033206216s
STEP: Saw pod success
Dec 24 02:47:12.774: INFO: Pod "pod-secrets-9bb5ee82-3d7b-42b0-8874-c02cd31da2b2" satisfied condition "Succeeded or Failed"
Dec 24 02:47:12.777: INFO: Trying to get logs from node c1-4 pod pod-secrets-9bb5ee82-3d7b-42b0-8874-c02cd31da2b2 container secret-volume-test: <nil>
STEP: delete the pod
Dec 24 02:47:12.813: INFO: Waiting for pod pod-secrets-9bb5ee82-3d7b-42b0-8874-c02cd31da2b2 to disappear
Dec 24 02:47:12.820: INFO: Pod pod-secrets-9bb5ee82-3d7b-42b0-8874-c02cd31da2b2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:47:12.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6819" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":84,"skipped":1300,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:47:12.845: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Dec 24 02:47:12.891: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 24 02:47:12.900: INFO: Waiting for terminating namespaces to be deleted...
Dec 24 02:47:12.901: INFO: 
Logging pods the apiserver thinks is on node c1-4 before test
Dec 24 02:47:12.917: INFO: calico-node-gf9xk from kube-system started at 2020-12-17 09:20:20 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.917: INFO: 	Container calico-node ready: true, restart count 0
Dec 24 02:47:12.917: INFO: kube-proxy-nnl7j from kube-system started at 2020-12-17 09:18:43 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.917: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 24 02:47:12.917: INFO: update-demo-nautilus-rnvvd from kubectl-8136 started at 2020-12-24 02:46:59 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.917: INFO: 	Container update-demo ready: false, restart count 0
Dec 24 02:47:12.917: INFO: virt-handler-5nkqw from kubevirt started at 2020-12-23 10:54:33 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.917: INFO: 	Container virt-handler ready: false, restart count 0
Dec 24 02:47:12.917: INFO: csi-cephfsplugin-ghqns from rook-ceph started at 2020-12-23 10:54:33 +0000 UTC (3 container statuses recorded)
Dec 24 02:47:12.917: INFO: 	Container csi-cephfsplugin ready: false, restart count 0
Dec 24 02:47:12.917: INFO: 	Container driver-registrar ready: false, restart count 0
Dec 24 02:47:12.917: INFO: 	Container liveness-prometheus ready: false, restart count 0
Dec 24 02:47:12.917: INFO: csi-rbdplugin-z74d4 from rook-ceph started at 2020-12-23 10:54:33 +0000 UTC (3 container statuses recorded)
Dec 24 02:47:12.917: INFO: 	Container csi-rbdplugin ready: false, restart count 0
Dec 24 02:47:12.917: INFO: 	Container driver-registrar ready: false, restart count 0
Dec 24 02:47:12.917: INFO: 	Container liveness-prometheus ready: false, restart count 0
Dec 24 02:47:12.917: INFO: rook-ceph-crashcollector-c1-4-b5fd6cbf-jrn9q from rook-ceph started at 2020-12-24 02:42:56 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.917: INFO: 	Container ceph-crash ready: true, restart count 0
Dec 24 02:47:12.917: INFO: rook-ceph-crashcollector-c1-4-b5fd6cbf-r86bh from rook-ceph started at 2020-12-23 10:29:44 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.917: INFO: 	Container ceph-crash ready: false, restart count 0
Dec 24 02:47:12.917: INFO: rook-ceph-mon-b-6c84dd66-29k4w from rook-ceph started at 2020-12-24 02:42:56 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.917: INFO: 	Container mon ready: true, restart count 0
Dec 24 02:47:12.917: INFO: rook-ceph-mon-b-6c84dd66-xj8wb from rook-ceph started at 2020-12-23 10:29:43 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.917: INFO: 	Container mon ready: false, restart count 0
Dec 24 02:47:12.917: INFO: rook-ceph-osd-1-59b659d744-k6c5s from rook-ceph started at 2020-12-23 10:29:43 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.917: INFO: 	Container osd ready: false, restart count 0
Dec 24 02:47:12.917: INFO: rook-ceph-osd-1-59b659d744-nnqc9 from rook-ceph started at 2020-12-24 02:42:54 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.917: INFO: 	Container osd ready: true, restart count 0
Dec 24 02:47:12.917: INFO: rook-discover-svzsw from rook-ceph started at 2020-12-23 10:54:33 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.917: INFO: 	Container rook-discover ready: false, restart count 0
Dec 24 02:47:12.917: INFO: busybox-user-65534-f833a2d6-bdf3-4eae-bb8a-8da3c502f2a6 from security-context-test-8001 started at 2020-12-24 02:47:08 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.917: INFO: 	Container busybox-user-65534-f833a2d6-bdf3-4eae-bb8a-8da3c502f2a6 ready: false, restart count 0
Dec 24 02:47:12.917: INFO: sonobuoy from sonobuoy started at 2020-12-24 02:17:41 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.917: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 24 02:47:12.917: INFO: sonobuoy-e2e-job-683f472080954d4f from sonobuoy started at 2020-12-24 02:17:43 +0000 UTC (2 container statuses recorded)
Dec 24 02:47:12.917: INFO: 	Container e2e ready: true, restart count 0
Dec 24 02:47:12.917: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 24 02:47:12.917: INFO: sonobuoy-systemd-logs-daemon-set-eb313291d8024436-cqjvw from sonobuoy started at 2020-12-24 02:17:43 +0000 UTC (2 container statuses recorded)
Dec 24 02:47:12.917: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 24 02:47:12.917: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 24 02:47:12.917: INFO: 
Logging pods the apiserver thinks is on node c1-5 before test
Dec 24 02:47:12.933: INFO: cdi-apiserver-79dbd664dd-jvbpv from cdi started at 2020-12-23 09:46:37 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.933: INFO: 	Container cdi-apiserver ready: true, restart count 0
Dec 24 02:47:12.933: INFO: snapshot-controller-0 from default started at 2020-12-18 04:06:22 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.933: INFO: 	Container snapshot-controller ready: true, restart count 0
Dec 24 02:47:12.933: INFO: calico-node-tt5dp from kube-system started at 2020-12-17 09:20:20 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.933: INFO: 	Container calico-node ready: true, restart count 0
Dec 24 02:47:12.933: INFO: kube-proxy-2lcn2 from kube-system started at 2020-12-17 09:18:32 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.933: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 24 02:47:12.933: INFO: update-demo-nautilus-6t52f from kubectl-8136 started at 2020-12-24 02:46:59 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.933: INFO: 	Container update-demo ready: false, restart count 0
Dec 24 02:47:12.933: INFO: virt-api-76b44ffc65-kcmtd from kubevirt started at 2020-12-23 09:46:37 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.933: INFO: 	Container virt-api ready: true, restart count 0
Dec 24 02:47:12.933: INFO: virt-controller-84b5d4b779-lbv8p from kubevirt started at 2020-12-23 09:46:37 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.933: INFO: 	Container virt-controller ready: true, restart count 0
Dec 24 02:47:12.933: INFO: virt-handler-scjdh from kubevirt started at 2020-12-17 09:24:03 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.933: INFO: 	Container virt-handler ready: true, restart count 7
Dec 24 02:47:12.933: INFO: virt-operator-867fcd786d-hqk4k from kubevirt started at 2020-12-23 09:46:37 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.933: INFO: 	Container virt-operator ready: true, restart count 0
Dec 24 02:47:12.933: INFO: csi-cephfsplugin-provisioner-d878cdf8b-6g8td from rook-ceph started at 2020-12-18 04:06:36 +0000 UTC (6 container statuses recorded)
Dec 24 02:47:12.933: INFO: 	Container csi-attacher ready: true, restart count 1
Dec 24 02:47:12.933: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 24 02:47:12.933: INFO: 	Container csi-provisioner ready: true, restart count 1
Dec 24 02:47:12.933: INFO: 	Container csi-resizer ready: true, restart count 1
Dec 24 02:47:12.933: INFO: 	Container csi-snapshotter ready: true, restart count 1
Dec 24 02:47:12.933: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 02:47:12.933: INFO: csi-cephfsplugin-xd5rd from rook-ceph started at 2020-12-18 04:06:35 +0000 UTC (3 container statuses recorded)
Dec 24 02:47:12.933: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 24 02:47:12.933: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 24 02:47:12.933: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 02:47:12.933: INFO: csi-rbdplugin-drpwm from rook-ceph started at 2020-12-18 04:06:35 +0000 UTC (3 container statuses recorded)
Dec 24 02:47:12.933: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 24 02:47:12.933: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 24 02:47:12.933: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 02:47:12.933: INFO: csi-rbdplugin-provisioner-7588bc4-dxkjr from rook-ceph started at 2020-12-18 04:06:35 +0000 UTC (6 container statuses recorded)
Dec 24 02:47:12.933: INFO: 	Container csi-attacher ready: true, restart count 1
Dec 24 02:47:12.933: INFO: 	Container csi-provisioner ready: true, restart count 2
Dec 24 02:47:12.933: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 24 02:47:12.933: INFO: 	Container csi-resizer ready: true, restart count 1
Dec 24 02:47:12.933: INFO: 	Container csi-snapshotter ready: true, restart count 1
Dec 24 02:47:12.933: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 02:47:12.933: INFO: rook-ceph-crashcollector-c1-5-7fdd9898c6-mm2r6 from rook-ceph started at 2020-12-18 04:07:17 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.933: INFO: 	Container ceph-crash ready: true, restart count 0
Dec 24 02:47:12.933: INFO: rook-ceph-mon-c-bcf7d945c-mvssc from rook-ceph started at 2020-12-18 04:07:17 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.933: INFO: 	Container mon ready: true, restart count 0
Dec 24 02:47:12.933: INFO: rook-ceph-operator-775d4b6c5f-khccc from rook-ceph started at 2020-12-18 04:06:25 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.933: INFO: 	Container rook-ceph-operator ready: true, restart count 0
Dec 24 02:47:12.933: INFO: rook-ceph-osd-2-5478c54cc5-s282q from rook-ceph started at 2020-12-18 04:07:41 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.933: INFO: 	Container osd ready: true, restart count 0
Dec 24 02:47:12.933: INFO: rook-ceph-osd-prepare-c1-5-l2wd8 from rook-ceph started at 2020-12-18 05:07:16 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.933: INFO: 	Container provision ready: false, restart count 0
Dec 24 02:47:12.933: INFO: rook-ceph-tools-6967fc698d-n2nzp from rook-ceph started at 2020-12-23 09:46:37 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.933: INFO: 	Container rook-ceph-tools ready: true, restart count 0
Dec 24 02:47:12.933: INFO: rook-discover-ds9s8 from rook-ceph started at 2020-12-18 04:06:28 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.933: INFO: 	Container rook-discover ready: true, restart count 0
Dec 24 02:47:12.933: INFO: sonobuoy-systemd-logs-daemon-set-eb313291d8024436-82n6r from sonobuoy started at 2020-12-24 02:17:43 +0000 UTC (2 container statuses recorded)
Dec 24 02:47:12.933: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 24 02:47:12.933: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 24 02:47:12.933: INFO: 
Logging pods the apiserver thinks is on node c1-6 before test
Dec 24 02:47:12.949: INFO: cdi-deployment-6c8975f4b-6bk46 from cdi started at 2020-12-21 09:29:47 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.949: INFO: 	Container cdi-controller ready: true, restart count 1
Dec 24 02:47:12.949: INFO: cdi-operator-5d5654db65-5hdrv from cdi started at 2020-12-21 09:29:21 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.949: INFO: 	Container cdi-operator ready: true, restart count 2
Dec 24 02:47:12.949: INFO: cdi-uploadproxy-5765998fff-qh2cb from cdi started at 2020-12-23 09:46:37 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.949: INFO: 	Container cdi-uploadproxy ready: true, restart count 0
Dec 24 02:47:12.949: INFO: calico-node-5rrck from kube-system started at 2020-12-17 09:20:20 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.949: INFO: 	Container calico-node ready: true, restart count 0
Dec 24 02:47:12.949: INFO: kube-proxy-jmmvs from kube-system started at 2020-12-17 09:19:08 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.949: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 24 02:47:12.949: INFO: virt-api-76b44ffc65-b4xmf from kubevirt started at 2020-12-17 09:23:40 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.949: INFO: 	Container virt-api ready: true, restart count 0
Dec 24 02:47:12.949: INFO: virt-controller-84b5d4b779-krqk6 from kubevirt started at 2020-12-17 09:24:03 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.949: INFO: 	Container virt-controller ready: true, restart count 5
Dec 24 02:47:12.949: INFO: virt-handler-jk5bd from kubevirt started at 2020-12-17 09:24:03 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.949: INFO: 	Container virt-handler ready: true, restart count 0
Dec 24 02:47:12.949: INFO: virt-operator-867fcd786d-9xnf6 from kubevirt started at 2020-12-17 09:23:28 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.949: INFO: 	Container virt-operator ready: true, restart count 2
Dec 24 02:47:12.949: INFO: csi-cephfsplugin-provisioner-d878cdf8b-9v92z from rook-ceph started at 2020-12-23 09:46:37 +0000 UTC (6 container statuses recorded)
Dec 24 02:47:12.949: INFO: 	Container csi-attacher ready: true, restart count 0
Dec 24 02:47:12.949: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 24 02:47:12.949: INFO: 	Container csi-provisioner ready: true, restart count 0
Dec 24 02:47:12.949: INFO: 	Container csi-resizer ready: true, restart count 0
Dec 24 02:47:12.949: INFO: 	Container csi-snapshotter ready: true, restart count 0
Dec 24 02:47:12.949: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 02:47:12.949: INFO: csi-cephfsplugin-q5hm7 from rook-ceph started at 2020-12-18 04:06:35 +0000 UTC (3 container statuses recorded)
Dec 24 02:47:12.949: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 24 02:47:12.949: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 24 02:47:12.949: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 02:47:12.949: INFO: csi-rbdplugin-provisioner-7588bc4-r297x from rook-ceph started at 2020-12-18 04:06:35 +0000 UTC (6 container statuses recorded)
Dec 24 02:47:12.949: INFO: 	Container csi-attacher ready: true, restart count 0
Dec 24 02:47:12.949: INFO: 	Container csi-provisioner ready: true, restart count 0
Dec 24 02:47:12.949: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 24 02:47:12.949: INFO: 	Container csi-resizer ready: true, restart count 0
Dec 24 02:47:12.949: INFO: 	Container csi-snapshotter ready: true, restart count 0
Dec 24 02:47:12.949: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 02:47:12.949: INFO: csi-rbdplugin-xfdqp from rook-ceph started at 2020-12-18 04:06:35 +0000 UTC (3 container statuses recorded)
Dec 24 02:47:12.949: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 24 02:47:12.949: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 24 02:47:12.949: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 02:47:12.949: INFO: rook-ceph-crashcollector-c1-6-5868d4fd57-6c5gp from rook-ceph started at 2020-12-18 04:08:07 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.949: INFO: 	Container ceph-crash ready: true, restart count 0
Dec 24 02:47:12.949: INFO: rook-ceph-mds-myfs-a-59cfdfc57f-zb7rk from rook-ceph started at 2020-12-23 09:46:37 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.949: INFO: 	Container mds ready: true, restart count 0
Dec 24 02:47:12.949: INFO: rook-ceph-mds-myfs-b-6d859f5b5c-r2rqz from rook-ceph started at 2020-12-18 04:08:07 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.949: INFO: 	Container mds ready: true, restart count 0
Dec 24 02:47:12.949: INFO: rook-ceph-mgr-a-8b957cf9b-9nkkq from rook-ceph started at 2020-12-18 04:07:30 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.949: INFO: 	Container mgr ready: true, restart count 0
Dec 24 02:47:12.949: INFO: rook-ceph-mon-a-77f88c5ff8-hlrmz from rook-ceph started at 2020-12-18 04:06:59 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.949: INFO: 	Container mon ready: true, restart count 0
Dec 24 02:47:12.949: INFO: rook-ceph-osd-0-5897494d6d-k6ml7 from rook-ceph started at 2020-12-18 04:07:39 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.949: INFO: 	Container osd ready: true, restart count 0
Dec 24 02:47:12.949: INFO: rook-ceph-osd-prepare-c1-6-jmhbg from rook-ceph started at 2020-12-18 05:07:18 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.949: INFO: 	Container provision ready: false, restart count 0
Dec 24 02:47:12.949: INFO: rook-discover-4kd85 from rook-ceph started at 2020-12-18 04:06:28 +0000 UTC (1 container statuses recorded)
Dec 24 02:47:12.949: INFO: 	Container rook-discover ready: true, restart count 0
Dec 24 02:47:12.949: INFO: sonobuoy-systemd-logs-daemon-set-eb313291d8024436-79g22 from sonobuoy started at 2020-12-24 02:17:43 +0000 UTC (2 container statuses recorded)
Dec 24 02:47:12.949: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 24 02:47:12.949: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: verifying the node has the label node c1-4
STEP: verifying the node has the label node c1-5
STEP: verifying the node has the label node c1-6
Dec 24 02:47:13.064: INFO: Pod cdi-apiserver-79dbd664dd-jvbpv requesting resource cpu=0m on Node c1-5
Dec 24 02:47:13.064: INFO: Pod cdi-deployment-6c8975f4b-6bk46 requesting resource cpu=0m on Node c1-6
Dec 24 02:47:13.064: INFO: Pod cdi-operator-5d5654db65-5hdrv requesting resource cpu=0m on Node c1-6
Dec 24 02:47:13.064: INFO: Pod cdi-uploadproxy-5765998fff-qh2cb requesting resource cpu=0m on Node c1-6
Dec 24 02:47:13.064: INFO: Pod snapshot-controller-0 requesting resource cpu=0m on Node c1-5
Dec 24 02:47:13.064: INFO: Pod calico-node-5rrck requesting resource cpu=250m on Node c1-6
Dec 24 02:47:13.064: INFO: Pod calico-node-gf9xk requesting resource cpu=250m on Node c1-4
Dec 24 02:47:13.064: INFO: Pod calico-node-tt5dp requesting resource cpu=250m on Node c1-5
Dec 24 02:47:13.064: INFO: Pod kube-proxy-2lcn2 requesting resource cpu=0m on Node c1-5
Dec 24 02:47:13.064: INFO: Pod kube-proxy-jmmvs requesting resource cpu=0m on Node c1-6
Dec 24 02:47:13.064: INFO: Pod kube-proxy-nnl7j requesting resource cpu=0m on Node c1-4
Dec 24 02:47:13.064: INFO: Pod update-demo-nautilus-6t52f requesting resource cpu=0m on Node c1-5
Dec 24 02:47:13.064: INFO: Pod update-demo-nautilus-rnvvd requesting resource cpu=0m on Node c1-4
Dec 24 02:47:13.064: INFO: Pod virt-api-76b44ffc65-b4xmf requesting resource cpu=0m on Node c1-6
Dec 24 02:47:13.064: INFO: Pod virt-api-76b44ffc65-kcmtd requesting resource cpu=0m on Node c1-5
Dec 24 02:47:13.064: INFO: Pod virt-controller-84b5d4b779-krqk6 requesting resource cpu=0m on Node c1-6
Dec 24 02:47:13.064: INFO: Pod virt-controller-84b5d4b779-lbv8p requesting resource cpu=0m on Node c1-5
Dec 24 02:47:13.064: INFO: Pod virt-handler-5nkqw requesting resource cpu=0m on Node c1-4
Dec 24 02:47:13.064: INFO: Pod virt-handler-jk5bd requesting resource cpu=0m on Node c1-6
Dec 24 02:47:13.064: INFO: Pod virt-handler-scjdh requesting resource cpu=0m on Node c1-5
Dec 24 02:47:13.064: INFO: Pod virt-operator-867fcd786d-9xnf6 requesting resource cpu=0m on Node c1-6
Dec 24 02:47:13.064: INFO: Pod virt-operator-867fcd786d-hqk4k requesting resource cpu=0m on Node c1-5
Dec 24 02:47:13.064: INFO: Pod csi-cephfsplugin-ghqns requesting resource cpu=0m on Node c1-4
Dec 24 02:47:13.064: INFO: Pod csi-cephfsplugin-provisioner-d878cdf8b-6g8td requesting resource cpu=0m on Node c1-5
Dec 24 02:47:13.064: INFO: Pod csi-cephfsplugin-provisioner-d878cdf8b-9v92z requesting resource cpu=0m on Node c1-6
Dec 24 02:47:13.064: INFO: Pod csi-cephfsplugin-q5hm7 requesting resource cpu=0m on Node c1-6
Dec 24 02:47:13.064: INFO: Pod csi-cephfsplugin-xd5rd requesting resource cpu=0m on Node c1-5
Dec 24 02:47:13.064: INFO: Pod csi-rbdplugin-drpwm requesting resource cpu=0m on Node c1-5
Dec 24 02:47:13.064: INFO: Pod csi-rbdplugin-provisioner-7588bc4-dxkjr requesting resource cpu=0m on Node c1-5
Dec 24 02:47:13.064: INFO: Pod csi-rbdplugin-provisioner-7588bc4-r297x requesting resource cpu=0m on Node c1-6
Dec 24 02:47:13.064: INFO: Pod csi-rbdplugin-xfdqp requesting resource cpu=0m on Node c1-6
Dec 24 02:47:13.064: INFO: Pod csi-rbdplugin-z74d4 requesting resource cpu=0m on Node c1-4
Dec 24 02:47:13.064: INFO: Pod rook-ceph-crashcollector-c1-4-b5fd6cbf-jrn9q requesting resource cpu=0m on Node c1-4
Dec 24 02:47:13.064: INFO: Pod rook-ceph-crashcollector-c1-4-b5fd6cbf-r86bh requesting resource cpu=0m on Node c1-4
Dec 24 02:47:13.064: INFO: Pod rook-ceph-crashcollector-c1-5-7fdd9898c6-mm2r6 requesting resource cpu=0m on Node c1-5
Dec 24 02:47:13.064: INFO: Pod rook-ceph-crashcollector-c1-6-5868d4fd57-6c5gp requesting resource cpu=0m on Node c1-6
Dec 24 02:47:13.064: INFO: Pod rook-ceph-mds-myfs-a-59cfdfc57f-zb7rk requesting resource cpu=0m on Node c1-6
Dec 24 02:47:13.064: INFO: Pod rook-ceph-mds-myfs-b-6d859f5b5c-r2rqz requesting resource cpu=0m on Node c1-6
Dec 24 02:47:13.064: INFO: Pod rook-ceph-mgr-a-8b957cf9b-9nkkq requesting resource cpu=0m on Node c1-6
Dec 24 02:47:13.064: INFO: Pod rook-ceph-mon-a-77f88c5ff8-hlrmz requesting resource cpu=0m on Node c1-6
Dec 24 02:47:13.064: INFO: Pod rook-ceph-mon-b-6c84dd66-29k4w requesting resource cpu=0m on Node c1-4
Dec 24 02:47:13.064: INFO: Pod rook-ceph-mon-b-6c84dd66-xj8wb requesting resource cpu=0m on Node c1-4
Dec 24 02:47:13.064: INFO: Pod rook-ceph-mon-c-bcf7d945c-mvssc requesting resource cpu=0m on Node c1-5
Dec 24 02:47:13.064: INFO: Pod rook-ceph-operator-775d4b6c5f-khccc requesting resource cpu=0m on Node c1-5
Dec 24 02:47:13.064: INFO: Pod rook-ceph-osd-0-5897494d6d-k6ml7 requesting resource cpu=0m on Node c1-6
Dec 24 02:47:13.064: INFO: Pod rook-ceph-osd-1-59b659d744-k6c5s requesting resource cpu=0m on Node c1-4
Dec 24 02:47:13.064: INFO: Pod rook-ceph-osd-1-59b659d744-nnqc9 requesting resource cpu=0m on Node c1-4
Dec 24 02:47:13.064: INFO: Pod rook-ceph-osd-2-5478c54cc5-s282q requesting resource cpu=0m on Node c1-5
Dec 24 02:47:13.064: INFO: Pod rook-ceph-tools-6967fc698d-n2nzp requesting resource cpu=0m on Node c1-5
Dec 24 02:47:13.064: INFO: Pod rook-discover-4kd85 requesting resource cpu=0m on Node c1-6
Dec 24 02:47:13.064: INFO: Pod rook-discover-ds9s8 requesting resource cpu=0m on Node c1-5
Dec 24 02:47:13.064: INFO: Pod rook-discover-svzsw requesting resource cpu=0m on Node c1-4
Dec 24 02:47:13.064: INFO: Pod sonobuoy requesting resource cpu=0m on Node c1-4
Dec 24 02:47:13.064: INFO: Pod sonobuoy-e2e-job-683f472080954d4f requesting resource cpu=0m on Node c1-4
Dec 24 02:47:13.064: INFO: Pod sonobuoy-systemd-logs-daemon-set-eb313291d8024436-79g22 requesting resource cpu=0m on Node c1-6
Dec 24 02:47:13.064: INFO: Pod sonobuoy-systemd-logs-daemon-set-eb313291d8024436-82n6r requesting resource cpu=0m on Node c1-5
Dec 24 02:47:13.064: INFO: Pod sonobuoy-systemd-logs-daemon-set-eb313291d8024436-cqjvw requesting resource cpu=0m on Node c1-4
STEP: Starting Pods to consume most of the cluster CPU.
Dec 24 02:47:13.064: INFO: Creating a pod which consumes cpu=11025m on Node c1-4
Dec 24 02:47:13.073: INFO: Creating a pod which consumes cpu=11025m on Node c1-5
Dec 24 02:47:13.098: INFO: Creating a pod which consumes cpu=11025m on Node c1-6
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-827dfad0-73fa-4771-b9f0-92167056f1c2.16538718f1cfac74], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3042/filler-pod-827dfad0-73fa-4771-b9f0-92167056f1c2 to c1-5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-827dfad0-73fa-4771-b9f0-92167056f1c2.1653871927cba5bb], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-827dfad0-73fa-4771-b9f0-92167056f1c2.1653871939d0a739], Reason = [Created], Message = [Created container filler-pod-827dfad0-73fa-4771-b9f0-92167056f1c2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-827dfad0-73fa-4771-b9f0-92167056f1c2.165387193c8939e4], Reason = [Started], Message = [Started container filler-pod-827dfad0-73fa-4771-b9f0-92167056f1c2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-aac19438-8ba4-4a79-af39-0b8a85729762.16538718f355843a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3042/filler-pod-aac19438-8ba4-4a79-af39-0b8a85729762 to c1-6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-aac19438-8ba4-4a79-af39-0b8a85729762.1653871928926e3c], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-aac19438-8ba4-4a79-af39-0b8a85729762.1653871936d57879], Reason = [Created], Message = [Created container filler-pod-aac19438-8ba4-4a79-af39-0b8a85729762]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-aac19438-8ba4-4a79-af39-0b8a85729762.16538719392a8f34], Reason = [Started], Message = [Started container filler-pod-aac19438-8ba4-4a79-af39-0b8a85729762]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d8956af0-df07-4f6b-a1d4-3d42bb949e35.16538718f14e8519], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3042/filler-pod-d8956af0-df07-4f6b-a1d4-3d42bb949e35 to c1-4]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d8956af0-df07-4f6b-a1d4-3d42bb949e35.165387192540ded2], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d8956af0-df07-4f6b-a1d4-3d42bb949e35.165387193271bbb9], Reason = [Created], Message = [Created container filler-pod-d8956af0-df07-4f6b-a1d4-3d42bb949e35]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d8956af0-df07-4f6b-a1d4-3d42bb949e35.16538719347b8562], Reason = [Started], Message = [Started container filler-pod-d8956af0-df07-4f6b-a1d4-3d42bb949e35]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.165387196d236409], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 3 Insufficient cpu.]
STEP: removing the label node off the node c1-4
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node c1-5
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node c1-6
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:47:16.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3042" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":305,"completed":85,"skipped":1315,"failed":0}
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:47:16.243: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Dec 24 02:47:16.288: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:47:19.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4672" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":305,"completed":86,"skipped":1316,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:47:19.532: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-835d70c1-856a-4d84-9876-b4ed8ef262b8
STEP: Creating a pod to test consume secrets
Dec 24 02:47:19.614: INFO: Waiting up to 5m0s for pod "pod-secrets-2f8e6935-6c96-452d-95bb-96d532ae1384" in namespace "secrets-774" to be "Succeeded or Failed"
Dec 24 02:47:19.622: INFO: Pod "pod-secrets-2f8e6935-6c96-452d-95bb-96d532ae1384": Phase="Pending", Reason="", readiness=false. Elapsed: 7.890022ms
Dec 24 02:47:21.624: INFO: Pod "pod-secrets-2f8e6935-6c96-452d-95bb-96d532ae1384": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009957513s
STEP: Saw pod success
Dec 24 02:47:21.624: INFO: Pod "pod-secrets-2f8e6935-6c96-452d-95bb-96d532ae1384" satisfied condition "Succeeded or Failed"
Dec 24 02:47:21.628: INFO: Trying to get logs from node c1-4 pod pod-secrets-2f8e6935-6c96-452d-95bb-96d532ae1384 container secret-volume-test: <nil>
STEP: delete the pod
Dec 24 02:47:21.672: INFO: Waiting for pod pod-secrets-2f8e6935-6c96-452d-95bb-96d532ae1384 to disappear
Dec 24 02:47:21.685: INFO: Pod pod-secrets-2f8e6935-6c96-452d-95bb-96d532ae1384 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:47:21.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-774" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":87,"skipped":1325,"failed":0}
S
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:47:21.691: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Dec 24 02:47:21.810: INFO: Waiting up to 5m0s for pod "downward-api-ab4bc66d-620a-4001-9c79-b973b9a2eb20" in namespace "downward-api-6634" to be "Succeeded or Failed"
Dec 24 02:47:21.835: INFO: Pod "downward-api-ab4bc66d-620a-4001-9c79-b973b9a2eb20": Phase="Pending", Reason="", readiness=false. Elapsed: 25.482541ms
Dec 24 02:47:23.838: INFO: Pod "downward-api-ab4bc66d-620a-4001-9c79-b973b9a2eb20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02849442s
STEP: Saw pod success
Dec 24 02:47:23.838: INFO: Pod "downward-api-ab4bc66d-620a-4001-9c79-b973b9a2eb20" satisfied condition "Succeeded or Failed"
Dec 24 02:47:23.841: INFO: Trying to get logs from node c1-4 pod downward-api-ab4bc66d-620a-4001-9c79-b973b9a2eb20 container dapi-container: <nil>
STEP: delete the pod
Dec 24 02:47:23.865: INFO: Waiting for pod downward-api-ab4bc66d-620a-4001-9c79-b973b9a2eb20 to disappear
Dec 24 02:47:23.869: INFO: Pod downward-api-ab4bc66d-620a-4001-9c79-b973b9a2eb20 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:47:23.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6634" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":305,"completed":88,"skipped":1326,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:47:23.877: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:47:23.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2954" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":305,"completed":89,"skipped":1393,"failed":0}

------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:47:23.946: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Dec 24 02:47:24.025: INFO: starting watch
STEP: patching
STEP: updating
Dec 24 02:47:24.034: INFO: waiting for watch events with expected annotations
Dec 24 02:47:24.034: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:47:24.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-854" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":305,"completed":90,"skipped":1393,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:47:24.095: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create deployment with httpd image
Dec 24 02:47:24.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 create -f -'
Dec 24 02:47:24.488: INFO: stderr: ""
Dec 24 02:47:24.488: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Dec 24 02:47:24.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 diff -f -'
Dec 24 02:47:25.046: INFO: rc: 1
Dec 24 02:47:25.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 delete -f -'
Dec 24 02:47:25.162: INFO: stderr: ""
Dec 24 02:47:25.162: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:47:25.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4982" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":305,"completed":91,"skipped":1402,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:47:25.170: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 02:47:25.226: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 24 02:47:29.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 --namespace=crd-publish-openapi-4790 create -f -'
Dec 24 02:47:31.562: INFO: stderr: ""
Dec 24 02:47:31.562: INFO: stdout: "e2e-test-crd-publish-openapi-1061-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 24 02:47:31.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 --namespace=crd-publish-openapi-4790 delete e2e-test-crd-publish-openapi-1061-crds test-cr'
Dec 24 02:47:31.671: INFO: stderr: ""
Dec 24 02:47:31.671: INFO: stdout: "e2e-test-crd-publish-openapi-1061-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Dec 24 02:47:31.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 --namespace=crd-publish-openapi-4790 apply -f -'
Dec 24 02:47:31.995: INFO: stderr: ""
Dec 24 02:47:31.995: INFO: stdout: "e2e-test-crd-publish-openapi-1061-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 24 02:47:31.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 --namespace=crd-publish-openapi-4790 delete e2e-test-crd-publish-openapi-1061-crds test-cr'
Dec 24 02:47:32.100: INFO: stderr: ""
Dec 24 02:47:32.100: INFO: stdout: "e2e-test-crd-publish-openapi-1061-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Dec 24 02:47:32.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 explain e2e-test-crd-publish-openapi-1061-crds'
Dec 24 02:47:32.432: INFO: stderr: ""
Dec 24 02:47:32.432: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1061-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:47:36.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4790" for this suite.

• [SLOW TEST:11.826 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":305,"completed":92,"skipped":1408,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:47:36.996: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 24 02:47:41.131: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 24 02:47:41.169: INFO: Pod pod-with-poststart-http-hook still exists
Dec 24 02:47:43.169: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 24 02:47:43.172: INFO: Pod pod-with-poststart-http-hook still exists
Dec 24 02:47:45.169: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 24 02:47:45.173: INFO: Pod pod-with-poststart-http-hook still exists
Dec 24 02:47:47.169: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 24 02:47:47.173: INFO: Pod pod-with-poststart-http-hook still exists
Dec 24 02:47:49.169: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 24 02:47:49.172: INFO: Pod pod-with-poststart-http-hook still exists
Dec 24 02:47:51.169: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 24 02:47:51.173: INFO: Pod pod-with-poststart-http-hook still exists
Dec 24 02:47:53.169: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 24 02:47:53.172: INFO: Pod pod-with-poststart-http-hook still exists
Dec 24 02:47:55.169: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 24 02:47:55.172: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:47:55.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3970" for this suite.

• [SLOW TEST:18.185 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":305,"completed":93,"skipped":1435,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:47:55.181: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-4ba97490-e0bf-4f20-842e-9b5affb9ce23
STEP: Creating a pod to test consume configMaps
Dec 24 02:47:55.232: INFO: Waiting up to 5m0s for pod "pod-configmaps-745cde68-527d-444b-9613-7d1a928f48be" in namespace "configmap-9077" to be "Succeeded or Failed"
Dec 24 02:47:55.248: INFO: Pod "pod-configmaps-745cde68-527d-444b-9613-7d1a928f48be": Phase="Pending", Reason="", readiness=false. Elapsed: 15.646151ms
Dec 24 02:47:57.250: INFO: Pod "pod-configmaps-745cde68-527d-444b-9613-7d1a928f48be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018266606s
STEP: Saw pod success
Dec 24 02:47:57.250: INFO: Pod "pod-configmaps-745cde68-527d-444b-9613-7d1a928f48be" satisfied condition "Succeeded or Failed"
Dec 24 02:47:57.252: INFO: Trying to get logs from node c1-4 pod pod-configmaps-745cde68-527d-444b-9613-7d1a928f48be container configmap-volume-test: <nil>
STEP: delete the pod
Dec 24 02:47:57.272: INFO: Waiting for pod pod-configmaps-745cde68-527d-444b-9613-7d1a928f48be to disappear
Dec 24 02:47:57.288: INFO: Pod pod-configmaps-745cde68-527d-444b-9613-7d1a928f48be no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:47:57.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9077" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":94,"skipped":1444,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:47:57.323: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 24 02:47:57.369: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bf96ed08-8190-4ab9-ab87-5de2a2e1076a" in namespace "downward-api-891" to be "Succeeded or Failed"
Dec 24 02:47:57.374: INFO: Pod "downwardapi-volume-bf96ed08-8190-4ab9-ab87-5de2a2e1076a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.107455ms
Dec 24 02:47:59.377: INFO: Pod "downwardapi-volume-bf96ed08-8190-4ab9-ab87-5de2a2e1076a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007943938s
STEP: Saw pod success
Dec 24 02:47:59.377: INFO: Pod "downwardapi-volume-bf96ed08-8190-4ab9-ab87-5de2a2e1076a" satisfied condition "Succeeded or Failed"
Dec 24 02:47:59.380: INFO: Trying to get logs from node c1-4 pod downwardapi-volume-bf96ed08-8190-4ab9-ab87-5de2a2e1076a container client-container: <nil>
STEP: delete the pod
Dec 24 02:47:59.421: INFO: Waiting for pod downwardapi-volume-bf96ed08-8190-4ab9-ab87-5de2a2e1076a to disappear
Dec 24 02:47:59.425: INFO: Pod downwardapi-volume-bf96ed08-8190-4ab9-ab87-5de2a2e1076a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:47:59.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-891" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":95,"skipped":1461,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:47:59.433: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service multi-endpoint-test in namespace services-8145
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8145 to expose endpoints map[]
Dec 24 02:47:59.511: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Dec 24 02:48:00.523: INFO: successfully validated that service multi-endpoint-test in namespace services-8145 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-8145
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8145 to expose endpoints map[pod1:[100]]
Dec 24 02:48:03.558: INFO: successfully validated that service multi-endpoint-test in namespace services-8145 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-8145
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8145 to expose endpoints map[pod1:[100] pod2:[101]]
Dec 24 02:48:05.596: INFO: successfully validated that service multi-endpoint-test in namespace services-8145 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-8145
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8145 to expose endpoints map[pod2:[101]]
Dec 24 02:48:05.623: INFO: successfully validated that service multi-endpoint-test in namespace services-8145 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-8145
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8145 to expose endpoints map[]
Dec 24 02:48:05.651: INFO: successfully validated that service multi-endpoint-test in namespace services-8145 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:48:05.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8145" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:6.276 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":305,"completed":96,"skipped":1476,"failed":0}
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:48:05.709: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:48:18.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3876" for this suite.
STEP: Destroying namespace "nsdeletetest-5052" for this suite.
Dec 24 02:48:18.884: INFO: Namespace nsdeletetest-5052 was already deleted
STEP: Destroying namespace "nsdeletetest-4103" for this suite.

• [SLOW TEST:13.178 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":305,"completed":97,"skipped":1476,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:48:18.887: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-5bae5d0b-76cb-4a09-8a9a-6930a32b586e
STEP: Creating a pod to test consume configMaps
Dec 24 02:48:18.959: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5bf9eb34-d232-4557-8ed1-68a7b39f5459" in namespace "projected-619" to be "Succeeded or Failed"
Dec 24 02:48:18.964: INFO: Pod "pod-projected-configmaps-5bf9eb34-d232-4557-8ed1-68a7b39f5459": Phase="Pending", Reason="", readiness=false. Elapsed: 5.618ms
Dec 24 02:48:20.985: INFO: Pod "pod-projected-configmaps-5bf9eb34-d232-4557-8ed1-68a7b39f5459": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026365374s
STEP: Saw pod success
Dec 24 02:48:20.985: INFO: Pod "pod-projected-configmaps-5bf9eb34-d232-4557-8ed1-68a7b39f5459" satisfied condition "Succeeded or Failed"
Dec 24 02:48:20.987: INFO: Trying to get logs from node c1-4 pod pod-projected-configmaps-5bf9eb34-d232-4557-8ed1-68a7b39f5459 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 24 02:48:21.006: INFO: Waiting for pod pod-projected-configmaps-5bf9eb34-d232-4557-8ed1-68a7b39f5459 to disappear
Dec 24 02:48:21.010: INFO: Pod pod-projected-configmaps-5bf9eb34-d232-4557-8ed1-68a7b39f5459 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:48:21.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-619" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":98,"skipped":1487,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:48:21.018: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:163
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:48:21.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1740" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":305,"completed":99,"skipped":1529,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:48:21.106: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 02:48:21.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 create -f - --namespace=kubectl-2973'
Dec 24 02:48:21.595: INFO: stderr: ""
Dec 24 02:48:21.595: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Dec 24 02:48:21.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 create -f - --namespace=kubectl-2973'
Dec 24 02:48:21.880: INFO: stderr: ""
Dec 24 02:48:21.880: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Dec 24 02:48:22.883: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 24 02:48:22.883: INFO: Found 0 / 1
Dec 24 02:48:23.883: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 24 02:48:23.883: INFO: Found 1 / 1
Dec 24 02:48:23.883: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 24 02:48:23.885: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 24 02:48:23.885: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 24 02:48:23.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 describe pod agnhost-primary-4d77q --namespace=kubectl-2973'
Dec 24 02:48:24.022: INFO: stderr: ""
Dec 24 02:48:24.023: INFO: stdout: "Name:         agnhost-primary-4d77q\nNamespace:    kubectl-2973\nPriority:     0\nNode:         c1-4/172.21.3.5\nStart Time:   Thu, 24 Dec 2020 02:48:21 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 10.244.113.73/32\n              cni.projectcalico.org/podIPs: 10.244.113.73/32\nStatus:       Running\nIP:           10.244.113.73\nIPs:\n  IP:           10.244.113.73\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://4cded6e9e51f24c71f82fea2eb2c68d701bbc46767b22707bedb63811e2846f3\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 24 Dec 2020 02:48:22 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-dg2q6 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-dg2q6:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-dg2q6\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-2973/agnhost-primary-4d77q to c1-4\n  Normal  Pulled     2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.20\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Dec 24 02:48:24.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 describe rc agnhost-primary --namespace=kubectl-2973'
Dec 24 02:48:24.128: INFO: stderr: ""
Dec 24 02:48:24.128: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2973\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-4d77q\n"
Dec 24 02:48:24.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 describe service agnhost-primary --namespace=kubectl-2973'
Dec 24 02:48:24.216: INFO: stderr: ""
Dec 24 02:48:24.216: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2973\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP:                10.96.86.53\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.113.73:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec 24 02:48:24.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 describe node c1-3'
Dec 24 02:48:24.347: INFO: stderr: ""
Dec 24 02:48:24.347: INFO: stdout: "Name:               c1-3\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=c1-3\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/crio/crio.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 172.31.3.3/16\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.244.120.0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 17 Dec 2020 09:17:40 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  c1-3\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 24 Dec 2020 02:48:23 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 21 Dec 2020 06:34:24 +0000   Mon, 21 Dec 2020 06:34:24 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Thu, 24 Dec 2020 02:44:38 +0000   Thu, 17 Dec 2020 09:17:40 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 24 Dec 2020 02:44:38 +0000   Thu, 17 Dec 2020 09:17:40 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 24 Dec 2020 02:44:38 +0000   Thu, 17 Dec 2020 09:17:40 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 24 Dec 2020 02:44:38 +0000   Thu, 17 Dec 2020 09:18:01 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  172.21.3.4\n  Hostname:    c1-3\nCapacity:\n  cpu:                16\n  ephemeral-storage:  51175Mi\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             31930128Ki\n  pods:               110\nAllocatable:\n  cpu:                16\n  ephemeral-storage:  48294789041\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             31827728Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 30f6d5e75c834c3a858a2d0d347d0816\n  System UUID:                E1F21AF8-B030-11E6-8289-DCE82AEDB8FA\n  Boot ID:                    d2eaf721-2256-4274-acac-21bb0456f6cc\n  Kernel Version:             3.10.0-1062.el7.x86_64\n  OS Image:                   ProLinux 7\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  cri-o://1.19.0\n  Kubelet Version:            v1.19.4\n  Kube-Proxy Version:         v1.19.4\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-kube-controllers-5c6f6b67db-t6h5f                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         17h\n  kube-system                 calico-node-9cvr2                                          250m (1%)     0 (0%)      0 (0%)           0 (0%)         6d17h\n  kube-system                 coredns-f9fd979d6-5thml                                    100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     6d17h\n  kube-system                 coredns-f9fd979d6-xnspm                                    100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     6d17h\n  kube-system                 etcd-c1-3                                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d17h\n  kube-system                 kube-apiserver-c1-3                                        250m (1%)     0 (0%)      0 (0%)           0 (0%)         6d17h\n  kube-system                 kube-controller-manager-c1-3                               200m (1%)     0 (0%)      0 (0%)           0 (0%)         6d17h\n  kube-system                 kube-proxy-g9npn                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d17h\n  kube-system                 kube-scheduler-c1-3                                        100m (0%)     0 (0%)      0 (0%)           0 (0%)         6d17h\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-eb313291d8024436-bf6bv    0 (0%)        0 (0%)      0 (0%)           0 (0%)         30m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                1 (6%)      0 (0%)\n  memory             140Mi (0%)  340Mi (1%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
Dec 24 02:48:24.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 describe namespace kubectl-2973'
Dec 24 02:48:24.445: INFO: stderr: ""
Dec 24 02:48:24.445: INFO: stdout: "Name:         kubectl-2973\nLabels:       e2e-framework=kubectl\n              e2e-run=b09050e8-3c82-4a73-ae58-9703fde3db88\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:48:24.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2973" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":305,"completed":100,"skipped":1531,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:48:24.452: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-7702
STEP: creating service affinity-nodeport-transition in namespace services-7702
STEP: creating replication controller affinity-nodeport-transition in namespace services-7702
I1224 02:48:24.548824      24 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-7702, replica count: 3
I1224 02:48:27.599246      24 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 24 02:48:27.608: INFO: Creating new exec pod
Dec 24 02:48:32.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-7702 execpod-affinity6nmv5 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Dec 24 02:48:32.814: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Dec 24 02:48:32.814: INFO: stdout: ""
Dec 24 02:48:32.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-7702 execpod-affinity6nmv5 -- /bin/sh -x -c nc -zv -t -w 2 10.96.238.99 80'
Dec 24 02:48:32.981: INFO: stderr: "+ nc -zv -t -w 2 10.96.238.99 80\nConnection to 10.96.238.99 80 port [tcp/http] succeeded!\n"
Dec 24 02:48:32.982: INFO: stdout: ""
Dec 24 02:48:32.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-7702 execpod-affinity6nmv5 -- /bin/sh -x -c nc -zv -t -w 2 172.21.3.6 31875'
Dec 24 02:48:33.146: INFO: stderr: "+ nc -zv -t -w 2 172.21.3.6 31875\nConnection to 172.21.3.6 31875 port [tcp/31875] succeeded!\n"
Dec 24 02:48:33.146: INFO: stdout: ""
Dec 24 02:48:33.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-7702 execpod-affinity6nmv5 -- /bin/sh -x -c nc -zv -t -w 2 172.21.3.5 31875'
Dec 24 02:48:33.315: INFO: stderr: "+ nc -zv -t -w 2 172.21.3.5 31875\nConnection to 172.21.3.5 31875 port [tcp/31875] succeeded!\n"
Dec 24 02:48:33.315: INFO: stdout: ""
Dec 24 02:48:33.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-7702 execpod-affinity6nmv5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.3.5:31875/ ; done'
Dec 24 02:48:33.572: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31875/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31875/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31875/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31875/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31875/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31875/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31875/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31875/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31875/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31875/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31875/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31875/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31875/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31875/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31875/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31875/\n"
Dec 24 02:48:33.572: INFO: stdout: "\naffinity-nodeport-transition-mllc8\naffinity-nodeport-transition-t5b6l\naffinity-nodeport-transition-qkpr4\naffinity-nodeport-transition-mllc8\naffinity-nodeport-transition-t5b6l\naffinity-nodeport-transition-mllc8\naffinity-nodeport-transition-t5b6l\naffinity-nodeport-transition-mllc8\naffinity-nodeport-transition-mllc8\naffinity-nodeport-transition-mllc8\naffinity-nodeport-transition-qkpr4\naffinity-nodeport-transition-qkpr4\naffinity-nodeport-transition-qkpr4\naffinity-nodeport-transition-mllc8\naffinity-nodeport-transition-mllc8\naffinity-nodeport-transition-mllc8"
Dec 24 02:48:33.572: INFO: Received response from host: affinity-nodeport-transition-mllc8
Dec 24 02:48:33.572: INFO: Received response from host: affinity-nodeport-transition-t5b6l
Dec 24 02:48:33.572: INFO: Received response from host: affinity-nodeport-transition-qkpr4
Dec 24 02:48:33.572: INFO: Received response from host: affinity-nodeport-transition-mllc8
Dec 24 02:48:33.572: INFO: Received response from host: affinity-nodeport-transition-t5b6l
Dec 24 02:48:33.572: INFO: Received response from host: affinity-nodeport-transition-mllc8
Dec 24 02:48:33.572: INFO: Received response from host: affinity-nodeport-transition-t5b6l
Dec 24 02:48:33.572: INFO: Received response from host: affinity-nodeport-transition-mllc8
Dec 24 02:48:33.572: INFO: Received response from host: affinity-nodeport-transition-mllc8
Dec 24 02:48:33.572: INFO: Received response from host: affinity-nodeport-transition-mllc8
Dec 24 02:48:33.572: INFO: Received response from host: affinity-nodeport-transition-qkpr4
Dec 24 02:48:33.572: INFO: Received response from host: affinity-nodeport-transition-qkpr4
Dec 24 02:48:33.572: INFO: Received response from host: affinity-nodeport-transition-qkpr4
Dec 24 02:48:33.572: INFO: Received response from host: affinity-nodeport-transition-mllc8
Dec 24 02:48:33.572: INFO: Received response from host: affinity-nodeport-transition-mllc8
Dec 24 02:48:33.572: INFO: Received response from host: affinity-nodeport-transition-mllc8
Dec 24 02:48:33.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-7702 execpod-affinity6nmv5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.3.5:31875/ ; done'
Dec 24 02:48:33.833: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31875/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31875/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31875/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31875/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31875/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31875/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31875/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31875/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31875/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31875/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31875/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31875/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31875/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31875/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31875/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31875/\n"
Dec 24 02:48:33.833: INFO: stdout: "\naffinity-nodeport-transition-t5b6l\naffinity-nodeport-transition-t5b6l\naffinity-nodeport-transition-t5b6l\naffinity-nodeport-transition-t5b6l\naffinity-nodeport-transition-t5b6l\naffinity-nodeport-transition-t5b6l\naffinity-nodeport-transition-t5b6l\naffinity-nodeport-transition-t5b6l\naffinity-nodeport-transition-t5b6l\naffinity-nodeport-transition-t5b6l\naffinity-nodeport-transition-t5b6l\naffinity-nodeport-transition-t5b6l\naffinity-nodeport-transition-t5b6l\naffinity-nodeport-transition-t5b6l\naffinity-nodeport-transition-t5b6l\naffinity-nodeport-transition-t5b6l"
Dec 24 02:48:33.833: INFO: Received response from host: affinity-nodeport-transition-t5b6l
Dec 24 02:48:33.833: INFO: Received response from host: affinity-nodeport-transition-t5b6l
Dec 24 02:48:33.833: INFO: Received response from host: affinity-nodeport-transition-t5b6l
Dec 24 02:48:33.833: INFO: Received response from host: affinity-nodeport-transition-t5b6l
Dec 24 02:48:33.833: INFO: Received response from host: affinity-nodeport-transition-t5b6l
Dec 24 02:48:33.833: INFO: Received response from host: affinity-nodeport-transition-t5b6l
Dec 24 02:48:33.833: INFO: Received response from host: affinity-nodeport-transition-t5b6l
Dec 24 02:48:33.833: INFO: Received response from host: affinity-nodeport-transition-t5b6l
Dec 24 02:48:33.833: INFO: Received response from host: affinity-nodeport-transition-t5b6l
Dec 24 02:48:33.833: INFO: Received response from host: affinity-nodeport-transition-t5b6l
Dec 24 02:48:33.833: INFO: Received response from host: affinity-nodeport-transition-t5b6l
Dec 24 02:48:33.833: INFO: Received response from host: affinity-nodeport-transition-t5b6l
Dec 24 02:48:33.833: INFO: Received response from host: affinity-nodeport-transition-t5b6l
Dec 24 02:48:33.833: INFO: Received response from host: affinity-nodeport-transition-t5b6l
Dec 24 02:48:33.833: INFO: Received response from host: affinity-nodeport-transition-t5b6l
Dec 24 02:48:33.833: INFO: Received response from host: affinity-nodeport-transition-t5b6l
Dec 24 02:48:33.833: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-7702, will wait for the garbage collector to delete the pods
Dec 24 02:48:33.916: INFO: Deleting ReplicationController affinity-nodeport-transition took: 4.638759ms
Dec 24 02:48:34.016: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.17058ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:48:48.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7702" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:24.519 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":101,"skipped":1586,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:48:48.972: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-7a45b2e6-9ec7-44e6-84bd-3da365f86113
STEP: Creating a pod to test consume secrets
Dec 24 02:48:49.037: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-146c1ee7-f579-4345-9702-0369934d07d1" in namespace "projected-2873" to be "Succeeded or Failed"
Dec 24 02:48:49.041: INFO: Pod "pod-projected-secrets-146c1ee7-f579-4345-9702-0369934d07d1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.493695ms
Dec 24 02:48:51.044: INFO: Pod "pod-projected-secrets-146c1ee7-f579-4345-9702-0369934d07d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007184498s
STEP: Saw pod success
Dec 24 02:48:51.044: INFO: Pod "pod-projected-secrets-146c1ee7-f579-4345-9702-0369934d07d1" satisfied condition "Succeeded or Failed"
Dec 24 02:48:51.046: INFO: Trying to get logs from node c1-4 pod pod-projected-secrets-146c1ee7-f579-4345-9702-0369934d07d1 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 24 02:48:51.074: INFO: Waiting for pod pod-projected-secrets-146c1ee7-f579-4345-9702-0369934d07d1 to disappear
Dec 24 02:48:51.082: INFO: Pod pod-projected-secrets-146c1ee7-f579-4345-9702-0369934d07d1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:48:51.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2873" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":102,"skipped":1590,"failed":0}
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:48:51.089: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:48:51.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3892" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":305,"completed":103,"skipped":1600,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:48:51.232: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 02:48:51.277: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Dec 24 02:48:53.319: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:48:54.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6474" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":305,"completed":104,"skipped":1607,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:48:54.352: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Dec 24 02:48:54.437: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 24 02:49:54.523: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Dec 24 02:49:54.557: INFO: Created pod: pod0-sched-preemption-low-priority
Dec 24 02:49:54.583: INFO: Created pod: pod1-sched-preemption-medium-priority
Dec 24 02:49:54.606: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:50:26.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3518" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:92.414 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":305,"completed":105,"skipped":1610,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:50:26.767: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 02:50:26.826: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:50:27.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5515" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":305,"completed":106,"skipped":1612,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:50:28.015: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-4eb6f4be-73c4-4db9-b4a3-28c9a9d2a585
STEP: Creating a pod to test consume configMaps
Dec 24 02:50:28.116: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8a42372a-0bb8-4f57-8444-1c7617c745ab" in namespace "projected-8627" to be "Succeeded or Failed"
Dec 24 02:50:28.135: INFO: Pod "pod-projected-configmaps-8a42372a-0bb8-4f57-8444-1c7617c745ab": Phase="Pending", Reason="", readiness=false. Elapsed: 18.834603ms
Dec 24 02:50:30.137: INFO: Pod "pod-projected-configmaps-8a42372a-0bb8-4f57-8444-1c7617c745ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021120589s
STEP: Saw pod success
Dec 24 02:50:30.137: INFO: Pod "pod-projected-configmaps-8a42372a-0bb8-4f57-8444-1c7617c745ab" satisfied condition "Succeeded or Failed"
Dec 24 02:50:30.138: INFO: Trying to get logs from node c1-4 pod pod-projected-configmaps-8a42372a-0bb8-4f57-8444-1c7617c745ab container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 24 02:50:30.167: INFO: Waiting for pod pod-projected-configmaps-8a42372a-0bb8-4f57-8444-1c7617c745ab to disappear
Dec 24 02:50:30.171: INFO: Pod pod-projected-configmaps-8a42372a-0bb8-4f57-8444-1c7617c745ab no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:50:30.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8627" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":107,"skipped":1614,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:50:30.177: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 24 02:50:30.224: INFO: Waiting up to 5m0s for pod "pod-81969af8-ef33-4682-a512-7b3022bb5146" in namespace "emptydir-9107" to be "Succeeded or Failed"
Dec 24 02:50:30.261: INFO: Pod "pod-81969af8-ef33-4682-a512-7b3022bb5146": Phase="Pending", Reason="", readiness=false. Elapsed: 37.030005ms
Dec 24 02:50:32.264: INFO: Pod "pod-81969af8-ef33-4682-a512-7b3022bb5146": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03964811s
STEP: Saw pod success
Dec 24 02:50:32.264: INFO: Pod "pod-81969af8-ef33-4682-a512-7b3022bb5146" satisfied condition "Succeeded or Failed"
Dec 24 02:50:32.265: INFO: Trying to get logs from node c1-4 pod pod-81969af8-ef33-4682-a512-7b3022bb5146 container test-container: <nil>
STEP: delete the pod
Dec 24 02:50:32.333: INFO: Waiting for pod pod-81969af8-ef33-4682-a512-7b3022bb5146 to disappear
Dec 24 02:50:32.337: INFO: Pod pod-81969af8-ef33-4682-a512-7b3022bb5146 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:50:32.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9107" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":108,"skipped":1614,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:50:32.344: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 24 02:50:32.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-7485'
Dec 24 02:50:32.467: INFO: stderr: ""
Dec 24 02:50:32.467: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Dec 24 02:50:32.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pod e2e-test-httpd-pod -o json --namespace=kubectl-7485'
Dec 24 02:50:32.549: INFO: stderr: ""
Dec 24 02:50:32.549: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2020-12-24T02:50:32Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-12-24T02:50:32Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:message\": {},\n                                \"f:reason\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:message\": {},\n                                \"f:reason\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-12-24T02:50:32Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-7485\",\n        \"resourceVersion\": \"3445447\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-7485/pods/e2e-test-httpd-pod\",\n        \"uid\": \"5ed87e27-ff62-4434-b9fd-081c36252d14\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-j7rbc\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"c1-4\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-j7rbc\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-j7rbc\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-12-24T02:50:32Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-12-24T02:50:32Z\",\n                \"message\": \"containers with unready status: [e2e-test-httpd-pod]\",\n                \"reason\": \"ContainersNotReady\",\n                \"status\": \"False\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-12-24T02:50:32Z\",\n                \"message\": \"containers with unready status: [e2e-test-httpd-pod]\",\n                \"reason\": \"ContainersNotReady\",\n                \"status\": \"False\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-12-24T02:50:32Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": false,\n                \"restartCount\": 0,\n                \"started\": false,\n                \"state\": {\n                    \"waiting\": {\n                        \"reason\": \"ContainerCreating\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.21.3.5\",\n        \"phase\": \"Pending\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-12-24T02:50:32Z\"\n    }\n}\n"
Dec 24 02:50:32.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 replace -f - --dry-run server --namespace=kubectl-7485'
Dec 24 02:50:32.954: INFO: stderr: "W1224 02:50:32.608253    2249 helpers.go:553] --dry-run is deprecated and can be replaced with --dry-run=client.\n"
Dec 24 02:50:32.954: INFO: stdout: "pod/e2e-test-httpd-pod replaced (dry run)\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Dec 24 02:50:32.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 delete pods e2e-test-httpd-pod --namespace=kubectl-7485'
Dec 24 02:50:43.235: INFO: stderr: ""
Dec 24 02:50:43.235: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:50:43.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7485" for this suite.

• [SLOW TEST:10.899 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:919
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":305,"completed":109,"skipped":1615,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:50:43.243: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 24 02:50:47.352: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 24 02:50:47.362: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 24 02:50:49.362: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 24 02:50:49.373: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 24 02:50:51.362: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 24 02:50:51.365: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 24 02:50:53.362: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 24 02:50:53.365: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:50:53.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2978" for this suite.

• [SLOW TEST:10.131 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":305,"completed":110,"skipped":1622,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:50:53.374: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-2032
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Dec 24 02:50:53.482: INFO: Found 0 stateful pods, waiting for 3
Dec 24 02:51:03.485: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 24 02:51:03.485: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 24 02:51:03.485: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Dec 24 02:51:03.508: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Dec 24 02:51:13.540: INFO: Updating stateful set ss2
Dec 24 02:51:13.564: INFO: Waiting for Pod statefulset-2032/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Dec 24 02:51:23.666: INFO: Found 2 stateful pods, waiting for 3
Dec 24 02:51:33.670: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 24 02:51:33.670: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 24 02:51:33.670: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Dec 24 02:51:33.692: INFO: Updating stateful set ss2
Dec 24 02:51:33.722: INFO: Waiting for Pod statefulset-2032/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 24 02:51:43.750: INFO: Updating stateful set ss2
Dec 24 02:51:43.757: INFO: Waiting for StatefulSet statefulset-2032/ss2 to complete update
Dec 24 02:51:43.757: INFO: Waiting for Pod statefulset-2032/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Dec 24 02:51:53.770: INFO: Deleting all statefulset in ns statefulset-2032
Dec 24 02:51:53.772: INFO: Scaling statefulset ss2 to 0
Dec 24 02:52:13.790: INFO: Waiting for statefulset status.replicas updated to 0
Dec 24 02:52:13.792: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:52:13.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2032" for this suite.

• [SLOW TEST:80.455 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":305,"completed":111,"skipped":1643,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:52:13.829: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-projected-all-test-volume-1fe73102-855d-4db3-84a9-9c6332b54e55
STEP: Creating secret with name secret-projected-all-test-volume-62f7126e-c8bf-4222-b009-762f8b75894a
STEP: Creating a pod to test Check all projections for projected volume plugin
Dec 24 02:52:13.888: INFO: Waiting up to 5m0s for pod "projected-volume-5414df17-a45a-40a2-9e16-59a7891153b1" in namespace "projected-9508" to be "Succeeded or Failed"
Dec 24 02:52:13.892: INFO: Pod "projected-volume-5414df17-a45a-40a2-9e16-59a7891153b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.097268ms
Dec 24 02:52:15.895: INFO: Pod "projected-volume-5414df17-a45a-40a2-9e16-59a7891153b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006904703s
Dec 24 02:52:17.898: INFO: Pod "projected-volume-5414df17-a45a-40a2-9e16-59a7891153b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009735328s
STEP: Saw pod success
Dec 24 02:52:17.898: INFO: Pod "projected-volume-5414df17-a45a-40a2-9e16-59a7891153b1" satisfied condition "Succeeded or Failed"
Dec 24 02:52:17.900: INFO: Trying to get logs from node c1-4 pod projected-volume-5414df17-a45a-40a2-9e16-59a7891153b1 container projected-all-volume-test: <nil>
STEP: delete the pod
Dec 24 02:52:17.928: INFO: Waiting for pod projected-volume-5414df17-a45a-40a2-9e16-59a7891153b1 to disappear
Dec 24 02:52:17.955: INFO: Pod projected-volume-5414df17-a45a-40a2-9e16-59a7891153b1 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:52:17.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9508" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":305,"completed":112,"skipped":1648,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:52:17.963: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 24 02:52:21.043: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:52:21.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6487" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":113,"skipped":1661,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:52:21.092: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 02:52:25.237: INFO: Waiting up to 5m0s for pod "client-envvars-59878c8d-b79a-4f0e-b2aa-d0719480753c" in namespace "pods-6335" to be "Succeeded or Failed"
Dec 24 02:52:25.257: INFO: Pod "client-envvars-59878c8d-b79a-4f0e-b2aa-d0719480753c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.086355ms
Dec 24 02:52:27.259: INFO: Pod "client-envvars-59878c8d-b79a-4f0e-b2aa-d0719480753c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022469338s
STEP: Saw pod success
Dec 24 02:52:27.259: INFO: Pod "client-envvars-59878c8d-b79a-4f0e-b2aa-d0719480753c" satisfied condition "Succeeded or Failed"
Dec 24 02:52:27.261: INFO: Trying to get logs from node c1-4 pod client-envvars-59878c8d-b79a-4f0e-b2aa-d0719480753c container env3cont: <nil>
STEP: delete the pod
Dec 24 02:52:27.278: INFO: Waiting for pod client-envvars-59878c8d-b79a-4f0e-b2aa-d0719480753c to disappear
Dec 24 02:52:27.296: INFO: Pod client-envvars-59878c8d-b79a-4f0e-b2aa-d0719480753c no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:52:27.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6335" for this suite.

• [SLOW TEST:6.214 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":305,"completed":114,"skipped":1678,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:52:27.306: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 24 02:52:27.365: INFO: Waiting up to 5m0s for pod "downwardapi-volume-15db5110-c173-4bd2-a532-d6357ef5c0ff" in namespace "projected-1850" to be "Succeeded or Failed"
Dec 24 02:52:27.368: INFO: Pod "downwardapi-volume-15db5110-c173-4bd2-a532-d6357ef5c0ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.773931ms
Dec 24 02:52:29.384: INFO: Pod "downwardapi-volume-15db5110-c173-4bd2-a532-d6357ef5c0ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018472188s
STEP: Saw pod success
Dec 24 02:52:29.384: INFO: Pod "downwardapi-volume-15db5110-c173-4bd2-a532-d6357ef5c0ff" satisfied condition "Succeeded or Failed"
Dec 24 02:52:29.386: INFO: Trying to get logs from node c1-4 pod downwardapi-volume-15db5110-c173-4bd2-a532-d6357ef5c0ff container client-container: <nil>
STEP: delete the pod
Dec 24 02:52:29.422: INFO: Waiting for pod downwardapi-volume-15db5110-c173-4bd2-a532-d6357ef5c0ff to disappear
Dec 24 02:52:29.425: INFO: Pod downwardapi-volume-15db5110-c173-4bd2-a532-d6357ef5c0ff no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:52:29.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1850" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":115,"skipped":1682,"failed":0}
S
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:52:29.432: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-6493
STEP: creating service affinity-clusterip in namespace services-6493
STEP: creating replication controller affinity-clusterip in namespace services-6493
I1224 02:52:29.529303      24 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-6493, replica count: 3
I1224 02:52:32.580056      24 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 24 02:52:32.583: INFO: Creating new exec pod
Dec 24 02:52:35.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-6493 execpod-affinityv5zng -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Dec 24 02:52:35.802: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Dec 24 02:52:35.802: INFO: stdout: ""
Dec 24 02:52:35.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-6493 execpod-affinityv5zng -- /bin/sh -x -c nc -zv -t -w 2 10.96.235.188 80'
Dec 24 02:52:35.985: INFO: stderr: "+ nc -zv -t -w 2 10.96.235.188 80\nConnection to 10.96.235.188 80 port [tcp/http] succeeded!\n"
Dec 24 02:52:35.985: INFO: stdout: ""
Dec 24 02:52:35.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-6493 execpod-affinityv5zng -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.235.188:80/ ; done'
Dec 24 02:52:36.236: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.235.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.235.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.235.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.235.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.235.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.235.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.235.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.235.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.235.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.235.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.235.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.235.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.235.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.235.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.235.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.235.188:80/\n"
Dec 24 02:52:36.236: INFO: stdout: "\naffinity-clusterip-9w4pm\naffinity-clusterip-9w4pm\naffinity-clusterip-9w4pm\naffinity-clusterip-9w4pm\naffinity-clusterip-9w4pm\naffinity-clusterip-9w4pm\naffinity-clusterip-9w4pm\naffinity-clusterip-9w4pm\naffinity-clusterip-9w4pm\naffinity-clusterip-9w4pm\naffinity-clusterip-9w4pm\naffinity-clusterip-9w4pm\naffinity-clusterip-9w4pm\naffinity-clusterip-9w4pm\naffinity-clusterip-9w4pm\naffinity-clusterip-9w4pm"
Dec 24 02:52:36.236: INFO: Received response from host: affinity-clusterip-9w4pm
Dec 24 02:52:36.236: INFO: Received response from host: affinity-clusterip-9w4pm
Dec 24 02:52:36.236: INFO: Received response from host: affinity-clusterip-9w4pm
Dec 24 02:52:36.236: INFO: Received response from host: affinity-clusterip-9w4pm
Dec 24 02:52:36.236: INFO: Received response from host: affinity-clusterip-9w4pm
Dec 24 02:52:36.236: INFO: Received response from host: affinity-clusterip-9w4pm
Dec 24 02:52:36.236: INFO: Received response from host: affinity-clusterip-9w4pm
Dec 24 02:52:36.236: INFO: Received response from host: affinity-clusterip-9w4pm
Dec 24 02:52:36.236: INFO: Received response from host: affinity-clusterip-9w4pm
Dec 24 02:52:36.236: INFO: Received response from host: affinity-clusterip-9w4pm
Dec 24 02:52:36.236: INFO: Received response from host: affinity-clusterip-9w4pm
Dec 24 02:52:36.236: INFO: Received response from host: affinity-clusterip-9w4pm
Dec 24 02:52:36.236: INFO: Received response from host: affinity-clusterip-9w4pm
Dec 24 02:52:36.236: INFO: Received response from host: affinity-clusterip-9w4pm
Dec 24 02:52:36.236: INFO: Received response from host: affinity-clusterip-9w4pm
Dec 24 02:52:36.236: INFO: Received response from host: affinity-clusterip-9w4pm
Dec 24 02:52:36.236: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-6493, will wait for the garbage collector to delete the pods
Dec 24 02:52:36.310: INFO: Deleting ReplicationController affinity-clusterip took: 4.624082ms
Dec 24 02:52:36.410: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.186418ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:52:49.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6493" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:20.436 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":116,"skipped":1683,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:52:49.868: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service endpoint-test2 in namespace services-3329
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3329 to expose endpoints map[]
Dec 24 02:52:49.956: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Dec 24 02:52:50.989: INFO: successfully validated that service endpoint-test2 in namespace services-3329 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3329
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3329 to expose endpoints map[pod1:[80]]
Dec 24 02:52:53.028: INFO: successfully validated that service endpoint-test2 in namespace services-3329 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-3329
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3329 to expose endpoints map[pod1:[80] pod2:[80]]
Dec 24 02:52:55.074: INFO: successfully validated that service endpoint-test2 in namespace services-3329 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-3329
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3329 to expose endpoints map[pod2:[80]]
Dec 24 02:52:55.131: INFO: successfully validated that service endpoint-test2 in namespace services-3329 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-3329
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3329 to expose endpoints map[]
Dec 24 02:52:55.177: INFO: successfully validated that service endpoint-test2 in namespace services-3329 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:52:55.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3329" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:5.371 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":305,"completed":117,"skipped":1702,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:52:55.239: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
Dec 24 02:52:55.285: INFO: created test-event-1
Dec 24 02:52:55.291: INFO: created test-event-2
Dec 24 02:52:55.297: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Dec 24 02:52:55.315: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Dec 24 02:52:55.333: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:52:55.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6659" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":305,"completed":118,"skipped":1743,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:52:55.355: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 24 02:52:55.401: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b8dfa464-d36a-4b87-8034-f9ba3695de4d" in namespace "downward-api-9361" to be "Succeeded or Failed"
Dec 24 02:52:55.423: INFO: Pod "downwardapi-volume-b8dfa464-d36a-4b87-8034-f9ba3695de4d": Phase="Pending", Reason="", readiness=false. Elapsed: 22.240278ms
Dec 24 02:52:57.426: INFO: Pod "downwardapi-volume-b8dfa464-d36a-4b87-8034-f9ba3695de4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024759413s
STEP: Saw pod success
Dec 24 02:52:57.426: INFO: Pod "downwardapi-volume-b8dfa464-d36a-4b87-8034-f9ba3695de4d" satisfied condition "Succeeded or Failed"
Dec 24 02:52:57.428: INFO: Trying to get logs from node c1-4 pod downwardapi-volume-b8dfa464-d36a-4b87-8034-f9ba3695de4d container client-container: <nil>
STEP: delete the pod
Dec 24 02:52:57.464: INFO: Waiting for pod downwardapi-volume-b8dfa464-d36a-4b87-8034-f9ba3695de4d to disappear
Dec 24 02:52:57.468: INFO: Pod downwardapi-volume-b8dfa464-d36a-4b87-8034-f9ba3695de4d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:52:57.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9361" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":119,"skipped":1768,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:52:57.475: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-ff452611-67df-4b82-b9d4-0b2116b34acb
STEP: Creating a pod to test consume configMaps
Dec 24 02:52:57.554: INFO: Waiting up to 5m0s for pod "pod-configmaps-23f6ce90-24d2-4cf2-9932-af63d6bba68a" in namespace "configmap-5696" to be "Succeeded or Failed"
Dec 24 02:52:57.594: INFO: Pod "pod-configmaps-23f6ce90-24d2-4cf2-9932-af63d6bba68a": Phase="Pending", Reason="", readiness=false. Elapsed: 39.640266ms
Dec 24 02:52:59.597: INFO: Pod "pod-configmaps-23f6ce90-24d2-4cf2-9932-af63d6bba68a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.042566562s
STEP: Saw pod success
Dec 24 02:52:59.597: INFO: Pod "pod-configmaps-23f6ce90-24d2-4cf2-9932-af63d6bba68a" satisfied condition "Succeeded or Failed"
Dec 24 02:52:59.599: INFO: Trying to get logs from node c1-4 pod pod-configmaps-23f6ce90-24d2-4cf2-9932-af63d6bba68a container configmap-volume-test: <nil>
STEP: delete the pod
Dec 24 02:52:59.625: INFO: Waiting for pod pod-configmaps-23f6ce90-24d2-4cf2-9932-af63d6bba68a to disappear
Dec 24 02:52:59.629: INFO: Pod pod-configmaps-23f6ce90-24d2-4cf2-9932-af63d6bba68a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:52:59.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5696" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":120,"skipped":1770,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:52:59.636: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:53:02.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5391" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":305,"completed":121,"skipped":1777,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:53:02.735: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 24 02:53:02.836: INFO: Waiting up to 5m0s for pod "downwardapi-volume-05660c74-ba55-4c85-8aef-83801a635862" in namespace "downward-api-6699" to be "Succeeded or Failed"
Dec 24 02:53:02.841: INFO: Pod "downwardapi-volume-05660c74-ba55-4c85-8aef-83801a635862": Phase="Pending", Reason="", readiness=false. Elapsed: 5.669423ms
Dec 24 02:53:04.844: INFO: Pod "downwardapi-volume-05660c74-ba55-4c85-8aef-83801a635862": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008929082s
STEP: Saw pod success
Dec 24 02:53:04.845: INFO: Pod "downwardapi-volume-05660c74-ba55-4c85-8aef-83801a635862" satisfied condition "Succeeded or Failed"
Dec 24 02:53:04.847: INFO: Trying to get logs from node c1-4 pod downwardapi-volume-05660c74-ba55-4c85-8aef-83801a635862 container client-container: <nil>
STEP: delete the pod
Dec 24 02:53:04.886: INFO: Waiting for pod downwardapi-volume-05660c74-ba55-4c85-8aef-83801a635862 to disappear
Dec 24 02:53:04.893: INFO: Pod downwardapi-volume-05660c74-ba55-4c85-8aef-83801a635862 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:53:04.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6699" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":122,"skipped":1801,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:53:04.900: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 24 02:53:04.940: INFO: Waiting up to 5m0s for pod "pod-d8f95e32-cf40-4dfb-80d3-8f226eb5b550" in namespace "emptydir-6884" to be "Succeeded or Failed"
Dec 24 02:53:04.961: INFO: Pod "pod-d8f95e32-cf40-4dfb-80d3-8f226eb5b550": Phase="Pending", Reason="", readiness=false. Elapsed: 21.54283ms
Dec 24 02:53:06.968: INFO: Pod "pod-d8f95e32-cf40-4dfb-80d3-8f226eb5b550": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02786373s
STEP: Saw pod success
Dec 24 02:53:06.968: INFO: Pod "pod-d8f95e32-cf40-4dfb-80d3-8f226eb5b550" satisfied condition "Succeeded or Failed"
Dec 24 02:53:06.970: INFO: Trying to get logs from node c1-4 pod pod-d8f95e32-cf40-4dfb-80d3-8f226eb5b550 container test-container: <nil>
STEP: delete the pod
Dec 24 02:53:06.985: INFO: Waiting for pod pod-d8f95e32-cf40-4dfb-80d3-8f226eb5b550 to disappear
Dec 24 02:53:06.990: INFO: Pod pod-d8f95e32-cf40-4dfb-80d3-8f226eb5b550 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:53:06.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6884" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":123,"skipped":1808,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:53:06.997: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1224 02:53:13.120353      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Dec 24 02:54:15.135: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:54:15.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7671" for this suite.

• [SLOW TEST:68.150 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":305,"completed":124,"skipped":1816,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:54:15.148: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec 24 02:54:19.248: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 24 02:54:19.275: INFO: Pod pod-with-prestop-http-hook still exists
Dec 24 02:54:21.275: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 24 02:54:21.278: INFO: Pod pod-with-prestop-http-hook still exists
Dec 24 02:54:23.275: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 24 02:54:23.279: INFO: Pod pod-with-prestop-http-hook still exists
Dec 24 02:54:25.275: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 24 02:54:25.279: INFO: Pod pod-with-prestop-http-hook still exists
Dec 24 02:54:27.275: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 24 02:54:27.278: INFO: Pod pod-with-prestop-http-hook still exists
Dec 24 02:54:29.275: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 24 02:54:29.279: INFO: Pod pod-with-prestop-http-hook still exists
Dec 24 02:54:31.275: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 24 02:54:31.278: INFO: Pod pod-with-prestop-http-hook still exists
Dec 24 02:54:33.275: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 24 02:54:33.278: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:54:33.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7603" for this suite.

• [SLOW TEST:18.142 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":305,"completed":125,"skipped":1897,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:54:33.290: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 24 02:54:33.752: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 24 02:54:35.759: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744375273, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744375273, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744375273, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744375273, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 24 02:54:38.781: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:54:38.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5246" for this suite.
STEP: Destroying namespace "webhook-5246-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.717 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":305,"completed":126,"skipped":1903,"failed":0}
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:54:39.008: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:55:01.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1637" for this suite.

• [SLOW TEST:22.419 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":305,"completed":127,"skipped":1907,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:55:01.427: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Dec 24 02:55:01.478: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:55:07.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6820" for this suite.

• [SLOW TEST:6.190 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":305,"completed":128,"skipped":1952,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:55:07.617: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should delete a collection of pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pods
Dec 24 02:55:07.663: INFO: created test-pod-1
Dec 24 02:55:07.666: INFO: created test-pod-2
Dec 24 02:55:07.685: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:55:07.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6313" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":305,"completed":129,"skipped":1981,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:55:07.821: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 24 02:55:07.896: INFO: Waiting up to 5m0s for pod "downwardapi-volume-57c73dd3-4261-45c2-bbbb-c07087067d6d" in namespace "downward-api-4573" to be "Succeeded or Failed"
Dec 24 02:55:07.901: INFO: Pod "downwardapi-volume-57c73dd3-4261-45c2-bbbb-c07087067d6d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.386492ms
Dec 24 02:55:09.903: INFO: Pod "downwardapi-volume-57c73dd3-4261-45c2-bbbb-c07087067d6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00719582s
STEP: Saw pod success
Dec 24 02:55:09.904: INFO: Pod "downwardapi-volume-57c73dd3-4261-45c2-bbbb-c07087067d6d" satisfied condition "Succeeded or Failed"
Dec 24 02:55:09.906: INFO: Trying to get logs from node c1-4 pod downwardapi-volume-57c73dd3-4261-45c2-bbbb-c07087067d6d container client-container: <nil>
STEP: delete the pod
Dec 24 02:55:09.932: INFO: Waiting for pod downwardapi-volume-57c73dd3-4261-45c2-bbbb-c07087067d6d to disappear
Dec 24 02:55:09.935: INFO: Pod downwardapi-volume-57c73dd3-4261-45c2-bbbb-c07087067d6d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:55:09.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4573" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":305,"completed":130,"skipped":1992,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:55:09.942: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 02:55:10.027: INFO: The status of Pod test-webserver-28a8d21d-b31b-4ab7-81fa-68191b40741e is Pending, waiting for it to be Running (with Ready = true)
Dec 24 02:55:12.042: INFO: The status of Pod test-webserver-28a8d21d-b31b-4ab7-81fa-68191b40741e is Running (Ready = false)
Dec 24 02:55:14.030: INFO: The status of Pod test-webserver-28a8d21d-b31b-4ab7-81fa-68191b40741e is Running (Ready = false)
Dec 24 02:55:16.030: INFO: The status of Pod test-webserver-28a8d21d-b31b-4ab7-81fa-68191b40741e is Running (Ready = false)
Dec 24 02:55:18.030: INFO: The status of Pod test-webserver-28a8d21d-b31b-4ab7-81fa-68191b40741e is Running (Ready = false)
Dec 24 02:55:20.030: INFO: The status of Pod test-webserver-28a8d21d-b31b-4ab7-81fa-68191b40741e is Running (Ready = false)
Dec 24 02:55:22.036: INFO: The status of Pod test-webserver-28a8d21d-b31b-4ab7-81fa-68191b40741e is Running (Ready = false)
Dec 24 02:55:24.030: INFO: The status of Pod test-webserver-28a8d21d-b31b-4ab7-81fa-68191b40741e is Running (Ready = false)
Dec 24 02:55:26.030: INFO: The status of Pod test-webserver-28a8d21d-b31b-4ab7-81fa-68191b40741e is Running (Ready = false)
Dec 24 02:55:28.030: INFO: The status of Pod test-webserver-28a8d21d-b31b-4ab7-81fa-68191b40741e is Running (Ready = false)
Dec 24 02:55:30.030: INFO: The status of Pod test-webserver-28a8d21d-b31b-4ab7-81fa-68191b40741e is Running (Ready = false)
Dec 24 02:55:32.047: INFO: The status of Pod test-webserver-28a8d21d-b31b-4ab7-81fa-68191b40741e is Running (Ready = false)
Dec 24 02:55:34.030: INFO: The status of Pod test-webserver-28a8d21d-b31b-4ab7-81fa-68191b40741e is Running (Ready = false)
Dec 24 02:55:36.030: INFO: The status of Pod test-webserver-28a8d21d-b31b-4ab7-81fa-68191b40741e is Running (Ready = true)
Dec 24 02:55:36.033: INFO: Container started at 2020-12-24 02:55:11 +0000 UTC, pod became ready at 2020-12-24 02:55:34 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:55:36.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3846" for this suite.

• [SLOW TEST:26.098 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":305,"completed":131,"skipped":2014,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:55:36.041: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 02:55:36.082: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:55:38.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1077" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":305,"completed":132,"skipped":2054,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:55:38.189: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W1224 02:56:18.318138      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Dec 24 02:57:20.332: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Dec 24 02:57:20.332: INFO: Deleting pod "simpletest.rc-62nrl" in namespace "gc-7729"
Dec 24 02:57:20.385: INFO: Deleting pod "simpletest.rc-6775x" in namespace "gc-7729"
Dec 24 02:57:20.420: INFO: Deleting pod "simpletest.rc-6pj9s" in namespace "gc-7729"
Dec 24 02:57:20.454: INFO: Deleting pod "simpletest.rc-cjwgm" in namespace "gc-7729"
Dec 24 02:57:20.496: INFO: Deleting pod "simpletest.rc-kwq4g" in namespace "gc-7729"
Dec 24 02:57:20.539: INFO: Deleting pod "simpletest.rc-p5ptz" in namespace "gc-7729"
Dec 24 02:57:20.574: INFO: Deleting pod "simpletest.rc-pg5n9" in namespace "gc-7729"
Dec 24 02:57:20.616: INFO: Deleting pod "simpletest.rc-rfkq8" in namespace "gc-7729"
Dec 24 02:57:20.648: INFO: Deleting pod "simpletest.rc-t482v" in namespace "gc-7729"
Dec 24 02:57:20.682: INFO: Deleting pod "simpletest.rc-v5wnv" in namespace "gc-7729"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:57:20.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7729" for this suite.

• [SLOW TEST:102.552 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":305,"completed":133,"skipped":2072,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:57:20.741: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 24 02:57:20.808: INFO: Waiting up to 5m0s for pod "pod-a7b7cd01-3440-45d7-9026-c6b7c3e26ae1" in namespace "emptydir-1362" to be "Succeeded or Failed"
Dec 24 02:57:20.855: INFO: Pod "pod-a7b7cd01-3440-45d7-9026-c6b7c3e26ae1": Phase="Pending", Reason="", readiness=false. Elapsed: 46.765381ms
Dec 24 02:57:22.882: INFO: Pod "pod-a7b7cd01-3440-45d7-9026-c6b7c3e26ae1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.073938702s
STEP: Saw pod success
Dec 24 02:57:22.882: INFO: Pod "pod-a7b7cd01-3440-45d7-9026-c6b7c3e26ae1" satisfied condition "Succeeded or Failed"
Dec 24 02:57:22.884: INFO: Trying to get logs from node c1-4 pod pod-a7b7cd01-3440-45d7-9026-c6b7c3e26ae1 container test-container: <nil>
STEP: delete the pod
Dec 24 02:57:22.917: INFO: Waiting for pod pod-a7b7cd01-3440-45d7-9026-c6b7c3e26ae1 to disappear
Dec 24 02:57:22.923: INFO: Pod pod-a7b7cd01-3440-45d7-9026-c6b7c3e26ae1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:57:22.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1362" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":134,"skipped":2083,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:57:22.930: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 24 02:57:22.976: INFO: Waiting up to 5m0s for pod "pod-00010aea-ad2a-4a9a-8c76-4ea927e04ed1" in namespace "emptydir-301" to be "Succeeded or Failed"
Dec 24 02:57:23.010: INFO: Pod "pod-00010aea-ad2a-4a9a-8c76-4ea927e04ed1": Phase="Pending", Reason="", readiness=false. Elapsed: 33.310965ms
Dec 24 02:57:25.014: INFO: Pod "pod-00010aea-ad2a-4a9a-8c76-4ea927e04ed1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.037402024s
STEP: Saw pod success
Dec 24 02:57:25.014: INFO: Pod "pod-00010aea-ad2a-4a9a-8c76-4ea927e04ed1" satisfied condition "Succeeded or Failed"
Dec 24 02:57:25.016: INFO: Trying to get logs from node c1-4 pod pod-00010aea-ad2a-4a9a-8c76-4ea927e04ed1 container test-container: <nil>
STEP: delete the pod
Dec 24 02:57:25.066: INFO: Waiting for pod pod-00010aea-ad2a-4a9a-8c76-4ea927e04ed1 to disappear
Dec 24 02:57:25.072: INFO: Pod pod-00010aea-ad2a-4a9a-8c76-4ea927e04ed1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:57:25.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-301" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":135,"skipped":2094,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:57:25.079: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-9527bc94-ce05-49b2-a337-6aa3a0189c46
STEP: Creating a pod to test consume configMaps
Dec 24 02:57:25.154: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d3a1cda7-3ea7-45ce-a4fa-e168426b6365" in namespace "projected-5032" to be "Succeeded or Failed"
Dec 24 02:57:25.174: INFO: Pod "pod-projected-configmaps-d3a1cda7-3ea7-45ce-a4fa-e168426b6365": Phase="Pending", Reason="", readiness=false. Elapsed: 19.998571ms
Dec 24 02:57:27.177: INFO: Pod "pod-projected-configmaps-d3a1cda7-3ea7-45ce-a4fa-e168426b6365": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022692818s
STEP: Saw pod success
Dec 24 02:57:27.177: INFO: Pod "pod-projected-configmaps-d3a1cda7-3ea7-45ce-a4fa-e168426b6365" satisfied condition "Succeeded or Failed"
Dec 24 02:57:27.179: INFO: Trying to get logs from node c1-4 pod pod-projected-configmaps-d3a1cda7-3ea7-45ce-a4fa-e168426b6365 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 24 02:57:27.204: INFO: Waiting for pod pod-projected-configmaps-d3a1cda7-3ea7-45ce-a4fa-e168426b6365 to disappear
Dec 24 02:57:27.221: INFO: Pod pod-projected-configmaps-d3a1cda7-3ea7-45ce-a4fa-e168426b6365 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:57:27.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5032" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":136,"skipped":2104,"failed":0}
S
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:57:27.244: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7606 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7606;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7606 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7606;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7606.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7606.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7606.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7606.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7606.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7606.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7606.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7606.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7606.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7606.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7606.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7606.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7606.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 71.175.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.175.71_udp@PTR;check="$$(dig +tcp +noall +answer +search 71.175.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.175.71_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7606 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7606;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7606 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7606;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7606.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7606.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7606.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7606.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7606.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7606.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7606.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7606.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7606.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7606.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7606.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7606.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7606.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 71.175.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.175.71_udp@PTR;check="$$(dig +tcp +noall +answer +search 71.175.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.175.71_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 24 02:57:31.389: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:31.392: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:31.394: INFO: Unable to read wheezy_udp@dns-test-service.dns-7606 from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:31.397: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7606 from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:31.400: INFO: Unable to read wheezy_udp@dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:31.403: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:31.405: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:31.408: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:31.427: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:31.429: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:31.444: INFO: Unable to read jessie_udp@dns-test-service.dns-7606 from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:31.447: INFO: Unable to read jessie_tcp@dns-test-service.dns-7606 from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:31.449: INFO: Unable to read jessie_udp@dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:31.452: INFO: Unable to read jessie_tcp@dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:31.455: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:31.457: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:31.472: INFO: Lookups using dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7606 wheezy_tcp@dns-test-service.dns-7606 wheezy_udp@dns-test-service.dns-7606.svc wheezy_tcp@dns-test-service.dns-7606.svc wheezy_udp@_http._tcp.dns-test-service.dns-7606.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7606.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7606 jessie_tcp@dns-test-service.dns-7606 jessie_udp@dns-test-service.dns-7606.svc jessie_tcp@dns-test-service.dns-7606.svc jessie_udp@_http._tcp.dns-test-service.dns-7606.svc jessie_tcp@_http._tcp.dns-test-service.dns-7606.svc]

Dec 24 02:57:36.476: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:36.479: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:36.482: INFO: Unable to read wheezy_udp@dns-test-service.dns-7606 from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:36.485: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7606 from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:36.488: INFO: Unable to read wheezy_udp@dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:36.492: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:36.495: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:36.497: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:36.530: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:36.533: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:36.535: INFO: Unable to read jessie_udp@dns-test-service.dns-7606 from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:36.538: INFO: Unable to read jessie_tcp@dns-test-service.dns-7606 from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:36.541: INFO: Unable to read jessie_udp@dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:36.543: INFO: Unable to read jessie_tcp@dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:36.546: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:36.549: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:36.567: INFO: Lookups using dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7606 wheezy_tcp@dns-test-service.dns-7606 wheezy_udp@dns-test-service.dns-7606.svc wheezy_tcp@dns-test-service.dns-7606.svc wheezy_udp@_http._tcp.dns-test-service.dns-7606.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7606.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7606 jessie_tcp@dns-test-service.dns-7606 jessie_udp@dns-test-service.dns-7606.svc jessie_tcp@dns-test-service.dns-7606.svc jessie_udp@_http._tcp.dns-test-service.dns-7606.svc jessie_tcp@_http._tcp.dns-test-service.dns-7606.svc]

Dec 24 02:57:41.476: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:41.479: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:41.482: INFO: Unable to read wheezy_udp@dns-test-service.dns-7606 from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:41.485: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7606 from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:41.488: INFO: Unable to read wheezy_udp@dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:41.491: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:41.494: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:41.497: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:41.518: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:41.521: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:41.524: INFO: Unable to read jessie_udp@dns-test-service.dns-7606 from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:41.527: INFO: Unable to read jessie_tcp@dns-test-service.dns-7606 from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:41.529: INFO: Unable to read jessie_udp@dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:41.532: INFO: Unable to read jessie_tcp@dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:41.535: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:41.537: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:41.554: INFO: Lookups using dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7606 wheezy_tcp@dns-test-service.dns-7606 wheezy_udp@dns-test-service.dns-7606.svc wheezy_tcp@dns-test-service.dns-7606.svc wheezy_udp@_http._tcp.dns-test-service.dns-7606.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7606.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7606 jessie_tcp@dns-test-service.dns-7606 jessie_udp@dns-test-service.dns-7606.svc jessie_tcp@dns-test-service.dns-7606.svc jessie_udp@_http._tcp.dns-test-service.dns-7606.svc jessie_tcp@_http._tcp.dns-test-service.dns-7606.svc]

Dec 24 02:57:46.476: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:46.480: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:46.483: INFO: Unable to read wheezy_udp@dns-test-service.dns-7606 from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:46.486: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7606 from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:46.489: INFO: Unable to read wheezy_udp@dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:46.492: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:46.495: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:46.498: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:46.518: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:46.521: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:46.523: INFO: Unable to read jessie_udp@dns-test-service.dns-7606 from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:46.526: INFO: Unable to read jessie_tcp@dns-test-service.dns-7606 from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:46.528: INFO: Unable to read jessie_udp@dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:46.531: INFO: Unable to read jessie_tcp@dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:46.534: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:46.536: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:46.552: INFO: Lookups using dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7606 wheezy_tcp@dns-test-service.dns-7606 wheezy_udp@dns-test-service.dns-7606.svc wheezy_tcp@dns-test-service.dns-7606.svc wheezy_udp@_http._tcp.dns-test-service.dns-7606.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7606.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7606 jessie_tcp@dns-test-service.dns-7606 jessie_udp@dns-test-service.dns-7606.svc jessie_tcp@dns-test-service.dns-7606.svc jessie_udp@_http._tcp.dns-test-service.dns-7606.svc jessie_tcp@_http._tcp.dns-test-service.dns-7606.svc]

Dec 24 02:57:51.476: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:51.479: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:51.482: INFO: Unable to read wheezy_udp@dns-test-service.dns-7606 from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:51.485: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7606 from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:51.488: INFO: Unable to read wheezy_udp@dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:51.491: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:51.494: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:51.497: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:51.517: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:51.526: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:51.531: INFO: Unable to read jessie_udp@dns-test-service.dns-7606 from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:51.552: INFO: Unable to read jessie_tcp@dns-test-service.dns-7606 from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:51.560: INFO: Unable to read jessie_udp@dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:51.563: INFO: Unable to read jessie_tcp@dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:51.565: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:51.568: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:51.583: INFO: Lookups using dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7606 wheezy_tcp@dns-test-service.dns-7606 wheezy_udp@dns-test-service.dns-7606.svc wheezy_tcp@dns-test-service.dns-7606.svc wheezy_udp@_http._tcp.dns-test-service.dns-7606.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7606.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7606 jessie_tcp@dns-test-service.dns-7606 jessie_udp@dns-test-service.dns-7606.svc jessie_tcp@dns-test-service.dns-7606.svc jessie_udp@_http._tcp.dns-test-service.dns-7606.svc jessie_tcp@_http._tcp.dns-test-service.dns-7606.svc]

Dec 24 02:57:56.476: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:56.479: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:56.482: INFO: Unable to read wheezy_udp@dns-test-service.dns-7606 from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:56.485: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7606 from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:56.488: INFO: Unable to read wheezy_udp@dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:56.491: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:56.494: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:56.497: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:56.541: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:56.544: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:56.546: INFO: Unable to read jessie_udp@dns-test-service.dns-7606 from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:56.549: INFO: Unable to read jessie_tcp@dns-test-service.dns-7606 from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:56.552: INFO: Unable to read jessie_udp@dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:56.554: INFO: Unable to read jessie_tcp@dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:56.557: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:56.559: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7606.svc from pod dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14: the server could not find the requested resource (get pods dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14)
Dec 24 02:57:56.576: INFO: Lookups using dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7606 wheezy_tcp@dns-test-service.dns-7606 wheezy_udp@dns-test-service.dns-7606.svc wheezy_tcp@dns-test-service.dns-7606.svc wheezy_udp@_http._tcp.dns-test-service.dns-7606.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7606.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7606 jessie_tcp@dns-test-service.dns-7606 jessie_udp@dns-test-service.dns-7606.svc jessie_tcp@dns-test-service.dns-7606.svc jessie_udp@_http._tcp.dns-test-service.dns-7606.svc jessie_tcp@_http._tcp.dns-test-service.dns-7606.svc]

Dec 24 02:58:01.564: INFO: DNS probes using dns-7606/dns-test-8af6594f-27fc-428f-ae0d-6f84de16af14 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:58:01.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7606" for this suite.

• [SLOW TEST:34.522 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":305,"completed":137,"skipped":2105,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:58:01.766: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 02:58:01.837: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Dec 24 02:58:01.850: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 02:58:01.857: INFO: Number of nodes with available pods: 0
Dec 24 02:58:01.857: INFO: Node c1-4 is running more than one daemon pod
Dec 24 02:58:02.862: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 02:58:02.865: INFO: Number of nodes with available pods: 0
Dec 24 02:58:02.865: INFO: Node c1-4 is running more than one daemon pod
Dec 24 02:58:03.879: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 02:58:03.882: INFO: Number of nodes with available pods: 2
Dec 24 02:58:03.882: INFO: Node c1-4 is running more than one daemon pod
Dec 24 02:58:04.862: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 02:58:04.865: INFO: Number of nodes with available pods: 3
Dec 24 02:58:04.865: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Dec 24 02:58:04.910: INFO: Wrong image for pod: daemon-set-c58hs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:04.910: INFO: Wrong image for pod: daemon-set-mbcsn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:04.910: INFO: Wrong image for pod: daemon-set-qjn6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:04.931: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 02:58:05.935: INFO: Wrong image for pod: daemon-set-c58hs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:05.935: INFO: Wrong image for pod: daemon-set-mbcsn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:05.936: INFO: Wrong image for pod: daemon-set-qjn6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:05.940: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 02:58:06.934: INFO: Wrong image for pod: daemon-set-c58hs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:06.934: INFO: Wrong image for pod: daemon-set-mbcsn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:06.934: INFO: Wrong image for pod: daemon-set-qjn6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:06.939: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 02:58:07.934: INFO: Wrong image for pod: daemon-set-c58hs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:07.934: INFO: Pod daemon-set-c58hs is not available
Dec 24 02:58:07.934: INFO: Wrong image for pod: daemon-set-mbcsn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:07.934: INFO: Wrong image for pod: daemon-set-qjn6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:07.939: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 02:58:08.935: INFO: Wrong image for pod: daemon-set-c58hs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:08.935: INFO: Pod daemon-set-c58hs is not available
Dec 24 02:58:08.935: INFO: Wrong image for pod: daemon-set-mbcsn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:08.935: INFO: Wrong image for pod: daemon-set-qjn6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:08.939: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 02:58:09.934: INFO: Wrong image for pod: daemon-set-c58hs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:09.934: INFO: Pod daemon-set-c58hs is not available
Dec 24 02:58:09.934: INFO: Wrong image for pod: daemon-set-mbcsn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:09.934: INFO: Wrong image for pod: daemon-set-qjn6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:09.939: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 02:58:10.934: INFO: Wrong image for pod: daemon-set-c58hs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:10.934: INFO: Pod daemon-set-c58hs is not available
Dec 24 02:58:10.934: INFO: Wrong image for pod: daemon-set-mbcsn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:10.934: INFO: Wrong image for pod: daemon-set-qjn6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:10.939: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 02:58:11.934: INFO: Wrong image for pod: daemon-set-c58hs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:11.934: INFO: Pod daemon-set-c58hs is not available
Dec 24 02:58:11.934: INFO: Wrong image for pod: daemon-set-mbcsn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:11.934: INFO: Wrong image for pod: daemon-set-qjn6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:11.938: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 02:58:12.934: INFO: Wrong image for pod: daemon-set-c58hs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:12.934: INFO: Pod daemon-set-c58hs is not available
Dec 24 02:58:12.934: INFO: Wrong image for pod: daemon-set-mbcsn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:12.934: INFO: Wrong image for pod: daemon-set-qjn6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:12.939: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 02:58:13.941: INFO: Pod daemon-set-7zxgw is not available
Dec 24 02:58:13.941: INFO: Wrong image for pod: daemon-set-mbcsn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:13.941: INFO: Wrong image for pod: daemon-set-qjn6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:13.945: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 02:58:14.934: INFO: Pod daemon-set-7zxgw is not available
Dec 24 02:58:14.934: INFO: Wrong image for pod: daemon-set-mbcsn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:14.934: INFO: Wrong image for pod: daemon-set-qjn6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:14.938: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 02:58:15.963: INFO: Wrong image for pod: daemon-set-mbcsn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:15.963: INFO: Wrong image for pod: daemon-set-qjn6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:15.963: INFO: Pod daemon-set-qjn6j is not available
Dec 24 02:58:15.968: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 02:58:16.934: INFO: Wrong image for pod: daemon-set-mbcsn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:16.934: INFO: Wrong image for pod: daemon-set-qjn6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:16.935: INFO: Pod daemon-set-qjn6j is not available
Dec 24 02:58:16.939: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 02:58:17.934: INFO: Wrong image for pod: daemon-set-mbcsn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:17.934: INFO: Wrong image for pod: daemon-set-qjn6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:17.934: INFO: Pod daemon-set-qjn6j is not available
Dec 24 02:58:17.939: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 02:58:18.935: INFO: Wrong image for pod: daemon-set-mbcsn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:18.935: INFO: Wrong image for pod: daemon-set-qjn6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:18.935: INFO: Pod daemon-set-qjn6j is not available
Dec 24 02:58:18.939: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 02:58:19.934: INFO: Pod daemon-set-9kwc7 is not available
Dec 24 02:58:19.934: INFO: Wrong image for pod: daemon-set-mbcsn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:19.938: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 02:58:20.934: INFO: Pod daemon-set-9kwc7 is not available
Dec 24 02:58:20.934: INFO: Wrong image for pod: daemon-set-mbcsn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:20.939: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 02:58:21.934: INFO: Wrong image for pod: daemon-set-mbcsn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:21.938: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 02:58:22.935: INFO: Wrong image for pod: daemon-set-mbcsn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Dec 24 02:58:22.935: INFO: Pod daemon-set-mbcsn is not available
Dec 24 02:58:22.939: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 02:58:23.945: INFO: Pod daemon-set-2tdqd is not available
Dec 24 02:58:23.960: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Dec 24 02:58:23.965: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 02:58:23.967: INFO: Number of nodes with available pods: 2
Dec 24 02:58:23.967: INFO: Node c1-5 is running more than one daemon pod
Dec 24 02:58:24.972: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 02:58:24.975: INFO: Number of nodes with available pods: 2
Dec 24 02:58:24.975: INFO: Node c1-5 is running more than one daemon pod
Dec 24 02:58:25.972: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 02:58:25.975: INFO: Number of nodes with available pods: 3
Dec 24 02:58:25.975: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5409, will wait for the garbage collector to delete the pods
Dec 24 02:58:26.055: INFO: Deleting DaemonSet.extensions daemon-set took: 8.732697ms
Dec 24 02:58:27.056: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.000214319s
Dec 24 02:58:39.858: INFO: Number of nodes with available pods: 0
Dec 24 02:58:39.858: INFO: Number of running nodes: 0, number of available pods: 0
Dec 24 02:58:39.860: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5409/daemonsets","resourceVersion":"3450766"},"items":null}

Dec 24 02:58:39.862: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5409/pods","resourceVersion":"3450766"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:58:39.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5409" for this suite.

• [SLOW TEST:38.115 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":305,"completed":138,"skipped":2131,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:58:39.882: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Dec 24 02:58:44.477: INFO: Successfully updated pod "labelsupdate10037927-c155-4f50-b814-5c2e36471060"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:58:46.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7643" for this suite.

• [SLOW TEST:6.627 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":139,"skipped":2139,"failed":0}
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:58:46.509: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 24 02:58:46.595: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 02:58:46.597: INFO: Number of nodes with available pods: 0
Dec 24 02:58:46.597: INFO: Node c1-4 is running more than one daemon pod
Dec 24 02:58:47.602: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 02:58:47.605: INFO: Number of nodes with available pods: 0
Dec 24 02:58:47.605: INFO: Node c1-4 is running more than one daemon pod
Dec 24 02:58:48.602: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 02:58:48.605: INFO: Number of nodes with available pods: 2
Dec 24 02:58:48.605: INFO: Node c1-5 is running more than one daemon pod
Dec 24 02:58:49.603: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 02:58:49.606: INFO: Number of nodes with available pods: 3
Dec 24 02:58:49.606: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Dec 24 02:58:49.630: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 02:58:49.633: INFO: Number of nodes with available pods: 3
Dec 24 02:58:49.633: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4379, will wait for the garbage collector to delete the pods
Dec 24 02:58:50.707: INFO: Deleting DaemonSet.extensions daemon-set took: 5.036326ms
Dec 24 02:58:51.707: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.000245715s
Dec 24 02:58:59.818: INFO: Number of nodes with available pods: 0
Dec 24 02:58:59.818: INFO: Number of running nodes: 0, number of available pods: 0
Dec 24 02:58:59.820: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4379/daemonsets","resourceVersion":"3451069"},"items":null}

Dec 24 02:58:59.822: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4379/pods","resourceVersion":"3451069"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:58:59.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4379" for this suite.

• [SLOW TEST:13.332 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":305,"completed":140,"skipped":2139,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:58:59.841: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 24 02:59:00.371: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 24 02:59:03.394: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:59:03.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3880" for this suite.
STEP: Destroying namespace "webhook-3880-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":305,"completed":141,"skipped":2140,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:59:03.567: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Dec 24 02:59:03.654: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:59:27.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-886" for this suite.

• [SLOW TEST:24.374 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":305,"completed":142,"skipped":2151,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:59:27.942: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 24 02:59:27.984: INFO: Waiting up to 5m0s for pod "downwardapi-volume-640fe89b-7b51-43f0-b07a-ac2ae12318ed" in namespace "projected-6657" to be "Succeeded or Failed"
Dec 24 02:59:28.000: INFO: Pod "downwardapi-volume-640fe89b-7b51-43f0-b07a-ac2ae12318ed": Phase="Pending", Reason="", readiness=false. Elapsed: 15.601411ms
Dec 24 02:59:30.003: INFO: Pod "downwardapi-volume-640fe89b-7b51-43f0-b07a-ac2ae12318ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01848942s
Dec 24 02:59:32.006: INFO: Pod "downwardapi-volume-640fe89b-7b51-43f0-b07a-ac2ae12318ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021361842s
STEP: Saw pod success
Dec 24 02:59:32.006: INFO: Pod "downwardapi-volume-640fe89b-7b51-43f0-b07a-ac2ae12318ed" satisfied condition "Succeeded or Failed"
Dec 24 02:59:32.008: INFO: Trying to get logs from node c1-4 pod downwardapi-volume-640fe89b-7b51-43f0-b07a-ac2ae12318ed container client-container: <nil>
STEP: delete the pod
Dec 24 02:59:32.025: INFO: Waiting for pod downwardapi-volume-640fe89b-7b51-43f0-b07a-ac2ae12318ed to disappear
Dec 24 02:59:32.029: INFO: Pod downwardapi-volume-640fe89b-7b51-43f0-b07a-ac2ae12318ed no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:59:32.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6657" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":143,"skipped":2160,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:59:32.037: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Dec 24 02:59:32.121: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2103 /api/v1/namespaces/watch-2103/configmaps/e2e-watch-test-watch-closed 8553a3ef-9601-4cb1-9d7d-4f70c9ebe84c 3451423 0 2020-12-24 02:59:32 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-12-24 02:59:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 24 02:59:32.121: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2103 /api/v1/namespaces/watch-2103/configmaps/e2e-watch-test-watch-closed 8553a3ef-9601-4cb1-9d7d-4f70c9ebe84c 3451425 0 2020-12-24 02:59:32 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-12-24 02:59:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Dec 24 02:59:32.134: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2103 /api/v1/namespaces/watch-2103/configmaps/e2e-watch-test-watch-closed 8553a3ef-9601-4cb1-9d7d-4f70c9ebe84c 3451426 0 2020-12-24 02:59:32 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-12-24 02:59:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 24 02:59:32.134: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2103 /api/v1/namespaces/watch-2103/configmaps/e2e-watch-test-watch-closed 8553a3ef-9601-4cb1-9d7d-4f70c9ebe84c 3451427 0 2020-12-24 02:59:32 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-12-24 02:59:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 02:59:32.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2103" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":305,"completed":144,"skipped":2170,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 02:59:32.144: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 03:01:32.228: INFO: Deleting pod "var-expansion-717af7cc-6653-4725-aacc-2c73c0c656f3" in namespace "var-expansion-9726"
Dec 24 03:01:32.231: INFO: Wait up to 5m0s for pod "var-expansion-717af7cc-6653-4725-aacc-2c73c0c656f3" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:01:34.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9726" for this suite.

• [SLOW TEST:122.108 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":305,"completed":145,"skipped":2240,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:01:34.253: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 24 03:01:34.301: INFO: Waiting up to 5m0s for pod "pod-cbb3773a-c092-4116-a125-3c3d19703d3b" in namespace "emptydir-654" to be "Succeeded or Failed"
Dec 24 03:01:34.305: INFO: Pod "pod-cbb3773a-c092-4116-a125-3c3d19703d3b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.206651ms
Dec 24 03:01:36.311: INFO: Pod "pod-cbb3773a-c092-4116-a125-3c3d19703d3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009800525s
STEP: Saw pod success
Dec 24 03:01:36.311: INFO: Pod "pod-cbb3773a-c092-4116-a125-3c3d19703d3b" satisfied condition "Succeeded or Failed"
Dec 24 03:01:36.313: INFO: Trying to get logs from node c1-4 pod pod-cbb3773a-c092-4116-a125-3c3d19703d3b container test-container: <nil>
STEP: delete the pod
Dec 24 03:01:36.341: INFO: Waiting for pod pod-cbb3773a-c092-4116-a125-3c3d19703d3b to disappear
Dec 24 03:01:36.346: INFO: Pod pod-cbb3773a-c092-4116-a125-3c3d19703d3b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:01:36.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-654" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":146,"skipped":2261,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:01:36.366: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:01:36.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-725" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":305,"completed":147,"skipped":2273,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:01:36.496: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:01:36.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8505" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":305,"completed":148,"skipped":2309,"failed":0}
SSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:01:36.612: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-14cba870-bf26-4e7c-a9d0-4a0e0c684d38 in namespace container-probe-4736
Dec 24 03:01:38.685: INFO: Started pod busybox-14cba870-bf26-4e7c-a9d0-4a0e0c684d38 in namespace container-probe-4736
STEP: checking the pod's current state and verifying that restartCount is present
Dec 24 03:01:38.687: INFO: Initial restart count of pod busybox-14cba870-bf26-4e7c-a9d0-4a0e0c684d38 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:05:39.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4736" for this suite.

• [SLOW TEST:242.611 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":149,"skipped":2314,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:05:39.224: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-834c3143-ab1c-4807-a982-0981135a5ef9
STEP: Creating a pod to test consume secrets
Dec 24 03:05:39.314: INFO: Waiting up to 5m0s for pod "pod-secrets-8b56fd91-411f-4699-b198-b4773014588f" in namespace "secrets-9534" to be "Succeeded or Failed"
Dec 24 03:05:39.320: INFO: Pod "pod-secrets-8b56fd91-411f-4699-b198-b4773014588f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.731193ms
Dec 24 03:05:41.323: INFO: Pod "pod-secrets-8b56fd91-411f-4699-b198-b4773014588f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008283514s
STEP: Saw pod success
Dec 24 03:05:41.323: INFO: Pod "pod-secrets-8b56fd91-411f-4699-b198-b4773014588f" satisfied condition "Succeeded or Failed"
Dec 24 03:05:41.325: INFO: Trying to get logs from node c1-4 pod pod-secrets-8b56fd91-411f-4699-b198-b4773014588f container secret-volume-test: <nil>
STEP: delete the pod
Dec 24 03:05:41.361: INFO: Waiting for pod pod-secrets-8b56fd91-411f-4699-b198-b4773014588f to disappear
Dec 24 03:05:41.366: INFO: Pod pod-secrets-8b56fd91-411f-4699-b198-b4773014588f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:05:41.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9534" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":150,"skipped":2324,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:05:41.373: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Dec 24 03:05:41.439: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:05:44.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8316" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":305,"completed":151,"skipped":2343,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:05:44.803: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating secret secrets-4983/secret-test-a9ef7ae0-0f59-402f-a9ea-ec57dd97a4ca
STEP: Creating a pod to test consume secrets
Dec 24 03:05:44.854: INFO: Waiting up to 5m0s for pod "pod-configmaps-e1949e18-91da-4f81-9f18-6a64fba6aa2d" in namespace "secrets-4983" to be "Succeeded or Failed"
Dec 24 03:05:44.858: INFO: Pod "pod-configmaps-e1949e18-91da-4f81-9f18-6a64fba6aa2d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012038ms
Dec 24 03:05:46.860: INFO: Pod "pod-configmaps-e1949e18-91da-4f81-9f18-6a64fba6aa2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006443908s
STEP: Saw pod success
Dec 24 03:05:46.860: INFO: Pod "pod-configmaps-e1949e18-91da-4f81-9f18-6a64fba6aa2d" satisfied condition "Succeeded or Failed"
Dec 24 03:05:46.863: INFO: Trying to get logs from node c1-4 pod pod-configmaps-e1949e18-91da-4f81-9f18-6a64fba6aa2d container env-test: <nil>
STEP: delete the pod
Dec 24 03:05:46.893: INFO: Waiting for pod pod-configmaps-e1949e18-91da-4f81-9f18-6a64fba6aa2d to disappear
Dec 24 03:05:46.898: INFO: Pod pod-configmaps-e1949e18-91da-4f81-9f18-6a64fba6aa2d no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:05:46.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4983" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":152,"skipped":2347,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:05:46.904: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:05:57.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6042" for this suite.

• [SLOW TEST:11.088 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":305,"completed":153,"skipped":2357,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:05:57.993: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Dec 24 03:05:58.469: INFO: Pod name wrapped-volume-race-77ff5bc9-451b-4a62-91fb-647679a54cb1: Found 0 pods out of 5
Dec 24 03:06:03.476: INFO: Pod name wrapped-volume-race-77ff5bc9-451b-4a62-91fb-647679a54cb1: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-77ff5bc9-451b-4a62-91fb-647679a54cb1 in namespace emptydir-wrapper-3299, will wait for the garbage collector to delete the pods
Dec 24 03:06:13.552: INFO: Deleting ReplicationController wrapped-volume-race-77ff5bc9-451b-4a62-91fb-647679a54cb1 took: 5.175146ms
Dec 24 03:06:14.652: INFO: Terminating ReplicationController wrapped-volume-race-77ff5bc9-451b-4a62-91fb-647679a54cb1 pods took: 1.100235306s
STEP: Creating RC which spawns configmap-volume pods
Dec 24 03:06:28.979: INFO: Pod name wrapped-volume-race-d82a4465-f6bf-4e48-9027-554234c5ce67: Found 0 pods out of 5
Dec 24 03:06:33.985: INFO: Pod name wrapped-volume-race-d82a4465-f6bf-4e48-9027-554234c5ce67: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d82a4465-f6bf-4e48-9027-554234c5ce67 in namespace emptydir-wrapper-3299, will wait for the garbage collector to delete the pods
Dec 24 03:06:44.079: INFO: Deleting ReplicationController wrapped-volume-race-d82a4465-f6bf-4e48-9027-554234c5ce67 took: 7.871265ms
Dec 24 03:06:45.079: INFO: Terminating ReplicationController wrapped-volume-race-d82a4465-f6bf-4e48-9027-554234c5ce67 pods took: 1.000197563s
STEP: Creating RC which spawns configmap-volume pods
Dec 24 03:06:53.500: INFO: Pod name wrapped-volume-race-6d89d166-bdea-4aee-bccc-501b463d56f0: Found 0 pods out of 5
Dec 24 03:06:58.508: INFO: Pod name wrapped-volume-race-6d89d166-bdea-4aee-bccc-501b463d56f0: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-6d89d166-bdea-4aee-bccc-501b463d56f0 in namespace emptydir-wrapper-3299, will wait for the garbage collector to delete the pods
Dec 24 03:07:08.589: INFO: Deleting ReplicationController wrapped-volume-race-6d89d166-bdea-4aee-bccc-501b463d56f0 took: 5.171986ms
Dec 24 03:07:09.590: INFO: Terminating ReplicationController wrapped-volume-race-6d89d166-bdea-4aee-bccc-501b463d56f0 pods took: 1.000201405s
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:07:19.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3299" for this suite.

• [SLOW TEST:81.518 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":305,"completed":154,"skipped":2366,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:07:19.512: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Dec 24 03:07:20.182: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Dec 24 03:07:22.190: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376040, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376040, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376040, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376040, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 24 03:07:25.234: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 03:07:25.250: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:07:26.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5924" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.070 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":305,"completed":155,"skipped":2406,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:07:26.582: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 24 03:07:27.091: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 24 03:07:29.098: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376047, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376047, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376047, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376047, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 24 03:07:32.114: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:07:32.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2859" for this suite.
STEP: Destroying namespace "webhook-2859-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.694 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":305,"completed":156,"skipped":2426,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:07:32.276: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Dec 24 03:07:35.379: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:07:36.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2116" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":305,"completed":157,"skipped":2466,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:07:36.406: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override all
Dec 24 03:07:36.455: INFO: Waiting up to 5m0s for pod "client-containers-a06a7c51-510c-42b6-89e2-287b6eab037f" in namespace "containers-8680" to be "Succeeded or Failed"
Dec 24 03:07:36.459: INFO: Pod "client-containers-a06a7c51-510c-42b6-89e2-287b6eab037f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.194915ms
Dec 24 03:07:38.462: INFO: Pod "client-containers-a06a7c51-510c-42b6-89e2-287b6eab037f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006952695s
STEP: Saw pod success
Dec 24 03:07:38.462: INFO: Pod "client-containers-a06a7c51-510c-42b6-89e2-287b6eab037f" satisfied condition "Succeeded or Failed"
Dec 24 03:07:38.464: INFO: Trying to get logs from node c1-4 pod client-containers-a06a7c51-510c-42b6-89e2-287b6eab037f container test-container: <nil>
STEP: delete the pod
Dec 24 03:07:38.510: INFO: Waiting for pod client-containers-a06a7c51-510c-42b6-89e2-287b6eab037f to disappear
Dec 24 03:07:38.517: INFO: Pod client-containers-a06a7c51-510c-42b6-89e2-287b6eab037f no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:07:38.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8680" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":305,"completed":158,"skipped":2521,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:07:38.524: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 03:07:39.208: INFO: Checking APIGroup: apiregistration.k8s.io
Dec 24 03:07:39.209: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Dec 24 03:07:39.209: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Dec 24 03:07:39.209: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Dec 24 03:07:39.209: INFO: Checking APIGroup: extensions
Dec 24 03:07:39.209: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Dec 24 03:07:39.210: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Dec 24 03:07:39.210: INFO: extensions/v1beta1 matches extensions/v1beta1
Dec 24 03:07:39.210: INFO: Checking APIGroup: apps
Dec 24 03:07:39.210: INFO: PreferredVersion.GroupVersion: apps/v1
Dec 24 03:07:39.210: INFO: Versions found [{apps/v1 v1}]
Dec 24 03:07:39.210: INFO: apps/v1 matches apps/v1
Dec 24 03:07:39.210: INFO: Checking APIGroup: events.k8s.io
Dec 24 03:07:39.211: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Dec 24 03:07:39.211: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Dec 24 03:07:39.211: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Dec 24 03:07:39.211: INFO: Checking APIGroup: authentication.k8s.io
Dec 24 03:07:39.212: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Dec 24 03:07:39.212: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Dec 24 03:07:39.212: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Dec 24 03:07:39.212: INFO: Checking APIGroup: authorization.k8s.io
Dec 24 03:07:39.213: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Dec 24 03:07:39.213: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Dec 24 03:07:39.213: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Dec 24 03:07:39.213: INFO: Checking APIGroup: autoscaling
Dec 24 03:07:39.214: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Dec 24 03:07:39.214: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Dec 24 03:07:39.214: INFO: autoscaling/v1 matches autoscaling/v1
Dec 24 03:07:39.214: INFO: Checking APIGroup: batch
Dec 24 03:07:39.214: INFO: PreferredVersion.GroupVersion: batch/v1
Dec 24 03:07:39.214: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Dec 24 03:07:39.214: INFO: batch/v1 matches batch/v1
Dec 24 03:07:39.214: INFO: Checking APIGroup: certificates.k8s.io
Dec 24 03:07:39.215: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Dec 24 03:07:39.215: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Dec 24 03:07:39.215: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Dec 24 03:07:39.215: INFO: Checking APIGroup: networking.k8s.io
Dec 24 03:07:39.216: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Dec 24 03:07:39.216: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Dec 24 03:07:39.216: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Dec 24 03:07:39.216: INFO: Checking APIGroup: policy
Dec 24 03:07:39.217: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Dec 24 03:07:39.217: INFO: Versions found [{policy/v1beta1 v1beta1}]
Dec 24 03:07:39.217: INFO: policy/v1beta1 matches policy/v1beta1
Dec 24 03:07:39.217: INFO: Checking APIGroup: rbac.authorization.k8s.io
Dec 24 03:07:39.218: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Dec 24 03:07:39.218: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Dec 24 03:07:39.218: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Dec 24 03:07:39.218: INFO: Checking APIGroup: storage.k8s.io
Dec 24 03:07:39.218: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Dec 24 03:07:39.218: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Dec 24 03:07:39.218: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Dec 24 03:07:39.218: INFO: Checking APIGroup: admissionregistration.k8s.io
Dec 24 03:07:39.219: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Dec 24 03:07:39.219: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Dec 24 03:07:39.219: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Dec 24 03:07:39.219: INFO: Checking APIGroup: apiextensions.k8s.io
Dec 24 03:07:39.220: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Dec 24 03:07:39.220: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Dec 24 03:07:39.220: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Dec 24 03:07:39.220: INFO: Checking APIGroup: scheduling.k8s.io
Dec 24 03:07:39.221: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Dec 24 03:07:39.221: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Dec 24 03:07:39.221: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Dec 24 03:07:39.221: INFO: Checking APIGroup: coordination.k8s.io
Dec 24 03:07:39.221: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Dec 24 03:07:39.221: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Dec 24 03:07:39.221: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Dec 24 03:07:39.221: INFO: Checking APIGroup: node.k8s.io
Dec 24 03:07:39.222: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1beta1
Dec 24 03:07:39.222: INFO: Versions found [{node.k8s.io/v1beta1 v1beta1}]
Dec 24 03:07:39.222: INFO: node.k8s.io/v1beta1 matches node.k8s.io/v1beta1
Dec 24 03:07:39.222: INFO: Checking APIGroup: discovery.k8s.io
Dec 24 03:07:39.223: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Dec 24 03:07:39.223: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Dec 24 03:07:39.223: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Dec 24 03:07:39.223: INFO: Checking APIGroup: ceph.rook.io
Dec 24 03:07:39.224: INFO: PreferredVersion.GroupVersion: ceph.rook.io/v1
Dec 24 03:07:39.224: INFO: Versions found [{ceph.rook.io/v1 v1}]
Dec 24 03:07:39.224: INFO: ceph.rook.io/v1 matches ceph.rook.io/v1
Dec 24 03:07:39.224: INFO: Checking APIGroup: crd.projectcalico.org
Dec 24 03:07:39.225: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Dec 24 03:07:39.225: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Dec 24 03:07:39.225: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Dec 24 03:07:39.225: INFO: Checking APIGroup: cdi.kubevirt.io
Dec 24 03:07:39.226: INFO: PreferredVersion.GroupVersion: cdi.kubevirt.io/v1beta1
Dec 24 03:07:39.226: INFO: Versions found [{cdi.kubevirt.io/v1beta1 v1beta1} {cdi.kubevirt.io/v1alpha1 v1alpha1}]
Dec 24 03:07:39.226: INFO: cdi.kubevirt.io/v1beta1 matches cdi.kubevirt.io/v1beta1
Dec 24 03:07:39.226: INFO: Checking APIGroup: objectbucket.io
Dec 24 03:07:39.226: INFO: PreferredVersion.GroupVersion: objectbucket.io/v1alpha1
Dec 24 03:07:39.227: INFO: Versions found [{objectbucket.io/v1alpha1 v1alpha1}]
Dec 24 03:07:39.227: INFO: objectbucket.io/v1alpha1 matches objectbucket.io/v1alpha1
Dec 24 03:07:39.227: INFO: Checking APIGroup: snapshot.kubevirt.io
Dec 24 03:07:39.227: INFO: PreferredVersion.GroupVersion: snapshot.kubevirt.io/v1alpha1
Dec 24 03:07:39.227: INFO: Versions found [{snapshot.kubevirt.io/v1alpha1 v1alpha1}]
Dec 24 03:07:39.227: INFO: snapshot.kubevirt.io/v1alpha1 matches snapshot.kubevirt.io/v1alpha1
Dec 24 03:07:39.227: INFO: Checking APIGroup: upload.cdi.kubevirt.io
Dec 24 03:07:39.228: INFO: PreferredVersion.GroupVersion: upload.cdi.kubevirt.io/v1beta1
Dec 24 03:07:39.228: INFO: Versions found [{upload.cdi.kubevirt.io/v1beta1 v1beta1} {upload.cdi.kubevirt.io/v1alpha1 v1alpha1}]
Dec 24 03:07:39.228: INFO: upload.cdi.kubevirt.io/v1beta1 matches upload.cdi.kubevirt.io/v1beta1
Dec 24 03:07:39.228: INFO: Checking APIGroup: rook.io
Dec 24 03:07:39.229: INFO: PreferredVersion.GroupVersion: rook.io/v1alpha2
Dec 24 03:07:39.229: INFO: Versions found [{rook.io/v1alpha2 v1alpha2}]
Dec 24 03:07:39.229: INFO: rook.io/v1alpha2 matches rook.io/v1alpha2
Dec 24 03:07:39.229: INFO: Checking APIGroup: kubevirt.io
Dec 24 03:07:39.230: INFO: PreferredVersion.GroupVersion: kubevirt.io/v1alpha3
Dec 24 03:07:39.230: INFO: Versions found [{kubevirt.io/v1alpha3 v1alpha3}]
Dec 24 03:07:39.230: INFO: kubevirt.io/v1alpha3 matches kubevirt.io/v1alpha3
Dec 24 03:07:39.230: INFO: Checking APIGroup: subresources.kubevirt.io
Dec 24 03:07:39.230: INFO: PreferredVersion.GroupVersion: subresources.kubevirt.io/v1alpha3
Dec 24 03:07:39.230: INFO: Versions found [{subresources.kubevirt.io/v1alpha3 v1alpha3}]
Dec 24 03:07:39.230: INFO: subresources.kubevirt.io/v1alpha3 matches subresources.kubevirt.io/v1alpha3
Dec 24 03:07:39.230: INFO: Checking APIGroup: snapshot.storage.k8s.io
Dec 24 03:07:39.231: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1beta1
Dec 24 03:07:39.231: INFO: Versions found [{snapshot.storage.k8s.io/v1beta1 v1beta1}]
Dec 24 03:07:39.231: INFO: snapshot.storage.k8s.io/v1beta1 matches snapshot.storage.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:07:39.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-8622" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":305,"completed":159,"skipped":2537,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:07:39.239: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 24 03:07:39.318: INFO: Waiting up to 5m0s for pod "downwardapi-volume-07b346b6-3c7a-45d0-a270-3112afc8aee7" in namespace "downward-api-5341" to be "Succeeded or Failed"
Dec 24 03:07:39.331: INFO: Pod "downwardapi-volume-07b346b6-3c7a-45d0-a270-3112afc8aee7": Phase="Pending", Reason="", readiness=false. Elapsed: 12.978281ms
Dec 24 03:07:41.347: INFO: Pod "downwardapi-volume-07b346b6-3c7a-45d0-a270-3112afc8aee7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028175512s
STEP: Saw pod success
Dec 24 03:07:41.347: INFO: Pod "downwardapi-volume-07b346b6-3c7a-45d0-a270-3112afc8aee7" satisfied condition "Succeeded or Failed"
Dec 24 03:07:41.349: INFO: Trying to get logs from node c1-4 pod downwardapi-volume-07b346b6-3c7a-45d0-a270-3112afc8aee7 container client-container: <nil>
STEP: delete the pod
Dec 24 03:07:41.373: INFO: Waiting for pod downwardapi-volume-07b346b6-3c7a-45d0-a270-3112afc8aee7 to disappear
Dec 24 03:07:41.380: INFO: Pod downwardapi-volume-07b346b6-3c7a-45d0-a270-3112afc8aee7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:07:41.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5341" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":160,"skipped":2547,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:07:41.391: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-3758
Dec 24 03:07:43.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-3758 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Dec 24 03:07:44.878: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Dec 24 03:07:44.878: INFO: stdout: "iptables"
Dec 24 03:07:44.878: INFO: proxyMode: iptables
Dec 24 03:07:44.893: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Dec 24 03:07:44.901: INFO: Pod kube-proxy-mode-detector still exists
Dec 24 03:07:46.901: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Dec 24 03:07:46.903: INFO: Pod kube-proxy-mode-detector still exists
Dec 24 03:07:48.901: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Dec 24 03:07:48.912: INFO: Pod kube-proxy-mode-detector still exists
Dec 24 03:07:50.901: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Dec 24 03:07:50.904: INFO: Pod kube-proxy-mode-detector still exists
Dec 24 03:07:52.901: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Dec 24 03:07:52.904: INFO: Pod kube-proxy-mode-detector still exists
Dec 24 03:07:54.901: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Dec 24 03:07:54.904: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-3758
STEP: creating replication controller affinity-nodeport-timeout in namespace services-3758
I1224 03:07:54.961546      24 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-3758, replica count: 3
I1224 03:07:58.011904      24 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 24 03:07:58.021: INFO: Creating new exec pod
Dec 24 03:08:03.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-3758 execpod-affinityk9stz -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Dec 24 03:08:03.255: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Dec 24 03:08:03.255: INFO: stdout: ""
Dec 24 03:08:03.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-3758 execpod-affinityk9stz -- /bin/sh -x -c nc -zv -t -w 2 10.96.147.184 80'
Dec 24 03:08:03.470: INFO: stderr: "+ nc -zv -t -w 2 10.96.147.184 80\nConnection to 10.96.147.184 80 port [tcp/http] succeeded!\n"
Dec 24 03:08:03.470: INFO: stdout: ""
Dec 24 03:08:03.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-3758 execpod-affinityk9stz -- /bin/sh -x -c nc -zv -t -w 2 172.21.3.7 31473'
Dec 24 03:08:03.655: INFO: stderr: "+ nc -zv -t -w 2 172.21.3.7 31473\nConnection to 172.21.3.7 31473 port [tcp/31473] succeeded!\n"
Dec 24 03:08:03.656: INFO: stdout: ""
Dec 24 03:08:03.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-3758 execpod-affinityk9stz -- /bin/sh -x -c nc -zv -t -w 2 172.21.3.6 31473'
Dec 24 03:08:03.825: INFO: stderr: "+ nc -zv -t -w 2 172.21.3.6 31473\nConnection to 172.21.3.6 31473 port [tcp/31473] succeeded!\n"
Dec 24 03:08:03.825: INFO: stdout: ""
Dec 24 03:08:03.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-3758 execpod-affinityk9stz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.3.5:31473/ ; done'
Dec 24 03:08:04.114: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31473/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31473/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31473/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31473/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31473/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31473/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31473/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31473/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31473/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31473/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31473/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31473/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31473/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31473/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31473/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.3.5:31473/\n"
Dec 24 03:08:04.114: INFO: stdout: "\naffinity-nodeport-timeout-h2trb\naffinity-nodeport-timeout-h2trb\naffinity-nodeport-timeout-h2trb\naffinity-nodeport-timeout-h2trb\naffinity-nodeport-timeout-h2trb\naffinity-nodeport-timeout-h2trb\naffinity-nodeport-timeout-h2trb\naffinity-nodeport-timeout-h2trb\naffinity-nodeport-timeout-h2trb\naffinity-nodeport-timeout-h2trb\naffinity-nodeport-timeout-h2trb\naffinity-nodeport-timeout-h2trb\naffinity-nodeport-timeout-h2trb\naffinity-nodeport-timeout-h2trb\naffinity-nodeport-timeout-h2trb\naffinity-nodeport-timeout-h2trb"
Dec 24 03:08:04.114: INFO: Received response from host: affinity-nodeport-timeout-h2trb
Dec 24 03:08:04.114: INFO: Received response from host: affinity-nodeport-timeout-h2trb
Dec 24 03:08:04.114: INFO: Received response from host: affinity-nodeport-timeout-h2trb
Dec 24 03:08:04.114: INFO: Received response from host: affinity-nodeport-timeout-h2trb
Dec 24 03:08:04.114: INFO: Received response from host: affinity-nodeport-timeout-h2trb
Dec 24 03:08:04.114: INFO: Received response from host: affinity-nodeport-timeout-h2trb
Dec 24 03:08:04.114: INFO: Received response from host: affinity-nodeport-timeout-h2trb
Dec 24 03:08:04.114: INFO: Received response from host: affinity-nodeport-timeout-h2trb
Dec 24 03:08:04.114: INFO: Received response from host: affinity-nodeport-timeout-h2trb
Dec 24 03:08:04.114: INFO: Received response from host: affinity-nodeport-timeout-h2trb
Dec 24 03:08:04.114: INFO: Received response from host: affinity-nodeport-timeout-h2trb
Dec 24 03:08:04.114: INFO: Received response from host: affinity-nodeport-timeout-h2trb
Dec 24 03:08:04.114: INFO: Received response from host: affinity-nodeport-timeout-h2trb
Dec 24 03:08:04.114: INFO: Received response from host: affinity-nodeport-timeout-h2trb
Dec 24 03:08:04.114: INFO: Received response from host: affinity-nodeport-timeout-h2trb
Dec 24 03:08:04.114: INFO: Received response from host: affinity-nodeport-timeout-h2trb
Dec 24 03:08:04.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-3758 execpod-affinityk9stz -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.3.5:31473/'
Dec 24 03:08:04.287: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.3.5:31473/\n"
Dec 24 03:08:04.287: INFO: stdout: "affinity-nodeport-timeout-h2trb"
Dec 24 03:08:19.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-3758 execpod-affinityk9stz -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.3.5:31473/'
Dec 24 03:08:19.492: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.3.5:31473/\n"
Dec 24 03:08:19.492: INFO: stdout: "affinity-nodeport-timeout-68xmg"
Dec 24 03:08:19.492: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-3758, will wait for the garbage collector to delete the pods
Dec 24 03:08:19.562: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 4.312425ms
Dec 24 03:08:20.562: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 1.000176396s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:08:33.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3758" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:51.945 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":161,"skipped":2562,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:08:33.336: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 03:08:33.387: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 24 03:08:38.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 --namespace=crd-publish-openapi-1994 create -f -'
Dec 24 03:08:39.726: INFO: stderr: ""
Dec 24 03:08:39.726: INFO: stdout: "e2e-test-crd-publish-openapi-1134-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 24 03:08:39.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 --namespace=crd-publish-openapi-1994 delete e2e-test-crd-publish-openapi-1134-crds test-cr'
Dec 24 03:08:39.857: INFO: stderr: ""
Dec 24 03:08:39.857: INFO: stdout: "e2e-test-crd-publish-openapi-1134-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Dec 24 03:08:39.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 --namespace=crd-publish-openapi-1994 apply -f -'
Dec 24 03:08:40.216: INFO: stderr: ""
Dec 24 03:08:40.216: INFO: stdout: "e2e-test-crd-publish-openapi-1134-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 24 03:08:40.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 --namespace=crd-publish-openapi-1994 delete e2e-test-crd-publish-openapi-1134-crds test-cr'
Dec 24 03:08:40.341: INFO: stderr: ""
Dec 24 03:08:40.341: INFO: stdout: "e2e-test-crd-publish-openapi-1134-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Dec 24 03:08:40.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 explain e2e-test-crd-publish-openapi-1134-crds'
Dec 24 03:08:40.682: INFO: stderr: ""
Dec 24 03:08:40.682: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1134-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:08:45.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1994" for this suite.

• [SLOW TEST:12.025 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":305,"completed":162,"skipped":2573,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:08:45.361: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 24 03:08:46.139: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 24 03:08:48.147: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376126, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376126, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376126, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376126, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 24 03:08:51.175: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Dec 24 03:08:51.195: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:08:51.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4047" for this suite.
STEP: Destroying namespace "webhook-4047-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.961 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":305,"completed":163,"skipped":2579,"failed":0}
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:08:51.322: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 03:08:51.384: INFO: Create a RollingUpdate DaemonSet
Dec 24 03:08:51.387: INFO: Check that daemon pods launch on every node of the cluster
Dec 24 03:08:51.391: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 03:08:51.409: INFO: Number of nodes with available pods: 0
Dec 24 03:08:51.409: INFO: Node c1-4 is running more than one daemon pod
Dec 24 03:08:52.421: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 03:08:52.424: INFO: Number of nodes with available pods: 0
Dec 24 03:08:52.424: INFO: Node c1-4 is running more than one daemon pod
Dec 24 03:08:53.415: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 03:08:53.418: INFO: Number of nodes with available pods: 3
Dec 24 03:08:53.418: INFO: Number of running nodes: 3, number of available pods: 3
Dec 24 03:08:53.418: INFO: Update the DaemonSet to trigger a rollout
Dec 24 03:08:53.424: INFO: Updating DaemonSet daemon-set
Dec 24 03:09:03.451: INFO: Roll back the DaemonSet before rollout is complete
Dec 24 03:09:03.457: INFO: Updating DaemonSet daemon-set
Dec 24 03:09:03.457: INFO: Make sure DaemonSet rollback is complete
Dec 24 03:09:03.508: INFO: Wrong image for pod: daemon-set-45ckk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 24 03:09:03.508: INFO: Pod daemon-set-45ckk is not available
Dec 24 03:09:03.512: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 03:09:04.516: INFO: Wrong image for pod: daemon-set-45ckk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 24 03:09:04.516: INFO: Pod daemon-set-45ckk is not available
Dec 24 03:09:04.520: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 03:09:05.516: INFO: Wrong image for pod: daemon-set-45ckk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 24 03:09:05.516: INFO: Pod daemon-set-45ckk is not available
Dec 24 03:09:05.521: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 03:09:06.519: INFO: Wrong image for pod: daemon-set-45ckk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 24 03:09:06.519: INFO: Pod daemon-set-45ckk is not available
Dec 24 03:09:06.524: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 03:09:07.516: INFO: Wrong image for pod: daemon-set-45ckk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 24 03:09:07.516: INFO: Pod daemon-set-45ckk is not available
Dec 24 03:09:07.521: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 03:09:08.516: INFO: Wrong image for pod: daemon-set-45ckk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 24 03:09:08.516: INFO: Pod daemon-set-45ckk is not available
Dec 24 03:09:08.521: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 03:09:09.516: INFO: Wrong image for pod: daemon-set-45ckk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 24 03:09:09.516: INFO: Pod daemon-set-45ckk is not available
Dec 24 03:09:09.520: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 03:09:10.522: INFO: Wrong image for pod: daemon-set-45ckk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 24 03:09:10.522: INFO: Pod daemon-set-45ckk is not available
Dec 24 03:09:10.526: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 03:09:11.516: INFO: Wrong image for pod: daemon-set-45ckk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 24 03:09:11.516: INFO: Pod daemon-set-45ckk is not available
Dec 24 03:09:11.520: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 03:09:12.516: INFO: Wrong image for pod: daemon-set-45ckk. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 24 03:09:12.516: INFO: Pod daemon-set-45ckk is not available
Dec 24 03:09:12.521: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 03:09:13.516: INFO: Pod daemon-set-82dtb is not available
Dec 24 03:09:13.521: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1975, will wait for the garbage collector to delete the pods
Dec 24 03:09:13.583: INFO: Deleting DaemonSet.extensions daemon-set took: 4.494903ms
Dec 24 03:09:13.683: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.170456ms
Dec 24 03:09:19.785: INFO: Number of nodes with available pods: 0
Dec 24 03:09:19.785: INFO: Number of running nodes: 0, number of available pods: 0
Dec 24 03:09:19.787: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1975/daemonsets","resourceVersion":"3456921"},"items":null}

Dec 24 03:09:19.789: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1975/pods","resourceVersion":"3456921"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:09:19.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1975" for this suite.

• [SLOW TEST:28.485 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":305,"completed":164,"skipped":2581,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:09:19.808: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 03:09:19.850: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-5e865115-44f1-4a2c-85cc-72cd258e7580" in namespace "security-context-test-1900" to be "Succeeded or Failed"
Dec 24 03:09:19.854: INFO: Pod "busybox-privileged-false-5e865115-44f1-4a2c-85cc-72cd258e7580": Phase="Pending", Reason="", readiness=false. Elapsed: 3.95785ms
Dec 24 03:09:21.857: INFO: Pod "busybox-privileged-false-5e865115-44f1-4a2c-85cc-72cd258e7580": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00683524s
Dec 24 03:09:23.860: INFO: Pod "busybox-privileged-false-5e865115-44f1-4a2c-85cc-72cd258e7580": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009797884s
Dec 24 03:09:23.860: INFO: Pod "busybox-privileged-false-5e865115-44f1-4a2c-85cc-72cd258e7580" satisfied condition "Succeeded or Failed"
Dec 24 03:09:23.877: INFO: Got logs for pod "busybox-privileged-false-5e865115-44f1-4a2c-85cc-72cd258e7580": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:09:23.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1900" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":165,"skipped":2601,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:09:23.886: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-2746
STEP: creating service affinity-clusterip-transition in namespace services-2746
STEP: creating replication controller affinity-clusterip-transition in namespace services-2746
I1224 03:09:23.986652      24 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-2746, replica count: 3
I1224 03:09:27.037003      24 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 24 03:09:27.041: INFO: Creating new exec pod
Dec 24 03:09:30.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-2746 execpod-affinity68zwv -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Dec 24 03:09:30.262: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Dec 24 03:09:30.262: INFO: stdout: ""
Dec 24 03:09:30.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-2746 execpod-affinity68zwv -- /bin/sh -x -c nc -zv -t -w 2 10.96.115.247 80'
Dec 24 03:09:30.446: INFO: stderr: "+ nc -zv -t -w 2 10.96.115.247 80\nConnection to 10.96.115.247 80 port [tcp/http] succeeded!\n"
Dec 24 03:09:30.446: INFO: stdout: ""
Dec 24 03:09:30.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-2746 execpod-affinity68zwv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.115.247:80/ ; done'
Dec 24 03:09:30.745: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.115.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.115.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.115.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.115.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.115.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.115.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.115.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.115.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.115.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.115.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.115.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.115.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.115.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.115.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.115.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.115.247:80/\n"
Dec 24 03:09:30.745: INFO: stdout: "\naffinity-clusterip-transition-fgn9j\naffinity-clusterip-transition-sf98x\naffinity-clusterip-transition-sf98x\naffinity-clusterip-transition-sf98x\naffinity-clusterip-transition-nfpdn\naffinity-clusterip-transition-fgn9j\naffinity-clusterip-transition-nfpdn\naffinity-clusterip-transition-sf98x\naffinity-clusterip-transition-fgn9j\naffinity-clusterip-transition-fgn9j\naffinity-clusterip-transition-fgn9j\naffinity-clusterip-transition-fgn9j\naffinity-clusterip-transition-nfpdn\naffinity-clusterip-transition-fgn9j\naffinity-clusterip-transition-sf98x\naffinity-clusterip-transition-nfpdn"
Dec 24 03:09:30.745: INFO: Received response from host: affinity-clusterip-transition-fgn9j
Dec 24 03:09:30.745: INFO: Received response from host: affinity-clusterip-transition-sf98x
Dec 24 03:09:30.745: INFO: Received response from host: affinity-clusterip-transition-sf98x
Dec 24 03:09:30.745: INFO: Received response from host: affinity-clusterip-transition-sf98x
Dec 24 03:09:30.745: INFO: Received response from host: affinity-clusterip-transition-nfpdn
Dec 24 03:09:30.745: INFO: Received response from host: affinity-clusterip-transition-fgn9j
Dec 24 03:09:30.745: INFO: Received response from host: affinity-clusterip-transition-nfpdn
Dec 24 03:09:30.745: INFO: Received response from host: affinity-clusterip-transition-sf98x
Dec 24 03:09:30.745: INFO: Received response from host: affinity-clusterip-transition-fgn9j
Dec 24 03:09:30.745: INFO: Received response from host: affinity-clusterip-transition-fgn9j
Dec 24 03:09:30.745: INFO: Received response from host: affinity-clusterip-transition-fgn9j
Dec 24 03:09:30.745: INFO: Received response from host: affinity-clusterip-transition-fgn9j
Dec 24 03:09:30.745: INFO: Received response from host: affinity-clusterip-transition-nfpdn
Dec 24 03:09:30.745: INFO: Received response from host: affinity-clusterip-transition-fgn9j
Dec 24 03:09:30.745: INFO: Received response from host: affinity-clusterip-transition-sf98x
Dec 24 03:09:30.745: INFO: Received response from host: affinity-clusterip-transition-nfpdn
Dec 24 03:09:30.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-2746 execpod-affinity68zwv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.115.247:80/ ; done'
Dec 24 03:09:30.984: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.115.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.115.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.115.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.115.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.115.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.115.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.115.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.115.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.115.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.115.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.115.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.115.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.115.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.115.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.115.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.115.247:80/\n"
Dec 24 03:09:30.985: INFO: stdout: "\naffinity-clusterip-transition-nfpdn\naffinity-clusterip-transition-nfpdn\naffinity-clusterip-transition-nfpdn\naffinity-clusterip-transition-nfpdn\naffinity-clusterip-transition-nfpdn\naffinity-clusterip-transition-nfpdn\naffinity-clusterip-transition-nfpdn\naffinity-clusterip-transition-nfpdn\naffinity-clusterip-transition-nfpdn\naffinity-clusterip-transition-nfpdn\naffinity-clusterip-transition-nfpdn\naffinity-clusterip-transition-nfpdn\naffinity-clusterip-transition-nfpdn\naffinity-clusterip-transition-nfpdn\naffinity-clusterip-transition-nfpdn\naffinity-clusterip-transition-nfpdn"
Dec 24 03:09:30.985: INFO: Received response from host: affinity-clusterip-transition-nfpdn
Dec 24 03:09:30.985: INFO: Received response from host: affinity-clusterip-transition-nfpdn
Dec 24 03:09:30.985: INFO: Received response from host: affinity-clusterip-transition-nfpdn
Dec 24 03:09:30.985: INFO: Received response from host: affinity-clusterip-transition-nfpdn
Dec 24 03:09:30.985: INFO: Received response from host: affinity-clusterip-transition-nfpdn
Dec 24 03:09:30.985: INFO: Received response from host: affinity-clusterip-transition-nfpdn
Dec 24 03:09:30.985: INFO: Received response from host: affinity-clusterip-transition-nfpdn
Dec 24 03:09:30.985: INFO: Received response from host: affinity-clusterip-transition-nfpdn
Dec 24 03:09:30.985: INFO: Received response from host: affinity-clusterip-transition-nfpdn
Dec 24 03:09:30.985: INFO: Received response from host: affinity-clusterip-transition-nfpdn
Dec 24 03:09:30.985: INFO: Received response from host: affinity-clusterip-transition-nfpdn
Dec 24 03:09:30.985: INFO: Received response from host: affinity-clusterip-transition-nfpdn
Dec 24 03:09:30.985: INFO: Received response from host: affinity-clusterip-transition-nfpdn
Dec 24 03:09:30.985: INFO: Received response from host: affinity-clusterip-transition-nfpdn
Dec 24 03:09:30.985: INFO: Received response from host: affinity-clusterip-transition-nfpdn
Dec 24 03:09:30.985: INFO: Received response from host: affinity-clusterip-transition-nfpdn
Dec 24 03:09:30.985: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-2746, will wait for the garbage collector to delete the pods
Dec 24 03:09:31.103: INFO: Deleting ReplicationController affinity-clusterip-transition took: 4.334677ms
Dec 24 03:09:31.203: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.191851ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:09:39.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2746" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:15.970 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":166,"skipped":2687,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:09:39.857: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-6928
Dec 24 03:09:41.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-6928 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Dec 24 03:09:42.115: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Dec 24 03:09:42.115: INFO: stdout: "iptables"
Dec 24 03:09:42.115: INFO: proxyMode: iptables
Dec 24 03:09:42.120: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Dec 24 03:09:42.131: INFO: Pod kube-proxy-mode-detector still exists
Dec 24 03:09:44.131: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Dec 24 03:09:44.134: INFO: Pod kube-proxy-mode-detector still exists
Dec 24 03:09:46.131: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Dec 24 03:09:46.134: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-6928
STEP: creating replication controller affinity-clusterip-timeout in namespace services-6928
I1224 03:09:46.183789      24 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-6928, replica count: 3
I1224 03:09:49.234161      24 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 24 03:09:49.253: INFO: Creating new exec pod
Dec 24 03:09:54.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-6928 execpod-affinityg87k6 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Dec 24 03:09:54.476: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Dec 24 03:09:54.476: INFO: stdout: ""
Dec 24 03:09:54.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-6928 execpod-affinityg87k6 -- /bin/sh -x -c nc -zv -t -w 2 10.96.201.180 80'
Dec 24 03:09:54.647: INFO: stderr: "+ nc -zv -t -w 2 10.96.201.180 80\nConnection to 10.96.201.180 80 port [tcp/http] succeeded!\n"
Dec 24 03:09:54.647: INFO: stdout: ""
Dec 24 03:09:54.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-6928 execpod-affinityg87k6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.201.180:80/ ; done'
Dec 24 03:09:54.892: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.201.180:80/\n"
Dec 24 03:09:54.892: INFO: stdout: "\naffinity-clusterip-timeout-xqj89\naffinity-clusterip-timeout-xqj89\naffinity-clusterip-timeout-xqj89\naffinity-clusterip-timeout-xqj89\naffinity-clusterip-timeout-xqj89\naffinity-clusterip-timeout-xqj89\naffinity-clusterip-timeout-xqj89\naffinity-clusterip-timeout-xqj89\naffinity-clusterip-timeout-xqj89\naffinity-clusterip-timeout-xqj89\naffinity-clusterip-timeout-xqj89\naffinity-clusterip-timeout-xqj89\naffinity-clusterip-timeout-xqj89\naffinity-clusterip-timeout-xqj89\naffinity-clusterip-timeout-xqj89\naffinity-clusterip-timeout-xqj89"
Dec 24 03:09:54.892: INFO: Received response from host: affinity-clusterip-timeout-xqj89
Dec 24 03:09:54.892: INFO: Received response from host: affinity-clusterip-timeout-xqj89
Dec 24 03:09:54.892: INFO: Received response from host: affinity-clusterip-timeout-xqj89
Dec 24 03:09:54.892: INFO: Received response from host: affinity-clusterip-timeout-xqj89
Dec 24 03:09:54.892: INFO: Received response from host: affinity-clusterip-timeout-xqj89
Dec 24 03:09:54.892: INFO: Received response from host: affinity-clusterip-timeout-xqj89
Dec 24 03:09:54.892: INFO: Received response from host: affinity-clusterip-timeout-xqj89
Dec 24 03:09:54.892: INFO: Received response from host: affinity-clusterip-timeout-xqj89
Dec 24 03:09:54.892: INFO: Received response from host: affinity-clusterip-timeout-xqj89
Dec 24 03:09:54.892: INFO: Received response from host: affinity-clusterip-timeout-xqj89
Dec 24 03:09:54.892: INFO: Received response from host: affinity-clusterip-timeout-xqj89
Dec 24 03:09:54.892: INFO: Received response from host: affinity-clusterip-timeout-xqj89
Dec 24 03:09:54.892: INFO: Received response from host: affinity-clusterip-timeout-xqj89
Dec 24 03:09:54.892: INFO: Received response from host: affinity-clusterip-timeout-xqj89
Dec 24 03:09:54.892: INFO: Received response from host: affinity-clusterip-timeout-xqj89
Dec 24 03:09:54.892: INFO: Received response from host: affinity-clusterip-timeout-xqj89
Dec 24 03:09:54.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-6928 execpod-affinityg87k6 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.96.201.180:80/'
Dec 24 03:09:55.058: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.96.201.180:80/\n"
Dec 24 03:09:55.058: INFO: stdout: "affinity-clusterip-timeout-xqj89"
Dec 24 03:10:10.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-6928 execpod-affinityg87k6 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.96.201.180:80/'
Dec 24 03:10:10.253: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.96.201.180:80/\n"
Dec 24 03:10:10.253: INFO: stdout: "affinity-clusterip-timeout-xqj89"
Dec 24 03:10:25.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-6928 execpod-affinityg87k6 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.96.201.180:80/'
Dec 24 03:10:25.459: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.96.201.180:80/\n"
Dec 24 03:10:25.459: INFO: stdout: "affinity-clusterip-timeout-8wrvt"
Dec 24 03:10:25.459: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-6928, will wait for the garbage collector to delete the pods
Dec 24 03:10:25.556: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 4.435133ms
Dec 24 03:10:26.556: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 1.000209211s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:10:38.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6928" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:59.144 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":167,"skipped":2715,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:10:39.000: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Dec 24 03:10:39.088: INFO: Created pod &Pod{ObjectMeta:{dns-6265  dns-6265 /api/v1/namespaces/dns-6265/pods/dns-6265 12885fbe-7352-4548-99ea-16d7c7aac384 3457762 0 2020-12-24 03:10:39 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2020-12-24 03:10:39 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bc4mh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bc4mh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bc4mh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 24 03:10:39.098: INFO: The status of Pod dns-6265 is Pending, waiting for it to be Running (with Ready = true)
Dec 24 03:10:41.104: INFO: The status of Pod dns-6265 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Dec 24 03:10:41.104: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-6265 PodName:dns-6265 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 24 03:10:41.104: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Verifying customized DNS server is configured on pod...
Dec 24 03:10:41.223: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-6265 PodName:dns-6265 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 24 03:10:41.223: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
Dec 24 03:10:41.324: INFO: Deleting pod dns-6265...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:10:41.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6265" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":305,"completed":168,"skipped":2724,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:10:41.369: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 24 03:10:41.404: INFO: Waiting up to 5m0s for pod "pod-48878d8c-c47a-46c1-bd32-06b3b0035283" in namespace "emptydir-2477" to be "Succeeded or Failed"
Dec 24 03:10:41.418: INFO: Pod "pod-48878d8c-c47a-46c1-bd32-06b3b0035283": Phase="Pending", Reason="", readiness=false. Elapsed: 14.08174ms
Dec 24 03:10:43.421: INFO: Pod "pod-48878d8c-c47a-46c1-bd32-06b3b0035283": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016927877s
Dec 24 03:10:45.424: INFO: Pod "pod-48878d8c-c47a-46c1-bd32-06b3b0035283": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019891513s
STEP: Saw pod success
Dec 24 03:10:45.424: INFO: Pod "pod-48878d8c-c47a-46c1-bd32-06b3b0035283" satisfied condition "Succeeded or Failed"
Dec 24 03:10:45.426: INFO: Trying to get logs from node c1-4 pod pod-48878d8c-c47a-46c1-bd32-06b3b0035283 container test-container: <nil>
STEP: delete the pod
Dec 24 03:10:45.484: INFO: Waiting for pod pod-48878d8c-c47a-46c1-bd32-06b3b0035283 to disappear
Dec 24 03:10:45.487: INFO: Pod pod-48878d8c-c47a-46c1-bd32-06b3b0035283 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:10:45.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2477" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":169,"skipped":2748,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:10:45.494: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:10:52.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3592" for this suite.

• [SLOW TEST:7.085 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":305,"completed":170,"skipped":2761,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:10:52.579: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-1327a6bb-5e4f-4733-9b8e-4492f45a7b08
STEP: Creating a pod to test consume configMaps
Dec 24 03:10:52.645: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8e5c118d-0d89-4aab-a049-dd1450bf40f3" in namespace "projected-2964" to be "Succeeded or Failed"
Dec 24 03:10:52.649: INFO: Pod "pod-projected-configmaps-8e5c118d-0d89-4aab-a049-dd1450bf40f3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.093311ms
Dec 24 03:10:54.667: INFO: Pod "pod-projected-configmaps-8e5c118d-0d89-4aab-a049-dd1450bf40f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022297267s
STEP: Saw pod success
Dec 24 03:10:54.667: INFO: Pod "pod-projected-configmaps-8e5c118d-0d89-4aab-a049-dd1450bf40f3" satisfied condition "Succeeded or Failed"
Dec 24 03:10:54.669: INFO: Trying to get logs from node c1-4 pod pod-projected-configmaps-8e5c118d-0d89-4aab-a049-dd1450bf40f3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 24 03:10:54.700: INFO: Waiting for pod pod-projected-configmaps-8e5c118d-0d89-4aab-a049-dd1450bf40f3 to disappear
Dec 24 03:10:54.706: INFO: Pod pod-projected-configmaps-8e5c118d-0d89-4aab-a049-dd1450bf40f3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:10:54.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2964" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":171,"skipped":2762,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:10:54.717: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 24 03:10:54.765: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6d98df80-416e-4340-b3ba-36561a07682f" in namespace "projected-5590" to be "Succeeded or Failed"
Dec 24 03:10:54.787: INFO: Pod "downwardapi-volume-6d98df80-416e-4340-b3ba-36561a07682f": Phase="Pending", Reason="", readiness=false. Elapsed: 22.01252ms
Dec 24 03:10:56.790: INFO: Pod "downwardapi-volume-6d98df80-416e-4340-b3ba-36561a07682f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024970969s
STEP: Saw pod success
Dec 24 03:10:56.790: INFO: Pod "downwardapi-volume-6d98df80-416e-4340-b3ba-36561a07682f" satisfied condition "Succeeded or Failed"
Dec 24 03:10:56.792: INFO: Trying to get logs from node c1-4 pod downwardapi-volume-6d98df80-416e-4340-b3ba-36561a07682f container client-container: <nil>
STEP: delete the pod
Dec 24 03:10:56.814: INFO: Waiting for pod downwardapi-volume-6d98df80-416e-4340-b3ba-36561a07682f to disappear
Dec 24 03:10:56.821: INFO: Pod downwardapi-volume-6d98df80-416e-4340-b3ba-36561a07682f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:10:56.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5590" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":172,"skipped":2772,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:10:56.829: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Dec 24 03:10:56.866: INFO: namespace kubectl-3605
Dec 24 03:10:56.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 create -f - --namespace=kubectl-3605'
Dec 24 03:10:57.365: INFO: stderr: ""
Dec 24 03:10:57.365: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Dec 24 03:10:58.368: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 24 03:10:58.368: INFO: Found 0 / 1
Dec 24 03:10:59.368: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 24 03:10:59.368: INFO: Found 0 / 1
Dec 24 03:11:00.369: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 24 03:11:00.369: INFO: Found 1 / 1
Dec 24 03:11:00.369: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 24 03:11:00.371: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 24 03:11:00.371: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 24 03:11:00.371: INFO: wait on agnhost-primary startup in kubectl-3605 
Dec 24 03:11:00.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 logs agnhost-primary-55zhc agnhost-primary --namespace=kubectl-3605'
Dec 24 03:11:00.489: INFO: stderr: ""
Dec 24 03:11:00.489: INFO: stdout: "Paused\n"
STEP: exposing RC
Dec 24 03:11:00.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-3605'
Dec 24 03:11:00.611: INFO: stderr: ""
Dec 24 03:11:00.611: INFO: stdout: "service/rm2 exposed\n"
Dec 24 03:11:00.615: INFO: Service rm2 in namespace kubectl-3605 found.
STEP: exposing service
Dec 24 03:11:02.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-3605'
Dec 24 03:11:02.753: INFO: stderr: ""
Dec 24 03:11:02.753: INFO: stdout: "service/rm3 exposed\n"
Dec 24 03:11:02.773: INFO: Service rm3 in namespace kubectl-3605 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:11:04.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3605" for this suite.

• [SLOW TEST:7.967 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1246
    should create services for rc  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":305,"completed":173,"skipped":2803,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:11:04.796: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-8f0fd14e-cb65-4414-936b-5b511370515c
STEP: Creating a pod to test consume secrets
Dec 24 03:11:04.845: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c2257f54-1434-4243-b2c8-0116c1498b4f" in namespace "projected-6906" to be "Succeeded or Failed"
Dec 24 03:11:04.850: INFO: Pod "pod-projected-secrets-c2257f54-1434-4243-b2c8-0116c1498b4f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.306815ms
Dec 24 03:11:06.861: INFO: Pod "pod-projected-secrets-c2257f54-1434-4243-b2c8-0116c1498b4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015217738s
STEP: Saw pod success
Dec 24 03:11:06.861: INFO: Pod "pod-projected-secrets-c2257f54-1434-4243-b2c8-0116c1498b4f" satisfied condition "Succeeded or Failed"
Dec 24 03:11:06.863: INFO: Trying to get logs from node c1-4 pod pod-projected-secrets-c2257f54-1434-4243-b2c8-0116c1498b4f container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 24 03:11:06.880: INFO: Waiting for pod pod-projected-secrets-c2257f54-1434-4243-b2c8-0116c1498b4f to disappear
Dec 24 03:11:06.899: INFO: Pod pod-projected-secrets-c2257f54-1434-4243-b2c8-0116c1498b4f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:11:06.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6906" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":174,"skipped":2814,"failed":0}
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:11:06.907: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 24 03:11:09.993: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:11:10.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1177" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":175,"skipped":2820,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:11:10.030: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 03:11:10.119: INFO: Pod name rollover-pod: Found 0 pods out of 1
Dec 24 03:11:15.128: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 24 03:11:15.128: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec 24 03:11:17.131: INFO: Creating deployment "test-rollover-deployment"
Dec 24 03:11:17.143: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec 24 03:11:19.148: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec 24 03:11:19.153: INFO: Ensure that both replica sets have 1 created replica
Dec 24 03:11:19.157: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec 24 03:11:19.163: INFO: Updating deployment test-rollover-deployment
Dec 24 03:11:19.163: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec 24 03:11:21.174: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec 24 03:11:21.180: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec 24 03:11:21.185: INFO: all replica sets need to contain the pod-template-hash label
Dec 24 03:11:21.185: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376277, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376277, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376280, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376277, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 24 03:11:23.191: INFO: all replica sets need to contain the pod-template-hash label
Dec 24 03:11:23.191: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376277, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376277, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376280, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376277, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 24 03:11:25.191: INFO: all replica sets need to contain the pod-template-hash label
Dec 24 03:11:25.191: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376277, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376277, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376280, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376277, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 24 03:11:27.191: INFO: all replica sets need to contain the pod-template-hash label
Dec 24 03:11:27.191: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376277, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376277, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376280, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376277, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 24 03:11:29.191: INFO: all replica sets need to contain the pod-template-hash label
Dec 24 03:11:29.191: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376277, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376277, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376280, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376277, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 24 03:11:31.191: INFO: 
Dec 24 03:11:31.191: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Dec 24 03:11:31.197: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-9786 /apis/apps/v1/namespaces/deployment-9786/deployments/test-rollover-deployment 4b8eebdd-d758-4ada-a826-a3eae56853bb 3458454 2 2020-12-24 03:11:17 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-12-24 03:11:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-12-24 03:11:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003fc1dc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-12-24 03:11:17 +0000 UTC,LastTransitionTime:2020-12-24 03:11:17 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-5797c7764" has successfully progressed.,LastUpdateTime:2020-12-24 03:11:30 +0000 UTC,LastTransitionTime:2020-12-24 03:11:17 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 24 03:11:31.200: INFO: New ReplicaSet "test-rollover-deployment-5797c7764" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-5797c7764  deployment-9786 /apis/apps/v1/namespaces/deployment-9786/replicasets/test-rollover-deployment-5797c7764 5f492fc2-aeed-4c5b-aa00-8cc4ad7d7a92 3458442 2 2020-12-24 03:11:19 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 4b8eebdd-d758-4ada-a826-a3eae56853bb 0xc002db02f0 0xc002db02f1}] []  [{kube-controller-manager Update apps/v1 2020-12-24 03:11:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b8eebdd-d758-4ada-a826-a3eae56853bb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5797c7764,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002db0368 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 24 03:11:31.200: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec 24 03:11:31.200: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9786 /apis/apps/v1/namespaces/deployment-9786/replicasets/test-rollover-controller 33136cfd-0e81-4e63-80b6-1d25582c4a4c 3458452 2 2020-12-24 03:11:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 4b8eebdd-d758-4ada-a826-a3eae56853bb 0xc002db01e7 0xc002db01e8}] []  [{e2e.test Update apps/v1 2020-12-24 03:11:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-12-24 03:11:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b8eebdd-d758-4ada-a826-a3eae56853bb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002db0288 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 24 03:11:31.200: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-9786 /apis/apps/v1/namespaces/deployment-9786/replicasets/test-rollover-deployment-78bc8b888c 7f271d02-478c-418d-b407-0ebd633e6ac3 3458353 2 2020-12-24 03:11:17 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 4b8eebdd-d758-4ada-a826-a3eae56853bb 0xc002db03d7 0xc002db03d8}] []  [{kube-controller-manager Update apps/v1 2020-12-24 03:11:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b8eebdd-d758-4ada-a826-a3eae56853bb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002db0468 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 24 03:11:31.203: INFO: Pod "test-rollover-deployment-5797c7764-xjqrl" is available:
&Pod{ObjectMeta:{test-rollover-deployment-5797c7764-xjqrl test-rollover-deployment-5797c7764- deployment-9786 /api/v1/namespaces/deployment-9786/pods/test-rollover-deployment-5797c7764-xjqrl 93b76f11-4e33-4049-bece-e625054fdc89 3458376 0 2020-12-24 03:11:19 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[cni.projectcalico.org/podIP:10.244.113.125/32 cni.projectcalico.org/podIPs:10.244.113.125/32] [{apps/v1 ReplicaSet test-rollover-deployment-5797c7764 5f492fc2-aeed-4c5b-aa00-8cc4ad7d7a92 0xc002db0a20 0xc002db0a21}] []  [{calico Update v1 2020-12-24 03:11:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kube-controller-manager Update v1 2020-12-24 03:11:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f492fc2-aeed-4c5b-aa00-8cc4ad7d7a92\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-24 03:11:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.113.125\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sqtpg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sqtpg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sqtpg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:11:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:11:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:11:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:11:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.3.5,PodIP:10.244.113.125,StartTime:2020-12-24 03:11:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-24 03:11:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:cri-o://2816eb097f6c274252c3345f4638a05c528804b1aa5b7945e8e12237f8ed3260,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.113.125,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:11:31.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9786" for this suite.

• [SLOW TEST:21.202 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":305,"completed":176,"skipped":2821,"failed":0}
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:11:31.233: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-secret-s9zd
STEP: Creating a pod to test atomic-volume-subpath
Dec 24 03:11:31.313: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-s9zd" in namespace "subpath-4067" to be "Succeeded or Failed"
Dec 24 03:11:31.321: INFO: Pod "pod-subpath-test-secret-s9zd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.653418ms
Dec 24 03:11:33.324: INFO: Pod "pod-subpath-test-secret-s9zd": Phase="Running", Reason="", readiness=true. Elapsed: 2.010844763s
Dec 24 03:11:35.327: INFO: Pod "pod-subpath-test-secret-s9zd": Phase="Running", Reason="", readiness=true. Elapsed: 4.013583427s
Dec 24 03:11:37.344: INFO: Pod "pod-subpath-test-secret-s9zd": Phase="Running", Reason="", readiness=true. Elapsed: 6.031191448s
Dec 24 03:11:39.347: INFO: Pod "pod-subpath-test-secret-s9zd": Phase="Running", Reason="", readiness=true. Elapsed: 8.034035253s
Dec 24 03:11:41.350: INFO: Pod "pod-subpath-test-secret-s9zd": Phase="Running", Reason="", readiness=true. Elapsed: 10.037226896s
Dec 24 03:11:43.354: INFO: Pod "pod-subpath-test-secret-s9zd": Phase="Running", Reason="", readiness=true. Elapsed: 12.04046634s
Dec 24 03:11:45.356: INFO: Pod "pod-subpath-test-secret-s9zd": Phase="Running", Reason="", readiness=true. Elapsed: 14.043040583s
Dec 24 03:11:47.359: INFO: Pod "pod-subpath-test-secret-s9zd": Phase="Running", Reason="", readiness=true. Elapsed: 16.045960941s
Dec 24 03:11:49.362: INFO: Pod "pod-subpath-test-secret-s9zd": Phase="Running", Reason="", readiness=true. Elapsed: 18.048669629s
Dec 24 03:11:51.365: INFO: Pod "pod-subpath-test-secret-s9zd": Phase="Running", Reason="", readiness=true. Elapsed: 20.051502277s
Dec 24 03:11:53.367: INFO: Pod "pod-subpath-test-secret-s9zd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.054275521s
STEP: Saw pod success
Dec 24 03:11:53.367: INFO: Pod "pod-subpath-test-secret-s9zd" satisfied condition "Succeeded or Failed"
Dec 24 03:11:53.370: INFO: Trying to get logs from node c1-4 pod pod-subpath-test-secret-s9zd container test-container-subpath-secret-s9zd: <nil>
STEP: delete the pod
Dec 24 03:11:53.433: INFO: Waiting for pod pod-subpath-test-secret-s9zd to disappear
Dec 24 03:11:53.441: INFO: Pod pod-subpath-test-secret-s9zd no longer exists
STEP: Deleting pod pod-subpath-test-secret-s9zd
Dec 24 03:11:53.441: INFO: Deleting pod "pod-subpath-test-secret-s9zd" in namespace "subpath-4067"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:11:53.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4067" for this suite.

• [SLOW TEST:22.218 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":305,"completed":177,"skipped":2826,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:11:53.451: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 03:11:53.486: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 24 03:11:58.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 --namespace=crd-publish-openapi-7419 create -f -'
Dec 24 03:11:59.915: INFO: stderr: ""
Dec 24 03:11:59.915: INFO: stdout: "e2e-test-crd-publish-openapi-9262-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 24 03:11:59.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 --namespace=crd-publish-openapi-7419 delete e2e-test-crd-publish-openapi-9262-crds test-cr'
Dec 24 03:12:00.029: INFO: stderr: ""
Dec 24 03:12:00.029: INFO: stdout: "e2e-test-crd-publish-openapi-9262-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Dec 24 03:12:00.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 --namespace=crd-publish-openapi-7419 apply -f -'
Dec 24 03:12:00.381: INFO: stderr: ""
Dec 24 03:12:00.381: INFO: stdout: "e2e-test-crd-publish-openapi-9262-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 24 03:12:00.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 --namespace=crd-publish-openapi-7419 delete e2e-test-crd-publish-openapi-9262-crds test-cr'
Dec 24 03:12:00.473: INFO: stderr: ""
Dec 24 03:12:00.473: INFO: stdout: "e2e-test-crd-publish-openapi-9262-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Dec 24 03:12:00.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 explain e2e-test-crd-publish-openapi-9262-crds'
Dec 24 03:12:00.812: INFO: stderr: ""
Dec 24 03:12:00.812: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9262-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:12:05.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7419" for this suite.

• [SLOW TEST:12.524 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":305,"completed":178,"skipped":2828,"failed":0}
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:12:05.975: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 24 03:12:06.089: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 03:12:06.106: INFO: Number of nodes with available pods: 0
Dec 24 03:12:06.106: INFO: Node c1-4 is running more than one daemon pod
Dec 24 03:12:07.111: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 03:12:07.114: INFO: Number of nodes with available pods: 0
Dec 24 03:12:07.114: INFO: Node c1-4 is running more than one daemon pod
Dec 24 03:12:08.111: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 03:12:08.114: INFO: Number of nodes with available pods: 3
Dec 24 03:12:08.114: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Dec 24 03:12:08.148: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 03:12:08.151: INFO: Number of nodes with available pods: 2
Dec 24 03:12:08.151: INFO: Node c1-4 is running more than one daemon pod
Dec 24 03:12:09.156: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 03:12:09.159: INFO: Number of nodes with available pods: 2
Dec 24 03:12:09.159: INFO: Node c1-4 is running more than one daemon pod
Dec 24 03:12:10.158: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 03:12:10.160: INFO: Number of nodes with available pods: 2
Dec 24 03:12:10.160: INFO: Node c1-4 is running more than one daemon pod
Dec 24 03:12:11.155: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 03:12:11.157: INFO: Number of nodes with available pods: 2
Dec 24 03:12:11.157: INFO: Node c1-4 is running more than one daemon pod
Dec 24 03:12:12.155: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 03:12:12.158: INFO: Number of nodes with available pods: 2
Dec 24 03:12:12.158: INFO: Node c1-4 is running more than one daemon pod
Dec 24 03:12:13.156: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 03:12:13.159: INFO: Number of nodes with available pods: 2
Dec 24 03:12:13.159: INFO: Node c1-4 is running more than one daemon pod
Dec 24 03:12:14.155: INFO: DaemonSet pods can't tolerate node c1-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 24 03:12:14.158: INFO: Number of nodes with available pods: 3
Dec 24 03:12:14.158: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3866, will wait for the garbage collector to delete the pods
Dec 24 03:12:14.218: INFO: Deleting DaemonSet.extensions daemon-set took: 5.010134ms
Dec 24 03:12:15.218: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.000169368s
Dec 24 03:12:28.927: INFO: Number of nodes with available pods: 0
Dec 24 03:12:28.927: INFO: Number of running nodes: 0, number of available pods: 0
Dec 24 03:12:28.929: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3866/daemonsets","resourceVersion":"3459013"},"items":null}

Dec 24 03:12:28.930: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3866/pods","resourceVersion":"3459013"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:12:28.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3866" for this suite.

• [SLOW TEST:22.975 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":305,"completed":179,"skipped":2834,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:12:28.950: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 24 03:12:30.073: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 24 03:12:33.096: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:12:33.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5228" for this suite.
STEP: Destroying namespace "webhook-5228-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":305,"completed":180,"skipped":2841,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:12:33.291: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test env composition
Dec 24 03:12:33.360: INFO: Waiting up to 5m0s for pod "var-expansion-270751d9-cf6c-4b15-8da4-de69aa795913" in namespace "var-expansion-371" to be "Succeeded or Failed"
Dec 24 03:12:33.364: INFO: Pod "var-expansion-270751d9-cf6c-4b15-8da4-de69aa795913": Phase="Pending", Reason="", readiness=false. Elapsed: 4.215392ms
Dec 24 03:12:35.367: INFO: Pod "var-expansion-270751d9-cf6c-4b15-8da4-de69aa795913": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006911264s
Dec 24 03:12:37.370: INFO: Pod "var-expansion-270751d9-cf6c-4b15-8da4-de69aa795913": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010146651s
STEP: Saw pod success
Dec 24 03:12:37.370: INFO: Pod "var-expansion-270751d9-cf6c-4b15-8da4-de69aa795913" satisfied condition "Succeeded or Failed"
Dec 24 03:12:37.372: INFO: Trying to get logs from node c1-4 pod var-expansion-270751d9-cf6c-4b15-8da4-de69aa795913 container dapi-container: <nil>
STEP: delete the pod
Dec 24 03:12:37.413: INFO: Waiting for pod var-expansion-270751d9-cf6c-4b15-8da4-de69aa795913 to disappear
Dec 24 03:12:37.418: INFO: Pod var-expansion-270751d9-cf6c-4b15-8da4-de69aa795913 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:12:37.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-371" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":305,"completed":181,"skipped":2854,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:12:37.459: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
Dec 24 03:12:38.019: INFO: created pod pod-service-account-defaultsa
Dec 24 03:12:38.019: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec 24 03:12:38.024: INFO: created pod pod-service-account-mountsa
Dec 24 03:12:38.024: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec 24 03:12:38.039: INFO: created pod pod-service-account-nomountsa
Dec 24 03:12:38.039: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec 24 03:12:38.051: INFO: created pod pod-service-account-defaultsa-mountspec
Dec 24 03:12:38.051: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec 24 03:12:38.062: INFO: created pod pod-service-account-mountsa-mountspec
Dec 24 03:12:38.062: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec 24 03:12:38.077: INFO: created pod pod-service-account-nomountsa-mountspec
Dec 24 03:12:38.077: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec 24 03:12:38.085: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec 24 03:12:38.085: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec 24 03:12:38.115: INFO: created pod pod-service-account-mountsa-nomountspec
Dec 24 03:12:38.115: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec 24 03:12:38.119: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec 24 03:12:38.119: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:12:38.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1229" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":305,"completed":182,"skipped":2892,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:12:38.150: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 03:12:38.201: INFO: Creating deployment "webserver-deployment"
Dec 24 03:12:38.206: INFO: Waiting for observed generation 1
Dec 24 03:12:40.244: INFO: Waiting for all required pods to come up
Dec 24 03:12:40.247: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Dec 24 03:12:48.253: INFO: Waiting for deployment "webserver-deployment" to complete
Dec 24 03:12:48.258: INFO: Updating deployment "webserver-deployment" with a non-existent image
Dec 24 03:12:48.277: INFO: Updating deployment webserver-deployment
Dec 24 03:12:48.277: INFO: Waiting for observed generation 2
Dec 24 03:12:50.284: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec 24 03:12:50.286: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec 24 03:12:50.288: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 24 03:12:50.295: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec 24 03:12:50.295: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec 24 03:12:50.297: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 24 03:12:50.301: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Dec 24 03:12:50.301: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Dec 24 03:12:50.308: INFO: Updating deployment webserver-deployment
Dec 24 03:12:50.308: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Dec 24 03:12:50.349: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec 24 03:12:50.384: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Dec 24 03:12:50.419: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8067 /apis/apps/v1/namespaces/deployment-8067/deployments/webserver-deployment 56d21083-0d2e-45a7-9c66-e73bfb871560 3459719 3 2020-12-24 03:12:38 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-12-24 03:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-12-24 03:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007c70fc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2020-12-24 03:12:48 +0000 UTC,LastTransitionTime:2020-12-24 03:12:38 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-12-24 03:12:50 +0000 UTC,LastTransitionTime:2020-12-24 03:12:50 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Dec 24 03:12:50.441: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-8067 /apis/apps/v1/namespaces/deployment-8067/replicasets/webserver-deployment-795d758f88 f7c72d8b-ff6f-4668-a6df-84d2196863e0 3459754 3 2020-12-24 03:12:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 56d21083-0d2e-45a7-9c66-e73bfb871560 0xc007c718b7 0xc007c718b8}] []  [{kube-controller-manager Update apps/v1 2020-12-24 03:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56d21083-0d2e-45a7-9c66-e73bfb871560\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007c71968 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 24 03:12:50.441: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Dec 24 03:12:50.441: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-8067 /apis/apps/v1/namespaces/deployment-8067/replicasets/webserver-deployment-dd94f59b7 f05b8ba4-73b7-4de6-88b7-beb016a192ec 3459748 3 2020-12-24 03:12:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 56d21083-0d2e-45a7-9c66-e73bfb871560 0xc007c719e7 0xc007c719e8}] []  [{kube-controller-manager Update apps/v1 2020-12-24 03:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56d21083-0d2e-45a7-9c66-e73bfb871560\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007c71a98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Dec 24 03:12:50.463: INFO: Pod "webserver-deployment-795d758f88-2cnbs" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-2cnbs webserver-deployment-795d758f88- deployment-8067 /api/v1/namespaces/deployment-8067/pods/webserver-deployment-795d758f88-2cnbs 3d00bc88-a24c-4009-92dc-0fa3459b2b74 3459690 0 2020-12-24 03:12:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.244.26.115/32 cni.projectcalico.org/podIPs:10.244.26.115/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 f7c72d8b-ff6f-4668-a6df-84d2196863e0 0xc007c138b7 0xc007c138b8}] []  [{kube-controller-manager Update v1 2020-12-24 03:12:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7c72d8b-ff6f-4668-a6df-84d2196863e0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-24 03:12:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-12-24 03:12:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lf6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lf6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lf6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.3.6,PodIP:,StartTime:2020-12-24 03:12:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 24 03:12:50.463: INFO: Pod "webserver-deployment-795d758f88-6sqlh" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-6sqlh webserver-deployment-795d758f88- deployment-8067 /api/v1/namespaces/deployment-8067/pods/webserver-deployment-795d758f88-6sqlh bd29fbc2-774a-466f-8968-95fd871f8d7f 3459739 0 2020-12-24 03:12:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 f7c72d8b-ff6f-4668-a6df-84d2196863e0 0xc007c13c57 0xc007c13c58}] []  [{kube-controller-manager Update v1 2020-12-24 03:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7c72d8b-ff6f-4668-a6df-84d2196863e0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lf6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lf6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lf6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 24 03:12:50.463: INFO: Pod "webserver-deployment-795d758f88-8f7s4" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-8f7s4 webserver-deployment-795d758f88- deployment-8067 /api/v1/namespaces/deployment-8067/pods/webserver-deployment-795d758f88-8f7s4 21e0162b-edb2-4f56-952f-3de82026acf7 3459671 0 2020-12-24 03:12:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.244.113.86/32 cni.projectcalico.org/podIPs:10.244.113.86/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 f7c72d8b-ff6f-4668-a6df-84d2196863e0 0xc007c13eb7 0xc007c13eb8}] []  [{calico Update v1 2020-12-24 03:12:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kube-controller-manager Update v1 2020-12-24 03:12:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7c72d8b-ff6f-4668-a6df-84d2196863e0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lf6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lf6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lf6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 24 03:12:50.463: INFO: Pod "webserver-deployment-795d758f88-8m2qq" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-8m2qq webserver-deployment-795d758f88- deployment-8067 /api/v1/namespaces/deployment-8067/pods/webserver-deployment-795d758f88-8m2qq 84396a52-6386-4b16-987f-243a9a629ccd 3459727 0 2020-12-24 03:12:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 f7c72d8b-ff6f-4668-a6df-84d2196863e0 0xc007d7a107 0xc007d7a108}] []  [{kube-controller-manager Update v1 2020-12-24 03:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7c72d8b-ff6f-4668-a6df-84d2196863e0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lf6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lf6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lf6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 24 03:12:50.463: INFO: Pod "webserver-deployment-795d758f88-bkzd7" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-bkzd7 webserver-deployment-795d758f88- deployment-8067 /api/v1/namespaces/deployment-8067/pods/webserver-deployment-795d758f88-bkzd7 e701bf4c-458c-4d2f-accd-95580b196694 3459682 0 2020-12-24 03:12:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.244.26.113/32 cni.projectcalico.org/podIPs:10.244.26.113/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 f7c72d8b-ff6f-4668-a6df-84d2196863e0 0xc007d7a2a7 0xc007d7a2a8}] []  [{kube-controller-manager Update v1 2020-12-24 03:12:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7c72d8b-ff6f-4668-a6df-84d2196863e0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-24 03:12:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-12-24 03:12:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lf6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lf6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lf6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.3.6,PodIP:,StartTime:2020-12-24 03:12:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 24 03:12:50.463: INFO: Pod "webserver-deployment-795d758f88-c5zlf" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-c5zlf webserver-deployment-795d758f88- deployment-8067 /api/v1/namespaces/deployment-8067/pods/webserver-deployment-795d758f88-c5zlf 5c8a85d0-e7e4-421e-be74-913723fd9ac7 3459743 0 2020-12-24 03:12:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 f7c72d8b-ff6f-4668-a6df-84d2196863e0 0xc007d7a4e7 0xc007d7a4e8}] []  [{kube-controller-manager Update v1 2020-12-24 03:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7c72d8b-ff6f-4668-a6df-84d2196863e0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lf6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lf6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lf6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 24 03:12:50.463: INFO: Pod "webserver-deployment-795d758f88-djnvd" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-djnvd webserver-deployment-795d758f88- deployment-8067 /api/v1/namespaces/deployment-8067/pods/webserver-deployment-795d758f88-djnvd 45f26ddc-3504-48a0-bdd7-6a41c569e4a2 3459725 0 2020-12-24 03:12:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 f7c72d8b-ff6f-4668-a6df-84d2196863e0 0xc007d7a697 0xc007d7a698}] []  [{kube-controller-manager Update v1 2020-12-24 03:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7c72d8b-ff6f-4668-a6df-84d2196863e0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lf6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lf6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lf6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 24 03:12:50.464: INFO: Pod "webserver-deployment-795d758f88-jwq46" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-jwq46 webserver-deployment-795d758f88- deployment-8067 /api/v1/namespaces/deployment-8067/pods/webserver-deployment-795d758f88-jwq46 6c94e669-7b99-4782-8908-1d56b90dd530 3459749 0 2020-12-24 03:12:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 f7c72d8b-ff6f-4668-a6df-84d2196863e0 0xc007d7a8a7 0xc007d7a8a8}] []  [{kube-controller-manager Update v1 2020-12-24 03:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7c72d8b-ff6f-4668-a6df-84d2196863e0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lf6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lf6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lf6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 24 03:12:50.464: INFO: Pod "webserver-deployment-795d758f88-k8vnj" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-k8vnj webserver-deployment-795d758f88- deployment-8067 /api/v1/namespaces/deployment-8067/pods/webserver-deployment-795d758f88-k8vnj 92253511-0008-442e-8743-b133d564fe45 3459756 0 2020-12-24 03:12:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 f7c72d8b-ff6f-4668-a6df-84d2196863e0 0xc007d7aae7 0xc007d7aae8}] []  [{kube-controller-manager Update v1 2020-12-24 03:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7c72d8b-ff6f-4668-a6df-84d2196863e0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-24 03:12:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lf6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lf6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lf6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.3.7,PodIP:,StartTime:2020-12-24 03:12:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 24 03:12:50.464: INFO: Pod "webserver-deployment-795d758f88-v7tcc" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-v7tcc webserver-deployment-795d758f88- deployment-8067 /api/v1/namespaces/deployment-8067/pods/webserver-deployment-795d758f88-v7tcc bdc32bc8-6366-4331-b500-43709aac69e4 3459742 0 2020-12-24 03:12:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 f7c72d8b-ff6f-4668-a6df-84d2196863e0 0xc007d7ad77 0xc007d7ad78}] []  [{kube-controller-manager Update v1 2020-12-24 03:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7c72d8b-ff6f-4668-a6df-84d2196863e0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lf6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lf6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lf6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 24 03:12:50.464: INFO: Pod "webserver-deployment-795d758f88-wp669" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-wp669 webserver-deployment-795d758f88- deployment-8067 /api/v1/namespaces/deployment-8067/pods/webserver-deployment-795d758f88-wp669 9bcb093d-1371-40d1-8e9f-ecbf5b6cf04a 3459680 0 2020-12-24 03:12:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.244.113.80/32 cni.projectcalico.org/podIPs:10.244.113.80/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 f7c72d8b-ff6f-4668-a6df-84d2196863e0 0xc007d7af27 0xc007d7af28}] []  [{kube-controller-manager Update v1 2020-12-24 03:12:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7c72d8b-ff6f-4668-a6df-84d2196863e0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-24 03:12:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lf6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lf6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lf6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 24 03:12:50.464: INFO: Pod "webserver-deployment-795d758f88-wttzm" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-wttzm webserver-deployment-795d758f88- deployment-8067 /api/v1/namespaces/deployment-8067/pods/webserver-deployment-795d758f88-wttzm 698f4731-239a-4860-b664-45d86882c421 3459673 0 2020-12-24 03:12:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.244.92.240/32 cni.projectcalico.org/podIPs:10.244.92.240/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 f7c72d8b-ff6f-4668-a6df-84d2196863e0 0xc007d7b127 0xc007d7b128}] []  [{calico Update v1 2020-12-24 03:12:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kube-controller-manager Update v1 2020-12-24 03:12:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7c72d8b-ff6f-4668-a6df-84d2196863e0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-24 03:12:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lf6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lf6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lf6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.3.7,PodIP:,StartTime:2020-12-24 03:12:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 24 03:12:50.464: INFO: Pod "webserver-deployment-795d758f88-zs79p" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-zs79p webserver-deployment-795d758f88- deployment-8067 /api/v1/namespaces/deployment-8067/pods/webserver-deployment-795d758f88-zs79p 30911848-7571-476a-820a-85ea18a2c845 3459737 0 2020-12-24 03:12:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 f7c72d8b-ff6f-4668-a6df-84d2196863e0 0xc007d7b3b7 0xc007d7b3b8}] []  [{kube-controller-manager Update v1 2020-12-24 03:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7c72d8b-ff6f-4668-a6df-84d2196863e0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lf6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lf6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lf6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 24 03:12:50.464: INFO: Pod "webserver-deployment-dd94f59b7-2brhd" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-2brhd webserver-deployment-dd94f59b7- deployment-8067 /api/v1/namespaces/deployment-8067/pods/webserver-deployment-dd94f59b7-2brhd 5ebc5911-129f-46df-bbfe-cf18d795276b 3459747 0 2020-12-24 03:12:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f05b8ba4-73b7-4de6-88b7-beb016a192ec 0xc007d7b5f7 0xc007d7b5f8}] []  [{kube-controller-manager Update v1 2020-12-24 03:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f05b8ba4-73b7-4de6-88b7-beb016a192ec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lf6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lf6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lf6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 24 03:12:50.464: INFO: Pod "webserver-deployment-dd94f59b7-56hbd" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-56hbd webserver-deployment-dd94f59b7- deployment-8067 /api/v1/namespaces/deployment-8067/pods/webserver-deployment-dd94f59b7-56hbd aef458a7-3182-403e-9969-540461e735d1 3459437 0 2020-12-24 03:12:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.26.114/32 cni.projectcalico.org/podIPs:10.244.26.114/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f05b8ba4-73b7-4de6-88b7-beb016a192ec 0xc007d7b847 0xc007d7b848}] []  [{kube-controller-manager Update v1 2020-12-24 03:12:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f05b8ba4-73b7-4de6-88b7-beb016a192ec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-24 03:12:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-24 03:12:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.26.114\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lf6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lf6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lf6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.3.6,PodIP:10.244.26.114,StartTime:2020-12-24 03:12:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-24 03:12:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://4308a4d6966ea06e99e737f62537ccd121f189b2f899d51171ec44eb31d965be,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.26.114,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 24 03:12:50.465: INFO: Pod "webserver-deployment-dd94f59b7-6cbtl" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-6cbtl webserver-deployment-dd94f59b7- deployment-8067 /api/v1/namespaces/deployment-8067/pods/webserver-deployment-dd94f59b7-6cbtl 3ccacb66-80fe-438a-a7f1-f7ae827f8cfd 3459418 0 2020-12-24 03:12:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.92.233/32 cni.projectcalico.org/podIPs:10.244.92.233/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f05b8ba4-73b7-4de6-88b7-beb016a192ec 0xc007d7bd17 0xc007d7bd18}] []  [{kube-controller-manager Update v1 2020-12-24 03:12:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f05b8ba4-73b7-4de6-88b7-beb016a192ec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-24 03:12:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-24 03:12:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.92.233\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lf6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lf6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lf6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.3.7,PodIP:10.244.92.233,StartTime:2020-12-24 03:12:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-24 03:12:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://03005a4b3a65d9c04dedf33b813e827b81395f8234f272bf460f556b44252d2e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.92.233,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 24 03:12:50.465: INFO: Pod "webserver-deployment-dd94f59b7-7l852" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-7l852 webserver-deployment-dd94f59b7- deployment-8067 /api/v1/namespaces/deployment-8067/pods/webserver-deployment-dd94f59b7-7l852 fe4ca53b-d66f-4c2b-b2fb-dac8282c61ff 3459746 0 2020-12-24 03:12:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f05b8ba4-73b7-4de6-88b7-beb016a192ec 0xc0080d6127 0xc0080d6128}] []  [{kube-controller-manager Update v1 2020-12-24 03:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f05b8ba4-73b7-4de6-88b7-beb016a192ec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lf6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lf6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lf6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 24 03:12:50.465: INFO: Pod "webserver-deployment-dd94f59b7-7ssrm" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-7ssrm webserver-deployment-dd94f59b7- deployment-8067 /api/v1/namespaces/deployment-8067/pods/webserver-deployment-dd94f59b7-7ssrm 24145b73-d3b7-498f-93db-458cef7988e5 3459729 0 2020-12-24 03:12:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f05b8ba4-73b7-4de6-88b7-beb016a192ec 0xc0080d6367 0xc0080d6368}] []  [{kube-controller-manager Update v1 2020-12-24 03:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f05b8ba4-73b7-4de6-88b7-beb016a192ec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lf6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lf6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lf6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 24 03:12:50.465: INFO: Pod "webserver-deployment-dd94f59b7-8jtf9" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-8jtf9 webserver-deployment-dd94f59b7- deployment-8067 /api/v1/namespaces/deployment-8067/pods/webserver-deployment-dd94f59b7-8jtf9 760babdb-5d36-4edf-8d2b-df857ae14dfb 3459411 0 2020-12-24 03:12:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.26.106/32 cni.projectcalico.org/podIPs:10.244.26.106/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f05b8ba4-73b7-4de6-88b7-beb016a192ec 0xc0080d6617 0xc0080d6618}] []  [{kube-controller-manager Update v1 2020-12-24 03:12:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f05b8ba4-73b7-4de6-88b7-beb016a192ec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-24 03:12:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-24 03:12:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.26.106\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lf6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lf6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lf6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.3.6,PodIP:10.244.26.106,StartTime:2020-12-24 03:12:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-24 03:12:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://f4475535d1bd6b7439f5bf9049db8e8d2246039a0a2899410c49941c2b9deef6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.26.106,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 24 03:12:50.465: INFO: Pod "webserver-deployment-dd94f59b7-922bk" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-922bk webserver-deployment-dd94f59b7- deployment-8067 /api/v1/namespaces/deployment-8067/pods/webserver-deployment-dd94f59b7-922bk 030390d5-99cd-43e2-a14e-685972f60106 3459440 0 2020-12-24 03:12:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.26.112/32 cni.projectcalico.org/podIPs:10.244.26.112/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f05b8ba4-73b7-4de6-88b7-beb016a192ec 0xc0080d6947 0xc0080d6948}] []  [{kube-controller-manager Update v1 2020-12-24 03:12:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f05b8ba4-73b7-4de6-88b7-beb016a192ec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-24 03:12:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-24 03:12:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.26.112\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lf6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lf6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lf6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.3.6,PodIP:10.244.26.112,StartTime:2020-12-24 03:12:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-24 03:12:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://4fc059bcac126e9773140ece297ead3c7162e053a1d179de6df8a4a9b0d02a87,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.26.112,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 24 03:12:50.465: INFO: Pod "webserver-deployment-dd94f59b7-cdsth" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-cdsth webserver-deployment-dd94f59b7- deployment-8067 /api/v1/namespaces/deployment-8067/pods/webserver-deployment-dd94f59b7-cdsth 5001e7c7-ba61-434c-9c28-68ba54daff49 3459744 0 2020-12-24 03:12:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f05b8ba4-73b7-4de6-88b7-beb016a192ec 0xc0080d6cf7 0xc0080d6cf8}] []  [{kube-controller-manager Update v1 2020-12-24 03:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f05b8ba4-73b7-4de6-88b7-beb016a192ec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lf6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lf6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lf6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 24 03:12:50.466: INFO: Pod "webserver-deployment-dd94f59b7-hzcxv" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-hzcxv webserver-deployment-dd94f59b7- deployment-8067 /api/v1/namespaces/deployment-8067/pods/webserver-deployment-dd94f59b7-hzcxv 9d5833b9-7e1b-4274-85f2-5e7cbd9a14f2 3459745 0 2020-12-24 03:12:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f05b8ba4-73b7-4de6-88b7-beb016a192ec 0xc0080d6f47 0xc0080d6f48}] []  [{kube-controller-manager Update v1 2020-12-24 03:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f05b8ba4-73b7-4de6-88b7-beb016a192ec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lf6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lf6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lf6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 24 03:12:50.466: INFO: Pod "webserver-deployment-dd94f59b7-khwt4" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-khwt4 webserver-deployment-dd94f59b7- deployment-8067 /api/v1/namespaces/deployment-8067/pods/webserver-deployment-dd94f59b7-khwt4 59a6818b-3acf-45a6-8039-4d8553f2bdc6 3459735 0 2020-12-24 03:12:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f05b8ba4-73b7-4de6-88b7-beb016a192ec 0xc0080d7107 0xc0080d7108}] []  [{kube-controller-manager Update v1 2020-12-24 03:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f05b8ba4-73b7-4de6-88b7-beb016a192ec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lf6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lf6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lf6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 24 03:12:50.466: INFO: Pod "webserver-deployment-dd94f59b7-lrn7k" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-lrn7k webserver-deployment-dd94f59b7- deployment-8067 /api/v1/namespaces/deployment-8067/pods/webserver-deployment-dd94f59b7-lrn7k 11cef3c4-41ae-430e-9dd3-fb0d36232e24 3459734 0 2020-12-24 03:12:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f05b8ba4-73b7-4de6-88b7-beb016a192ec 0xc0080d73c7 0xc0080d73c8}] []  [{kube-controller-manager Update v1 2020-12-24 03:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f05b8ba4-73b7-4de6-88b7-beb016a192ec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lf6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lf6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lf6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 24 03:12:50.466: INFO: Pod "webserver-deployment-dd94f59b7-mznd6" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-mznd6 webserver-deployment-dd94f59b7- deployment-8067 /api/v1/namespaces/deployment-8067/pods/webserver-deployment-dd94f59b7-mznd6 9295d513-5613-43e2-9953-b0f1fa972a17 3459444 0 2020-12-24 03:12:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.92.226/32 cni.projectcalico.org/podIPs:10.244.92.226/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f05b8ba4-73b7-4de6-88b7-beb016a192ec 0xc0080d75d7 0xc0080d75d8}] []  [{kube-controller-manager Update v1 2020-12-24 03:12:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f05b8ba4-73b7-4de6-88b7-beb016a192ec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-24 03:12:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-24 03:12:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.92.226\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lf6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lf6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lf6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.3.7,PodIP:10.244.92.226,StartTime:2020-12-24 03:12:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-24 03:12:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://c83e8e8babc77c15c8d5ce95cd92e1ebb06269ede5c3cac61048a6dd1c5a7d78,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.92.226,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 24 03:12:50.466: INFO: Pod "webserver-deployment-dd94f59b7-ngrcv" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-ngrcv webserver-deployment-dd94f59b7- deployment-8067 /api/v1/namespaces/deployment-8067/pods/webserver-deployment-dd94f59b7-ngrcv 6bf8b98c-a237-4d2d-ba5e-693c9da5dab7 3459755 0 2020-12-24 03:12:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f05b8ba4-73b7-4de6-88b7-beb016a192ec 0xc0080d79b7 0xc0080d79b8}] []  [{kube-controller-manager Update v1 2020-12-24 03:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f05b8ba4-73b7-4de6-88b7-beb016a192ec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-24 03:12:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lf6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lf6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lf6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.3.6,PodIP:,StartTime:2020-12-24 03:12:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 24 03:12:50.466: INFO: Pod "webserver-deployment-dd94f59b7-p2qjw" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-p2qjw webserver-deployment-dd94f59b7- deployment-8067 /api/v1/namespaces/deployment-8067/pods/webserver-deployment-dd94f59b7-p2qjw f1eb502d-9a33-4659-861e-3eb4f6e8208f 3459713 0 2020-12-24 03:12:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f05b8ba4-73b7-4de6-88b7-beb016a192ec 0xc0080d7c57 0xc0080d7c58}] []  [{kube-controller-manager Update v1 2020-12-24 03:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f05b8ba4-73b7-4de6-88b7-beb016a192ec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lf6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lf6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lf6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 24 03:12:50.466: INFO: Pod "webserver-deployment-dd94f59b7-pgxjr" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-pgxjr webserver-deployment-dd94f59b7- deployment-8067 /api/v1/namespaces/deployment-8067/pods/webserver-deployment-dd94f59b7-pgxjr 0d9057b4-cb1b-469c-87ee-d5d7e9eafe54 3459736 0 2020-12-24 03:12:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f05b8ba4-73b7-4de6-88b7-beb016a192ec 0xc0080d7dd7 0xc0080d7dd8}] []  [{kube-controller-manager Update v1 2020-12-24 03:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f05b8ba4-73b7-4de6-88b7-beb016a192ec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lf6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lf6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lf6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 24 03:12:50.467: INFO: Pod "webserver-deployment-dd94f59b7-r4559" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-r4559 webserver-deployment-dd94f59b7- deployment-8067 /api/v1/namespaces/deployment-8067/pods/webserver-deployment-dd94f59b7-r4559 7b5ff9c1-2291-4896-8a9c-af93f670624a 3459589 0 2020-12-24 03:12:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.113.110/32 cni.projectcalico.org/podIPs:10.244.113.110/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f05b8ba4-73b7-4de6-88b7-beb016a192ec 0xc0080d7f37 0xc0080d7f38}] []  [{kube-controller-manager Update v1 2020-12-24 03:12:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f05b8ba4-73b7-4de6-88b7-beb016a192ec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-24 03:12:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-24 03:12:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.113.110\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lf6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lf6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lf6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.3.5,PodIP:10.244.113.110,StartTime:2020-12-24 03:12:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-24 03:12:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://d1f297deb9c675656aed43d23556b2bf76a5a2eed3416097346636d7041594cd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.113.110,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 24 03:12:50.467: INFO: Pod "webserver-deployment-dd94f59b7-vpbw7" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-vpbw7 webserver-deployment-dd94f59b7- deployment-8067 /api/v1/namespaces/deployment-8067/pods/webserver-deployment-dd94f59b7-vpbw7 f754087f-2eac-45e0-be58-789ef6f4ad3b 3459579 0 2020-12-24 03:12:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.113.122/32 cni.projectcalico.org/podIPs:10.244.113.122/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f05b8ba4-73b7-4de6-88b7-beb016a192ec 0xc00810a297 0xc00810a298}] []  [{kube-controller-manager Update v1 2020-12-24 03:12:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f05b8ba4-73b7-4de6-88b7-beb016a192ec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-24 03:12:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-24 03:12:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.113.122\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lf6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lf6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lf6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.3.5,PodIP:10.244.113.122,StartTime:2020-12-24 03:12:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-24 03:12:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://d2d4e82f996622ff216e5987e65ba0e743a0271f77570c0126dc1fa18c376e45,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.113.122,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 24 03:12:50.467: INFO: Pod "webserver-deployment-dd94f59b7-wpw72" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-wpw72 webserver-deployment-dd94f59b7- deployment-8067 /api/v1/namespaces/deployment-8067/pods/webserver-deployment-dd94f59b7-wpw72 d595b921-7ce4-4423-8645-6996f1e72707 3459741 0 2020-12-24 03:12:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f05b8ba4-73b7-4de6-88b7-beb016a192ec 0xc00810a537 0xc00810a538}] []  [{kube-controller-manager Update v1 2020-12-24 03:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f05b8ba4-73b7-4de6-88b7-beb016a192ec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lf6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lf6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lf6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 24 03:12:50.467: INFO: Pod "webserver-deployment-dd94f59b7-z7m8m" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-z7m8m webserver-deployment-dd94f59b7- deployment-8067 /api/v1/namespaces/deployment-8067/pods/webserver-deployment-dd94f59b7-z7m8m ed0f40b4-6f3a-4a4c-b613-d17295d1b76f 3459447 0 2020-12-24 03:12:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.92.225/32 cni.projectcalico.org/podIPs:10.244.92.225/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f05b8ba4-73b7-4de6-88b7-beb016a192ec 0xc00810a767 0xc00810a768}] []  [{kube-controller-manager Update v1 2020-12-24 03:12:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f05b8ba4-73b7-4de6-88b7-beb016a192ec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-24 03:12:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-24 03:12:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.92.225\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lf6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lf6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lf6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.3.7,PodIP:10.244.92.225,StartTime:2020-12-24 03:12:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-24 03:12:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://ef748144470b1b3205ae2ba2922c4362ab9c3e3170c6a35fceb039d4866575a4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.92.225,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 24 03:12:50.467: INFO: Pod "webserver-deployment-dd94f59b7-zqf5x" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-zqf5x webserver-deployment-dd94f59b7- deployment-8067 /api/v1/namespaces/deployment-8067/pods/webserver-deployment-dd94f59b7-zqf5x 630996b9-5042-43ff-a906-6bc8ec0d4e38 3459706 0 2020-12-24 03:12:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f05b8ba4-73b7-4de6-88b7-beb016a192ec 0xc00810ab17 0xc00810ab18}] []  [{kube-controller-manager Update v1 2020-12-24 03:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f05b8ba4-73b7-4de6-88b7-beb016a192ec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lf6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lf6d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lf6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:12:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:12:50.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8067" for this suite.

• [SLOW TEST:12.387 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":305,"completed":183,"skipped":2920,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:12:50.538: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-793c74a8-5266-4247-82a8-3560b6a169e4
STEP: Creating a pod to test consume configMaps
Dec 24 03:12:50.670: INFO: Waiting up to 5m0s for pod "pod-configmaps-bb98cd34-d6e5-482a-b186-52eceaf60ab7" in namespace "configmap-7120" to be "Succeeded or Failed"
Dec 24 03:12:50.676: INFO: Pod "pod-configmaps-bb98cd34-d6e5-482a-b186-52eceaf60ab7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.192412ms
Dec 24 03:12:52.681: INFO: Pod "pod-configmaps-bb98cd34-d6e5-482a-b186-52eceaf60ab7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010708146s
Dec 24 03:12:54.703: INFO: Pod "pod-configmaps-bb98cd34-d6e5-482a-b186-52eceaf60ab7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033441587s
Dec 24 03:12:56.719: INFO: Pod "pod-configmaps-bb98cd34-d6e5-482a-b186-52eceaf60ab7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.049567564s
Dec 24 03:12:58.722: INFO: Pod "pod-configmaps-bb98cd34-d6e5-482a-b186-52eceaf60ab7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.051929864s
STEP: Saw pod success
Dec 24 03:12:58.722: INFO: Pod "pod-configmaps-bb98cd34-d6e5-482a-b186-52eceaf60ab7" satisfied condition "Succeeded or Failed"
Dec 24 03:12:58.724: INFO: Trying to get logs from node c1-4 pod pod-configmaps-bb98cd34-d6e5-482a-b186-52eceaf60ab7 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 24 03:12:58.761: INFO: Waiting for pod pod-configmaps-bb98cd34-d6e5-482a-b186-52eceaf60ab7 to disappear
Dec 24 03:12:58.769: INFO: Pod pod-configmaps-bb98cd34-d6e5-482a-b186-52eceaf60ab7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:12:58.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7120" for this suite.

• [SLOW TEST:8.237 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":184,"skipped":2926,"failed":0}
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:12:58.775: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override command
Dec 24 03:12:58.839: INFO: Waiting up to 5m0s for pod "client-containers-d353e4aa-fe0d-4075-a282-74df25a7142d" in namespace "containers-4086" to be "Succeeded or Failed"
Dec 24 03:12:58.843: INFO: Pod "client-containers-d353e4aa-fe0d-4075-a282-74df25a7142d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.33447ms
Dec 24 03:13:00.849: INFO: Pod "client-containers-d353e4aa-fe0d-4075-a282-74df25a7142d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010438071s
Dec 24 03:13:02.852: INFO: Pod "client-containers-d353e4aa-fe0d-4075-a282-74df25a7142d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013313006s
Dec 24 03:13:04.855: INFO: Pod "client-containers-d353e4aa-fe0d-4075-a282-74df25a7142d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016381558s
STEP: Saw pod success
Dec 24 03:13:04.855: INFO: Pod "client-containers-d353e4aa-fe0d-4075-a282-74df25a7142d" satisfied condition "Succeeded or Failed"
Dec 24 03:13:04.857: INFO: Trying to get logs from node c1-4 pod client-containers-d353e4aa-fe0d-4075-a282-74df25a7142d container test-container: <nil>
STEP: delete the pod
Dec 24 03:13:04.873: INFO: Waiting for pod client-containers-d353e4aa-fe0d-4075-a282-74df25a7142d to disappear
Dec 24 03:13:04.878: INFO: Pod client-containers-d353e4aa-fe0d-4075-a282-74df25a7142d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:13:04.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4086" for this suite.

• [SLOW TEST:6.120 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":305,"completed":185,"skipped":2927,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:13:04.896: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Dec 24 03:13:07.449: INFO: Successfully updated pod "adopt-release-ctjkr"
STEP: Checking that the Job readopts the Pod
Dec 24 03:13:07.449: INFO: Waiting up to 15m0s for pod "adopt-release-ctjkr" in namespace "job-2246" to be "adopted"
Dec 24 03:13:07.460: INFO: Pod "adopt-release-ctjkr": Phase="Running", Reason="", readiness=true. Elapsed: 11.351029ms
Dec 24 03:13:09.463: INFO: Pod "adopt-release-ctjkr": Phase="Running", Reason="", readiness=true. Elapsed: 2.014148622s
Dec 24 03:13:09.463: INFO: Pod "adopt-release-ctjkr" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Dec 24 03:13:09.969: INFO: Successfully updated pod "adopt-release-ctjkr"
STEP: Checking that the Job releases the Pod
Dec 24 03:13:09.969: INFO: Waiting up to 15m0s for pod "adopt-release-ctjkr" in namespace "job-2246" to be "released"
Dec 24 03:13:09.987: INFO: Pod "adopt-release-ctjkr": Phase="Running", Reason="", readiness=true. Elapsed: 17.656029ms
Dec 24 03:13:09.987: INFO: Pod "adopt-release-ctjkr" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:13:09.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2246" for this suite.

• [SLOW TEST:5.126 seconds]
[sig-apps] Job
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":305,"completed":186,"skipped":2941,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:13:10.022: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pod templates
Dec 24 03:13:10.084: INFO: created test-podtemplate-1
Dec 24 03:13:10.090: INFO: created test-podtemplate-2
Dec 24 03:13:10.095: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Dec 24 03:13:10.101: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Dec 24 03:13:10.147: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:13:10.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-4279" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":305,"completed":187,"skipped":2965,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:13:10.188: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 24 03:13:10.228: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4b123780-1f04-4041-b8f9-df134a0c209a" in namespace "projected-5456" to be "Succeeded or Failed"
Dec 24 03:13:10.243: INFO: Pod "downwardapi-volume-4b123780-1f04-4041-b8f9-df134a0c209a": Phase="Pending", Reason="", readiness=false. Elapsed: 15.058996ms
Dec 24 03:13:12.246: INFO: Pod "downwardapi-volume-4b123780-1f04-4041-b8f9-df134a0c209a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017720682s
STEP: Saw pod success
Dec 24 03:13:12.246: INFO: Pod "downwardapi-volume-4b123780-1f04-4041-b8f9-df134a0c209a" satisfied condition "Succeeded or Failed"
Dec 24 03:13:12.248: INFO: Trying to get logs from node c1-4 pod downwardapi-volume-4b123780-1f04-4041-b8f9-df134a0c209a container client-container: <nil>
STEP: delete the pod
Dec 24 03:13:12.267: INFO: Waiting for pod downwardapi-volume-4b123780-1f04-4041-b8f9-df134a0c209a to disappear
Dec 24 03:13:12.273: INFO: Pod downwardapi-volume-4b123780-1f04-4041-b8f9-df134a0c209a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:13:12.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5456" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":188,"skipped":2979,"failed":0}
SSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:13:12.280: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-6410/configmap-test-56cbb615-f48d-478e-b16b-95c5785b18b1
STEP: Creating a pod to test consume configMaps
Dec 24 03:13:12.356: INFO: Waiting up to 5m0s for pod "pod-configmaps-9cd55014-db08-4c40-bad5-bee3c34b351b" in namespace "configmap-6410" to be "Succeeded or Failed"
Dec 24 03:13:12.365: INFO: Pod "pod-configmaps-9cd55014-db08-4c40-bad5-bee3c34b351b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.857214ms
Dec 24 03:13:14.368: INFO: Pod "pod-configmaps-9cd55014-db08-4c40-bad5-bee3c34b351b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012040791s
Dec 24 03:13:16.371: INFO: Pod "pod-configmaps-9cd55014-db08-4c40-bad5-bee3c34b351b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014805606s
STEP: Saw pod success
Dec 24 03:13:16.371: INFO: Pod "pod-configmaps-9cd55014-db08-4c40-bad5-bee3c34b351b" satisfied condition "Succeeded or Failed"
Dec 24 03:13:16.373: INFO: Trying to get logs from node c1-4 pod pod-configmaps-9cd55014-db08-4c40-bad5-bee3c34b351b container env-test: <nil>
STEP: delete the pod
Dec 24 03:13:16.389: INFO: Waiting for pod pod-configmaps-9cd55014-db08-4c40-bad5-bee3c34b351b to disappear
Dec 24 03:13:16.394: INFO: Pod pod-configmaps-9cd55014-db08-4c40-bad5-bee3c34b351b no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:13:16.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6410" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":305,"completed":189,"skipped":2984,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:13:16.401: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Dec 24 03:13:16.485: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 24 03:14:16.568: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Dec 24 03:14:16.588: INFO: Created pod: pod0-sched-preemption-low-priority
Dec 24 03:14:16.635: INFO: Created pod: pod1-sched-preemption-medium-priority
Dec 24 03:14:16.685: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:14:24.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6231" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:68.432 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":305,"completed":190,"skipped":2996,"failed":0}
SS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:14:24.834: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-1672, will wait for the garbage collector to delete the pods
Dec 24 03:14:28.941: INFO: Deleting Job.batch foo took: 4.731921ms
Dec 24 03:14:29.941: INFO: Terminating Job.batch foo pods took: 1.000212692s
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:15:02.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1672" for this suite.

• [SLOW TEST:37.418 seconds]
[sig-apps] Job
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":305,"completed":191,"skipped":2998,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:15:02.252: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:15:04.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1966" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":305,"completed":192,"skipped":3013,"failed":0}

------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:15:04.373: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-91df0b1e-4371-4ed3-a101-2a170a4d85dc
STEP: Creating a pod to test consume secrets
Dec 24 03:15:04.455: INFO: Waiting up to 5m0s for pod "pod-secrets-4490305c-ccda-40f6-b905-8e3a2ce6d690" in namespace "secrets-6251" to be "Succeeded or Failed"
Dec 24 03:15:04.480: INFO: Pod "pod-secrets-4490305c-ccda-40f6-b905-8e3a2ce6d690": Phase="Pending", Reason="", readiness=false. Elapsed: 24.671962ms
Dec 24 03:15:06.483: INFO: Pod "pod-secrets-4490305c-ccda-40f6-b905-8e3a2ce6d690": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027383049s
STEP: Saw pod success
Dec 24 03:15:06.483: INFO: Pod "pod-secrets-4490305c-ccda-40f6-b905-8e3a2ce6d690" satisfied condition "Succeeded or Failed"
Dec 24 03:15:06.485: INFO: Trying to get logs from node c1-4 pod pod-secrets-4490305c-ccda-40f6-b905-8e3a2ce6d690 container secret-volume-test: <nil>
STEP: delete the pod
Dec 24 03:15:06.538: INFO: Waiting for pod pod-secrets-4490305c-ccda-40f6-b905-8e3a2ce6d690 to disappear
Dec 24 03:15:06.541: INFO: Pod pod-secrets-4490305c-ccda-40f6-b905-8e3a2ce6d690 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:15:06.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6251" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":193,"skipped":3013,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:15:06.549: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Dec 24 03:15:06.591: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 24 03:15:06.598: INFO: Waiting for terminating namespaces to be deleted...
Dec 24 03:15:06.600: INFO: 
Logging pods the apiserver thinks is on node c1-4 before test
Dec 24 03:15:06.615: INFO: calico-node-gf9xk from kube-system started at 2020-12-17 09:20:20 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.615: INFO: 	Container calico-node ready: true, restart count 0
Dec 24 03:15:06.615: INFO: kube-proxy-nnl7j from kube-system started at 2020-12-17 09:18:43 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.615: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 24 03:15:06.615: INFO: busybox-scheduling-39f7ed17-1911-4269-aa70-3eece9751d60 from kubelet-test-1966 started at 2020-12-24 03:15:02 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.615: INFO: 	Container busybox-scheduling-39f7ed17-1911-4269-aa70-3eece9751d60 ready: true, restart count 0
Dec 24 03:15:06.615: INFO: virt-handler-5nkqw from kubevirt started at 2020-12-23 10:54:33 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.615: INFO: 	Container virt-handler ready: false, restart count 0
Dec 24 03:15:06.615: INFO: csi-cephfsplugin-ghqns from rook-ceph started at 2020-12-23 10:54:33 +0000 UTC (3 container statuses recorded)
Dec 24 03:15:06.615: INFO: 	Container csi-cephfsplugin ready: false, restart count 0
Dec 24 03:15:06.615: INFO: 	Container driver-registrar ready: false, restart count 0
Dec 24 03:15:06.615: INFO: 	Container liveness-prometheus ready: false, restart count 0
Dec 24 03:15:06.615: INFO: csi-rbdplugin-z74d4 from rook-ceph started at 2020-12-23 10:54:33 +0000 UTC (3 container statuses recorded)
Dec 24 03:15:06.615: INFO: 	Container csi-rbdplugin ready: false, restart count 0
Dec 24 03:15:06.615: INFO: 	Container driver-registrar ready: false, restart count 0
Dec 24 03:15:06.615: INFO: 	Container liveness-prometheus ready: false, restart count 0
Dec 24 03:15:06.615: INFO: rook-ceph-crashcollector-c1-4-b5fd6cbf-jrn9q from rook-ceph started at 2020-12-24 02:42:56 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.615: INFO: 	Container ceph-crash ready: true, restart count 0
Dec 24 03:15:06.615: INFO: rook-ceph-crashcollector-c1-4-b5fd6cbf-r86bh from rook-ceph started at 2020-12-23 10:29:44 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.615: INFO: 	Container ceph-crash ready: false, restart count 0
Dec 24 03:15:06.615: INFO: rook-ceph-mon-b-6c84dd66-29k4w from rook-ceph started at 2020-12-24 02:42:56 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.615: INFO: 	Container mon ready: true, restart count 0
Dec 24 03:15:06.615: INFO: rook-ceph-mon-b-6c84dd66-xj8wb from rook-ceph started at 2020-12-23 10:29:43 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.615: INFO: 	Container mon ready: false, restart count 0
Dec 24 03:15:06.615: INFO: rook-ceph-osd-1-59b659d744-k6c5s from rook-ceph started at 2020-12-23 10:29:43 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.615: INFO: 	Container osd ready: false, restart count 0
Dec 24 03:15:06.615: INFO: rook-ceph-osd-1-59b659d744-nnqc9 from rook-ceph started at 2020-12-24 02:42:54 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.615: INFO: 	Container osd ready: true, restart count 0
Dec 24 03:15:06.615: INFO: rook-discover-svzsw from rook-ceph started at 2020-12-23 10:54:33 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.615: INFO: 	Container rook-discover ready: false, restart count 0
Dec 24 03:15:06.615: INFO: sonobuoy from sonobuoy started at 2020-12-24 02:17:41 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.615: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 24 03:15:06.615: INFO: sonobuoy-e2e-job-683f472080954d4f from sonobuoy started at 2020-12-24 02:17:43 +0000 UTC (2 container statuses recorded)
Dec 24 03:15:06.615: INFO: 	Container e2e ready: true, restart count 0
Dec 24 03:15:06.615: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 24 03:15:06.615: INFO: sonobuoy-systemd-logs-daemon-set-eb313291d8024436-cqjvw from sonobuoy started at 2020-12-24 02:17:43 +0000 UTC (2 container statuses recorded)
Dec 24 03:15:06.615: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 24 03:15:06.615: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 24 03:15:06.615: INFO: 
Logging pods the apiserver thinks is on node c1-5 before test
Dec 24 03:15:06.630: INFO: cdi-apiserver-79dbd664dd-jvbpv from cdi started at 2020-12-23 09:46:37 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.630: INFO: 	Container cdi-apiserver ready: true, restart count 0
Dec 24 03:15:06.630: INFO: snapshot-controller-0 from default started at 2020-12-18 04:06:22 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.630: INFO: 	Container snapshot-controller ready: true, restart count 0
Dec 24 03:15:06.630: INFO: calico-node-tt5dp from kube-system started at 2020-12-17 09:20:20 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.630: INFO: 	Container calico-node ready: true, restart count 0
Dec 24 03:15:06.630: INFO: kube-proxy-2lcn2 from kube-system started at 2020-12-17 09:18:32 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.630: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 24 03:15:06.630: INFO: virt-api-76b44ffc65-kcmtd from kubevirt started at 2020-12-23 09:46:37 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.630: INFO: 	Container virt-api ready: true, restart count 0
Dec 24 03:15:06.630: INFO: virt-controller-84b5d4b779-lbv8p from kubevirt started at 2020-12-23 09:46:37 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.630: INFO: 	Container virt-controller ready: true, restart count 0
Dec 24 03:15:06.630: INFO: virt-handler-scjdh from kubevirt started at 2020-12-17 09:24:03 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.630: INFO: 	Container virt-handler ready: true, restart count 7
Dec 24 03:15:06.630: INFO: virt-operator-867fcd786d-hqk4k from kubevirt started at 2020-12-23 09:46:37 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.630: INFO: 	Container virt-operator ready: true, restart count 0
Dec 24 03:15:06.630: INFO: csi-cephfsplugin-provisioner-d878cdf8b-6g8td from rook-ceph started at 2020-12-18 04:06:36 +0000 UTC (6 container statuses recorded)
Dec 24 03:15:06.630: INFO: 	Container csi-attacher ready: true, restart count 1
Dec 24 03:15:06.630: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 24 03:15:06.630: INFO: 	Container csi-provisioner ready: true, restart count 1
Dec 24 03:15:06.630: INFO: 	Container csi-resizer ready: true, restart count 1
Dec 24 03:15:06.630: INFO: 	Container csi-snapshotter ready: true, restart count 1
Dec 24 03:15:06.630: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 03:15:06.630: INFO: csi-cephfsplugin-xd5rd from rook-ceph started at 2020-12-18 04:06:35 +0000 UTC (3 container statuses recorded)
Dec 24 03:15:06.630: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 24 03:15:06.630: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 24 03:15:06.630: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 03:15:06.630: INFO: csi-rbdplugin-drpwm from rook-ceph started at 2020-12-18 04:06:35 +0000 UTC (3 container statuses recorded)
Dec 24 03:15:06.630: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 24 03:15:06.630: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 24 03:15:06.630: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 03:15:06.630: INFO: csi-rbdplugin-provisioner-7588bc4-dxkjr from rook-ceph started at 2020-12-18 04:06:35 +0000 UTC (6 container statuses recorded)
Dec 24 03:15:06.630: INFO: 	Container csi-attacher ready: true, restart count 1
Dec 24 03:15:06.630: INFO: 	Container csi-provisioner ready: true, restart count 2
Dec 24 03:15:06.630: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 24 03:15:06.630: INFO: 	Container csi-resizer ready: true, restart count 1
Dec 24 03:15:06.630: INFO: 	Container csi-snapshotter ready: true, restart count 1
Dec 24 03:15:06.630: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 03:15:06.630: INFO: rook-ceph-crashcollector-c1-5-7fdd9898c6-mm2r6 from rook-ceph started at 2020-12-18 04:07:17 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.630: INFO: 	Container ceph-crash ready: true, restart count 0
Dec 24 03:15:06.630: INFO: rook-ceph-mon-c-bcf7d945c-mvssc from rook-ceph started at 2020-12-18 04:07:17 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.630: INFO: 	Container mon ready: true, restart count 0
Dec 24 03:15:06.630: INFO: rook-ceph-operator-775d4b6c5f-khccc from rook-ceph started at 2020-12-18 04:06:25 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.630: INFO: 	Container rook-ceph-operator ready: true, restart count 0
Dec 24 03:15:06.630: INFO: rook-ceph-osd-2-5478c54cc5-s282q from rook-ceph started at 2020-12-18 04:07:41 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.630: INFO: 	Container osd ready: true, restart count 0
Dec 24 03:15:06.630: INFO: rook-ceph-osd-prepare-c1-5-l2wd8 from rook-ceph started at 2020-12-18 05:07:16 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.630: INFO: 	Container provision ready: false, restart count 0
Dec 24 03:15:06.630: INFO: rook-ceph-tools-6967fc698d-n2nzp from rook-ceph started at 2020-12-23 09:46:37 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.630: INFO: 	Container rook-ceph-tools ready: true, restart count 0
Dec 24 03:15:06.630: INFO: rook-discover-ds9s8 from rook-ceph started at 2020-12-18 04:06:28 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.630: INFO: 	Container rook-discover ready: true, restart count 0
Dec 24 03:15:06.630: INFO: sonobuoy-systemd-logs-daemon-set-eb313291d8024436-82n6r from sonobuoy started at 2020-12-24 02:17:43 +0000 UTC (2 container statuses recorded)
Dec 24 03:15:06.630: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 24 03:15:06.630: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 24 03:15:06.630: INFO: 
Logging pods the apiserver thinks is on node c1-6 before test
Dec 24 03:15:06.648: INFO: cdi-deployment-6c8975f4b-6bk46 from cdi started at 2020-12-21 09:29:47 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.648: INFO: 	Container cdi-controller ready: true, restart count 1
Dec 24 03:15:06.648: INFO: cdi-operator-5d5654db65-5hdrv from cdi started at 2020-12-21 09:29:21 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.648: INFO: 	Container cdi-operator ready: true, restart count 2
Dec 24 03:15:06.648: INFO: cdi-uploadproxy-5765998fff-qh2cb from cdi started at 2020-12-23 09:46:37 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.648: INFO: 	Container cdi-uploadproxy ready: true, restart count 0
Dec 24 03:15:06.648: INFO: calico-node-5rrck from kube-system started at 2020-12-17 09:20:20 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.648: INFO: 	Container calico-node ready: true, restart count 0
Dec 24 03:15:06.648: INFO: kube-proxy-jmmvs from kube-system started at 2020-12-17 09:19:08 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.648: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 24 03:15:06.648: INFO: virt-api-76b44ffc65-b4xmf from kubevirt started at 2020-12-17 09:23:40 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.649: INFO: 	Container virt-api ready: true, restart count 0
Dec 24 03:15:06.649: INFO: virt-controller-84b5d4b779-krqk6 from kubevirt started at 2020-12-17 09:24:03 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.649: INFO: 	Container virt-controller ready: true, restart count 5
Dec 24 03:15:06.649: INFO: virt-handler-jk5bd from kubevirt started at 2020-12-17 09:24:03 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.649: INFO: 	Container virt-handler ready: true, restart count 0
Dec 24 03:15:06.649: INFO: virt-operator-867fcd786d-9xnf6 from kubevirt started at 2020-12-17 09:23:28 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.649: INFO: 	Container virt-operator ready: true, restart count 2
Dec 24 03:15:06.649: INFO: csi-cephfsplugin-provisioner-d878cdf8b-9v92z from rook-ceph started at 2020-12-23 09:46:37 +0000 UTC (6 container statuses recorded)
Dec 24 03:15:06.649: INFO: 	Container csi-attacher ready: true, restart count 0
Dec 24 03:15:06.649: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 24 03:15:06.649: INFO: 	Container csi-provisioner ready: true, restart count 0
Dec 24 03:15:06.649: INFO: 	Container csi-resizer ready: true, restart count 0
Dec 24 03:15:06.649: INFO: 	Container csi-snapshotter ready: true, restart count 0
Dec 24 03:15:06.649: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 03:15:06.649: INFO: csi-cephfsplugin-q5hm7 from rook-ceph started at 2020-12-18 04:06:35 +0000 UTC (3 container statuses recorded)
Dec 24 03:15:06.649: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 24 03:15:06.649: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 24 03:15:06.649: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 03:15:06.649: INFO: csi-rbdplugin-provisioner-7588bc4-r297x from rook-ceph started at 2020-12-18 04:06:35 +0000 UTC (6 container statuses recorded)
Dec 24 03:15:06.649: INFO: 	Container csi-attacher ready: true, restart count 0
Dec 24 03:15:06.649: INFO: 	Container csi-provisioner ready: true, restart count 0
Dec 24 03:15:06.649: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 24 03:15:06.649: INFO: 	Container csi-resizer ready: true, restart count 0
Dec 24 03:15:06.649: INFO: 	Container csi-snapshotter ready: true, restart count 0
Dec 24 03:15:06.649: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 03:15:06.649: INFO: csi-rbdplugin-xfdqp from rook-ceph started at 2020-12-18 04:06:35 +0000 UTC (3 container statuses recorded)
Dec 24 03:15:06.649: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 24 03:15:06.649: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 24 03:15:06.649: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 03:15:06.649: INFO: rook-ceph-crashcollector-c1-6-5868d4fd57-6c5gp from rook-ceph started at 2020-12-18 04:08:07 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.649: INFO: 	Container ceph-crash ready: true, restart count 0
Dec 24 03:15:06.649: INFO: rook-ceph-mds-myfs-a-59cfdfc57f-zb7rk from rook-ceph started at 2020-12-23 09:46:37 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.649: INFO: 	Container mds ready: true, restart count 0
Dec 24 03:15:06.649: INFO: rook-ceph-mds-myfs-b-6d859f5b5c-r2rqz from rook-ceph started at 2020-12-18 04:08:07 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.649: INFO: 	Container mds ready: true, restart count 0
Dec 24 03:15:06.649: INFO: rook-ceph-mgr-a-8b957cf9b-9nkkq from rook-ceph started at 2020-12-18 04:07:30 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.649: INFO: 	Container mgr ready: true, restart count 0
Dec 24 03:15:06.649: INFO: rook-ceph-mon-a-77f88c5ff8-hlrmz from rook-ceph started at 2020-12-18 04:06:59 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.649: INFO: 	Container mon ready: true, restart count 0
Dec 24 03:15:06.649: INFO: rook-ceph-osd-0-5897494d6d-k6ml7 from rook-ceph started at 2020-12-18 04:07:39 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.649: INFO: 	Container osd ready: true, restart count 0
Dec 24 03:15:06.649: INFO: rook-ceph-osd-prepare-c1-6-jmhbg from rook-ceph started at 2020-12-18 05:07:18 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.649: INFO: 	Container provision ready: false, restart count 0
Dec 24 03:15:06.649: INFO: rook-discover-4kd85 from rook-ceph started at 2020-12-18 04:06:28 +0000 UTC (1 container statuses recorded)
Dec 24 03:15:06.649: INFO: 	Container rook-discover ready: true, restart count 0
Dec 24 03:15:06.649: INFO: sonobuoy-systemd-logs-daemon-set-eb313291d8024436-79g22 from sonobuoy started at 2020-12-24 02:17:43 +0000 UTC (2 container statuses recorded)
Dec 24 03:15:06.649: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 24 03:15:06.649: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-87c29dbb-7a53-40d7-b9c9-2017197a68b9 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-87c29dbb-7a53-40d7-b9c9-2017197a68b9 off the node c1-4
STEP: verifying the node doesn't have the label kubernetes.io/e2e-87c29dbb-7a53-40d7-b9c9-2017197a68b9
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:15:16.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7628" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:10.329 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":305,"completed":194,"skipped":3016,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:15:16.879: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Dec 24 03:15:16.957: INFO: Waiting up to 5m0s for pod "downward-api-787f12eb-5772-4013-990f-8354ae275911" in namespace "downward-api-5557" to be "Succeeded or Failed"
Dec 24 03:15:16.960: INFO: Pod "downward-api-787f12eb-5772-4013-990f-8354ae275911": Phase="Pending", Reason="", readiness=false. Elapsed: 2.963681ms
Dec 24 03:15:18.962: INFO: Pod "downward-api-787f12eb-5772-4013-990f-8354ae275911": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005775026s
Dec 24 03:15:20.965: INFO: Pod "downward-api-787f12eb-5772-4013-990f-8354ae275911": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008745988s
STEP: Saw pod success
Dec 24 03:15:20.965: INFO: Pod "downward-api-787f12eb-5772-4013-990f-8354ae275911" satisfied condition "Succeeded or Failed"
Dec 24 03:15:20.968: INFO: Trying to get logs from node c1-4 pod downward-api-787f12eb-5772-4013-990f-8354ae275911 container dapi-container: <nil>
STEP: delete the pod
Dec 24 03:15:21.025: INFO: Waiting for pod downward-api-787f12eb-5772-4013-990f-8354ae275911 to disappear
Dec 24 03:15:21.028: INFO: Pod downward-api-787f12eb-5772-4013-990f-8354ae275911 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:15:21.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5557" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":305,"completed":195,"skipped":3081,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:15:21.035: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on node default medium
Dec 24 03:15:21.082: INFO: Waiting up to 5m0s for pod "pod-9d760814-54a3-4026-89e8-45fa30376184" in namespace "emptydir-6458" to be "Succeeded or Failed"
Dec 24 03:15:21.086: INFO: Pod "pod-9d760814-54a3-4026-89e8-45fa30376184": Phase="Pending", Reason="", readiness=false. Elapsed: 3.906958ms
Dec 24 03:15:23.088: INFO: Pod "pod-9d760814-54a3-4026-89e8-45fa30376184": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006094893s
Dec 24 03:15:25.090: INFO: Pod "pod-9d760814-54a3-4026-89e8-45fa30376184": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008838104s
STEP: Saw pod success
Dec 24 03:15:25.091: INFO: Pod "pod-9d760814-54a3-4026-89e8-45fa30376184" satisfied condition "Succeeded or Failed"
Dec 24 03:15:25.093: INFO: Trying to get logs from node c1-4 pod pod-9d760814-54a3-4026-89e8-45fa30376184 container test-container: <nil>
STEP: delete the pod
Dec 24 03:15:25.117: INFO: Waiting for pod pod-9d760814-54a3-4026-89e8-45fa30376184 to disappear
Dec 24 03:15:25.120: INFO: Pod pod-9d760814-54a3-4026-89e8-45fa30376184 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:15:25.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6458" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":196,"skipped":3088,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:15:25.127: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 24 03:15:25.173: INFO: Waiting up to 5m0s for pod "pod-79168f78-6bd9-4d5b-96ad-e23f10221dfc" in namespace "emptydir-9496" to be "Succeeded or Failed"
Dec 24 03:15:25.177: INFO: Pod "pod-79168f78-6bd9-4d5b-96ad-e23f10221dfc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.210118ms
Dec 24 03:15:27.201: INFO: Pod "pod-79168f78-6bd9-4d5b-96ad-e23f10221dfc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027946426s
STEP: Saw pod success
Dec 24 03:15:27.201: INFO: Pod "pod-79168f78-6bd9-4d5b-96ad-e23f10221dfc" satisfied condition "Succeeded or Failed"
Dec 24 03:15:27.203: INFO: Trying to get logs from node c1-4 pod pod-79168f78-6bd9-4d5b-96ad-e23f10221dfc container test-container: <nil>
STEP: delete the pod
Dec 24 03:15:27.237: INFO: Waiting for pod pod-79168f78-6bd9-4d5b-96ad-e23f10221dfc to disappear
Dec 24 03:15:27.246: INFO: Pod pod-79168f78-6bd9-4d5b-96ad-e23f10221dfc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:15:27.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9496" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":197,"skipped":3098,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:15:27.253: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:15:38.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-499" for this suite.

• [SLOW TEST:11.098 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":305,"completed":198,"skipped":3109,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:15:38.351: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Dec 24 03:15:40.460: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-2178 PodName:pod-sharedvolume-6a8ea0db-504c-4b9d-9698-35e15575a530 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 24 03:15:40.460: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
Dec 24 03:15:40.557: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:15:40.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2178" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":305,"completed":199,"skipped":3127,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:15:40.579: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 24 03:15:40.632: INFO: Waiting up to 5m0s for pod "downwardapi-volume-70c17a22-e31f-4444-bbc5-15db26de0a78" in namespace "downward-api-1140" to be "Succeeded or Failed"
Dec 24 03:15:40.636: INFO: Pod "downwardapi-volume-70c17a22-e31f-4444-bbc5-15db26de0a78": Phase="Pending", Reason="", readiness=false. Elapsed: 4.131293ms
Dec 24 03:15:42.639: INFO: Pod "downwardapi-volume-70c17a22-e31f-4444-bbc5-15db26de0a78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006953871s
STEP: Saw pod success
Dec 24 03:15:42.639: INFO: Pod "downwardapi-volume-70c17a22-e31f-4444-bbc5-15db26de0a78" satisfied condition "Succeeded or Failed"
Dec 24 03:15:42.641: INFO: Trying to get logs from node c1-4 pod downwardapi-volume-70c17a22-e31f-4444-bbc5-15db26de0a78 container client-container: <nil>
STEP: delete the pod
Dec 24 03:15:42.655: INFO: Waiting for pod downwardapi-volume-70c17a22-e31f-4444-bbc5-15db26de0a78 to disappear
Dec 24 03:15:42.688: INFO: Pod downwardapi-volume-70c17a22-e31f-4444-bbc5-15db26de0a78 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:15:42.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1140" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":200,"skipped":3127,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:15:42.695: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-532
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-532
STEP: Creating statefulset with conflicting port in namespace statefulset-532
STEP: Waiting until pod test-pod will start running in namespace statefulset-532
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-532
Dec 24 03:15:46.837: INFO: Observed stateful pod in namespace: statefulset-532, name: ss-0, uid: 1479fb88-4688-435f-a2e4-580163501923, status phase: Failed. Waiting for statefulset controller to delete.
Dec 24 03:15:46.839: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-532
STEP: Removing pod with conflicting port in namespace statefulset-532
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-532 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Dec 24 03:15:50.900: INFO: Deleting all statefulset in ns statefulset-532
Dec 24 03:15:50.902: INFO: Scaling statefulset ss to 0
Dec 24 03:16:00.915: INFO: Waiting for statefulset status.replicas updated to 0
Dec 24 03:16:00.917: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:16:00.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-532" for this suite.

• [SLOW TEST:18.239 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":305,"completed":201,"skipped":3165,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:16:00.935: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 24 03:16:03.512: INFO: Successfully updated pod "pod-update-9b85f621-b009-4055-b816-19c172778813"
STEP: verifying the updated pod is in kubernetes
Dec 24 03:16:03.521: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:16:03.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8440" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":305,"completed":202,"skipped":3178,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:16:03.545: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:16:03.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8088" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":305,"completed":203,"skipped":3198,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] version v1
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:16:03.606: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-mrtch in namespace proxy-1633
I1224 03:16:03.692098      24 runners.go:190] Created replication controller with name: proxy-service-mrtch, namespace: proxy-1633, replica count: 1
I1224 03:16:04.742511      24 runners.go:190] proxy-service-mrtch Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1224 03:16:05.742728      24 runners.go:190] proxy-service-mrtch Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1224 03:16:06.743057      24 runners.go:190] proxy-service-mrtch Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1224 03:16:07.743232      24 runners.go:190] proxy-service-mrtch Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1224 03:16:08.743382      24 runners.go:190] proxy-service-mrtch Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1224 03:16:09.743595      24 runners.go:190] proxy-service-mrtch Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 24 03:16:09.751: INFO: setup took 6.105773446s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Dec 24 03:16:09.763: INFO: (0) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:162/proxy/: bar (200; 12.605067ms)
Dec 24 03:16:09.763: INFO: (0) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:162/proxy/: bar (200; 12.433739ms)
Dec 24 03:16:09.764: INFO: (0) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/rewriteme">test<... (200; 12.489164ms)
Dec 24 03:16:09.766: INFO: (0) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname2/proxy/: bar (200; 14.698323ms)
Dec 24 03:16:09.767: INFO: (0) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/rewriteme">... (200; 15.717851ms)
Dec 24 03:16:09.767: INFO: (0) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/rewriteme">test</a> (200; 15.730436ms)
Dec 24 03:16:09.767: INFO: (0) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname2/proxy/: bar (200; 15.760556ms)
Dec 24 03:16:09.767: INFO: (0) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname1/proxy/: foo (200; 15.797201ms)
Dec 24 03:16:09.767: INFO: (0) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:160/proxy/: foo (200; 16.247517ms)
Dec 24 03:16:09.768: INFO: (0) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:160/proxy/: foo (200; 16.711267ms)
Dec 24 03:16:09.768: INFO: (0) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname1/proxy/: foo (200; 16.743452ms)
Dec 24 03:16:09.777: INFO: (0) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname1/proxy/: tls baz (200; 25.520576ms)
Dec 24 03:16:09.777: INFO: (0) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:462/proxy/: tls qux (200; 25.471522ms)
Dec 24 03:16:09.777: INFO: (0) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:460/proxy/: tls baz (200; 25.62543ms)
Dec 24 03:16:09.777: INFO: (0) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/tlsrewritem... (200; 25.571257ms)
Dec 24 03:16:09.777: INFO: (0) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname2/proxy/: tls qux (200; 25.714997ms)
Dec 24 03:16:09.781: INFO: (1) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:160/proxy/: foo (200; 4.561674ms)
Dec 24 03:16:09.781: INFO: (1) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/rewriteme">test<... (200; 4.490715ms)
Dec 24 03:16:09.781: INFO: (1) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:162/proxy/: bar (200; 4.726215ms)
Dec 24 03:16:09.782: INFO: (1) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:460/proxy/: tls baz (200; 4.775098ms)
Dec 24 03:16:09.782: INFO: (1) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:162/proxy/: bar (200; 4.877066ms)
Dec 24 03:16:09.782: INFO: (1) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname1/proxy/: tls baz (200; 5.051084ms)
Dec 24 03:16:09.782: INFO: (1) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/rewriteme">test</a> (200; 5.099439ms)
Dec 24 03:16:09.782: INFO: (1) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:160/proxy/: foo (200; 5.053701ms)
Dec 24 03:16:09.782: INFO: (1) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/rewriteme">... (200; 5.192025ms)
Dec 24 03:16:09.782: INFO: (1) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname2/proxy/: bar (200; 5.22095ms)
Dec 24 03:16:09.782: INFO: (1) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:462/proxy/: tls qux (200; 5.325882ms)
Dec 24 03:16:09.782: INFO: (1) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/tlsrewritem... (200; 5.331319ms)
Dec 24 03:16:09.801: INFO: (1) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname2/proxy/: bar (200; 24.492274ms)
Dec 24 03:16:09.801: INFO: (1) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname1/proxy/: foo (200; 24.535029ms)
Dec 24 03:16:09.801: INFO: (1) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname1/proxy/: foo (200; 24.613628ms)
Dec 24 03:16:09.801: INFO: (1) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname2/proxy/: tls qux (200; 24.706327ms)
Dec 24 03:16:09.805: INFO: (2) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:160/proxy/: foo (200; 3.694576ms)
Dec 24 03:16:09.805: INFO: (2) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:162/proxy/: bar (200; 3.824375ms)
Dec 24 03:16:09.806: INFO: (2) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/rewriteme">test<... (200; 4.107491ms)
Dec 24 03:16:09.806: INFO: (2) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/rewriteme">test</a> (200; 4.099529ms)
Dec 24 03:16:09.806: INFO: (2) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:160/proxy/: foo (200; 4.116668ms)
Dec 24 03:16:09.806: INFO: (2) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/tlsrewritem... (200; 4.108882ms)
Dec 24 03:16:09.806: INFO: (2) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:162/proxy/: bar (200; 4.351154ms)
Dec 24 03:16:09.806: INFO: (2) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:462/proxy/: tls qux (200; 4.37593ms)
Dec 24 03:16:09.806: INFO: (2) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/rewriteme">... (200; 4.363785ms)
Dec 24 03:16:09.806: INFO: (2) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:460/proxy/: tls baz (200; 4.767044ms)
Dec 24 03:16:09.807: INFO: (2) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname2/proxy/: bar (200; 5.170018ms)
Dec 24 03:16:09.807: INFO: (2) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname1/proxy/: foo (200; 5.155026ms)
Dec 24 03:16:09.807: INFO: (2) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname2/proxy/: tls qux (200; 5.1453ms)
Dec 24 03:16:09.807: INFO: (2) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname2/proxy/: bar (200; 5.250007ms)
Dec 24 03:16:09.807: INFO: (2) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname1/proxy/: foo (200; 5.299127ms)
Dec 24 03:16:09.807: INFO: (2) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname1/proxy/: tls baz (200; 5.301612ms)
Dec 24 03:16:09.813: INFO: (3) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname2/proxy/: bar (200; 5.565793ms)
Dec 24 03:16:09.813: INFO: (3) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname1/proxy/: foo (200; 5.646502ms)
Dec 24 03:16:09.813: INFO: (3) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname1/proxy/: foo (200; 5.565725ms)
Dec 24 03:16:09.813: INFO: (3) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname2/proxy/: tls qux (200; 5.591555ms)
Dec 24 03:16:09.813: INFO: (3) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname2/proxy/: bar (200; 5.586406ms)
Dec 24 03:16:09.813: INFO: (3) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/rewriteme">test</a> (200; 5.625369ms)
Dec 24 03:16:09.813: INFO: (3) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/rewriteme">test<... (200; 5.631566ms)
Dec 24 03:16:09.813: INFO: (3) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname1/proxy/: tls baz (200; 5.727299ms)
Dec 24 03:16:09.813: INFO: (3) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:460/proxy/: tls baz (200; 5.698204ms)
Dec 24 03:16:09.813: INFO: (3) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:462/proxy/: tls qux (200; 5.763053ms)
Dec 24 03:16:09.813: INFO: (3) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:162/proxy/: bar (200; 6.086799ms)
Dec 24 03:16:09.813: INFO: (3) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:160/proxy/: foo (200; 6.129461ms)
Dec 24 03:16:09.813: INFO: (3) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/rewriteme">... (200; 6.055168ms)
Dec 24 03:16:09.813: INFO: (3) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:162/proxy/: bar (200; 6.070073ms)
Dec 24 03:16:09.813: INFO: (3) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:160/proxy/: foo (200; 6.078552ms)
Dec 24 03:16:09.813: INFO: (3) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/tlsrewritem... (200; 6.235763ms)
Dec 24 03:16:09.817: INFO: (4) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/rewriteme">test<... (200; 3.432886ms)
Dec 24 03:16:09.817: INFO: (4) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:460/proxy/: tls baz (200; 3.462907ms)
Dec 24 03:16:09.817: INFO: (4) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:160/proxy/: foo (200; 3.570345ms)
Dec 24 03:16:09.817: INFO: (4) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:160/proxy/: foo (200; 3.646564ms)
Dec 24 03:16:09.817: INFO: (4) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:462/proxy/: tls qux (200; 3.65983ms)
Dec 24 03:16:09.817: INFO: (4) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/tlsrewritem... (200; 3.649795ms)
Dec 24 03:16:09.818: INFO: (4) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:162/proxy/: bar (200; 4.099898ms)
Dec 24 03:16:09.818: INFO: (4) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname1/proxy/: foo (200; 4.643179ms)
Dec 24 03:16:09.818: INFO: (4) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname2/proxy/: bar (200; 4.894177ms)
Dec 24 03:16:09.818: INFO: (4) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:162/proxy/: bar (200; 4.874926ms)
Dec 24 03:16:09.818: INFO: (4) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/rewriteme">test</a> (200; 4.985709ms)
Dec 24 03:16:09.818: INFO: (4) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname1/proxy/: tls baz (200; 4.921904ms)
Dec 24 03:16:09.818: INFO: (4) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname1/proxy/: foo (200; 4.963966ms)
Dec 24 03:16:09.818: INFO: (4) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/rewriteme">... (200; 5.064963ms)
Dec 24 03:16:09.818: INFO: (4) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname2/proxy/: bar (200; 5.018881ms)
Dec 24 03:16:09.818: INFO: (4) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname2/proxy/: tls qux (200; 5.083915ms)
Dec 24 03:16:09.822: INFO: (5) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/tlsrewritem... (200; 3.396597ms)
Dec 24 03:16:09.822: INFO: (5) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:162/proxy/: bar (200; 3.911277ms)
Dec 24 03:16:09.822: INFO: (5) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/rewriteme">... (200; 3.9032ms)
Dec 24 03:16:09.822: INFO: (5) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:462/proxy/: tls qux (200; 3.950679ms)
Dec 24 03:16:09.823: INFO: (5) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:460/proxy/: tls baz (200; 4.121051ms)
Dec 24 03:16:09.823: INFO: (5) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:160/proxy/: foo (200; 4.073152ms)
Dec 24 03:16:09.823: INFO: (5) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/rewriteme">test</a> (200; 4.040729ms)
Dec 24 03:16:09.823: INFO: (5) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/rewriteme">test<... (200; 4.146245ms)
Dec 24 03:16:09.823: INFO: (5) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:160/proxy/: foo (200; 4.152515ms)
Dec 24 03:16:09.823: INFO: (5) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:162/proxy/: bar (200; 4.482946ms)
Dec 24 03:16:09.824: INFO: (5) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname1/proxy/: foo (200; 5.328566ms)
Dec 24 03:16:09.824: INFO: (5) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname2/proxy/: bar (200; 5.358243ms)
Dec 24 03:16:09.824: INFO: (5) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname1/proxy/: foo (200; 5.238935ms)
Dec 24 03:16:09.824: INFO: (5) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname2/proxy/: tls qux (200; 5.262964ms)
Dec 24 03:16:09.824: INFO: (5) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname2/proxy/: bar (200; 5.296097ms)
Dec 24 03:16:09.824: INFO: (5) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname1/proxy/: tls baz (200; 5.321721ms)
Dec 24 03:16:09.828: INFO: (6) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:162/proxy/: bar (200; 4.19331ms)
Dec 24 03:16:09.828: INFO: (6) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname2/proxy/: bar (200; 4.270466ms)
Dec 24 03:16:09.828: INFO: (6) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:160/proxy/: foo (200; 4.157248ms)
Dec 24 03:16:09.829: INFO: (6) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:462/proxy/: tls qux (200; 5.168326ms)
Dec 24 03:16:09.829: INFO: (6) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname2/proxy/: bar (200; 5.359993ms)
Dec 24 03:16:09.829: INFO: (6) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname1/proxy/: foo (200; 5.279552ms)
Dec 24 03:16:09.829: INFO: (6) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/rewriteme">test</a> (200; 5.280606ms)
Dec 24 03:16:09.829: INFO: (6) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname2/proxy/: tls qux (200; 5.35584ms)
Dec 24 03:16:09.829: INFO: (6) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname1/proxy/: foo (200; 5.316518ms)
Dec 24 03:16:09.829: INFO: (6) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/tlsrewritem... (200; 5.323587ms)
Dec 24 03:16:09.829: INFO: (6) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname1/proxy/: tls baz (200; 5.427843ms)
Dec 24 03:16:09.829: INFO: (6) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/rewriteme">test<... (200; 5.565368ms)
Dec 24 03:16:09.830: INFO: (6) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/rewriteme">... (200; 5.701689ms)
Dec 24 03:16:09.830: INFO: (6) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:160/proxy/: foo (200; 5.780757ms)
Dec 24 03:16:09.830: INFO: (6) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:162/proxy/: bar (200; 5.703435ms)
Dec 24 03:16:09.830: INFO: (6) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:460/proxy/: tls baz (200; 5.820576ms)
Dec 24 03:16:09.833: INFO: (7) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/rewriteme">test<... (200; 3.637079ms)
Dec 24 03:16:09.833: INFO: (7) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:162/proxy/: bar (200; 3.65175ms)
Dec 24 03:16:09.834: INFO: (7) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/rewriteme">test</a> (200; 4.004623ms)
Dec 24 03:16:09.834: INFO: (7) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:460/proxy/: tls baz (200; 4.136397ms)
Dec 24 03:16:09.834: INFO: (7) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:160/proxy/: foo (200; 4.154873ms)
Dec 24 03:16:09.834: INFO: (7) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/tlsrewritem... (200; 4.164437ms)
Dec 24 03:16:09.834: INFO: (7) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname1/proxy/: foo (200; 4.257912ms)
Dec 24 03:16:09.834: INFO: (7) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:162/proxy/: bar (200; 4.417837ms)
Dec 24 03:16:09.835: INFO: (7) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname1/proxy/: tls baz (200; 5.229774ms)
Dec 24 03:16:09.835: INFO: (7) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname2/proxy/: bar (200; 5.319969ms)
Dec 24 03:16:09.835: INFO: (7) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname2/proxy/: bar (200; 5.266748ms)
Dec 24 03:16:09.835: INFO: (7) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/rewriteme">... (200; 5.289558ms)
Dec 24 03:16:09.835: INFO: (7) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:160/proxy/: foo (200; 5.31217ms)
Dec 24 03:16:09.835: INFO: (7) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname2/proxy/: tls qux (200; 5.366663ms)
Dec 24 03:16:09.835: INFO: (7) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname1/proxy/: foo (200; 5.395373ms)
Dec 24 03:16:09.835: INFO: (7) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:462/proxy/: tls qux (200; 5.321375ms)
Dec 24 03:16:09.839: INFO: (8) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/rewriteme">test<... (200; 3.832838ms)
Dec 24 03:16:09.840: INFO: (8) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname1/proxy/: foo (200; 4.376533ms)
Dec 24 03:16:09.840: INFO: (8) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/tlsrewritem... (200; 4.387124ms)
Dec 24 03:16:09.840: INFO: (8) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:162/proxy/: bar (200; 4.467123ms)
Dec 24 03:16:09.840: INFO: (8) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname1/proxy/: foo (200; 4.666229ms)
Dec 24 03:16:09.840: INFO: (8) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname1/proxy/: tls baz (200; 4.656318ms)
Dec 24 03:16:09.840: INFO: (8) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname2/proxy/: tls qux (200; 4.66493ms)
Dec 24 03:16:09.840: INFO: (8) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname2/proxy/: bar (200; 4.698729ms)
Dec 24 03:16:09.840: INFO: (8) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/rewriteme">test</a> (200; 4.690283ms)
Dec 24 03:16:09.840: INFO: (8) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:160/proxy/: foo (200; 4.722033ms)
Dec 24 03:16:09.840: INFO: (8) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:162/proxy/: bar (200; 4.772801ms)
Dec 24 03:16:09.840: INFO: (8) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname2/proxy/: bar (200; 4.929717ms)
Dec 24 03:16:09.840: INFO: (8) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/rewriteme">... (200; 5.130683ms)
Dec 24 03:16:09.840: INFO: (8) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:460/proxy/: tls baz (200; 5.077985ms)
Dec 24 03:16:09.840: INFO: (8) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:160/proxy/: foo (200; 5.099887ms)
Dec 24 03:16:09.840: INFO: (8) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:462/proxy/: tls qux (200; 5.175774ms)
Dec 24 03:16:09.844: INFO: (9) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/tlsrewritem... (200; 3.636953ms)
Dec 24 03:16:09.844: INFO: (9) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:162/proxy/: bar (200; 3.752315ms)
Dec 24 03:16:09.844: INFO: (9) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:162/proxy/: bar (200; 3.705361ms)
Dec 24 03:16:09.844: INFO: (9) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:462/proxy/: tls qux (200; 3.703832ms)
Dec 24 03:16:09.845: INFO: (9) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname1/proxy/: foo (200; 4.055875ms)
Dec 24 03:16:09.845: INFO: (9) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname1/proxy/: foo (200; 4.314627ms)
Dec 24 03:16:09.845: INFO: (9) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname2/proxy/: bar (200; 4.476396ms)
Dec 24 03:16:09.845: INFO: (9) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname2/proxy/: tls qux (200; 4.514951ms)
Dec 24 03:16:09.845: INFO: (9) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname2/proxy/: bar (200; 4.557761ms)
Dec 24 03:16:09.845: INFO: (9) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:160/proxy/: foo (200; 4.545066ms)
Dec 24 03:16:09.845: INFO: (9) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/rewriteme">test</a> (200; 4.631367ms)
Dec 24 03:16:09.845: INFO: (9) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/rewriteme">... (200; 4.622278ms)
Dec 24 03:16:09.845: INFO: (9) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:460/proxy/: tls baz (200; 4.743991ms)
Dec 24 03:16:09.845: INFO: (9) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname1/proxy/: tls baz (200; 4.621582ms)
Dec 24 03:16:09.845: INFO: (9) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:160/proxy/: foo (200; 4.650189ms)
Dec 24 03:16:09.845: INFO: (9) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/rewriteme">test<... (200; 4.834232ms)
Dec 24 03:16:09.848: INFO: (10) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:162/proxy/: bar (200; 2.90597ms)
Dec 24 03:16:09.849: INFO: (10) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:160/proxy/: foo (200; 3.170527ms)
Dec 24 03:16:09.849: INFO: (10) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/tlsrewritem... (200; 3.181896ms)
Dec 24 03:16:09.849: INFO: (10) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname2/proxy/: tls qux (200; 3.722455ms)
Dec 24 03:16:09.849: INFO: (10) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:462/proxy/: tls qux (200; 3.562241ms)
Dec 24 03:16:09.850: INFO: (10) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname1/proxy/: foo (200; 4.059417ms)
Dec 24 03:16:09.850: INFO: (10) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname1/proxy/: foo (200; 4.068676ms)
Dec 24 03:16:09.850: INFO: (10) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname2/proxy/: bar (200; 4.072998ms)
Dec 24 03:16:09.850: INFO: (10) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname1/proxy/: tls baz (200; 4.163416ms)
Dec 24 03:16:09.850: INFO: (10) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname2/proxy/: bar (200; 4.176448ms)
Dec 24 03:16:09.850: INFO: (10) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/rewriteme">... (200; 4.665581ms)
Dec 24 03:16:09.850: INFO: (10) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:162/proxy/: bar (200; 4.553114ms)
Dec 24 03:16:09.850: INFO: (10) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:160/proxy/: foo (200; 4.540798ms)
Dec 24 03:16:09.850: INFO: (10) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:460/proxy/: tls baz (200; 4.562375ms)
Dec 24 03:16:09.850: INFO: (10) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/rewriteme">test</a> (200; 4.643094ms)
Dec 24 03:16:09.850: INFO: (10) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/rewriteme">test<... (200; 4.598499ms)
Dec 24 03:16:09.853: INFO: (11) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/rewriteme">test</a> (200; 2.375508ms)
Dec 24 03:16:09.853: INFO: (11) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:462/proxy/: tls qux (200; 2.649735ms)
Dec 24 03:16:09.853: INFO: (11) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:160/proxy/: foo (200; 2.568313ms)
Dec 24 03:16:09.854: INFO: (11) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:160/proxy/: foo (200; 3.442726ms)
Dec 24 03:16:09.854: INFO: (11) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:162/proxy/: bar (200; 3.432856ms)
Dec 24 03:16:09.854: INFO: (11) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/rewriteme">... (200; 3.457271ms)
Dec 24 03:16:09.854: INFO: (11) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/rewriteme">test<... (200; 3.555387ms)
Dec 24 03:16:09.854: INFO: (11) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/tlsrewritem... (200; 3.589331ms)
Dec 24 03:16:09.854: INFO: (11) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:460/proxy/: tls baz (200; 4.107844ms)
Dec 24 03:16:09.855: INFO: (11) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:162/proxy/: bar (200; 4.682323ms)
Dec 24 03:16:09.855: INFO: (11) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname1/proxy/: foo (200; 4.621033ms)
Dec 24 03:16:09.855: INFO: (11) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname1/proxy/: foo (200; 4.763389ms)
Dec 24 03:16:09.855: INFO: (11) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname2/proxy/: tls qux (200; 4.761908ms)
Dec 24 03:16:09.855: INFO: (11) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname2/proxy/: bar (200; 4.803402ms)
Dec 24 03:16:09.855: INFO: (11) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname1/proxy/: tls baz (200; 4.879653ms)
Dec 24 03:16:09.855: INFO: (11) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname2/proxy/: bar (200; 5.011572ms)
Dec 24 03:16:09.859: INFO: (12) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:162/proxy/: bar (200; 3.26655ms)
Dec 24 03:16:09.859: INFO: (12) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname2/proxy/: tls qux (200; 3.326136ms)
Dec 24 03:16:09.859: INFO: (12) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/rewriteme">test<... (200; 3.290371ms)
Dec 24 03:16:09.859: INFO: (12) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:160/proxy/: foo (200; 3.315741ms)
Dec 24 03:16:09.859: INFO: (12) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:460/proxy/: tls baz (200; 3.478258ms)
Dec 24 03:16:09.859: INFO: (12) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/rewriteme">... (200; 3.730989ms)
Dec 24 03:16:09.860: INFO: (12) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname1/proxy/: tls baz (200; 4.618315ms)
Dec 24 03:16:09.860: INFO: (12) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname1/proxy/: foo (200; 4.602486ms)
Dec 24 03:16:09.860: INFO: (12) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/tlsrewritem... (200; 4.63091ms)
Dec 24 03:16:09.860: INFO: (12) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname1/proxy/: foo (200; 4.588821ms)
Dec 24 03:16:09.860: INFO: (12) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/rewriteme">test</a> (200; 4.632384ms)
Dec 24 03:16:09.860: INFO: (12) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname2/proxy/: bar (200; 4.635004ms)
Dec 24 03:16:09.860: INFO: (12) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname2/proxy/: bar (200; 4.650744ms)
Dec 24 03:16:09.860: INFO: (12) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:462/proxy/: tls qux (200; 4.761317ms)
Dec 24 03:16:09.860: INFO: (12) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:160/proxy/: foo (200; 5.027953ms)
Dec 24 03:16:09.860: INFO: (12) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:162/proxy/: bar (200; 5.0213ms)
Dec 24 03:16:09.864: INFO: (13) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:162/proxy/: bar (200; 3.433737ms)
Dec 24 03:16:09.864: INFO: (13) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:160/proxy/: foo (200; 3.546973ms)
Dec 24 03:16:09.864: INFO: (13) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:162/proxy/: bar (200; 3.494164ms)
Dec 24 03:16:09.864: INFO: (13) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/tlsrewritem... (200; 3.493913ms)
Dec 24 03:16:09.864: INFO: (13) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname2/proxy/: bar (200; 3.836377ms)
Dec 24 03:16:09.864: INFO: (13) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname1/proxy/: foo (200; 3.998284ms)
Dec 24 03:16:09.865: INFO: (13) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname2/proxy/: bar (200; 4.168892ms)
Dec 24 03:16:09.865: INFO: (13) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname1/proxy/: foo (200; 4.254897ms)
Dec 24 03:16:09.865: INFO: (13) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname1/proxy/: tls baz (200; 4.244074ms)
Dec 24 03:16:09.865: INFO: (13) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/rewriteme">test<... (200; 4.238911ms)
Dec 24 03:16:09.865: INFO: (13) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname2/proxy/: tls qux (200; 4.214932ms)
Dec 24 03:16:09.865: INFO: (13) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/rewriteme">... (200; 4.565287ms)
Dec 24 03:16:09.865: INFO: (13) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:160/proxy/: foo (200; 4.480783ms)
Dec 24 03:16:09.865: INFO: (13) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:462/proxy/: tls qux (200; 4.57286ms)
Dec 24 03:16:09.865: INFO: (13) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/rewriteme">test</a> (200; 4.536205ms)
Dec 24 03:16:09.865: INFO: (13) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:460/proxy/: tls baz (200; 4.577098ms)
Dec 24 03:16:09.867: INFO: (14) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:462/proxy/: tls qux (200; 2.335872ms)
Dec 24 03:16:09.868: INFO: (14) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/rewriteme">test</a> (200; 3.322987ms)
Dec 24 03:16:09.868: INFO: (14) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/rewriteme">test<... (200; 3.369342ms)
Dec 24 03:16:09.868: INFO: (14) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname1/proxy/: tls baz (200; 3.405976ms)
Dec 24 03:16:09.869: INFO: (14) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname2/proxy/: bar (200; 4.07031ms)
Dec 24 03:16:09.869: INFO: (14) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:460/proxy/: tls baz (200; 4.171701ms)
Dec 24 03:16:09.869: INFO: (14) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname1/proxy/: foo (200; 4.133598ms)
Dec 24 03:16:09.869: INFO: (14) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname2/proxy/: tls qux (200; 4.176257ms)
Dec 24 03:16:09.869: INFO: (14) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname1/proxy/: foo (200; 4.180218ms)
Dec 24 03:16:09.869: INFO: (14) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/rewriteme">... (200; 4.251497ms)
Dec 24 03:16:09.869: INFO: (14) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname2/proxy/: bar (200; 4.329428ms)
Dec 24 03:16:09.869: INFO: (14) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/tlsrewritem... (200; 4.3091ms)
Dec 24 03:16:09.870: INFO: (14) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:160/proxy/: foo (200; 4.493202ms)
Dec 24 03:16:09.870: INFO: (14) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:162/proxy/: bar (200; 4.59121ms)
Dec 24 03:16:09.870: INFO: (14) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:162/proxy/: bar (200; 4.541749ms)
Dec 24 03:16:09.870: INFO: (14) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:160/proxy/: foo (200; 4.581726ms)
Dec 24 03:16:09.873: INFO: (15) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/rewriteme">... (200; 3.151322ms)
Dec 24 03:16:09.873: INFO: (15) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:162/proxy/: bar (200; 3.198628ms)
Dec 24 03:16:09.873: INFO: (15) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/rewriteme">test</a> (200; 3.184166ms)
Dec 24 03:16:09.873: INFO: (15) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/rewriteme">test<... (200; 3.186595ms)
Dec 24 03:16:09.873: INFO: (15) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:160/proxy/: foo (200; 3.203956ms)
Dec 24 03:16:09.873: INFO: (15) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:160/proxy/: foo (200; 3.232226ms)
Dec 24 03:16:09.873: INFO: (15) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/tlsrewritem... (200; 3.214863ms)
Dec 24 03:16:09.873: INFO: (15) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:162/proxy/: bar (200; 3.51611ms)
Dec 24 03:16:09.875: INFO: (15) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:462/proxy/: tls qux (200; 5.604837ms)
Dec 24 03:16:09.875: INFO: (15) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname1/proxy/: foo (200; 5.695782ms)
Dec 24 03:16:09.876: INFO: (15) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname2/proxy/: bar (200; 6.5862ms)
Dec 24 03:16:09.877: INFO: (15) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname1/proxy/: foo (200; 7.200921ms)
Dec 24 03:16:09.877: INFO: (15) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname2/proxy/: bar (200; 7.130255ms)
Dec 24 03:16:09.877: INFO: (15) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:460/proxy/: tls baz (200; 7.233486ms)
Dec 24 03:16:09.877: INFO: (15) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname1/proxy/: tls baz (200; 7.307326ms)
Dec 24 03:16:09.877: INFO: (15) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname2/proxy/: tls qux (200; 7.279437ms)
Dec 24 03:16:09.881: INFO: (16) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:162/proxy/: bar (200; 3.624411ms)
Dec 24 03:16:09.881: INFO: (16) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:462/proxy/: tls qux (200; 3.659782ms)
Dec 24 03:16:09.882: INFO: (16) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:162/proxy/: bar (200; 4.384652ms)
Dec 24 03:16:09.882: INFO: (16) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname1/proxy/: foo (200; 4.490262ms)
Dec 24 03:16:09.882: INFO: (16) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname1/proxy/: tls baz (200; 4.431764ms)
Dec 24 03:16:09.882: INFO: (16) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname1/proxy/: foo (200; 4.40055ms)
Dec 24 03:16:09.882: INFO: (16) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/tlsrewritem... (200; 4.429253ms)
Dec 24 03:16:09.882: INFO: (16) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname2/proxy/: bar (200; 4.428961ms)
Dec 24 03:16:09.882: INFO: (16) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname2/proxy/: tls qux (200; 4.511083ms)
Dec 24 03:16:09.882: INFO: (16) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:460/proxy/: tls baz (200; 4.718146ms)
Dec 24 03:16:09.882: INFO: (16) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/rewriteme">test<... (200; 4.75261ms)
Dec 24 03:16:09.882: INFO: (16) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:160/proxy/: foo (200; 4.79512ms)
Dec 24 03:16:09.882: INFO: (16) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:160/proxy/: foo (200; 4.788336ms)
Dec 24 03:16:09.882: INFO: (16) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname2/proxy/: bar (200; 4.73731ms)
Dec 24 03:16:09.882: INFO: (16) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/rewriteme">test</a> (200; 4.823018ms)
Dec 24 03:16:09.882: INFO: (16) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/rewriteme">... (200; 4.846032ms)
Dec 24 03:16:09.885: INFO: (17) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/rewriteme">test</a> (200; 2.855358ms)
Dec 24 03:16:09.885: INFO: (17) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/rewriteme">... (200; 3.021685ms)
Dec 24 03:16:09.885: INFO: (17) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:160/proxy/: foo (200; 2.957732ms)
Dec 24 03:16:09.886: INFO: (17) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname1/proxy/: foo (200; 3.937666ms)
Dec 24 03:16:09.886: INFO: (17) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname2/proxy/: bar (200; 3.945378ms)
Dec 24 03:16:09.886: INFO: (17) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname2/proxy/: bar (200; 3.971707ms)
Dec 24 03:16:09.886: INFO: (17) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname1/proxy/: foo (200; 3.960991ms)
Dec 24 03:16:09.886: INFO: (17) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname2/proxy/: tls qux (200; 4.335088ms)
Dec 24 03:16:09.886: INFO: (17) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:162/proxy/: bar (200; 4.309544ms)
Dec 24 03:16:09.886: INFO: (17) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:460/proxy/: tls baz (200; 4.384391ms)
Dec 24 03:16:09.886: INFO: (17) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/rewriteme">test<... (200; 4.327055ms)
Dec 24 03:16:09.886: INFO: (17) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:162/proxy/: bar (200; 4.419582ms)
Dec 24 03:16:09.886: INFO: (17) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname1/proxy/: tls baz (200; 4.357993ms)
Dec 24 03:16:09.886: INFO: (17) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/tlsrewritem... (200; 4.317631ms)
Dec 24 03:16:09.886: INFO: (17) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:160/proxy/: foo (200; 4.358812ms)
Dec 24 03:16:09.886: INFO: (17) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:462/proxy/: tls qux (200; 4.553198ms)
Dec 24 03:16:09.890: INFO: (18) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:162/proxy/: bar (200; 3.485964ms)
Dec 24 03:16:09.890: INFO: (18) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:462/proxy/: tls qux (200; 3.612581ms)
Dec 24 03:16:09.890: INFO: (18) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:160/proxy/: foo (200; 3.68943ms)
Dec 24 03:16:09.890: INFO: (18) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/tlsrewritem... (200; 3.692353ms)
Dec 24 03:16:09.890: INFO: (18) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname2/proxy/: tls qux (200; 3.741559ms)
Dec 24 03:16:09.890: INFO: (18) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname1/proxy/: foo (200; 3.778014ms)
Dec 24 03:16:09.891: INFO: (18) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:162/proxy/: bar (200; 3.987059ms)
Dec 24 03:16:09.891: INFO: (18) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/rewriteme">test<... (200; 4.009991ms)
Dec 24 03:16:09.891: INFO: (18) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/rewriteme">... (200; 4.082552ms)
Dec 24 03:16:09.891: INFO: (18) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname2/proxy/: bar (200; 4.019242ms)
Dec 24 03:16:09.891: INFO: (18) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname1/proxy/: foo (200; 4.126722ms)
Dec 24 03:16:09.891: INFO: (18) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname1/proxy/: tls baz (200; 4.070936ms)
Dec 24 03:16:09.891: INFO: (18) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:460/proxy/: tls baz (200; 4.298512ms)
Dec 24 03:16:09.891: INFO: (18) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname2/proxy/: bar (200; 4.234968ms)
Dec 24 03:16:09.891: INFO: (18) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:160/proxy/: foo (200; 4.260487ms)
Dec 24 03:16:09.891: INFO: (18) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/rewriteme">test</a> (200; 4.315603ms)
Dec 24 03:16:09.894: INFO: (19) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:160/proxy/: foo (200; 3.319698ms)
Dec 24 03:16:09.894: INFO: (19) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname1/proxy/: tls baz (200; 3.285145ms)
Dec 24 03:16:09.894: INFO: (19) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:1080/proxy/rewriteme">test<... (200; 3.289068ms)
Dec 24 03:16:09.895: INFO: (19) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl/proxy/rewriteme">test</a> (200; 3.732691ms)
Dec 24 03:16:09.895: INFO: (19) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname1/proxy/: foo (200; 3.705726ms)
Dec 24 03:16:09.895: INFO: (19) /api/v1/namespaces/proxy-1633/services/https:proxy-service-mrtch:tlsportname2/proxy/: tls qux (200; 3.771958ms)
Dec 24 03:16:09.895: INFO: (19) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:1080/proxy/rewriteme">... (200; 3.721902ms)
Dec 24 03:16:09.895: INFO: (19) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname2/proxy/: bar (200; 3.720565ms)
Dec 24 03:16:09.895: INFO: (19) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:162/proxy/: bar (200; 3.721895ms)
Dec 24 03:16:09.895: INFO: (19) /api/v1/namespaces/proxy-1633/services/proxy-service-mrtch:portname1/proxy/: foo (200; 3.747454ms)
Dec 24 03:16:09.895: INFO: (19) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:460/proxy/: tls baz (200; 3.817264ms)
Dec 24 03:16:09.895: INFO: (19) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/: <a href="/api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:443/proxy/tlsrewritem... (200; 3.731206ms)
Dec 24 03:16:09.895: INFO: (19) /api/v1/namespaces/proxy-1633/services/http:proxy-service-mrtch:portname2/proxy/: bar (200; 3.973061ms)
Dec 24 03:16:09.895: INFO: (19) /api/v1/namespaces/proxy-1633/pods/proxy-service-mrtch-g85vl:162/proxy/: bar (200; 4.166268ms)
Dec 24 03:16:09.895: INFO: (19) /api/v1/namespaces/proxy-1633/pods/http:proxy-service-mrtch-g85vl:160/proxy/: foo (200; 4.096212ms)
Dec 24 03:16:09.895: INFO: (19) /api/v1/namespaces/proxy-1633/pods/https:proxy-service-mrtch-g85vl:462/proxy/: tls qux (200; 4.183129ms)
STEP: deleting ReplicationController proxy-service-mrtch in namespace proxy-1633, will wait for the garbage collector to delete the pods
Dec 24 03:16:09.952: INFO: Deleting ReplicationController proxy-service-mrtch took: 5.000766ms
Dec 24 03:16:10.952: INFO: Terminating ReplicationController proxy-service-mrtch pods took: 1.000187534s
[AfterEach] version v1
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:16:13.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1633" for this suite.

• [SLOW TEST:9.854 seconds]
[sig-network] Proxy
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":305,"completed":204,"skipped":3234,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:16:13.461: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-c6ed2f26-5d0e-40c5-ae76-74d104b82679
STEP: Creating a pod to test consume configMaps
Dec 24 03:16:13.536: INFO: Waiting up to 5m0s for pod "pod-configmaps-4bcf587e-13b1-49ba-ac9b-a2d0d4156f8e" in namespace "configmap-2368" to be "Succeeded or Failed"
Dec 24 03:16:13.542: INFO: Pod "pod-configmaps-4bcf587e-13b1-49ba-ac9b-a2d0d4156f8e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.976915ms
Dec 24 03:16:15.545: INFO: Pod "pod-configmaps-4bcf587e-13b1-49ba-ac9b-a2d0d4156f8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008305837s
STEP: Saw pod success
Dec 24 03:16:15.545: INFO: Pod "pod-configmaps-4bcf587e-13b1-49ba-ac9b-a2d0d4156f8e" satisfied condition "Succeeded or Failed"
Dec 24 03:16:15.547: INFO: Trying to get logs from node c1-4 pod pod-configmaps-4bcf587e-13b1-49ba-ac9b-a2d0d4156f8e container configmap-volume-test: <nil>
STEP: delete the pod
Dec 24 03:16:15.585: INFO: Waiting for pod pod-configmaps-4bcf587e-13b1-49ba-ac9b-a2d0d4156f8e to disappear
Dec 24 03:16:15.598: INFO: Pod pod-configmaps-4bcf587e-13b1-49ba-ac9b-a2d0d4156f8e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:16:15.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2368" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":205,"skipped":3250,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:16:15.621: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-62f3711e-3f8d-4c81-b465-4e1af6d2e8cd
STEP: Creating a pod to test consume configMaps
Dec 24 03:16:15.686: INFO: Waiting up to 5m0s for pod "pod-configmaps-f0eb67c0-49a7-4b5a-b79f-517bf699fa4c" in namespace "configmap-1732" to be "Succeeded or Failed"
Dec 24 03:16:15.700: INFO: Pod "pod-configmaps-f0eb67c0-49a7-4b5a-b79f-517bf699fa4c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.919504ms
Dec 24 03:16:17.703: INFO: Pod "pod-configmaps-f0eb67c0-49a7-4b5a-b79f-517bf699fa4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01670211s
STEP: Saw pod success
Dec 24 03:16:17.703: INFO: Pod "pod-configmaps-f0eb67c0-49a7-4b5a-b79f-517bf699fa4c" satisfied condition "Succeeded or Failed"
Dec 24 03:16:17.705: INFO: Trying to get logs from node c1-4 pod pod-configmaps-f0eb67c0-49a7-4b5a-b79f-517bf699fa4c container configmap-volume-test: <nil>
STEP: delete the pod
Dec 24 03:16:17.721: INFO: Waiting for pod pod-configmaps-f0eb67c0-49a7-4b5a-b79f-517bf699fa4c to disappear
Dec 24 03:16:17.725: INFO: Pod pod-configmaps-f0eb67c0-49a7-4b5a-b79f-517bf699fa4c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:16:17.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1732" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":206,"skipped":3271,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:16:17.732: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 03:18:17.808: INFO: Deleting pod "var-expansion-586c6574-e796-48cb-be82-d36cd18d5943" in namespace "var-expansion-9003"
Dec 24 03:18:17.812: INFO: Wait up to 5m0s for pod "var-expansion-586c6574-e796-48cb-be82-d36cd18d5943" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:18:19.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9003" for this suite.

• [SLOW TEST:122.104 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":305,"completed":207,"skipped":3292,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:18:19.836: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-projected-b7zp
STEP: Creating a pod to test atomic-volume-subpath
Dec 24 03:18:19.913: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-b7zp" in namespace "subpath-239" to be "Succeeded or Failed"
Dec 24 03:18:19.922: INFO: Pod "pod-subpath-test-projected-b7zp": Phase="Pending", Reason="", readiness=false. Elapsed: 9.262958ms
Dec 24 03:18:21.934: INFO: Pod "pod-subpath-test-projected-b7zp": Phase="Running", Reason="", readiness=true. Elapsed: 2.021126017s
Dec 24 03:18:23.937: INFO: Pod "pod-subpath-test-projected-b7zp": Phase="Running", Reason="", readiness=true. Elapsed: 4.024098739s
Dec 24 03:18:25.940: INFO: Pod "pod-subpath-test-projected-b7zp": Phase="Running", Reason="", readiness=true. Elapsed: 6.027096361s
Dec 24 03:18:27.943: INFO: Pod "pod-subpath-test-projected-b7zp": Phase="Running", Reason="", readiness=true. Elapsed: 8.030183407s
Dec 24 03:18:29.946: INFO: Pod "pod-subpath-test-projected-b7zp": Phase="Running", Reason="", readiness=true. Elapsed: 10.033322768s
Dec 24 03:18:31.949: INFO: Pod "pod-subpath-test-projected-b7zp": Phase="Running", Reason="", readiness=true. Elapsed: 12.036399021s
Dec 24 03:18:33.952: INFO: Pod "pod-subpath-test-projected-b7zp": Phase="Running", Reason="", readiness=true. Elapsed: 14.039426453s
Dec 24 03:18:35.955: INFO: Pod "pod-subpath-test-projected-b7zp": Phase="Running", Reason="", readiness=true. Elapsed: 16.042083765s
Dec 24 03:18:37.958: INFO: Pod "pod-subpath-test-projected-b7zp": Phase="Running", Reason="", readiness=true. Elapsed: 18.04495759s
Dec 24 03:18:39.961: INFO: Pod "pod-subpath-test-projected-b7zp": Phase="Running", Reason="", readiness=true. Elapsed: 20.048060346s
Dec 24 03:18:41.964: INFO: Pod "pod-subpath-test-projected-b7zp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.050971614s
STEP: Saw pod success
Dec 24 03:18:41.964: INFO: Pod "pod-subpath-test-projected-b7zp" satisfied condition "Succeeded or Failed"
Dec 24 03:18:41.966: INFO: Trying to get logs from node c1-4 pod pod-subpath-test-projected-b7zp container test-container-subpath-projected-b7zp: <nil>
STEP: delete the pod
Dec 24 03:18:41.994: INFO: Waiting for pod pod-subpath-test-projected-b7zp to disappear
Dec 24 03:18:42.000: INFO: Pod pod-subpath-test-projected-b7zp no longer exists
STEP: Deleting pod pod-subpath-test-projected-b7zp
Dec 24 03:18:42.000: INFO: Deleting pod "pod-subpath-test-projected-b7zp" in namespace "subpath-239"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:18:42.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-239" for this suite.

• [SLOW TEST:22.172 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":305,"completed":208,"skipped":3303,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:18:42.009: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 24 03:18:42.924: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 24 03:18:44.932: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376722, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376722, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376722, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744376722, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 24 03:18:47.947: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:19:00.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7911" for this suite.
STEP: Destroying namespace "webhook-7911-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:18.153 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":305,"completed":209,"skipped":3307,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:19:00.162: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 03:19:00.196: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Dec 24 03:19:04.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 --namespace=crd-publish-openapi-6464 create -f -'
Dec 24 03:19:06.576: INFO: stderr: ""
Dec 24 03:19:06.576: INFO: stdout: "e2e-test-crd-publish-openapi-8353-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 24 03:19:06.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 --namespace=crd-publish-openapi-6464 delete e2e-test-crd-publish-openapi-8353-crds test-foo'
Dec 24 03:19:06.689: INFO: stderr: ""
Dec 24 03:19:06.689: INFO: stdout: "e2e-test-crd-publish-openapi-8353-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Dec 24 03:19:06.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 --namespace=crd-publish-openapi-6464 apply -f -'
Dec 24 03:19:07.037: INFO: stderr: ""
Dec 24 03:19:07.037: INFO: stdout: "e2e-test-crd-publish-openapi-8353-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 24 03:19:07.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 --namespace=crd-publish-openapi-6464 delete e2e-test-crd-publish-openapi-8353-crds test-foo'
Dec 24 03:19:07.135: INFO: stderr: ""
Dec 24 03:19:07.135: INFO: stdout: "e2e-test-crd-publish-openapi-8353-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Dec 24 03:19:07.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 --namespace=crd-publish-openapi-6464 create -f -'
Dec 24 03:19:07.482: INFO: rc: 1
Dec 24 03:19:07.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 --namespace=crd-publish-openapi-6464 apply -f -'
Dec 24 03:19:07.819: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Dec 24 03:19:07.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 --namespace=crd-publish-openapi-6464 create -f -'
Dec 24 03:19:08.194: INFO: rc: 1
Dec 24 03:19:08.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 --namespace=crd-publish-openapi-6464 apply -f -'
Dec 24 03:19:08.543: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Dec 24 03:19:08.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 explain e2e-test-crd-publish-openapi-8353-crds'
Dec 24 03:19:08.883: INFO: stderr: ""
Dec 24 03:19:08.883: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8353-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Dec 24 03:19:08.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 explain e2e-test-crd-publish-openapi-8353-crds.metadata'
Dec 24 03:19:09.228: INFO: stderr: ""
Dec 24 03:19:09.228: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8353-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Dec 24 03:19:09.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 explain e2e-test-crd-publish-openapi-8353-crds.spec'
Dec 24 03:19:09.573: INFO: stderr: ""
Dec 24 03:19:09.573: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8353-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Dec 24 03:19:09.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 explain e2e-test-crd-publish-openapi-8353-crds.spec.bars'
Dec 24 03:19:09.901: INFO: stderr: ""
Dec 24 03:19:09.901: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8353-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Dec 24 03:19:09.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 explain e2e-test-crd-publish-openapi-8353-crds.spec.bars2'
Dec 24 03:19:10.280: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:19:15.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6464" for this suite.

• [SLOW TEST:15.257 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":305,"completed":210,"skipped":3319,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:19:15.419: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:19:17.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-959" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":211,"skipped":3332,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:19:17.492: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:19:23.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6156" for this suite.
STEP: Destroying namespace "nsdeletetest-878" for this suite.
Dec 24 03:19:23.762: INFO: Namespace nsdeletetest-878 was already deleted
STEP: Destroying namespace "nsdeletetest-2547" for this suite.

• [SLOW TEST:6.273 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":305,"completed":212,"skipped":3347,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:19:23.765: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:19:29.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5878" for this suite.

• [SLOW TEST:6.072 seconds]
[sig-apps] Job
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":305,"completed":213,"skipped":3354,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:19:29.837: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 24 03:19:31.179: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 24 03:19:34.208: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:19:34.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7966" for this suite.
STEP: Destroying namespace "webhook-7966-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":305,"completed":214,"skipped":3383,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:19:34.333: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Dec 24 03:19:34.415: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6921 /api/v1/namespaces/watch-6921/configmaps/e2e-watch-test-label-changed 0a17080a-f6af-4244-907a-eec7d17993e1 3464468 0 2020-12-24 03:19:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-12-24 03:19:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 24 03:19:34.415: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6921 /api/v1/namespaces/watch-6921/configmaps/e2e-watch-test-label-changed 0a17080a-f6af-4244-907a-eec7d17993e1 3464469 0 2020-12-24 03:19:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-12-24 03:19:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 24 03:19:34.415: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6921 /api/v1/namespaces/watch-6921/configmaps/e2e-watch-test-label-changed 0a17080a-f6af-4244-907a-eec7d17993e1 3464470 0 2020-12-24 03:19:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-12-24 03:19:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Dec 24 03:19:44.448: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6921 /api/v1/namespaces/watch-6921/configmaps/e2e-watch-test-label-changed 0a17080a-f6af-4244-907a-eec7d17993e1 3464598 0 2020-12-24 03:19:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-12-24 03:19:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 24 03:19:44.449: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6921 /api/v1/namespaces/watch-6921/configmaps/e2e-watch-test-label-changed 0a17080a-f6af-4244-907a-eec7d17993e1 3464599 0 2020-12-24 03:19:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-12-24 03:19:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 24 03:19:44.449: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6921 /api/v1/namespaces/watch-6921/configmaps/e2e-watch-test-label-changed 0a17080a-f6af-4244-907a-eec7d17993e1 3464600 0 2020-12-24 03:19:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-12-24 03:19:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:19:44.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6921" for this suite.

• [SLOW TEST:10.138 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":305,"completed":215,"skipped":3406,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:19:44.471: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 03:19:44.539: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-96c4378d-e400-4ca9-8823-e4f8de8b17db" in namespace "security-context-test-7426" to be "Succeeded or Failed"
Dec 24 03:19:44.543: INFO: Pod "alpine-nnp-false-96c4378d-e400-4ca9-8823-e4f8de8b17db": Phase="Pending", Reason="", readiness=false. Elapsed: 4.094345ms
Dec 24 03:19:46.546: INFO: Pod "alpine-nnp-false-96c4378d-e400-4ca9-8823-e4f8de8b17db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006998144s
Dec 24 03:19:46.546: INFO: Pod "alpine-nnp-false-96c4378d-e400-4ca9-8823-e4f8de8b17db" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:19:46.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7426" for this suite.
•{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":216,"skipped":3428,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:19:46.560: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name secret-emptykey-test-e471bb1b-dc5c-437f-b9f6-bd0a45878019
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:19:46.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9433" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":305,"completed":217,"skipped":3435,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:19:46.634: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Dec 24 03:19:46.684: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2911 /api/v1/namespaces/watch-2911/configmaps/e2e-watch-test-configmap-a 2b6d9b04-ca11-4168-857e-6f7402ac8b10 3464644 0 2020-12-24 03:19:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-24 03:19:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 24 03:19:46.684: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2911 /api/v1/namespaces/watch-2911/configmaps/e2e-watch-test-configmap-a 2b6d9b04-ca11-4168-857e-6f7402ac8b10 3464644 0 2020-12-24 03:19:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-24 03:19:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Dec 24 03:19:56.689: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2911 /api/v1/namespaces/watch-2911/configmaps/e2e-watch-test-configmap-a 2b6d9b04-ca11-4168-857e-6f7402ac8b10 3464736 0 2020-12-24 03:19:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-24 03:19:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 24 03:19:56.689: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2911 /api/v1/namespaces/watch-2911/configmaps/e2e-watch-test-configmap-a 2b6d9b04-ca11-4168-857e-6f7402ac8b10 3464736 0 2020-12-24 03:19:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-24 03:19:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Dec 24 03:20:06.696: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2911 /api/v1/namespaces/watch-2911/configmaps/e2e-watch-test-configmap-a 2b6d9b04-ca11-4168-857e-6f7402ac8b10 3464804 0 2020-12-24 03:19:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-24 03:20:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 24 03:20:06.696: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2911 /api/v1/namespaces/watch-2911/configmaps/e2e-watch-test-configmap-a 2b6d9b04-ca11-4168-857e-6f7402ac8b10 3464804 0 2020-12-24 03:19:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-24 03:20:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Dec 24 03:20:16.702: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2911 /api/v1/namespaces/watch-2911/configmaps/e2e-watch-test-configmap-a 2b6d9b04-ca11-4168-857e-6f7402ac8b10 3464869 0 2020-12-24 03:19:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-24 03:20:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 24 03:20:16.702: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2911 /api/v1/namespaces/watch-2911/configmaps/e2e-watch-test-configmap-a 2b6d9b04-ca11-4168-857e-6f7402ac8b10 3464869 0 2020-12-24 03:19:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-24 03:20:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Dec 24 03:20:26.709: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2911 /api/v1/namespaces/watch-2911/configmaps/e2e-watch-test-configmap-b 89840be2-cb8e-47e1-b4ad-ef521b23a1ab 3464935 0 2020-12-24 03:20:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-12-24 03:20:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 24 03:20:26.709: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2911 /api/v1/namespaces/watch-2911/configmaps/e2e-watch-test-configmap-b 89840be2-cb8e-47e1-b4ad-ef521b23a1ab 3464935 0 2020-12-24 03:20:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-12-24 03:20:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Dec 24 03:20:36.714: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2911 /api/v1/namespaces/watch-2911/configmaps/e2e-watch-test-configmap-b 89840be2-cb8e-47e1-b4ad-ef521b23a1ab 3465000 0 2020-12-24 03:20:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-12-24 03:20:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 24 03:20:36.714: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2911 /api/v1/namespaces/watch-2911/configmaps/e2e-watch-test-configmap-b 89840be2-cb8e-47e1-b4ad-ef521b23a1ab 3465000 0 2020-12-24 03:20:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-12-24 03:20:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:20:46.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2911" for this suite.

• [SLOW TEST:60.088 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":305,"completed":218,"skipped":3453,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:20:46.723: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-cb68c351-a11b-4aa5-90be-d5092450d26c
STEP: Creating a pod to test consume configMaps
Dec 24 03:20:46.815: INFO: Waiting up to 5m0s for pod "pod-configmaps-832e2769-51f4-4fae-b1bd-c6b248780915" in namespace "configmap-507" to be "Succeeded or Failed"
Dec 24 03:20:46.831: INFO: Pod "pod-configmaps-832e2769-51f4-4fae-b1bd-c6b248780915": Phase="Pending", Reason="", readiness=false. Elapsed: 16.374347ms
Dec 24 03:20:48.834: INFO: Pod "pod-configmaps-832e2769-51f4-4fae-b1bd-c6b248780915": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019444666s
Dec 24 03:20:50.837: INFO: Pod "pod-configmaps-832e2769-51f4-4fae-b1bd-c6b248780915": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022233437s
STEP: Saw pod success
Dec 24 03:20:50.837: INFO: Pod "pod-configmaps-832e2769-51f4-4fae-b1bd-c6b248780915" satisfied condition "Succeeded or Failed"
Dec 24 03:20:50.839: INFO: Trying to get logs from node c1-4 pod pod-configmaps-832e2769-51f4-4fae-b1bd-c6b248780915 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 24 03:20:50.855: INFO: Waiting for pod pod-configmaps-832e2769-51f4-4fae-b1bd-c6b248780915 to disappear
Dec 24 03:20:50.884: INFO: Pod pod-configmaps-832e2769-51f4-4fae-b1bd-c6b248780915 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:20:50.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-507" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":219,"skipped":3459,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:20:50.892: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2952.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2952.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2952.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2952.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2952.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2952.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2952.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2952.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2952.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2952.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 24 03:20:52.978: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:20:52.980: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:20:52.983: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:20:52.985: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:20:52.993: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:20:52.995: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:20:52.998: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:20:53.000: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:20:53.004: INFO: Lookups using dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2952.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2952.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local jessie_udp@dns-test-service-2.dns-2952.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2952.svc.cluster.local]

Dec 24 03:20:58.008: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:20:58.011: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:20:58.014: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:20:58.017: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:20:58.026: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:20:58.029: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:20:58.032: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:20:58.035: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:20:58.041: INFO: Lookups using dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2952.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2952.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local jessie_udp@dns-test-service-2.dns-2952.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2952.svc.cluster.local]

Dec 24 03:21:03.008: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:21:03.011: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:21:03.014: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:21:03.017: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:21:03.026: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:21:03.029: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:21:03.032: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:21:03.035: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:21:03.041: INFO: Lookups using dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2952.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2952.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local jessie_udp@dns-test-service-2.dns-2952.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2952.svc.cluster.local]

Dec 24 03:21:08.008: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:21:08.011: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:21:08.014: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:21:08.017: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:21:08.026: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:21:08.029: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:21:08.032: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:21:08.035: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:21:08.041: INFO: Lookups using dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2952.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2952.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local jessie_udp@dns-test-service-2.dns-2952.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2952.svc.cluster.local]

Dec 24 03:21:13.011: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:21:13.016: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:21:13.022: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:21:13.056: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:21:13.063: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:21:13.066: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:21:13.068: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:21:13.070: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:21:13.075: INFO: Lookups using dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2952.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2952.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local jessie_udp@dns-test-service-2.dns-2952.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2952.svc.cluster.local]

Dec 24 03:21:18.009: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:21:18.012: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:21:18.015: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:21:18.018: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:21:18.027: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:21:18.030: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:21:18.033: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:21:18.036: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2952.svc.cluster.local from pod dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3: the server could not find the requested resource (get pods dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3)
Dec 24 03:21:18.042: INFO: Lookups using dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2952.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2952.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2952.svc.cluster.local jessie_udp@dns-test-service-2.dns-2952.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2952.svc.cluster.local]

Dec 24 03:21:23.040: INFO: DNS probes using dns-2952/dns-test-066d204f-09a9-4ec7-aef6-3c5986dc34d3 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:21:23.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2952" for this suite.

• [SLOW TEST:32.262 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":305,"completed":220,"skipped":3485,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:21:23.154: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Dec 24 03:21:25.752: INFO: Successfully updated pod "annotationupdate2bf440d2-5abe-4c8e-a8da-40373a925dac"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:21:27.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7022" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":221,"skipped":3487,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:21:27.781: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-fd03ada2-9dda-4dc5-ab3e-194610e57e9f
STEP: Creating secret with name s-test-opt-upd-548a08ff-6b64-43ed-a20b-c65134485ba6
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-fd03ada2-9dda-4dc5-ab3e-194610e57e9f
STEP: Updating secret s-test-opt-upd-548a08ff-6b64-43ed-a20b-c65134485ba6
STEP: Creating secret with name s-test-opt-create-fbb23fb7-8e2d-4a6d-838c-36f1ae122256
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:22:38.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9777" for this suite.

• [SLOW TEST:70.356 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":222,"skipped":3489,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:22:38.138: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-075b25b9-dd85-4513-88ba-5d759fe61213
STEP: Creating a pod to test consume secrets
Dec 24 03:22:38.202: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-be0d7763-6941-43c3-992b-4a95f98d9930" in namespace "projected-4042" to be "Succeeded or Failed"
Dec 24 03:22:38.206: INFO: Pod "pod-projected-secrets-be0d7763-6941-43c3-992b-4a95f98d9930": Phase="Pending", Reason="", readiness=false. Elapsed: 3.046795ms
Dec 24 03:22:40.208: INFO: Pod "pod-projected-secrets-be0d7763-6941-43c3-992b-4a95f98d9930": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005293418s
STEP: Saw pod success
Dec 24 03:22:40.208: INFO: Pod "pod-projected-secrets-be0d7763-6941-43c3-992b-4a95f98d9930" satisfied condition "Succeeded or Failed"
Dec 24 03:22:40.210: INFO: Trying to get logs from node c1-4 pod pod-projected-secrets-be0d7763-6941-43c3-992b-4a95f98d9930 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 24 03:22:40.224: INFO: Waiting for pod pod-projected-secrets-be0d7763-6941-43c3-992b-4a95f98d9930 to disappear
Dec 24 03:22:40.228: INFO: Pod pod-projected-secrets-be0d7763-6941-43c3-992b-4a95f98d9930 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:22:40.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4042" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":223,"skipped":3506,"failed":0}

------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:22:40.235: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Dec 24 03:22:40.326: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
Dec 24 03:22:44.881: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:23:02.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1846" for this suite.

• [SLOW TEST:22.733 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":305,"completed":224,"skipped":3506,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:23:02.967: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 24 03:23:03.014: INFO: Waiting up to 5m0s for pod "downwardapi-volume-85c794ce-e9dc-49c4-a1ed-784996f06bff" in namespace "projected-1640" to be "Succeeded or Failed"
Dec 24 03:23:03.040: INFO: Pod "downwardapi-volume-85c794ce-e9dc-49c4-a1ed-784996f06bff": Phase="Pending", Reason="", readiness=false. Elapsed: 26.068004ms
Dec 24 03:23:05.043: INFO: Pod "downwardapi-volume-85c794ce-e9dc-49c4-a1ed-784996f06bff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02908271s
Dec 24 03:23:07.046: INFO: Pod "downwardapi-volume-85c794ce-e9dc-49c4-a1ed-784996f06bff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032035046s
STEP: Saw pod success
Dec 24 03:23:07.046: INFO: Pod "downwardapi-volume-85c794ce-e9dc-49c4-a1ed-784996f06bff" satisfied condition "Succeeded or Failed"
Dec 24 03:23:07.049: INFO: Trying to get logs from node c1-4 pod downwardapi-volume-85c794ce-e9dc-49c4-a1ed-784996f06bff container client-container: <nil>
STEP: delete the pod
Dec 24 03:23:07.071: INFO: Waiting for pod downwardapi-volume-85c794ce-e9dc-49c4-a1ed-784996f06bff to disappear
Dec 24 03:23:07.091: INFO: Pod downwardapi-volume-85c794ce-e9dc-49c4-a1ed-784996f06bff no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:23:07.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1640" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":305,"completed":225,"skipped":3517,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:23:07.100: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 03:23:07.165: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Dec 24 03:23:07.173: INFO: Number of nodes with available pods: 0
Dec 24 03:23:07.173: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Dec 24 03:23:07.224: INFO: Number of nodes with available pods: 0
Dec 24 03:23:07.224: INFO: Node c1-4 is running more than one daemon pod
Dec 24 03:23:08.235: INFO: Number of nodes with available pods: 0
Dec 24 03:23:08.235: INFO: Node c1-4 is running more than one daemon pod
Dec 24 03:23:09.227: INFO: Number of nodes with available pods: 1
Dec 24 03:23:09.227: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Dec 24 03:23:09.247: INFO: Number of nodes with available pods: 1
Dec 24 03:23:09.247: INFO: Number of running nodes: 0, number of available pods: 1
Dec 24 03:23:10.251: INFO: Number of nodes with available pods: 0
Dec 24 03:23:10.251: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Dec 24 03:23:10.288: INFO: Number of nodes with available pods: 0
Dec 24 03:23:10.288: INFO: Node c1-4 is running more than one daemon pod
Dec 24 03:23:11.291: INFO: Number of nodes with available pods: 0
Dec 24 03:23:11.291: INFO: Node c1-4 is running more than one daemon pod
Dec 24 03:23:12.309: INFO: Number of nodes with available pods: 0
Dec 24 03:23:12.309: INFO: Node c1-4 is running more than one daemon pod
Dec 24 03:23:13.291: INFO: Number of nodes with available pods: 0
Dec 24 03:23:13.291: INFO: Node c1-4 is running more than one daemon pod
Dec 24 03:23:14.323: INFO: Number of nodes with available pods: 0
Dec 24 03:23:14.323: INFO: Node c1-4 is running more than one daemon pod
Dec 24 03:23:15.291: INFO: Number of nodes with available pods: 0
Dec 24 03:23:15.291: INFO: Node c1-4 is running more than one daemon pod
Dec 24 03:23:16.291: INFO: Number of nodes with available pods: 0
Dec 24 03:23:16.291: INFO: Node c1-4 is running more than one daemon pod
Dec 24 03:23:17.291: INFO: Number of nodes with available pods: 0
Dec 24 03:23:17.291: INFO: Node c1-4 is running more than one daemon pod
Dec 24 03:23:18.291: INFO: Number of nodes with available pods: 0
Dec 24 03:23:18.291: INFO: Node c1-4 is running more than one daemon pod
Dec 24 03:23:19.291: INFO: Number of nodes with available pods: 0
Dec 24 03:23:19.291: INFO: Node c1-4 is running more than one daemon pod
Dec 24 03:23:20.291: INFO: Number of nodes with available pods: 0
Dec 24 03:23:20.291: INFO: Node c1-4 is running more than one daemon pod
Dec 24 03:23:21.291: INFO: Number of nodes with available pods: 0
Dec 24 03:23:21.291: INFO: Node c1-4 is running more than one daemon pod
Dec 24 03:23:22.291: INFO: Number of nodes with available pods: 0
Dec 24 03:23:22.291: INFO: Node c1-4 is running more than one daemon pod
Dec 24 03:23:23.294: INFO: Number of nodes with available pods: 0
Dec 24 03:23:23.294: INFO: Node c1-4 is running more than one daemon pod
Dec 24 03:23:24.291: INFO: Number of nodes with available pods: 0
Dec 24 03:23:24.291: INFO: Node c1-4 is running more than one daemon pod
Dec 24 03:23:25.291: INFO: Number of nodes with available pods: 1
Dec 24 03:23:25.291: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6766, will wait for the garbage collector to delete the pods
Dec 24 03:23:25.352: INFO: Deleting DaemonSet.extensions daemon-set took: 4.417917ms
Dec 24 03:23:26.353: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.000217323s
Dec 24 03:23:33.359: INFO: Number of nodes with available pods: 0
Dec 24 03:23:33.359: INFO: Number of running nodes: 0, number of available pods: 0
Dec 24 03:23:33.361: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6766/daemonsets","resourceVersion":"3466457"},"items":null}

Dec 24 03:23:33.363: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6766/pods","resourceVersion":"3466457"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:23:33.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6766" for this suite.

• [SLOW TEST:26.294 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":305,"completed":226,"skipped":3549,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:23:33.395: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1839
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-1839
I1224 03:23:33.489612      24 runners.go:190] Created replication controller with name: externalname-service, namespace: services-1839, replica count: 2
Dec 24 03:23:36.540: INFO: Creating new exec pod
I1224 03:23:36.540022      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 24 03:23:39.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-1839 execpodlcs45 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Dec 24 03:23:39.782: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 24 03:23:39.782: INFO: stdout: ""
Dec 24 03:23:39.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-1839 execpodlcs45 -- /bin/sh -x -c nc -zv -t -w 2 10.96.125.125 80'
Dec 24 03:23:39.958: INFO: stderr: "+ nc -zv -t -w 2 10.96.125.125 80\nConnection to 10.96.125.125 80 port [tcp/http] succeeded!\n"
Dec 24 03:23:39.958: INFO: stdout: ""
Dec 24 03:23:39.958: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:23:39.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1839" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:6.621 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":305,"completed":227,"skipped":3554,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:23:40.016: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 24 03:23:40.712: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 24 03:23:43.749: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 03:23:43.752: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:23:44.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-728" for this suite.
STEP: Destroying namespace "webhook-728-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":305,"completed":228,"skipped":3558,"failed":0}

------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:23:45.005: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6683.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6683.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6683.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6683.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6683.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6683.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 24 03:23:49.173: INFO: DNS probes using dns-6683/dns-test-2e52c7e6-2205-418c-9715-a1efd212b8f3 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:23:49.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6683" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":305,"completed":229,"skipped":3558,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:23:49.266: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Dec 24 03:23:49.316: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Dec 24 03:23:49.329: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Dec 24 03:23:49.329: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Dec 24 03:23:49.355: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Dec 24 03:23:49.355: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Dec 24 03:23:49.370: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Dec 24 03:23:49.370: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Dec 24 03:23:56.404: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:23:56.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-7047" for this suite.

• [SLOW TEST:7.169 seconds]
[sig-scheduling] LimitRange
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":305,"completed":230,"skipped":3584,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:23:56.436: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Dec 24 03:23:56.510: INFO: PodSpec: initContainers in spec.initContainers
Dec 24 03:24:46.381: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-5717eea2-f794-4828-b0ea-69011ad6a2d2", GenerateName:"", Namespace:"init-container-743", SelfLink:"/api/v1/namespaces/init-container-743/pods/pod-init-5717eea2-f794-4828-b0ea-69011ad6a2d2", UID:"5b824aff-d9e7-4141-b177-482fab4ecee3", ResourceVersion:"3467264", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63744377036, loc:(*time.Location)(0x77108c0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"510256062"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.244.113.107/32", "cni.projectcalico.org/podIPs":"10.244.113.107/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc004780f60), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004780f80)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc004780fa0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004780fc0)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc004780fe0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004781000)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-7kqkx", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc005a16680), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7kqkx", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7kqkx", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7kqkx", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc006190f18), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"c1-4", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000ae6690), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc006190fc0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc006190fe0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc006190fe8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc006190fec), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc004c58690), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744377036, loc:(*time.Location)(0x77108c0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744377036, loc:(*time.Location)(0x77108c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744377036, loc:(*time.Location)(0x77108c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744377036, loc:(*time.Location)(0x77108c0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.21.3.5", PodIP:"10.244.113.107", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.113.107"}}, StartTime:(*v1.Time)(0xc004781020), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000ae67e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000ae6850)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"cri-o://27cf0af7b903631d26d0c18047c4f34812b685e980b17fde285e273277fe3017", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004781060), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004781040), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc00619108f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:24:46.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-743" for this suite.

• [SLOW TEST:49.958 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":305,"completed":231,"skipped":3597,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:24:46.395: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-fa56098f-ef0b-4956-9430-387f0eccbb01
STEP: Creating a pod to test consume configMaps
Dec 24 03:24:46.456: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6e8dd82c-654a-4ca6-a03d-9026461ff196" in namespace "projected-1995" to be "Succeeded or Failed"
Dec 24 03:24:46.474: INFO: Pod "pod-projected-configmaps-6e8dd82c-654a-4ca6-a03d-9026461ff196": Phase="Pending", Reason="", readiness=false. Elapsed: 18.643181ms
Dec 24 03:24:48.477: INFO: Pod "pod-projected-configmaps-6e8dd82c-654a-4ca6-a03d-9026461ff196": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021106846s
STEP: Saw pod success
Dec 24 03:24:48.477: INFO: Pod "pod-projected-configmaps-6e8dd82c-654a-4ca6-a03d-9026461ff196" satisfied condition "Succeeded or Failed"
Dec 24 03:24:48.479: INFO: Trying to get logs from node c1-4 pod pod-projected-configmaps-6e8dd82c-654a-4ca6-a03d-9026461ff196 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 24 03:24:48.504: INFO: Waiting for pod pod-projected-configmaps-6e8dd82c-654a-4ca6-a03d-9026461ff196 to disappear
Dec 24 03:24:48.511: INFO: Pod pod-projected-configmaps-6e8dd82c-654a-4ca6-a03d-9026461ff196 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:24:48.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1995" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":232,"skipped":3598,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:24:48.518: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-16f9c87c-ebd7-4181-ba8b-e7ece7d4ebea
STEP: Creating a pod to test consume secrets
Dec 24 03:24:48.597: INFO: Waiting up to 5m0s for pod "pod-secrets-f050c086-224b-4ce8-bfe8-5bd7b3d553e5" in namespace "secrets-2479" to be "Succeeded or Failed"
Dec 24 03:24:48.603: INFO: Pod "pod-secrets-f050c086-224b-4ce8-bfe8-5bd7b3d553e5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.537179ms
Dec 24 03:24:50.615: INFO: Pod "pod-secrets-f050c086-224b-4ce8-bfe8-5bd7b3d553e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017696691s
STEP: Saw pod success
Dec 24 03:24:50.615: INFO: Pod "pod-secrets-f050c086-224b-4ce8-bfe8-5bd7b3d553e5" satisfied condition "Succeeded or Failed"
Dec 24 03:24:50.617: INFO: Trying to get logs from node c1-4 pod pod-secrets-f050c086-224b-4ce8-bfe8-5bd7b3d553e5 container secret-volume-test: <nil>
STEP: delete the pod
Dec 24 03:24:50.636: INFO: Waiting for pod pod-secrets-f050c086-224b-4ce8-bfe8-5bd7b3d553e5 to disappear
Dec 24 03:24:50.643: INFO: Pod pod-secrets-f050c086-224b-4ce8-bfe8-5bd7b3d553e5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:24:50.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2479" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":233,"skipped":3600,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:24:50.650: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4891
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-4891
I1224 03:24:50.769512      24 runners.go:190] Created replication controller with name: externalname-service, namespace: services-4891, replica count: 2
Dec 24 03:24:53.819: INFO: Creating new exec pod
I1224 03:24:53.819811      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 24 03:24:56.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-4891 execpodnm94b -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Dec 24 03:24:57.036: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 24 03:24:57.036: INFO: stdout: ""
Dec 24 03:24:57.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-4891 execpodnm94b -- /bin/sh -x -c nc -zv -t -w 2 10.96.168.97 80'
Dec 24 03:24:57.220: INFO: stderr: "+ nc -zv -t -w 2 10.96.168.97 80\nConnection to 10.96.168.97 80 port [tcp/http] succeeded!\n"
Dec 24 03:24:57.220: INFO: stdout: ""
Dec 24 03:24:57.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-4891 execpodnm94b -- /bin/sh -x -c nc -zv -t -w 2 172.21.3.5 32256'
Dec 24 03:24:57.396: INFO: stderr: "+ nc -zv -t -w 2 172.21.3.5 32256\nConnection to 172.21.3.5 32256 port [tcp/32256] succeeded!\n"
Dec 24 03:24:57.396: INFO: stdout: ""
Dec 24 03:24:57.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-4891 execpodnm94b -- /bin/sh -x -c nc -zv -t -w 2 172.21.3.6 32256'
Dec 24 03:24:57.563: INFO: stderr: "+ nc -zv -t -w 2 172.21.3.6 32256\nConnection to 172.21.3.6 32256 port [tcp/32256] succeeded!\n"
Dec 24 03:24:57.563: INFO: stdout: ""
Dec 24 03:24:57.563: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:24:57.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4891" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:6.975 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":305,"completed":234,"skipped":3609,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:24:57.625: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 24 03:24:57.668: INFO: Waiting up to 5m0s for pod "pod-c9844fd2-dc5b-456b-92bd-d5fc4157336b" in namespace "emptydir-3544" to be "Succeeded or Failed"
Dec 24 03:24:57.686: INFO: Pod "pod-c9844fd2-dc5b-456b-92bd-d5fc4157336b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.150383ms
Dec 24 03:24:59.689: INFO: Pod "pod-c9844fd2-dc5b-456b-92bd-d5fc4157336b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020808247s
STEP: Saw pod success
Dec 24 03:24:59.689: INFO: Pod "pod-c9844fd2-dc5b-456b-92bd-d5fc4157336b" satisfied condition "Succeeded or Failed"
Dec 24 03:24:59.691: INFO: Trying to get logs from node c1-4 pod pod-c9844fd2-dc5b-456b-92bd-d5fc4157336b container test-container: <nil>
STEP: delete the pod
Dec 24 03:24:59.709: INFO: Waiting for pod pod-c9844fd2-dc5b-456b-92bd-d5fc4157336b to disappear
Dec 24 03:24:59.713: INFO: Pod pod-c9844fd2-dc5b-456b-92bd-d5fc4157336b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:24:59.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3544" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":235,"skipped":3615,"failed":0}

------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:24:59.720: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:25:01.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6600" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":305,"completed":236,"skipped":3615,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:25:01.879: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 24 03:25:01.939: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3cc08d91-3127-4d1b-b5b1-a583be78f6c3" in namespace "downward-api-9233" to be "Succeeded or Failed"
Dec 24 03:25:01.943: INFO: Pod "downwardapi-volume-3cc08d91-3127-4d1b-b5b1-a583be78f6c3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.313426ms
Dec 24 03:25:03.946: INFO: Pod "downwardapi-volume-3cc08d91-3127-4d1b-b5b1-a583be78f6c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006470072s
STEP: Saw pod success
Dec 24 03:25:03.946: INFO: Pod "downwardapi-volume-3cc08d91-3127-4d1b-b5b1-a583be78f6c3" satisfied condition "Succeeded or Failed"
Dec 24 03:25:03.948: INFO: Trying to get logs from node c1-4 pod downwardapi-volume-3cc08d91-3127-4d1b-b5b1-a583be78f6c3 container client-container: <nil>
STEP: delete the pod
Dec 24 03:25:03.968: INFO: Waiting for pod downwardapi-volume-3cc08d91-3127-4d1b-b5b1-a583be78f6c3 to disappear
Dec 24 03:25:03.988: INFO: Pod downwardapi-volume-3cc08d91-3127-4d1b-b5b1-a583be78f6c3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:25:03.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9233" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":237,"skipped":3631,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:25:03.996: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-81955da5-7d46-4f7b-b732-20c29114b33f
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-81955da5-7d46-4f7b-b732-20c29114b33f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:25:08.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8035" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":238,"skipped":3645,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:25:08.091: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 24 03:25:08.141: INFO: Waiting up to 5m0s for pod "downwardapi-volume-77762292-3e0e-4a7c-9824-a4620cecbac6" in namespace "projected-4567" to be "Succeeded or Failed"
Dec 24 03:25:08.144: INFO: Pod "downwardapi-volume-77762292-3e0e-4a7c-9824-a4620cecbac6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.805231ms
Dec 24 03:25:10.147: INFO: Pod "downwardapi-volume-77762292-3e0e-4a7c-9824-a4620cecbac6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006212504s
STEP: Saw pod success
Dec 24 03:25:10.147: INFO: Pod "downwardapi-volume-77762292-3e0e-4a7c-9824-a4620cecbac6" satisfied condition "Succeeded or Failed"
Dec 24 03:25:10.149: INFO: Trying to get logs from node c1-4 pod downwardapi-volume-77762292-3e0e-4a7c-9824-a4620cecbac6 container client-container: <nil>
STEP: delete the pod
Dec 24 03:25:10.167: INFO: Waiting for pod downwardapi-volume-77762292-3e0e-4a7c-9824-a4620cecbac6 to disappear
Dec 24 03:25:10.184: INFO: Pod downwardapi-volume-77762292-3e0e-4a7c-9824-a4620cecbac6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:25:10.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4567" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":239,"skipped":3649,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:25:10.194: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 24 03:25:10.254: INFO: Waiting up to 5m0s for pod "pod-9fe289c2-643f-48c0-bf11-a18ce5d9e562" in namespace "emptydir-1216" to be "Succeeded or Failed"
Dec 24 03:25:10.258: INFO: Pod "pod-9fe289c2-643f-48c0-bf11-a18ce5d9e562": Phase="Pending", Reason="", readiness=false. Elapsed: 3.718901ms
Dec 24 03:25:12.260: INFO: Pod "pod-9fe289c2-643f-48c0-bf11-a18ce5d9e562": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006627837s
STEP: Saw pod success
Dec 24 03:25:12.260: INFO: Pod "pod-9fe289c2-643f-48c0-bf11-a18ce5d9e562" satisfied condition "Succeeded or Failed"
Dec 24 03:25:12.263: INFO: Trying to get logs from node c1-4 pod pod-9fe289c2-643f-48c0-bf11-a18ce5d9e562 container test-container: <nil>
STEP: delete the pod
Dec 24 03:25:12.283: INFO: Waiting for pod pod-9fe289c2-643f-48c0-bf11-a18ce5d9e562 to disappear
Dec 24 03:25:12.286: INFO: Pod pod-9fe289c2-643f-48c0-bf11-a18ce5d9e562 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:25:12.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1216" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":240,"skipped":3668,"failed":0}
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:25:12.294: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-5240
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 24 03:25:12.349: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 24 03:25:12.425: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 24 03:25:14.427: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 24 03:25:16.431: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 24 03:25:18.436: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 24 03:25:20.427: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 24 03:25:22.428: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 24 03:25:24.428: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 24 03:25:26.428: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 24 03:25:28.428: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 24 03:25:30.427: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec 24 03:25:30.431: INFO: The status of Pod netserver-1 is Running (Ready = true)
Dec 24 03:25:30.436: INFO: The status of Pod netserver-2 is Running (Ready = false)
Dec 24 03:25:32.439: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Dec 24 03:25:36.467: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.113.95:8080/dial?request=hostname&protocol=http&host=10.244.113.73&port=8080&tries=1'] Namespace:pod-network-test-5240 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 24 03:25:36.467: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
Dec 24 03:25:36.572: INFO: Waiting for responses: map[]
Dec 24 03:25:36.575: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.113.95:8080/dial?request=hostname&protocol=http&host=10.244.26.125&port=8080&tries=1'] Namespace:pod-network-test-5240 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 24 03:25:36.575: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
Dec 24 03:25:36.674: INFO: Waiting for responses: map[]
Dec 24 03:25:36.677: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.113.95:8080/dial?request=hostname&protocol=http&host=10.244.92.249&port=8080&tries=1'] Namespace:pod-network-test-5240 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 24 03:25:36.677: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
Dec 24 03:25:36.772: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:25:36.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5240" for this suite.

• [SLOW TEST:24.486 seconds]
[sig-network] Networking
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":305,"completed":241,"skipped":3671,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:25:36.780: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5471.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5471.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5471.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5471.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 24 03:25:38.867: INFO: DNS probes using dns-test-7d1d0a20-8d74-4288-9737-ed9e60f33b11 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5471.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5471.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5471.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5471.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 24 03:25:42.956: INFO: File wheezy_udp@dns-test-service-3.dns-5471.svc.cluster.local from pod  dns-5471/dns-test-0f7c6389-3550-4170-94c0-bdbf7ab4c3b2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 24 03:25:42.959: INFO: File jessie_udp@dns-test-service-3.dns-5471.svc.cluster.local from pod  dns-5471/dns-test-0f7c6389-3550-4170-94c0-bdbf7ab4c3b2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 24 03:25:42.959: INFO: Lookups using dns-5471/dns-test-0f7c6389-3550-4170-94c0-bdbf7ab4c3b2 failed for: [wheezy_udp@dns-test-service-3.dns-5471.svc.cluster.local jessie_udp@dns-test-service-3.dns-5471.svc.cluster.local]

Dec 24 03:25:47.963: INFO: File wheezy_udp@dns-test-service-3.dns-5471.svc.cluster.local from pod  dns-5471/dns-test-0f7c6389-3550-4170-94c0-bdbf7ab4c3b2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 24 03:25:47.967: INFO: File jessie_udp@dns-test-service-3.dns-5471.svc.cluster.local from pod  dns-5471/dns-test-0f7c6389-3550-4170-94c0-bdbf7ab4c3b2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 24 03:25:47.967: INFO: Lookups using dns-5471/dns-test-0f7c6389-3550-4170-94c0-bdbf7ab4c3b2 failed for: [wheezy_udp@dns-test-service-3.dns-5471.svc.cluster.local jessie_udp@dns-test-service-3.dns-5471.svc.cluster.local]

Dec 24 03:25:52.963: INFO: File wheezy_udp@dns-test-service-3.dns-5471.svc.cluster.local from pod  dns-5471/dns-test-0f7c6389-3550-4170-94c0-bdbf7ab4c3b2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 24 03:25:52.966: INFO: File jessie_udp@dns-test-service-3.dns-5471.svc.cluster.local from pod  dns-5471/dns-test-0f7c6389-3550-4170-94c0-bdbf7ab4c3b2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 24 03:25:52.966: INFO: Lookups using dns-5471/dns-test-0f7c6389-3550-4170-94c0-bdbf7ab4c3b2 failed for: [wheezy_udp@dns-test-service-3.dns-5471.svc.cluster.local jessie_udp@dns-test-service-3.dns-5471.svc.cluster.local]

Dec 24 03:25:57.963: INFO: File wheezy_udp@dns-test-service-3.dns-5471.svc.cluster.local from pod  dns-5471/dns-test-0f7c6389-3550-4170-94c0-bdbf7ab4c3b2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 24 03:25:57.966: INFO: File jessie_udp@dns-test-service-3.dns-5471.svc.cluster.local from pod  dns-5471/dns-test-0f7c6389-3550-4170-94c0-bdbf7ab4c3b2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 24 03:25:57.966: INFO: Lookups using dns-5471/dns-test-0f7c6389-3550-4170-94c0-bdbf7ab4c3b2 failed for: [wheezy_udp@dns-test-service-3.dns-5471.svc.cluster.local jessie_udp@dns-test-service-3.dns-5471.svc.cluster.local]

Dec 24 03:26:02.963: INFO: File wheezy_udp@dns-test-service-3.dns-5471.svc.cluster.local from pod  dns-5471/dns-test-0f7c6389-3550-4170-94c0-bdbf7ab4c3b2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 24 03:26:02.966: INFO: File jessie_udp@dns-test-service-3.dns-5471.svc.cluster.local from pod  dns-5471/dns-test-0f7c6389-3550-4170-94c0-bdbf7ab4c3b2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 24 03:26:02.966: INFO: Lookups using dns-5471/dns-test-0f7c6389-3550-4170-94c0-bdbf7ab4c3b2 failed for: [wheezy_udp@dns-test-service-3.dns-5471.svc.cluster.local jessie_udp@dns-test-service-3.dns-5471.svc.cluster.local]

Dec 24 03:26:07.963: INFO: File wheezy_udp@dns-test-service-3.dns-5471.svc.cluster.local from pod  dns-5471/dns-test-0f7c6389-3550-4170-94c0-bdbf7ab4c3b2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 24 03:26:07.966: INFO: File jessie_udp@dns-test-service-3.dns-5471.svc.cluster.local from pod  dns-5471/dns-test-0f7c6389-3550-4170-94c0-bdbf7ab4c3b2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 24 03:26:07.966: INFO: Lookups using dns-5471/dns-test-0f7c6389-3550-4170-94c0-bdbf7ab4c3b2 failed for: [wheezy_udp@dns-test-service-3.dns-5471.svc.cluster.local jessie_udp@dns-test-service-3.dns-5471.svc.cluster.local]

Dec 24 03:26:12.966: INFO: DNS probes using dns-test-0f7c6389-3550-4170-94c0-bdbf7ab4c3b2 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5471.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5471.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5471.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5471.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 24 03:26:17.068: INFO: DNS probes using dns-test-aa0b9c1e-2007-4a05-826b-d4bcd69f7c98 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:26:17.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5471" for this suite.

• [SLOW TEST:40.404 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":305,"completed":242,"skipped":3759,"failed":0}
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:26:17.185: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-fld7
STEP: Creating a pod to test atomic-volume-subpath
Dec 24 03:26:17.240: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-fld7" in namespace "subpath-4713" to be "Succeeded or Failed"
Dec 24 03:26:17.255: INFO: Pod "pod-subpath-test-configmap-fld7": Phase="Pending", Reason="", readiness=false. Elapsed: 14.88726ms
Dec 24 03:26:19.258: INFO: Pod "pod-subpath-test-configmap-fld7": Phase="Running", Reason="", readiness=true. Elapsed: 2.017550616s
Dec 24 03:26:21.261: INFO: Pod "pod-subpath-test-configmap-fld7": Phase="Running", Reason="", readiness=true. Elapsed: 4.020535759s
Dec 24 03:26:23.264: INFO: Pod "pod-subpath-test-configmap-fld7": Phase="Running", Reason="", readiness=true. Elapsed: 6.023447667s
Dec 24 03:26:25.267: INFO: Pod "pod-subpath-test-configmap-fld7": Phase="Running", Reason="", readiness=true. Elapsed: 8.026672635s
Dec 24 03:26:27.270: INFO: Pod "pod-subpath-test-configmap-fld7": Phase="Running", Reason="", readiness=true. Elapsed: 10.029300095s
Dec 24 03:26:29.281: INFO: Pod "pod-subpath-test-configmap-fld7": Phase="Running", Reason="", readiness=true. Elapsed: 12.040322734s
Dec 24 03:26:31.284: INFO: Pod "pod-subpath-test-configmap-fld7": Phase="Running", Reason="", readiness=true. Elapsed: 14.04375911s
Dec 24 03:26:33.287: INFO: Pod "pod-subpath-test-configmap-fld7": Phase="Running", Reason="", readiness=true. Elapsed: 16.047127967s
Dec 24 03:26:35.291: INFO: Pod "pod-subpath-test-configmap-fld7": Phase="Running", Reason="", readiness=true. Elapsed: 18.050492095s
Dec 24 03:26:37.294: INFO: Pod "pod-subpath-test-configmap-fld7": Phase="Running", Reason="", readiness=true. Elapsed: 20.053826972s
Dec 24 03:26:39.297: INFO: Pod "pod-subpath-test-configmap-fld7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.056979837s
STEP: Saw pod success
Dec 24 03:26:39.297: INFO: Pod "pod-subpath-test-configmap-fld7" satisfied condition "Succeeded or Failed"
Dec 24 03:26:39.299: INFO: Trying to get logs from node c1-4 pod pod-subpath-test-configmap-fld7 container test-container-subpath-configmap-fld7: <nil>
STEP: delete the pod
Dec 24 03:26:39.320: INFO: Waiting for pod pod-subpath-test-configmap-fld7 to disappear
Dec 24 03:26:39.325: INFO: Pod pod-subpath-test-configmap-fld7 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-fld7
Dec 24 03:26:39.325: INFO: Deleting pod "pod-subpath-test-configmap-fld7" in namespace "subpath-4713"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:26:39.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4713" for this suite.

• [SLOW TEST:22.150 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":305,"completed":243,"skipped":3759,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:26:39.335: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-9285
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-9285
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9285
Dec 24 03:26:39.419: INFO: Found 0 stateful pods, waiting for 1
Dec 24 03:26:49.422: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Dec 24 03:26:49.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-9285 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 24 03:26:49.640: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 24 03:26:49.640: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 24 03:26:49.640: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 24 03:26:49.642: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 24 03:26:59.646: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 24 03:26:59.646: INFO: Waiting for statefulset status.replicas updated to 0
Dec 24 03:26:59.663: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999956s
Dec 24 03:27:00.666: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.991156173s
Dec 24 03:27:01.669: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.988360669s
Dec 24 03:27:02.672: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.985298657s
Dec 24 03:27:03.675: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.982078073s
Dec 24 03:27:04.679: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.978667701s
Dec 24 03:27:05.682: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.975048435s
Dec 24 03:27:06.685: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.972092351s
Dec 24 03:27:07.688: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.968956336s
Dec 24 03:27:08.691: INFO: Verifying statefulset ss doesn't scale past 1 for another 965.770601ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9285
Dec 24 03:27:09.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-9285 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:27:09.888: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 24 03:27:09.888: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 24 03:27:09.888: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 24 03:27:09.890: INFO: Found 1 stateful pods, waiting for 3
Dec 24 03:27:19.894: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 24 03:27:19.894: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 24 03:27:19.894: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Dec 24 03:27:19.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-9285 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 24 03:27:20.108: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 24 03:27:20.108: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 24 03:27:20.108: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 24 03:27:20.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-9285 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 24 03:27:20.305: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 24 03:27:20.305: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 24 03:27:20.305: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 24 03:27:20.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-9285 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 24 03:27:20.533: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 24 03:27:20.533: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 24 03:27:20.533: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 24 03:27:20.533: INFO: Waiting for statefulset status.replicas updated to 0
Dec 24 03:27:20.536: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Dec 24 03:27:30.542: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 24 03:27:30.542: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 24 03:27:30.542: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 24 03:27:30.556: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999951s
Dec 24 03:27:31.560: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.991818395s
Dec 24 03:27:32.564: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988410783s
Dec 24 03:27:33.567: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984183636s
Dec 24 03:27:34.571: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.98053376s
Dec 24 03:27:35.576: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.976819585s
Dec 24 03:27:36.580: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.971871098s
Dec 24 03:27:37.584: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.967856022s
Dec 24 03:27:38.589: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.964299067s
Dec 24 03:27:39.605: INFO: Verifying statefulset ss doesn't scale past 3 for another 959.105966ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9285
Dec 24 03:27:40.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-9285 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:27:40.794: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 24 03:27:40.794: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 24 03:27:40.794: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 24 03:27:40.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-9285 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:27:40.969: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 24 03:27:40.969: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 24 03:27:40.969: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 24 03:27:40.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-9285 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:27:41.160: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 24 03:27:41.160: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 24 03:27:41.160: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 24 03:27:41.160: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Dec 24 03:28:01.180: INFO: Deleting all statefulset in ns statefulset-9285
Dec 24 03:28:01.183: INFO: Scaling statefulset ss to 0
Dec 24 03:28:01.191: INFO: Waiting for statefulset status.replicas updated to 0
Dec 24 03:28:01.193: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:28:01.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9285" for this suite.

• [SLOW TEST:81.881 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":305,"completed":244,"skipped":3768,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:28:01.215: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:28:14.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8625" for this suite.

• [SLOW TEST:13.129 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":305,"completed":245,"skipped":3773,"failed":0}
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:28:14.345: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-2348
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Dec 24 03:28:14.424: INFO: Found 0 stateful pods, waiting for 3
Dec 24 03:28:24.427: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 24 03:28:24.427: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 24 03:28:24.427: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec 24 03:28:24.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-2348 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 24 03:28:24.691: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 24 03:28:24.691: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 24 03:28:24.691: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Dec 24 03:28:34.719: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Dec 24 03:28:44.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-2348 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:28:44.969: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 24 03:28:44.969: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 24 03:28:44.969: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 24 03:29:04.985: INFO: Waiting for StatefulSet statefulset-2348/ss2 to complete update
Dec 24 03:29:04.985: INFO: Waiting for Pod statefulset-2348/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 24 03:29:14.991: INFO: Waiting for StatefulSet statefulset-2348/ss2 to complete update
STEP: Rolling back to a previous revision
Dec 24 03:29:25.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-2348 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 24 03:29:26.440: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 24 03:29:26.440: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 24 03:29:26.440: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 24 03:29:36.472: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Dec 24 03:29:46.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-2348 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:29:46.725: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 24 03:29:46.725: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 24 03:29:46.725: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 24 03:29:56.741: INFO: Waiting for StatefulSet statefulset-2348/ss2 to complete update
Dec 24 03:29:56.741: INFO: Waiting for Pod statefulset-2348/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 24 03:29:56.741: INFO: Waiting for Pod statefulset-2348/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 24 03:29:56.741: INFO: Waiting for Pod statefulset-2348/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 24 03:30:06.747: INFO: Waiting for StatefulSet statefulset-2348/ss2 to complete update
Dec 24 03:30:06.747: INFO: Waiting for Pod statefulset-2348/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 24 03:30:06.747: INFO: Waiting for Pod statefulset-2348/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 24 03:30:16.747: INFO: Waiting for StatefulSet statefulset-2348/ss2 to complete update
Dec 24 03:30:16.747: INFO: Waiting for Pod statefulset-2348/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Dec 24 03:30:26.749: INFO: Deleting all statefulset in ns statefulset-2348
Dec 24 03:30:26.751: INFO: Scaling statefulset ss2 to 0
Dec 24 03:30:56.767: INFO: Waiting for statefulset status.replicas updated to 0
Dec 24 03:30:56.770: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:30:56.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2348" for this suite.

• [SLOW TEST:162.447 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":305,"completed":246,"skipped":3778,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:30:56.792: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:30:56.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6720" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":305,"completed":247,"skipped":3782,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:30:56.901: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:30:57.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-4262" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":305,"completed":248,"skipped":3854,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:30:57.007: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Dec 24 03:30:57.064: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 24 03:30:57.072: INFO: Waiting for terminating namespaces to be deleted...
Dec 24 03:30:57.074: INFO: 
Logging pods the apiserver thinks is on node c1-4 before test
Dec 24 03:30:57.090: INFO: calico-node-gf9xk from kube-system started at 2020-12-17 09:20:20 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.090: INFO: 	Container calico-node ready: true, restart count 0
Dec 24 03:30:57.090: INFO: kube-proxy-nnl7j from kube-system started at 2020-12-17 09:18:43 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.090: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 24 03:30:57.091: INFO: virt-handler-5nkqw from kubevirt started at 2020-12-23 10:54:33 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.091: INFO: 	Container virt-handler ready: false, restart count 0
Dec 24 03:30:57.091: INFO: csi-cephfsplugin-ghqns from rook-ceph started at 2020-12-23 10:54:33 +0000 UTC (3 container statuses recorded)
Dec 24 03:30:57.091: INFO: 	Container csi-cephfsplugin ready: false, restart count 0
Dec 24 03:30:57.091: INFO: 	Container driver-registrar ready: false, restart count 0
Dec 24 03:30:57.091: INFO: 	Container liveness-prometheus ready: false, restart count 0
Dec 24 03:30:57.091: INFO: csi-rbdplugin-z74d4 from rook-ceph started at 2020-12-23 10:54:33 +0000 UTC (3 container statuses recorded)
Dec 24 03:30:57.091: INFO: 	Container csi-rbdplugin ready: false, restart count 0
Dec 24 03:30:57.091: INFO: 	Container driver-registrar ready: false, restart count 0
Dec 24 03:30:57.091: INFO: 	Container liveness-prometheus ready: false, restart count 0
Dec 24 03:30:57.091: INFO: rook-ceph-crashcollector-c1-4-b5fd6cbf-jrn9q from rook-ceph started at 2020-12-24 02:42:56 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.091: INFO: 	Container ceph-crash ready: true, restart count 0
Dec 24 03:30:57.091: INFO: rook-ceph-crashcollector-c1-4-b5fd6cbf-r86bh from rook-ceph started at 2020-12-23 10:29:44 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.091: INFO: 	Container ceph-crash ready: false, restart count 0
Dec 24 03:30:57.091: INFO: rook-ceph-mon-b-6c84dd66-29k4w from rook-ceph started at 2020-12-24 02:42:56 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.091: INFO: 	Container mon ready: true, restart count 0
Dec 24 03:30:57.091: INFO: rook-ceph-mon-b-6c84dd66-xj8wb from rook-ceph started at 2020-12-23 10:29:43 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.091: INFO: 	Container mon ready: false, restart count 0
Dec 24 03:30:57.091: INFO: rook-ceph-osd-1-59b659d744-k6c5s from rook-ceph started at 2020-12-23 10:29:43 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.091: INFO: 	Container osd ready: false, restart count 0
Dec 24 03:30:57.091: INFO: rook-ceph-osd-1-59b659d744-nnqc9 from rook-ceph started at 2020-12-24 02:42:54 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.091: INFO: 	Container osd ready: true, restart count 0
Dec 24 03:30:57.091: INFO: rook-discover-svzsw from rook-ceph started at 2020-12-23 10:54:33 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.091: INFO: 	Container rook-discover ready: false, restart count 0
Dec 24 03:30:57.091: INFO: sonobuoy from sonobuoy started at 2020-12-24 02:17:41 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.091: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 24 03:30:57.091: INFO: sonobuoy-e2e-job-683f472080954d4f from sonobuoy started at 2020-12-24 02:17:43 +0000 UTC (2 container statuses recorded)
Dec 24 03:30:57.091: INFO: 	Container e2e ready: true, restart count 0
Dec 24 03:30:57.091: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 24 03:30:57.091: INFO: sonobuoy-systemd-logs-daemon-set-eb313291d8024436-cqjvw from sonobuoy started at 2020-12-24 02:17:43 +0000 UTC (2 container statuses recorded)
Dec 24 03:30:57.091: INFO: 	Container sonobuoy-worker ready: false, restart count 7
Dec 24 03:30:57.091: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 24 03:30:57.091: INFO: 
Logging pods the apiserver thinks is on node c1-5 before test
Dec 24 03:30:57.106: INFO: cdi-apiserver-79dbd664dd-jvbpv from cdi started at 2020-12-23 09:46:37 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.106: INFO: 	Container cdi-apiserver ready: true, restart count 0
Dec 24 03:30:57.106: INFO: snapshot-controller-0 from default started at 2020-12-18 04:06:22 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.106: INFO: 	Container snapshot-controller ready: true, restart count 0
Dec 24 03:30:57.106: INFO: calico-node-tt5dp from kube-system started at 2020-12-17 09:20:20 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.106: INFO: 	Container calico-node ready: true, restart count 0
Dec 24 03:30:57.106: INFO: kube-proxy-2lcn2 from kube-system started at 2020-12-17 09:18:32 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.106: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 24 03:30:57.106: INFO: virt-api-76b44ffc65-kcmtd from kubevirt started at 2020-12-23 09:46:37 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.106: INFO: 	Container virt-api ready: true, restart count 0
Dec 24 03:30:57.106: INFO: virt-controller-84b5d4b779-lbv8p from kubevirt started at 2020-12-23 09:46:37 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.106: INFO: 	Container virt-controller ready: true, restart count 0
Dec 24 03:30:57.106: INFO: virt-handler-scjdh from kubevirt started at 2020-12-17 09:24:03 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.106: INFO: 	Container virt-handler ready: true, restart count 7
Dec 24 03:30:57.106: INFO: virt-operator-867fcd786d-hqk4k from kubevirt started at 2020-12-23 09:46:37 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.106: INFO: 	Container virt-operator ready: true, restart count 0
Dec 24 03:30:57.106: INFO: csi-cephfsplugin-provisioner-d878cdf8b-6g8td from rook-ceph started at 2020-12-18 04:06:36 +0000 UTC (6 container statuses recorded)
Dec 24 03:30:57.106: INFO: 	Container csi-attacher ready: true, restart count 1
Dec 24 03:30:57.106: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 24 03:30:57.106: INFO: 	Container csi-provisioner ready: true, restart count 1
Dec 24 03:30:57.106: INFO: 	Container csi-resizer ready: true, restart count 1
Dec 24 03:30:57.106: INFO: 	Container csi-snapshotter ready: true, restart count 1
Dec 24 03:30:57.106: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 03:30:57.106: INFO: csi-cephfsplugin-xd5rd from rook-ceph started at 2020-12-18 04:06:35 +0000 UTC (3 container statuses recorded)
Dec 24 03:30:57.106: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 24 03:30:57.106: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 24 03:30:57.106: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 03:30:57.106: INFO: csi-rbdplugin-drpwm from rook-ceph started at 2020-12-18 04:06:35 +0000 UTC (3 container statuses recorded)
Dec 24 03:30:57.106: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 24 03:30:57.106: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 24 03:30:57.106: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 03:30:57.106: INFO: csi-rbdplugin-provisioner-7588bc4-dxkjr from rook-ceph started at 2020-12-18 04:06:35 +0000 UTC (6 container statuses recorded)
Dec 24 03:30:57.106: INFO: 	Container csi-attacher ready: true, restart count 1
Dec 24 03:30:57.107: INFO: 	Container csi-provisioner ready: true, restart count 2
Dec 24 03:30:57.107: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 24 03:30:57.107: INFO: 	Container csi-resizer ready: true, restart count 1
Dec 24 03:30:57.107: INFO: 	Container csi-snapshotter ready: true, restart count 1
Dec 24 03:30:57.107: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 03:30:57.107: INFO: rook-ceph-crashcollector-c1-5-7fdd9898c6-mm2r6 from rook-ceph started at 2020-12-18 04:07:17 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.107: INFO: 	Container ceph-crash ready: true, restart count 0
Dec 24 03:30:57.107: INFO: rook-ceph-mon-c-bcf7d945c-mvssc from rook-ceph started at 2020-12-18 04:07:17 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.107: INFO: 	Container mon ready: true, restart count 0
Dec 24 03:30:57.107: INFO: rook-ceph-operator-775d4b6c5f-khccc from rook-ceph started at 2020-12-18 04:06:25 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.107: INFO: 	Container rook-ceph-operator ready: true, restart count 0
Dec 24 03:30:57.107: INFO: rook-ceph-osd-2-5478c54cc5-s282q from rook-ceph started at 2020-12-18 04:07:41 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.107: INFO: 	Container osd ready: true, restart count 0
Dec 24 03:30:57.107: INFO: rook-ceph-osd-prepare-c1-5-l2wd8 from rook-ceph started at 2020-12-18 05:07:16 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.107: INFO: 	Container provision ready: false, restart count 0
Dec 24 03:30:57.107: INFO: rook-ceph-tools-6967fc698d-n2nzp from rook-ceph started at 2020-12-23 09:46:37 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.107: INFO: 	Container rook-ceph-tools ready: true, restart count 0
Dec 24 03:30:57.107: INFO: rook-discover-ds9s8 from rook-ceph started at 2020-12-18 04:06:28 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.107: INFO: 	Container rook-discover ready: true, restart count 0
Dec 24 03:30:57.107: INFO: sonobuoy-systemd-logs-daemon-set-eb313291d8024436-82n6r from sonobuoy started at 2020-12-24 02:17:43 +0000 UTC (2 container statuses recorded)
Dec 24 03:30:57.107: INFO: 	Container sonobuoy-worker ready: false, restart count 7
Dec 24 03:30:57.107: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 24 03:30:57.107: INFO: 
Logging pods the apiserver thinks is on node c1-6 before test
Dec 24 03:30:57.122: INFO: cdi-deployment-6c8975f4b-6bk46 from cdi started at 2020-12-21 09:29:47 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.122: INFO: 	Container cdi-controller ready: true, restart count 1
Dec 24 03:30:57.122: INFO: cdi-operator-5d5654db65-5hdrv from cdi started at 2020-12-21 09:29:21 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.122: INFO: 	Container cdi-operator ready: true, restart count 2
Dec 24 03:30:57.122: INFO: cdi-uploadproxy-5765998fff-qh2cb from cdi started at 2020-12-23 09:46:37 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.122: INFO: 	Container cdi-uploadproxy ready: true, restart count 0
Dec 24 03:30:57.122: INFO: calico-node-5rrck from kube-system started at 2020-12-17 09:20:20 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.122: INFO: 	Container calico-node ready: true, restart count 0
Dec 24 03:30:57.122: INFO: kube-proxy-jmmvs from kube-system started at 2020-12-17 09:19:08 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.122: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 24 03:30:57.122: INFO: virt-api-76b44ffc65-b4xmf from kubevirt started at 2020-12-17 09:23:40 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.122: INFO: 	Container virt-api ready: true, restart count 0
Dec 24 03:30:57.122: INFO: virt-controller-84b5d4b779-krqk6 from kubevirt started at 2020-12-17 09:24:03 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.122: INFO: 	Container virt-controller ready: true, restart count 5
Dec 24 03:30:57.122: INFO: virt-handler-jk5bd from kubevirt started at 2020-12-17 09:24:03 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.122: INFO: 	Container virt-handler ready: true, restart count 0
Dec 24 03:30:57.122: INFO: virt-operator-867fcd786d-9xnf6 from kubevirt started at 2020-12-17 09:23:28 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.122: INFO: 	Container virt-operator ready: true, restart count 2
Dec 24 03:30:57.122: INFO: csi-cephfsplugin-provisioner-d878cdf8b-9v92z from rook-ceph started at 2020-12-23 09:46:37 +0000 UTC (6 container statuses recorded)
Dec 24 03:30:57.122: INFO: 	Container csi-attacher ready: true, restart count 0
Dec 24 03:30:57.122: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 24 03:30:57.122: INFO: 	Container csi-provisioner ready: true, restart count 0
Dec 24 03:30:57.122: INFO: 	Container csi-resizer ready: true, restart count 0
Dec 24 03:30:57.122: INFO: 	Container csi-snapshotter ready: true, restart count 0
Dec 24 03:30:57.122: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 03:30:57.122: INFO: csi-cephfsplugin-q5hm7 from rook-ceph started at 2020-12-18 04:06:35 +0000 UTC (3 container statuses recorded)
Dec 24 03:30:57.122: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Dec 24 03:30:57.122: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 24 03:30:57.122: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 03:30:57.122: INFO: csi-rbdplugin-provisioner-7588bc4-r297x from rook-ceph started at 2020-12-18 04:06:35 +0000 UTC (6 container statuses recorded)
Dec 24 03:30:57.122: INFO: 	Container csi-attacher ready: true, restart count 0
Dec 24 03:30:57.122: INFO: 	Container csi-provisioner ready: true, restart count 0
Dec 24 03:30:57.122: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 24 03:30:57.122: INFO: 	Container csi-resizer ready: true, restart count 0
Dec 24 03:30:57.122: INFO: 	Container csi-snapshotter ready: true, restart count 0
Dec 24 03:30:57.122: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 03:30:57.122: INFO: csi-rbdplugin-xfdqp from rook-ceph started at 2020-12-18 04:06:35 +0000 UTC (3 container statuses recorded)
Dec 24 03:30:57.122: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Dec 24 03:30:57.122: INFO: 	Container driver-registrar ready: true, restart count 0
Dec 24 03:30:57.122: INFO: 	Container liveness-prometheus ready: true, restart count 0
Dec 24 03:30:57.122: INFO: rook-ceph-crashcollector-c1-6-5868d4fd57-6c5gp from rook-ceph started at 2020-12-18 04:08:07 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.122: INFO: 	Container ceph-crash ready: true, restart count 0
Dec 24 03:30:57.122: INFO: rook-ceph-mds-myfs-a-59cfdfc57f-zb7rk from rook-ceph started at 2020-12-23 09:46:37 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.122: INFO: 	Container mds ready: true, restart count 0
Dec 24 03:30:57.122: INFO: rook-ceph-mds-myfs-b-6d859f5b5c-r2rqz from rook-ceph started at 2020-12-18 04:08:07 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.122: INFO: 	Container mds ready: true, restart count 0
Dec 24 03:30:57.122: INFO: rook-ceph-mgr-a-8b957cf9b-9nkkq from rook-ceph started at 2020-12-18 04:07:30 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.123: INFO: 	Container mgr ready: true, restart count 0
Dec 24 03:30:57.123: INFO: rook-ceph-mon-a-77f88c5ff8-hlrmz from rook-ceph started at 2020-12-18 04:06:59 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.123: INFO: 	Container mon ready: true, restart count 0
Dec 24 03:30:57.123: INFO: rook-ceph-osd-0-5897494d6d-k6ml7 from rook-ceph started at 2020-12-18 04:07:39 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.123: INFO: 	Container osd ready: true, restart count 0
Dec 24 03:30:57.123: INFO: rook-ceph-osd-prepare-c1-6-jmhbg from rook-ceph started at 2020-12-18 05:07:18 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.123: INFO: 	Container provision ready: false, restart count 0
Dec 24 03:30:57.123: INFO: rook-discover-4kd85 from rook-ceph started at 2020-12-18 04:06:28 +0000 UTC (1 container statuses recorded)
Dec 24 03:30:57.123: INFO: 	Container rook-discover ready: true, restart count 0
Dec 24 03:30:57.123: INFO: sonobuoy-systemd-logs-daemon-set-eb313291d8024436-79g22 from sonobuoy started at 2020-12-24 02:17:43 +0000 UTC (2 container statuses recorded)
Dec 24 03:30:57.123: INFO: 	Container sonobuoy-worker ready: false, restart count 7
Dec 24 03:30:57.123: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-18decdcc-549d-45bb-8437-0c7737605807 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-18decdcc-549d-45bb-8437-0c7737605807 off the node c1-4
STEP: verifying the node doesn't have the label kubernetes.io/e2e-18decdcc-549d-45bb-8437-0c7737605807
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:36:01.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2942" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:304.277 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":305,"completed":249,"skipped":3855,"failed":0}
S
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:36:01.284: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 03:36:01.355: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:36:03.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9155" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":305,"completed":250,"skipped":3856,"failed":0}
SS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:36:03.437: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 24 03:36:06.502: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:36:06.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3433" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":305,"completed":251,"skipped":3858,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:36:06.542: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:36:06.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-9942" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":305,"completed":252,"skipped":3870,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:36:06.652: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:36:06.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-63" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":305,"completed":253,"skipped":3890,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:36:06.698: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod with failed condition
STEP: updating the pod
Dec 24 03:38:07.320: INFO: Successfully updated pod "var-expansion-56e93546-6a99-44af-95e4-4701d38d0613"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Dec 24 03:38:09.329: INFO: Deleting pod "var-expansion-56e93546-6a99-44af-95e4-4701d38d0613" in namespace "var-expansion-2737"
Dec 24 03:38:09.334: INFO: Wait up to 5m0s for pod "var-expansion-56e93546-6a99-44af-95e4-4701d38d0613" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:38:43.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2737" for this suite.

• [SLOW TEST:156.659 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":305,"completed":254,"skipped":3912,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:38:43.357: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-01c47edd-b9be-475b-bbf7-c7de6ac9a4dd
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:38:45.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7915" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":255,"skipped":3916,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:38:45.472: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-6309
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 24 03:38:45.553: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 24 03:38:45.609: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 24 03:38:47.612: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 24 03:38:49.613: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 24 03:38:51.612: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 24 03:38:53.613: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 24 03:38:55.613: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 24 03:38:57.612: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec 24 03:38:57.615: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec 24 03:38:59.618: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec 24 03:39:01.618: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec 24 03:39:03.618: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec 24 03:39:05.618: INFO: The status of Pod netserver-1 is Running (Ready = true)
Dec 24 03:39:05.622: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Dec 24 03:39:07.663: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.113.94:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6309 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 24 03:39:07.663: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
Dec 24 03:39:07.764: INFO: Found all expected endpoints: [netserver-0]
Dec 24 03:39:07.766: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.26.71:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6309 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 24 03:39:07.766: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
Dec 24 03:39:07.851: INFO: Found all expected endpoints: [netserver-1]
Dec 24 03:39:07.853: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.92.228:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6309 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 24 03:39:07.853: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
Dec 24 03:39:07.943: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:39:07.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6309" for this suite.

• [SLOW TEST:22.479 seconds]
[sig-network] Networking
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":256,"skipped":3968,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:39:07.951: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 24 03:39:07.990: INFO: Waiting up to 5m0s for pod "pod-d18010ea-d1e1-4993-a50e-da9aad9a917d" in namespace "emptydir-2942" to be "Succeeded or Failed"
Dec 24 03:39:07.992: INFO: Pod "pod-d18010ea-d1e1-4993-a50e-da9aad9a917d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.693496ms
Dec 24 03:39:09.994: INFO: Pod "pod-d18010ea-d1e1-4993-a50e-da9aad9a917d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004090574s
STEP: Saw pod success
Dec 24 03:39:09.994: INFO: Pod "pod-d18010ea-d1e1-4993-a50e-da9aad9a917d" satisfied condition "Succeeded or Failed"
Dec 24 03:39:09.996: INFO: Trying to get logs from node c1-4 pod pod-d18010ea-d1e1-4993-a50e-da9aad9a917d container test-container: <nil>
STEP: delete the pod
Dec 24 03:39:10.024: INFO: Waiting for pod pod-d18010ea-d1e1-4993-a50e-da9aad9a917d to disappear
Dec 24 03:39:10.032: INFO: Pod pod-d18010ea-d1e1-4993-a50e-da9aad9a917d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:39:10.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2942" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":257,"skipped":3969,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:39:10.060: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:39:27.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4231" for this suite.

• [SLOW TEST:17.116 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":305,"completed":258,"skipped":3992,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:39:27.176: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W1224 03:39:28.355158      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Dec 24 03:40:30.375: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:40:30.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3072" for this suite.

• [SLOW TEST:63.207 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":305,"completed":259,"skipped":4061,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:40:30.384: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:40:46.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7606" for this suite.

• [SLOW TEST:16.132 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":305,"completed":260,"skipped":4096,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:40:46.516: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-671ad8cb-bb48-4292-9c52-2b0f7af7bbc4
STEP: Creating a pod to test consume secrets
Dec 24 03:40:46.574: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b8088e72-46f1-4641-a7ad-3a004616dc47" in namespace "projected-4506" to be "Succeeded or Failed"
Dec 24 03:40:46.591: INFO: Pod "pod-projected-secrets-b8088e72-46f1-4641-a7ad-3a004616dc47": Phase="Pending", Reason="", readiness=false. Elapsed: 16.939781ms
Dec 24 03:40:48.594: INFO: Pod "pod-projected-secrets-b8088e72-46f1-4641-a7ad-3a004616dc47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019692675s
STEP: Saw pod success
Dec 24 03:40:48.594: INFO: Pod "pod-projected-secrets-b8088e72-46f1-4641-a7ad-3a004616dc47" satisfied condition "Succeeded or Failed"
Dec 24 03:40:48.596: INFO: Trying to get logs from node c1-4 pod pod-projected-secrets-b8088e72-46f1-4641-a7ad-3a004616dc47 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 24 03:40:48.657: INFO: Waiting for pod pod-projected-secrets-b8088e72-46f1-4641-a7ad-3a004616dc47 to disappear
Dec 24 03:40:48.664: INFO: Pod pod-projected-secrets-b8088e72-46f1-4641-a7ad-3a004616dc47 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:40:48.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4506" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":261,"skipped":4101,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:40:48.671: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-jwlr
STEP: Creating a pod to test atomic-volume-subpath
Dec 24 03:40:48.748: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-jwlr" in namespace "subpath-4842" to be "Succeeded or Failed"
Dec 24 03:40:48.750: INFO: Pod "pod-subpath-test-configmap-jwlr": Phase="Pending", Reason="", readiness=false. Elapsed: 1.7189ms
Dec 24 03:40:50.752: INFO: Pod "pod-subpath-test-configmap-jwlr": Phase="Running", Reason="", readiness=true. Elapsed: 2.004092862s
Dec 24 03:40:52.764: INFO: Pod "pod-subpath-test-configmap-jwlr": Phase="Running", Reason="", readiness=true. Elapsed: 4.01615947s
Dec 24 03:40:54.767: INFO: Pod "pod-subpath-test-configmap-jwlr": Phase="Running", Reason="", readiness=true. Elapsed: 6.018710998s
Dec 24 03:40:56.769: INFO: Pod "pod-subpath-test-configmap-jwlr": Phase="Running", Reason="", readiness=true. Elapsed: 8.021328608s
Dec 24 03:40:58.803: INFO: Pod "pod-subpath-test-configmap-jwlr": Phase="Running", Reason="", readiness=true. Elapsed: 10.054632749s
Dec 24 03:41:00.806: INFO: Pod "pod-subpath-test-configmap-jwlr": Phase="Running", Reason="", readiness=true. Elapsed: 12.057380943s
Dec 24 03:41:02.808: INFO: Pod "pod-subpath-test-configmap-jwlr": Phase="Running", Reason="", readiness=true. Elapsed: 14.060118247s
Dec 24 03:41:04.811: INFO: Pod "pod-subpath-test-configmap-jwlr": Phase="Running", Reason="", readiness=true. Elapsed: 16.0627866s
Dec 24 03:41:06.814: INFO: Pod "pod-subpath-test-configmap-jwlr": Phase="Running", Reason="", readiness=true. Elapsed: 18.06578742s
Dec 24 03:41:08.817: INFO: Pod "pod-subpath-test-configmap-jwlr": Phase="Running", Reason="", readiness=true. Elapsed: 20.069091888s
Dec 24 03:41:10.827: INFO: Pod "pod-subpath-test-configmap-jwlr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.0786555s
STEP: Saw pod success
Dec 24 03:41:10.827: INFO: Pod "pod-subpath-test-configmap-jwlr" satisfied condition "Succeeded or Failed"
Dec 24 03:41:10.829: INFO: Trying to get logs from node c1-4 pod pod-subpath-test-configmap-jwlr container test-container-subpath-configmap-jwlr: <nil>
STEP: delete the pod
Dec 24 03:41:10.856: INFO: Waiting for pod pod-subpath-test-configmap-jwlr to disappear
Dec 24 03:41:10.859: INFO: Pod pod-subpath-test-configmap-jwlr no longer exists
STEP: Deleting pod pod-subpath-test-configmap-jwlr
Dec 24 03:41:10.859: INFO: Deleting pod "pod-subpath-test-configmap-jwlr" in namespace "subpath-4842"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:41:10.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4842" for this suite.

• [SLOW TEST:22.198 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":305,"completed":262,"skipped":4120,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:41:10.870: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 03:41:10.947: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Dec 24 03:41:15.949: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 24 03:41:15.949: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Dec 24 03:41:15.979: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-550 /apis/apps/v1/namespaces/deployment-550/deployments/test-cleanup-deployment 26104ce9-2d5d-4694-84fd-a7a969d88884 3475426 1 2020-12-24 03:41:15 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2020-12-24 03:41:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008894918 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Dec 24 03:41:16.003: INFO: New ReplicaSet "test-cleanup-deployment-5d446bdd47" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5d446bdd47  deployment-550 /apis/apps/v1/namespaces/deployment-550/replicasets/test-cleanup-deployment-5d446bdd47 72bf238b-ef02-4853-8ec6-7f761e10917c 3475428 1 2020-12-24 03:41:15 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 26104ce9-2d5d-4694-84fd-a7a969d88884 0xc008894f27 0xc008894f28}] []  [{kube-controller-manager Update apps/v1 2020-12-24 03:41:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"26104ce9-2d5d-4694-84fd-a7a969d88884\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5d446bdd47,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008894fb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 24 03:41:16.003: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Dec 24 03:41:16.004: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-550 /apis/apps/v1/namespaces/deployment-550/replicasets/test-cleanup-controller 58f71cdf-d00a-4e2e-a138-f408853b592f 3475427 1 2020-12-24 03:41:10 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 26104ce9-2d5d-4694-84fd-a7a969d88884 0xc008894e17 0xc008894e18}] []  [{e2e.test Update apps/v1 2020-12-24 03:41:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-12-24 03:41:15 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"26104ce9-2d5d-4694-84fd-a7a969d88884\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc008894eb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 24 03:41:16.026: INFO: Pod "test-cleanup-controller-7k4bp" is available:
&Pod{ObjectMeta:{test-cleanup-controller-7k4bp test-cleanup-controller- deployment-550 /api/v1/namespaces/deployment-550/pods/test-cleanup-controller-7k4bp 340e09ca-0b97-45ac-9cac-d5bc450feaf3 3475394 0 2020-12-24 03:41:10 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:10.244.113.105/32 cni.projectcalico.org/podIPs:10.244.113.105/32] [{apps/v1 ReplicaSet test-cleanup-controller 58f71cdf-d00a-4e2e-a138-f408853b592f 0xc008895497 0xc008895498}] []  [{kube-controller-manager Update v1 2020-12-24 03:41:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58f71cdf-d00a-4e2e-a138-f408853b592f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-24 03:41:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-24 03:41:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.113.105\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-x27ms,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-x27ms,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-x27ms,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:41:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:41:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:41:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:41:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.3.5,PodIP:10.244.113.105,StartTime:2020-12-24 03:41:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-24 03:41:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://d68723f25af730db7d9adaf6031fbd5583e93420ce421830bef00fdc82bf9e0b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.113.105,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 24 03:41:16.027: INFO: Pod "test-cleanup-deployment-5d446bdd47-d4vzq" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-5d446bdd47-d4vzq test-cleanup-deployment-5d446bdd47- deployment-550 /api/v1/namespaces/deployment-550/pods/test-cleanup-deployment-5d446bdd47-d4vzq f8984011-b6e7-4bcf-8357-80658dbe9007 3475434 0 2020-12-24 03:41:15 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-5d446bdd47 72bf238b-ef02-4853-8ec6-7f761e10917c 0xc008895667 0xc008895668}] []  [{kube-controller-manager Update v1 2020-12-24 03:41:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72bf238b-ef02-4853-8ec6-7f761e10917c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-x27ms,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-x27ms,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-x27ms,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:41:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:41:16.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-550" for this suite.

• [SLOW TEST:5.184 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":305,"completed":263,"skipped":4127,"failed":0}
S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:41:16.055: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:41:16.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3583" for this suite.
STEP: Destroying namespace "nspatchtest-dcd6b146-ddd0-4f58-ac47-30296975efb1-34" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":305,"completed":264,"skipped":4128,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:41:16.215: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-6730
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 24 03:41:16.256: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 24 03:41:16.330: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 24 03:41:18.333: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 24 03:41:20.334: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 24 03:41:22.341: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 24 03:41:24.333: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 24 03:41:26.333: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 24 03:41:28.333: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 24 03:41:30.333: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 24 03:41:32.340: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 24 03:41:34.333: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 24 03:41:36.333: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 24 03:41:38.333: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec 24 03:41:38.338: INFO: The status of Pod netserver-1 is Running (Ready = true)
Dec 24 03:41:38.342: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Dec 24 03:41:40.366: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.113.99:8080/dial?request=hostname&protocol=udp&host=10.244.113.102&port=8081&tries=1'] Namespace:pod-network-test-6730 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 24 03:41:40.366: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
Dec 24 03:41:40.458: INFO: Waiting for responses: map[]
Dec 24 03:41:40.460: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.113.99:8080/dial?request=hostname&protocol=udp&host=10.244.26.73&port=8081&tries=1'] Namespace:pod-network-test-6730 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 24 03:41:40.460: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
Dec 24 03:41:40.563: INFO: Waiting for responses: map[]
Dec 24 03:41:40.565: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.113.99:8080/dial?request=hostname&protocol=udp&host=10.244.92.214&port=8081&tries=1'] Namespace:pod-network-test-6730 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 24 03:41:40.565: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
Dec 24 03:41:40.646: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:41:40.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6730" for this suite.

• [SLOW TEST:24.439 seconds]
[sig-network] Networking
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":305,"completed":265,"skipped":4168,"failed":0}
SSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:41:40.655: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-1c6089b2-e702-40ac-8034-c3fd4238d008 in namespace container-probe-2966
Dec 24 03:41:42.721: INFO: Started pod liveness-1c6089b2-e702-40ac-8034-c3fd4238d008 in namespace container-probe-2966
STEP: checking the pod's current state and verifying that restartCount is present
Dec 24 03:41:42.723: INFO: Initial restart count of pod liveness-1c6089b2-e702-40ac-8034-c3fd4238d008 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:45:43.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2966" for this suite.

• [SLOW TEST:242.732 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":305,"completed":266,"skipped":4173,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:45:43.387: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with configMap that has name projected-configmap-test-upd-5793ce96-33d3-4742-9878-329a7fe703f2
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-5793ce96-33d3-4742-9878-329a7fe703f2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:46:59.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9782" for this suite.

• [SLOW TEST:76.335 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":267,"skipped":4186,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:46:59.722: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:47:59.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7321" for this suite.

• [SLOW TEST:60.079 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":305,"completed":268,"skipped":4211,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:47:59.801: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:47:59.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5344" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":269,"skipped":4229,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:47:59.916: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-57cc454b-9dc5-45e5-8e69-41180cd3fcb5 in namespace container-probe-5338
Dec 24 03:48:01.992: INFO: Started pod liveness-57cc454b-9dc5-45e5-8e69-41180cd3fcb5 in namespace container-probe-5338
STEP: checking the pod's current state and verifying that restartCount is present
Dec 24 03:48:01.994: INFO: Initial restart count of pod liveness-57cc454b-9dc5-45e5-8e69-41180cd3fcb5 is 0
Dec 24 03:48:18.037: INFO: Restart count of pod container-probe-5338/liveness-57cc454b-9dc5-45e5-8e69-41180cd3fcb5 is now 1 (16.043086087s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:48:18.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5338" for this suite.

• [SLOW TEST:18.154 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":270,"skipped":4248,"failed":0}
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:48:18.071: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
STEP: reading a file in the container
Dec 24 03:48:20.619: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3549 pod-service-account-300a21ae-4318-4378-ad7f-7e1b0da41884 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Dec 24 03:48:22.054: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3549 pod-service-account-300a21ae-4318-4378-ad7f-7e1b0da41884 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Dec 24 03:48:22.236: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3549 pod-service-account-300a21ae-4318-4378-ad7f-7e1b0da41884 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:48:22.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3549" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":305,"completed":271,"skipped":4256,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:48:22.415: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl replace
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1581
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 24 03:48:22.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-8881'
Dec 24 03:48:22.559: INFO: stderr: ""
Dec 24 03:48:22.559: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Dec 24 03:48:27.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 get pod e2e-test-httpd-pod --namespace=kubectl-8881 -o json'
Dec 24 03:48:27.719: INFO: stderr: ""
Dec 24 03:48:27.719: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.244.113.70/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.244.113.70/32\"\n        },\n        \"creationTimestamp\": \"2020-12-24T03:48:22Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-12-24T03:48:22Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \".\": {},\n                            \"f:cni.projectcalico.org/podIP\": {},\n                            \"f:cni.projectcalico.org/podIPs\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-12-24T03:48:23Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"10.244.113.70\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-12-24T03:48:23Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8881\",\n        \"resourceVersion\": \"3478462\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-8881/pods/e2e-test-httpd-pod\",\n        \"uid\": \"453e8b4d-3dcf-4c17-bf6a-1ee70467d7f2\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-wwp2t\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"c1-4\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-wwp2t\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-wwp2t\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-12-24T03:48:22Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-12-24T03:48:23Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-12-24T03:48:23Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-12-24T03:48:22Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://58c9488e59add240018d74d32a4f35d43d2ea0b7dad0ac85bbf17196aa54f8f2\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-12-24T03:48:23Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.21.3.5\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.113.70\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.113.70\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-12-24T03:48:22Z\"\n    }\n}\n"
STEP: replace the image in the pod
Dec 24 03:48:27.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 replace -f - --namespace=kubectl-8881'
Dec 24 03:48:28.183: INFO: stderr: ""
Dec 24 03:48:28.183: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1586
Dec 24 03:48:28.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 delete pods e2e-test-httpd-pod --namespace=kubectl-8881'
Dec 24 03:48:33.239: INFO: stderr: ""
Dec 24 03:48:33.239: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:48:33.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8881" for this suite.

• [SLOW TEST:10.832 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1577
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":305,"completed":272,"skipped":4284,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:48:33.247: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 24 03:48:33.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-8013'
Dec 24 03:48:33.417: INFO: stderr: ""
Dec 24 03:48:33.417: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1550
Dec 24 03:48:33.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 delete pods e2e-test-httpd-pod --namespace=kubectl-8013'
Dec 24 03:48:35.807: INFO: stderr: ""
Dec 24 03:48:35.807: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:48:35.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8013" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":305,"completed":273,"skipped":4320,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:48:35.814: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-835203e5-e44c-4aff-a3db-e58b9a40ae1c
STEP: Creating configMap with name cm-test-opt-upd-1adf99db-629d-4594-8647-253fcedf20bb
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-835203e5-e44c-4aff-a3db-e58b9a40ae1c
STEP: Updating configmap cm-test-opt-upd-1adf99db-629d-4594-8647-253fcedf20bb
STEP: Creating configMap with name cm-test-opt-create-eb1aefcc-2545-4e07-9d62-e6a6116f749f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:48:39.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2972" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":274,"skipped":4331,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:48:39.976: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 24 03:48:42.562: INFO: Successfully updated pod "pod-update-activedeadlineseconds-36682ab5-dd61-49bc-9c42-6f0a71aa3c1f"
Dec 24 03:48:42.562: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-36682ab5-dd61-49bc-9c42-6f0a71aa3c1f" in namespace "pods-4139" to be "terminated due to deadline exceeded"
Dec 24 03:48:42.579: INFO: Pod "pod-update-activedeadlineseconds-36682ab5-dd61-49bc-9c42-6f0a71aa3c1f": Phase="Running", Reason="", readiness=true. Elapsed: 16.853663ms
Dec 24 03:48:44.597: INFO: Pod "pod-update-activedeadlineseconds-36682ab5-dd61-49bc-9c42-6f0a71aa3c1f": Phase="Running", Reason="", readiness=true. Elapsed: 2.034959741s
Dec 24 03:48:46.602: INFO: Pod "pod-update-activedeadlineseconds-36682ab5-dd61-49bc-9c42-6f0a71aa3c1f": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.039715537s
Dec 24 03:48:46.602: INFO: Pod "pod-update-activedeadlineseconds-36682ab5-dd61-49bc-9c42-6f0a71aa3c1f" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:48:46.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4139" for this suite.

• [SLOW TEST:6.634 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":305,"completed":275,"skipped":4344,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:48:46.610: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-e9f71802-9c17-4651-898e-bb81ebd1be41
STEP: Creating a pod to test consume configMaps
Dec 24 03:48:46.678: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-07355fc1-ecbc-436a-9158-a0f3927b271a" in namespace "projected-7691" to be "Succeeded or Failed"
Dec 24 03:48:46.704: INFO: Pod "pod-projected-configmaps-07355fc1-ecbc-436a-9158-a0f3927b271a": Phase="Pending", Reason="", readiness=false. Elapsed: 26.640933ms
Dec 24 03:48:48.707: INFO: Pod "pod-projected-configmaps-07355fc1-ecbc-436a-9158-a0f3927b271a": Phase="Running", Reason="", readiness=true. Elapsed: 2.029595039s
Dec 24 03:48:50.710: INFO: Pod "pod-projected-configmaps-07355fc1-ecbc-436a-9158-a0f3927b271a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032450457s
STEP: Saw pod success
Dec 24 03:48:50.710: INFO: Pod "pod-projected-configmaps-07355fc1-ecbc-436a-9158-a0f3927b271a" satisfied condition "Succeeded or Failed"
Dec 24 03:48:50.713: INFO: Trying to get logs from node c1-4 pod pod-projected-configmaps-07355fc1-ecbc-436a-9158-a0f3927b271a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 24 03:48:50.734: INFO: Waiting for pod pod-projected-configmaps-07355fc1-ecbc-436a-9158-a0f3927b271a to disappear
Dec 24 03:48:50.739: INFO: Pod pod-projected-configmaps-07355fc1-ecbc-436a-9158-a0f3927b271a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:48:50.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7691" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":276,"skipped":4355,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:48:50.747: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override arguments
Dec 24 03:48:50.792: INFO: Waiting up to 5m0s for pod "client-containers-b2796cb8-b5cf-44ab-898f-eea4041bec61" in namespace "containers-855" to be "Succeeded or Failed"
Dec 24 03:48:50.837: INFO: Pod "client-containers-b2796cb8-b5cf-44ab-898f-eea4041bec61": Phase="Pending", Reason="", readiness=false. Elapsed: 44.979204ms
Dec 24 03:48:52.840: INFO: Pod "client-containers-b2796cb8-b5cf-44ab-898f-eea4041bec61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.047741552s
STEP: Saw pod success
Dec 24 03:48:52.840: INFO: Pod "client-containers-b2796cb8-b5cf-44ab-898f-eea4041bec61" satisfied condition "Succeeded or Failed"
Dec 24 03:48:52.842: INFO: Trying to get logs from node c1-4 pod client-containers-b2796cb8-b5cf-44ab-898f-eea4041bec61 container test-container: <nil>
STEP: delete the pod
Dec 24 03:48:52.866: INFO: Waiting for pod client-containers-b2796cb8-b5cf-44ab-898f-eea4041bec61 to disappear
Dec 24 03:48:52.871: INFO: Pod client-containers-b2796cb8-b5cf-44ab-898f-eea4041bec61 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:48:52.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-855" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":305,"completed":277,"skipped":4376,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:48:52.878: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4144.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4144.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 24 03:48:54.986: INFO: DNS probes using dns-4144/dns-test-cc6614b1-2e17-45b9-ae52-d480d60a3628 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:48:55.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4144" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":305,"completed":278,"skipped":4377,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:48:55.021: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 03:48:55.111: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"2a0cbd95-d858-4ac7-a1c3-7802cb1c6564", Controller:(*bool)(0xc007305ae2), BlockOwnerDeletion:(*bool)(0xc007305ae3)}}
Dec 24 03:48:55.122: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"2ac4e619-7863-44e3-8018-30be0fa8114e", Controller:(*bool)(0xc007305cea), BlockOwnerDeletion:(*bool)(0xc007305ceb)}}
Dec 24 03:48:55.134: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"fe31a2b7-f2b1-4c69-a0a3-a60d568424f3", Controller:(*bool)(0xc007305efa), BlockOwnerDeletion:(*bool)(0xc007305efb)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:49:00.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5196" for this suite.

• [SLOW TEST:5.166 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":305,"completed":279,"skipped":4382,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:49:00.188: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:49:02.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2671" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":280,"skipped":4420,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:49:02.318: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Dec 24 03:49:04.910: INFO: Successfully updated pod "labelsupdatec7ebf02e-50ba-419a-b460-bca4501ac8db"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:49:06.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9710" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":281,"skipped":4444,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:49:06.939: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 24 03:49:07.432: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 24 03:49:09.440: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744378547, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744378547, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744378547, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744378547, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 24 03:49:12.461: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 03:49:12.463: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8289-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:49:13.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9992" for this suite.
STEP: Destroying namespace "webhook-9992-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.758 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":305,"completed":282,"skipped":4470,"failed":0}
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:49:13.698: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-628
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating statefulset ss in namespace statefulset-628
Dec 24 03:49:13.752: INFO: Found 0 stateful pods, waiting for 1
Dec 24 03:49:23.755: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Dec 24 03:49:23.772: INFO: Deleting all statefulset in ns statefulset-628
Dec 24 03:49:23.788: INFO: Scaling statefulset ss to 0
Dec 24 03:49:33.825: INFO: Waiting for statefulset status.replicas updated to 0
Dec 24 03:49:33.828: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:49:33.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-628" for this suite.

• [SLOW TEST:20.161 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":305,"completed":283,"skipped":4470,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:49:33.859: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-77b341e5-d715-4b14-b12a-668e5001fead
STEP: Creating a pod to test consume secrets
Dec 24 03:49:33.982: INFO: Waiting up to 5m0s for pod "pod-secrets-40eaa70a-78be-469b-b87d-0949dcfeeaee" in namespace "secrets-4988" to be "Succeeded or Failed"
Dec 24 03:49:33.990: INFO: Pod "pod-secrets-40eaa70a-78be-469b-b87d-0949dcfeeaee": Phase="Pending", Reason="", readiness=false. Elapsed: 7.521941ms
Dec 24 03:49:35.993: INFO: Pod "pod-secrets-40eaa70a-78be-469b-b87d-0949dcfeeaee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010866435s
Dec 24 03:49:37.996: INFO: Pod "pod-secrets-40eaa70a-78be-469b-b87d-0949dcfeeaee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013474907s
STEP: Saw pod success
Dec 24 03:49:37.996: INFO: Pod "pod-secrets-40eaa70a-78be-469b-b87d-0949dcfeeaee" satisfied condition "Succeeded or Failed"
Dec 24 03:49:37.998: INFO: Trying to get logs from node c1-4 pod pod-secrets-40eaa70a-78be-469b-b87d-0949dcfeeaee container secret-volume-test: <nil>
STEP: delete the pod
Dec 24 03:49:38.018: INFO: Waiting for pod pod-secrets-40eaa70a-78be-469b-b87d-0949dcfeeaee to disappear
Dec 24 03:49:38.025: INFO: Pod pod-secrets-40eaa70a-78be-469b-b87d-0949dcfeeaee no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:49:38.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4988" for this suite.
STEP: Destroying namespace "secret-namespace-1947" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":305,"completed":284,"skipped":4529,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:49:38.055: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating api versions
Dec 24 03:49:38.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 api-versions'
Dec 24 03:49:38.291: INFO: stderr: ""
Dec 24 03:49:38.291: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncdi.kubevirt.io/v1alpha1\ncdi.kubevirt.io/v1beta1\nceph.rook.io/v1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nkubevirt.io/v1alpha3\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\nobjectbucket.io/v1alpha1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nrook.io/v1alpha2\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsnapshot.kubevirt.io/v1alpha1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nsubresources.kubevirt.io/v1alpha3\nupload.cdi.kubevirt.io/v1alpha1\nupload.cdi.kubevirt.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:49:38.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6988" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":305,"completed":285,"skipped":4533,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:49:38.299: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Dec 24 03:49:38.363: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
Dec 24 03:49:43.435: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:50:01.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-569" for this suite.

• [SLOW TEST:23.148 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":305,"completed":286,"skipped":4557,"failed":0}
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:50:01.447: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-3888
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating stateful set ss in namespace statefulset-3888
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3888
Dec 24 03:50:01.537: INFO: Found 0 stateful pods, waiting for 1
Dec 24 03:50:11.540: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Dec 24 03:50:11.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 24 03:50:11.763: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 24 03:50:11.763: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 24 03:50:11.763: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 24 03:50:11.766: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 24 03:50:21.769: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 24 03:50:21.769: INFO: Waiting for statefulset status.replicas updated to 0
Dec 24 03:50:21.793: INFO: POD   NODE  PHASE    GRACE  CONDITIONS
Dec 24 03:50:21.793: INFO: ss-0  c1-4  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:01 +0000 UTC  }]
Dec 24 03:50:21.793: INFO: 
Dec 24 03:50:21.793: INFO: StatefulSet ss has not reached scale 3, at 1
Dec 24 03:50:22.796: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996255561s
Dec 24 03:50:23.801: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992644106s
Dec 24 03:50:24.805: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987796366s
Dec 24 03:50:25.809: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984016558s
Dec 24 03:50:26.812: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.980296635s
Dec 24 03:50:27.819: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.976706718s
Dec 24 03:50:28.822: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.970377638s
Dec 24 03:50:29.826: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.966730884s
Dec 24 03:50:30.834: INFO: Verifying statefulset ss doesn't scale past 3 for another 963.085417ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3888
Dec 24 03:50:31.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:50:32.044: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 24 03:50:32.044: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 24 03:50:32.044: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 24 03:50:32.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:50:32.243: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 24 03:50:32.243: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 24 03:50:32.243: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 24 03:50:32.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:50:32.443: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 24 03:50:32.443: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 24 03:50:32.443: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 24 03:50:32.446: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Dec 24 03:50:42.450: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 24 03:50:42.450: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 24 03:50:42.450: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Dec 24 03:50:42.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 24 03:50:42.655: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 24 03:50:42.655: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 24 03:50:42.655: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 24 03:50:42.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 24 03:50:42.882: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 24 03:50:42.882: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 24 03:50:42.882: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 24 03:50:42.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 24 03:50:43.106: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 24 03:50:43.106: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 24 03:50:43.106: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 24 03:50:43.106: INFO: Waiting for statefulset status.replicas updated to 0
Dec 24 03:50:43.109: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Dec 24 03:50:53.122: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 24 03:50:53.122: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 24 03:50:53.122: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 24 03:50:53.145: INFO: POD   NODE  PHASE    GRACE  CONDITIONS
Dec 24 03:50:53.145: INFO: ss-0  c1-4  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:01 +0000 UTC  }]
Dec 24 03:50:53.145: INFO: ss-1  c1-5  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:21 +0000 UTC  }]
Dec 24 03:50:53.145: INFO: ss-2  c1-6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:21 +0000 UTC  }]
Dec 24 03:50:53.145: INFO: 
Dec 24 03:50:53.145: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 24 03:50:54.149: INFO: POD   NODE  PHASE    GRACE  CONDITIONS
Dec 24 03:50:54.149: INFO: ss-0  c1-4  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:01 +0000 UTC  }]
Dec 24 03:50:54.149: INFO: ss-1  c1-5  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:21 +0000 UTC  }]
Dec 24 03:50:54.149: INFO: ss-2  c1-6  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:21 +0000 UTC  }]
Dec 24 03:50:54.149: INFO: 
Dec 24 03:50:54.149: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 24 03:50:55.153: INFO: POD   NODE  PHASE    GRACE  CONDITIONS
Dec 24 03:50:55.153: INFO: ss-0  c1-4  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:01 +0000 UTC  }]
Dec 24 03:50:55.153: INFO: ss-1  c1-5  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:21 +0000 UTC  }]
Dec 24 03:50:55.153: INFO: ss-2  c1-6  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:21 +0000 UTC  }]
Dec 24 03:50:55.153: INFO: 
Dec 24 03:50:55.153: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 24 03:50:56.157: INFO: POD   NODE  PHASE    GRACE  CONDITIONS
Dec 24 03:50:56.157: INFO: ss-0  c1-4  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:01 +0000 UTC  }]
Dec 24 03:50:56.157: INFO: ss-2  c1-6  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:21 +0000 UTC  }]
Dec 24 03:50:56.157: INFO: 
Dec 24 03:50:56.157: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 24 03:50:57.160: INFO: POD   NODE  PHASE    GRACE  CONDITIONS
Dec 24 03:50:57.160: INFO: ss-0  c1-4  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:01 +0000 UTC  }]
Dec 24 03:50:57.160: INFO: ss-2  c1-6  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:21 +0000 UTC  }]
Dec 24 03:50:57.160: INFO: 
Dec 24 03:50:57.160: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 24 03:50:58.163: INFO: POD   NODE  PHASE    GRACE  CONDITIONS
Dec 24 03:50:58.163: INFO: ss-0  c1-4  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:01 +0000 UTC  }]
Dec 24 03:50:58.164: INFO: ss-2  c1-6  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:21 +0000 UTC  }]
Dec 24 03:50:58.164: INFO: 
Dec 24 03:50:58.164: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 24 03:50:59.167: INFO: POD   NODE  PHASE    GRACE  CONDITIONS
Dec 24 03:50:59.167: INFO: ss-0  c1-4  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:01 +0000 UTC  }]
Dec 24 03:50:59.167: INFO: ss-2  c1-6  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:21 +0000 UTC  }]
Dec 24 03:50:59.167: INFO: 
Dec 24 03:50:59.167: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 24 03:51:00.170: INFO: POD   NODE  PHASE    GRACE  CONDITIONS
Dec 24 03:51:00.170: INFO: ss-0  c1-4  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:01 +0000 UTC  }]
Dec 24 03:51:00.170: INFO: 
Dec 24 03:51:00.170: INFO: StatefulSet ss has not reached scale 0, at 1
Dec 24 03:51:01.173: INFO: POD   NODE  PHASE    GRACE  CONDITIONS
Dec 24 03:51:01.173: INFO: ss-0  c1-4  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:01 +0000 UTC  }]
Dec 24 03:51:01.173: INFO: 
Dec 24 03:51:01.173: INFO: StatefulSet ss has not reached scale 0, at 1
Dec 24 03:51:02.177: INFO: POD   NODE  PHASE    GRACE  CONDITIONS
Dec 24 03:51:02.177: INFO: ss-0  c1-4  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-24 03:50:01 +0000 UTC  }]
Dec 24 03:51:02.177: INFO: 
Dec 24 03:51:02.177: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3888
Dec 24 03:51:03.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:51:03.293: INFO: rc: 1
Dec 24 03:51:03.293: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 24 03:51:13.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:51:13.404: INFO: rc: 1
Dec 24 03:51:13.404: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 24 03:51:23.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:51:23.511: INFO: rc: 1
Dec 24 03:51:23.511: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 24 03:51:33.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:51:33.627: INFO: rc: 1
Dec 24 03:51:33.627: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 24 03:51:43.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:51:43.745: INFO: rc: 1
Dec 24 03:51:43.746: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 24 03:51:53.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:51:53.861: INFO: rc: 1
Dec 24 03:51:53.861: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 24 03:52:03.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:52:03.973: INFO: rc: 1
Dec 24 03:52:03.973: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 24 03:52:13.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:52:14.082: INFO: rc: 1
Dec 24 03:52:14.082: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 24 03:52:24.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:52:24.181: INFO: rc: 1
Dec 24 03:52:24.181: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 24 03:52:34.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:52:34.299: INFO: rc: 1
Dec 24 03:52:34.299: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 24 03:52:44.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:52:44.411: INFO: rc: 1
Dec 24 03:52:44.411: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 24 03:52:54.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:52:54.522: INFO: rc: 1
Dec 24 03:52:54.522: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 24 03:53:04.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:53:04.633: INFO: rc: 1
Dec 24 03:53:04.633: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 24 03:53:14.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:53:14.748: INFO: rc: 1
Dec 24 03:53:14.748: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 24 03:53:24.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:53:24.863: INFO: rc: 1
Dec 24 03:53:24.863: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 24 03:53:34.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:53:34.971: INFO: rc: 1
Dec 24 03:53:34.971: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 24 03:53:44.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:53:45.091: INFO: rc: 1
Dec 24 03:53:45.091: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 24 03:53:55.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:53:55.206: INFO: rc: 1
Dec 24 03:53:55.206: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 24 03:54:05.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:54:05.317: INFO: rc: 1
Dec 24 03:54:05.317: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 24 03:54:15.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:54:15.428: INFO: rc: 1
Dec 24 03:54:15.428: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 24 03:54:25.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:54:25.538: INFO: rc: 1
Dec 24 03:54:25.538: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 24 03:54:35.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:54:35.651: INFO: rc: 1
Dec 24 03:54:35.651: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 24 03:54:45.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:54:45.753: INFO: rc: 1
Dec 24 03:54:45.753: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 24 03:54:55.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:54:55.864: INFO: rc: 1
Dec 24 03:54:55.864: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 24 03:55:05.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:55:05.979: INFO: rc: 1
Dec 24 03:55:05.979: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 24 03:55:15.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:55:16.089: INFO: rc: 1
Dec 24 03:55:16.089: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 24 03:55:26.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:55:26.203: INFO: rc: 1
Dec 24 03:55:26.203: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 24 03:55:36.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:55:36.312: INFO: rc: 1
Dec 24 03:55:36.312: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 24 03:55:46.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:55:46.423: INFO: rc: 1
Dec 24 03:55:46.423: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 24 03:55:56.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:55:56.535: INFO: rc: 1
Dec 24 03:55:56.535: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 24 03:56:06.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=statefulset-3888 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 24 03:56:06.650: INFO: rc: 1
Dec 24 03:56:06.650: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: 
Dec 24 03:56:06.650: INFO: Scaling statefulset ss to 0
Dec 24 03:56:06.658: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Dec 24 03:56:06.660: INFO: Deleting all statefulset in ns statefulset-3888
Dec 24 03:56:06.662: INFO: Scaling statefulset ss to 0
Dec 24 03:56:06.669: INFO: Waiting for statefulset status.replicas updated to 0
Dec 24 03:56:06.701: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:56:06.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3888" for this suite.

• [SLOW TEST:365.270 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":305,"completed":287,"skipped":4558,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:56:06.717: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 03:56:06.763: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: creating replication controller svc-latency-rc in namespace svc-latency-783
I1224 03:56:06.776837      24 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-783, replica count: 1
I1224 03:56:07.827241      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1224 03:56:08.827464      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 24 03:56:08.939: INFO: Created: latency-svc-g466g
Dec 24 03:56:08.948: INFO: Got endpoints: latency-svc-g466g [21.034116ms]
Dec 24 03:56:08.968: INFO: Created: latency-svc-sspmw
Dec 24 03:56:08.985: INFO: Got endpoints: latency-svc-sspmw [36.377455ms]
Dec 24 03:56:08.996: INFO: Created: latency-svc-tkk2m
Dec 24 03:56:09.024: INFO: Got endpoints: latency-svc-tkk2m [75.866678ms]
Dec 24 03:56:09.036: INFO: Created: latency-svc-lxgnz
Dec 24 03:56:09.044: INFO: Got endpoints: latency-svc-lxgnz [95.554853ms]
Dec 24 03:56:09.064: INFO: Created: latency-svc-tdrf9
Dec 24 03:56:09.073: INFO: Got endpoints: latency-svc-tdrf9 [124.596576ms]
Dec 24 03:56:09.093: INFO: Created: latency-svc-d8xc5
Dec 24 03:56:09.110: INFO: Got endpoints: latency-svc-d8xc5 [161.773595ms]
Dec 24 03:56:09.138: INFO: Created: latency-svc-2mhjq
Dec 24 03:56:09.142: INFO: Got endpoints: latency-svc-2mhjq [193.434288ms]
Dec 24 03:56:09.156: INFO: Created: latency-svc-6lvl7
Dec 24 03:56:09.159: INFO: Got endpoints: latency-svc-6lvl7 [210.279866ms]
Dec 24 03:56:09.179: INFO: Created: latency-svc-tbz7w
Dec 24 03:56:09.182: INFO: Got endpoints: latency-svc-tbz7w [233.338567ms]
Dec 24 03:56:09.202: INFO: Created: latency-svc-bkgvd
Dec 24 03:56:09.205: INFO: Got endpoints: latency-svc-bkgvd [256.429119ms]
Dec 24 03:56:09.225: INFO: Created: latency-svc-x5gjk
Dec 24 03:56:09.228: INFO: Got endpoints: latency-svc-x5gjk [279.181788ms]
Dec 24 03:56:09.264: INFO: Created: latency-svc-vnzsv
Dec 24 03:56:09.268: INFO: Got endpoints: latency-svc-vnzsv [319.54637ms]
Dec 24 03:56:09.282: INFO: Created: latency-svc-7fh7q
Dec 24 03:56:09.285: INFO: Got endpoints: latency-svc-7fh7q [336.751296ms]
Dec 24 03:56:09.305: INFO: Created: latency-svc-7t9gm
Dec 24 03:56:09.316: INFO: Got endpoints: latency-svc-7t9gm [367.347594ms]
Dec 24 03:56:09.327: INFO: Created: latency-svc-lzqfx
Dec 24 03:56:09.331: INFO: Got endpoints: latency-svc-lzqfx [382.239651ms]
Dec 24 03:56:09.350: INFO: Created: latency-svc-xjw5r
Dec 24 03:56:09.354: INFO: Got endpoints: latency-svc-xjw5r [405.384992ms]
Dec 24 03:56:09.385: INFO: Created: latency-svc-p49rt
Dec 24 03:56:09.388: INFO: Got endpoints: latency-svc-p49rt [403.534166ms]
Dec 24 03:56:09.407: INFO: Created: latency-svc-vrgzb
Dec 24 03:56:09.419: INFO: Got endpoints: latency-svc-vrgzb [394.936961ms]
Dec 24 03:56:09.430: INFO: Created: latency-svc-tmb7m
Dec 24 03:56:09.434: INFO: Got endpoints: latency-svc-tmb7m [390.145939ms]
Dec 24 03:56:09.453: INFO: Created: latency-svc-pt7lr
Dec 24 03:56:09.457: INFO: Got endpoints: latency-svc-pt7lr [383.757509ms]
Dec 24 03:56:09.476: INFO: Created: latency-svc-bvk6r
Dec 24 03:56:09.498: INFO: Got endpoints: latency-svc-bvk6r [388.296551ms]
Dec 24 03:56:09.505: INFO: Created: latency-svc-2pknc
Dec 24 03:56:09.516: INFO: Got endpoints: latency-svc-2pknc [374.138015ms]
Dec 24 03:56:09.527: INFO: Created: latency-svc-99qtl
Dec 24 03:56:09.532: INFO: Got endpoints: latency-svc-99qtl [372.988076ms]
Dec 24 03:56:09.550: INFO: Created: latency-svc-fd6jt
Dec 24 03:56:09.554: INFO: Got endpoints: latency-svc-fd6jt [372.63945ms]
Dec 24 03:56:09.573: INFO: Created: latency-svc-dk5tp
Dec 24 03:56:09.579: INFO: Got endpoints: latency-svc-dk5tp [373.979484ms]
Dec 24 03:56:09.596: INFO: Created: latency-svc-n4jg5
Dec 24 03:56:09.619: INFO: Got endpoints: latency-svc-n4jg5 [391.482192ms]
Dec 24 03:56:09.630: INFO: Created: latency-svc-g425g
Dec 24 03:56:09.635: INFO: Got endpoints: latency-svc-g425g [366.778431ms]
Dec 24 03:56:09.653: INFO: Created: latency-svc-dpqt6
Dec 24 03:56:09.658: INFO: Got endpoints: latency-svc-dpqt6 [372.546988ms]
Dec 24 03:56:09.676: INFO: Created: latency-svc-mpjp5
Dec 24 03:56:09.681: INFO: Got endpoints: latency-svc-mpjp5 [364.827216ms]
Dec 24 03:56:09.699: INFO: Created: latency-svc-hnb79
Dec 24 03:56:09.710: INFO: Got endpoints: latency-svc-hnb79 [379.428219ms]
Dec 24 03:56:09.739: INFO: Created: latency-svc-tmmbc
Dec 24 03:56:09.750: INFO: Got endpoints: latency-svc-tmmbc [396.576246ms]
Dec 24 03:56:09.751: INFO: Created: latency-svc-xcnp5
Dec 24 03:56:09.762: INFO: Got endpoints: latency-svc-xcnp5 [373.592916ms]
Dec 24 03:56:09.785: INFO: Created: latency-svc-8x2nz
Dec 24 03:56:09.789: INFO: Got endpoints: latency-svc-8x2nz [370.273963ms]
Dec 24 03:56:09.808: INFO: Created: latency-svc-8wltx
Dec 24 03:56:09.819: INFO: Got endpoints: latency-svc-8wltx [384.753657ms]
Dec 24 03:56:09.830: INFO: Created: latency-svc-zdjk6
Dec 24 03:56:09.835: INFO: Got endpoints: latency-svc-zdjk6 [378.206872ms]
Dec 24 03:56:09.876: INFO: Created: latency-svc-bgwwq
Dec 24 03:56:09.888: INFO: Got endpoints: latency-svc-bgwwq [389.350205ms]
Dec 24 03:56:09.888: INFO: Created: latency-svc-kxkn2
Dec 24 03:56:09.899: INFO: Got endpoints: latency-svc-kxkn2 [382.97978ms]
Dec 24 03:56:09.910: INFO: Created: latency-svc-662kk
Dec 24 03:56:09.922: INFO: Got endpoints: latency-svc-662kk [390.147796ms]
Dec 24 03:56:09.933: INFO: Created: latency-svc-44b8v
Dec 24 03:56:09.938: INFO: Got endpoints: latency-svc-44b8v [383.886058ms]
Dec 24 03:56:09.956: INFO: Created: latency-svc-dt8wk
Dec 24 03:56:09.967: INFO: Got endpoints: latency-svc-dt8wk [388.405158ms]
Dec 24 03:56:10.007: INFO: Created: latency-svc-kvl2t
Dec 24 03:56:10.019: INFO: Created: latency-svc-hzk2g
Dec 24 03:56:10.019: INFO: Got endpoints: latency-svc-kvl2t [400.065573ms]
Dec 24 03:56:10.030: INFO: Got endpoints: latency-svc-hzk2g [395.575602ms]
Dec 24 03:56:10.042: INFO: Created: latency-svc-m98n5
Dec 24 03:56:10.047: INFO: Got endpoints: latency-svc-m98n5 [389.54501ms]
Dec 24 03:56:10.065: INFO: Created: latency-svc-qpbf4
Dec 24 03:56:10.070: INFO: Got endpoints: latency-svc-qpbf4 [389.345393ms]
Dec 24 03:56:10.088: INFO: Created: latency-svc-zbdd7
Dec 24 03:56:10.099: INFO: Got endpoints: latency-svc-zbdd7 [388.862815ms]
Dec 24 03:56:10.127: INFO: Created: latency-svc-5q5rr
Dec 24 03:56:10.133: INFO: Got endpoints: latency-svc-5q5rr [382.668961ms]
Dec 24 03:56:10.156: INFO: Created: latency-svc-9jdkf
Dec 24 03:56:10.168: INFO: Got endpoints: latency-svc-9jdkf [405.814234ms]
Dec 24 03:56:10.191: INFO: Created: latency-svc-rdsbg
Dec 24 03:56:10.196: INFO: Got endpoints: latency-svc-rdsbg [406.434966ms]
Dec 24 03:56:10.214: INFO: Created: latency-svc-jw9zz
Dec 24 03:56:10.225: INFO: Got endpoints: latency-svc-jw9zz [405.70361ms]
Dec 24 03:56:10.254: INFO: Created: latency-svc-4xrnh
Dec 24 03:56:10.259: INFO: Got endpoints: latency-svc-4xrnh [424.038082ms]
Dec 24 03:56:10.276: INFO: Created: latency-svc-xcn86
Dec 24 03:56:10.282: INFO: Got endpoints: latency-svc-xcn86 [394.143646ms]
Dec 24 03:56:10.299: INFO: Created: latency-svc-6rblj
Dec 24 03:56:10.305: INFO: Got endpoints: latency-svc-6rblj [406.171009ms]
Dec 24 03:56:10.322: INFO: Created: latency-svc-7x4dk
Dec 24 03:56:10.334: INFO: Got endpoints: latency-svc-7x4dk [411.728877ms]
Dec 24 03:56:10.345: INFO: Created: latency-svc-95vwk
Dec 24 03:56:10.384: INFO: Got endpoints: latency-svc-95vwk [445.70254ms]
Dec 24 03:56:10.385: INFO: Created: latency-svc-hb7b9
Dec 24 03:56:10.391: INFO: Got endpoints: latency-svc-hb7b9 [423.962958ms]
Dec 24 03:56:10.408: INFO: Created: latency-svc-54mnd
Dec 24 03:56:10.414: INFO: Got endpoints: latency-svc-54mnd [394.723405ms]
Dec 24 03:56:10.431: INFO: Created: latency-svc-nxjf4
Dec 24 03:56:10.437: INFO: Got endpoints: latency-svc-nxjf4 [406.441626ms]
Dec 24 03:56:10.455: INFO: Created: latency-svc-gg8g9
Dec 24 03:56:10.460: INFO: Got endpoints: latency-svc-gg8g9 [412.862111ms]
Dec 24 03:56:10.476: INFO: Created: latency-svc-gkcwr
Dec 24 03:56:10.498: INFO: Got endpoints: latency-svc-gkcwr [428.208595ms]
Dec 24 03:56:10.511: INFO: Created: latency-svc-9zq5x
Dec 24 03:56:10.517: INFO: Got endpoints: latency-svc-9zq5x [417.9879ms]
Dec 24 03:56:10.545: INFO: Created: latency-svc-vkz9c
Dec 24 03:56:10.551: INFO: Got endpoints: latency-svc-vkz9c [418.215948ms]
Dec 24 03:56:10.568: INFO: Created: latency-svc-db8vt
Dec 24 03:56:10.575: INFO: Got endpoints: latency-svc-db8vt [406.836079ms]
Dec 24 03:56:10.591: INFO: Created: latency-svc-4lpbk
Dec 24 03:56:10.597: INFO: Got endpoints: latency-svc-4lpbk [401.279453ms]
Dec 24 03:56:10.624: INFO: Created: latency-svc-m656j
Dec 24 03:56:10.648: INFO: Got endpoints: latency-svc-m656j [423.228638ms]
Dec 24 03:56:10.648: INFO: Created: latency-svc-8dz7g
Dec 24 03:56:10.671: INFO: Created: latency-svc-wfvbk
Dec 24 03:56:10.694: INFO: Got endpoints: latency-svc-8dz7g [434.734734ms]
Dec 24 03:56:10.694: INFO: Created: latency-svc-5x6m4
Dec 24 03:56:10.717: INFO: Created: latency-svc-gck4d
Dec 24 03:56:10.757: INFO: Created: latency-svc-8kwln
Dec 24 03:56:10.757: INFO: Got endpoints: latency-svc-wfvbk [474.676181ms]
Dec 24 03:56:10.779: INFO: Created: latency-svc-bv4ht
Dec 24 03:56:10.802: INFO: Created: latency-svc-d6xqq
Dec 24 03:56:10.802: INFO: Got endpoints: latency-svc-5x6m4 [497.165409ms]
Dec 24 03:56:10.825: INFO: Created: latency-svc-rdv7n
Dec 24 03:56:10.848: INFO: Got endpoints: latency-svc-gck4d [514.518001ms]
Dec 24 03:56:10.849: INFO: Created: latency-svc-7rkmp
Dec 24 03:56:10.882: INFO: Created: latency-svc-6wmhx
Dec 24 03:56:10.905: INFO: Created: latency-svc-m8str
Dec 24 03:56:10.905: INFO: Got endpoints: latency-svc-8kwln [521.372183ms]
Dec 24 03:56:10.928: INFO: Created: latency-svc-w4lgv
Dec 24 03:56:10.951: INFO: Created: latency-svc-4k6dt
Dec 24 03:56:10.951: INFO: Got endpoints: latency-svc-bv4ht [559.833575ms]
Dec 24 03:56:10.995: INFO: Created: latency-svc-cd4th
Dec 24 03:56:10.996: INFO: Got endpoints: latency-svc-d6xqq [581.724557ms]
Dec 24 03:56:11.014: INFO: Created: latency-svc-rvfkv
Dec 24 03:56:11.037: INFO: Created: latency-svc-mw6tr
Dec 24 03:56:11.044: INFO: Got endpoints: latency-svc-rdv7n [607.219619ms]
Dec 24 03:56:11.059: INFO: Created: latency-svc-snfb6
Dec 24 03:56:11.083: INFO: Created: latency-svc-5h95d
Dec 24 03:56:11.133: INFO: Created: latency-svc-k4zn8
Dec 24 03:56:11.133: INFO: Got endpoints: latency-svc-7rkmp [673.082368ms]
Dec 24 03:56:11.146: INFO: Created: latency-svc-77w69
Dec 24 03:56:11.146: INFO: Got endpoints: latency-svc-6wmhx [647.452525ms]
Dec 24 03:56:11.168: INFO: Created: latency-svc-qk8xl
Dec 24 03:56:11.191: INFO: Created: latency-svc-qqgm7
Dec 24 03:56:11.199: INFO: Got endpoints: latency-svc-m8str [681.729042ms]
Dec 24 03:56:11.214: INFO: Created: latency-svc-xc44r
Dec 24 03:56:11.254: INFO: Created: latency-svc-wh7hv
Dec 24 03:56:11.254: INFO: Got endpoints: latency-svc-w4lgv [703.037123ms]
Dec 24 03:56:11.277: INFO: Created: latency-svc-79m2n
Dec 24 03:56:11.300: INFO: Got endpoints: latency-svc-4k6dt [725.146374ms]
Dec 24 03:56:11.300: INFO: Created: latency-svc-zg4wc
Dec 24 03:56:11.323: INFO: Created: latency-svc-smsxr
Dec 24 03:56:11.346: INFO: Created: latency-svc-vgz67
Dec 24 03:56:11.346: INFO: Got endpoints: latency-svc-cd4th [748.3643ms]
Dec 24 03:56:11.385: INFO: Created: latency-svc-dvzdq
Dec 24 03:56:11.394: INFO: Got endpoints: latency-svc-rvfkv [745.457519ms]
Dec 24 03:56:11.414: INFO: Created: latency-svc-t47jb
Dec 24 03:56:11.445: INFO: Got endpoints: latency-svc-mw6tr [751.022407ms]
Dec 24 03:56:11.465: INFO: Created: latency-svc-jvmmd
Dec 24 03:56:11.492: INFO: Got endpoints: latency-svc-snfb6 [734.98058ms]
Dec 24 03:56:11.523: INFO: Created: latency-svc-vxqgn
Dec 24 03:56:11.541: INFO: Got endpoints: latency-svc-5h95d [738.972857ms]
Dec 24 03:56:11.574: INFO: Created: latency-svc-n5tpj
Dec 24 03:56:11.624: INFO: Got endpoints: latency-svc-k4zn8 [776.029026ms]
Dec 24 03:56:11.643: INFO: Got endpoints: latency-svc-77w69 [737.202945ms]
Dec 24 03:56:11.643: INFO: Created: latency-svc-6z7nh
Dec 24 03:56:11.671: INFO: Created: latency-svc-lqgvz
Dec 24 03:56:11.692: INFO: Got endpoints: latency-svc-qk8xl [740.641369ms]
Dec 24 03:56:11.756: INFO: Created: latency-svc-hfvsb
Dec 24 03:56:11.756: INFO: Got endpoints: latency-svc-qqgm7 [759.875504ms]
Dec 24 03:56:11.780: INFO: Created: latency-svc-kpm5x
Dec 24 03:56:11.791: INFO: Got endpoints: latency-svc-xc44r [747.290969ms]
Dec 24 03:56:11.831: INFO: Created: latency-svc-pmjhs
Dec 24 03:56:11.842: INFO: Got endpoints: latency-svc-wh7hv [708.310034ms]
Dec 24 03:56:11.887: INFO: Created: latency-svc-lg4dn
Dec 24 03:56:11.900: INFO: Got endpoints: latency-svc-79m2n [754.087545ms]
Dec 24 03:56:11.923: INFO: Created: latency-svc-f95mb
Dec 24 03:56:11.944: INFO: Got endpoints: latency-svc-zg4wc [744.985703ms]
Dec 24 03:56:11.974: INFO: Created: latency-svc-bhjn7
Dec 24 03:56:12.018: INFO: Got endpoints: latency-svc-smsxr [763.815135ms]
Dec 24 03:56:12.049: INFO: Got endpoints: latency-svc-vgz67 [748.791263ms]
Dec 24 03:56:12.049: INFO: Created: latency-svc-r494m
Dec 24 03:56:12.089: INFO: Created: latency-svc-t7cff
Dec 24 03:56:12.093: INFO: Got endpoints: latency-svc-dvzdq [747.189473ms]
Dec 24 03:56:12.146: INFO: Created: latency-svc-4vxkb
Dec 24 03:56:12.146: INFO: Got endpoints: latency-svc-t47jb [752.266185ms]
Dec 24 03:56:12.180: INFO: Created: latency-svc-rqlgf
Dec 24 03:56:12.191: INFO: Got endpoints: latency-svc-jvmmd [746.279311ms]
Dec 24 03:56:12.220: INFO: Created: latency-svc-j9sq8
Dec 24 03:56:12.253: INFO: Got endpoints: latency-svc-vxqgn [760.802134ms]
Dec 24 03:56:12.294: INFO: Got endpoints: latency-svc-n5tpj [753.020201ms]
Dec 24 03:56:12.294: INFO: Created: latency-svc-dcczk
Dec 24 03:56:12.329: INFO: Created: latency-svc-m2jgg
Dec 24 03:56:12.345: INFO: Got endpoints: latency-svc-6z7nh [720.231814ms]
Dec 24 03:56:12.397: INFO: Got endpoints: latency-svc-lqgvz [754.551571ms]
Dec 24 03:56:12.398: INFO: Created: latency-svc-g74pq
Dec 24 03:56:12.447: INFO: Created: latency-svc-48rhc
Dec 24 03:56:12.447: INFO: Got endpoints: latency-svc-hfvsb [755.301322ms]
Dec 24 03:56:12.493: INFO: Created: latency-svc-qh5p5
Dec 24 03:56:12.493: INFO: Got endpoints: latency-svc-kpm5x [736.916243ms]
Dec 24 03:56:12.523: INFO: Created: latency-svc-kphzg
Dec 24 03:56:12.541: INFO: Got endpoints: latency-svc-pmjhs [749.992335ms]
Dec 24 03:56:12.574: INFO: Created: latency-svc-dqx62
Dec 24 03:56:12.591: INFO: Got endpoints: latency-svc-lg4dn [749.632272ms]
Dec 24 03:56:12.643: INFO: Created: latency-svc-2wf7v
Dec 24 03:56:12.644: INFO: Got endpoints: latency-svc-f95mb [743.523867ms]
Dec 24 03:56:12.666: INFO: Created: latency-svc-5zp6l
Dec 24 03:56:12.691: INFO: Got endpoints: latency-svc-bhjn7 [747.285234ms]
Dec 24 03:56:12.718: INFO: Created: latency-svc-8vrkr
Dec 24 03:56:12.756: INFO: Got endpoints: latency-svc-r494m [738.17306ms]
Dec 24 03:56:12.780: INFO: Created: latency-svc-68z4l
Dec 24 03:56:12.791: INFO: Got endpoints: latency-svc-t7cff [742.811479ms]
Dec 24 03:56:12.820: INFO: Created: latency-svc-sncv7
Dec 24 03:56:12.841: INFO: Got endpoints: latency-svc-4vxkb [748.386002ms]
Dec 24 03:56:12.883: INFO: Created: latency-svc-ncb66
Dec 24 03:56:12.894: INFO: Got endpoints: latency-svc-rqlgf [748.577957ms]
Dec 24 03:56:12.917: INFO: Created: latency-svc-84n2p
Dec 24 03:56:12.945: INFO: Got endpoints: latency-svc-j9sq8 [753.933775ms]
Dec 24 03:56:12.996: INFO: Created: latency-svc-5snsl
Dec 24 03:56:12.996: INFO: Got endpoints: latency-svc-dcczk [743.005178ms]
Dec 24 03:56:13.032: INFO: Created: latency-svc-fnds4
Dec 24 03:56:13.041: INFO: Got endpoints: latency-svc-m2jgg [746.816253ms]
Dec 24 03:56:13.077: INFO: Created: latency-svc-pfn6x
Dec 24 03:56:13.091: INFO: Got endpoints: latency-svc-g74pq [746.567308ms]
Dec 24 03:56:13.129: INFO: Created: latency-svc-c4tk8
Dec 24 03:56:13.145: INFO: Got endpoints: latency-svc-48rhc [748.14034ms]
Dec 24 03:56:13.169: INFO: Created: latency-svc-9d8pc
Dec 24 03:56:13.191: INFO: Got endpoints: latency-svc-qh5p5 [744.291222ms]
Dec 24 03:56:13.247: INFO: Created: latency-svc-29jvv
Dec 24 03:56:13.247: INFO: Got endpoints: latency-svc-kphzg [754.514617ms]
Dec 24 03:56:13.272: INFO: Created: latency-svc-7h5kp
Dec 24 03:56:13.291: INFO: Got endpoints: latency-svc-dqx62 [749.915694ms]
Dec 24 03:56:13.335: INFO: Created: latency-svc-zpt74
Dec 24 03:56:13.367: INFO: Got endpoints: latency-svc-2wf7v [776.044383ms]
Dec 24 03:56:13.398: INFO: Created: latency-svc-n5clt
Dec 24 03:56:13.442: INFO: Got endpoints: latency-svc-5zp6l [797.939062ms]
Dec 24 03:56:13.481: INFO: Created: latency-svc-br4x4
Dec 24 03:56:13.492: INFO: Got endpoints: latency-svc-8vrkr [800.410128ms]
Dec 24 03:56:13.518: INFO: Created: latency-svc-z2ptk
Dec 24 03:56:13.546: INFO: Got endpoints: latency-svc-68z4l [789.573784ms]
Dec 24 03:56:13.569: INFO: Created: latency-svc-2gcm6
Dec 24 03:56:13.613: INFO: Got endpoints: latency-svc-sncv7 [821.115381ms]
Dec 24 03:56:13.644: INFO: Got endpoints: latency-svc-ncb66 [802.284082ms]
Dec 24 03:56:13.644: INFO: Created: latency-svc-gtd7l
Dec 24 03:56:13.666: INFO: Created: latency-svc-v8rxj
Dec 24 03:56:13.692: INFO: Got endpoints: latency-svc-84n2p [797.280114ms]
Dec 24 03:56:13.712: INFO: Created: latency-svc-znkwj
Dec 24 03:56:13.742: INFO: Got endpoints: latency-svc-5snsl [796.303396ms]
Dec 24 03:56:13.775: INFO: Created: latency-svc-dfk86
Dec 24 03:56:13.792: INFO: Got endpoints: latency-svc-fnds4 [796.156044ms]
Dec 24 03:56:13.826: INFO: Created: latency-svc-llz74
Dec 24 03:56:13.847: INFO: Got endpoints: latency-svc-pfn6x [805.425901ms]
Dec 24 03:56:13.884: INFO: Created: latency-svc-4df84
Dec 24 03:56:13.892: INFO: Got endpoints: latency-svc-c4tk8 [800.599195ms]
Dec 24 03:56:13.924: INFO: Created: latency-svc-6z7mc
Dec 24 03:56:13.984: INFO: Got endpoints: latency-svc-9d8pc [838.38036ms]
Dec 24 03:56:14.003: INFO: Got endpoints: latency-svc-29jvv [811.94778ms]
Dec 24 03:56:14.004: INFO: Created: latency-svc-pxpb7
Dec 24 03:56:14.026: INFO: Created: latency-svc-j8rjd
Dec 24 03:56:14.041: INFO: Got endpoints: latency-svc-7h5kp [794.237382ms]
Dec 24 03:56:14.066: INFO: Created: latency-svc-pgmhb
Dec 24 03:56:14.115: INFO: Got endpoints: latency-svc-zpt74 [823.948281ms]
Dec 24 03:56:14.141: INFO: Created: latency-svc-lgthp
Dec 24 03:56:14.147: INFO: Got endpoints: latency-svc-n5clt [779.746442ms]
Dec 24 03:56:14.175: INFO: Created: latency-svc-75k5w
Dec 24 03:56:14.193: INFO: Got endpoints: latency-svc-br4x4 [751.081776ms]
Dec 24 03:56:14.215: INFO: Created: latency-svc-qnzpn
Dec 24 03:56:14.242: INFO: Got endpoints: latency-svc-z2ptk [750.36765ms]
Dec 24 03:56:14.272: INFO: Created: latency-svc-wqpxf
Dec 24 03:56:14.301: INFO: Got endpoints: latency-svc-2gcm6 [754.487315ms]
Dec 24 03:56:14.324: INFO: Created: latency-svc-6wxmp
Dec 24 03:56:14.367: INFO: Got endpoints: latency-svc-gtd7l [754.349783ms]
Dec 24 03:56:14.381: INFO: Created: latency-svc-lgscm
Dec 24 03:56:14.392: INFO: Got endpoints: latency-svc-v8rxj [748.020878ms]
Dec 24 03:56:14.421: INFO: Created: latency-svc-5x5xk
Dec 24 03:56:14.441: INFO: Got endpoints: latency-svc-znkwj [749.539692ms]
Dec 24 03:56:14.510: INFO: Created: latency-svc-bz5w4
Dec 24 03:56:14.510: INFO: Got endpoints: latency-svc-dfk86 [768.460379ms]
Dec 24 03:56:14.524: INFO: Created: latency-svc-5cv27
Dec 24 03:56:14.548: INFO: Got endpoints: latency-svc-llz74 [755.742529ms]
Dec 24 03:56:14.569: INFO: Created: latency-svc-6fcgl
Dec 24 03:56:14.592: INFO: Got endpoints: latency-svc-4df84 [745.466998ms]
Dec 24 03:56:14.641: INFO: Created: latency-svc-rj74h
Dec 24 03:56:14.655: INFO: Got endpoints: latency-svc-6z7mc [763.2034ms]
Dec 24 03:56:14.678: INFO: Created: latency-svc-kqnwr
Dec 24 03:56:14.692: INFO: Got endpoints: latency-svc-pxpb7 [707.702697ms]
Dec 24 03:56:14.724: INFO: Created: latency-svc-bgb2s
Dec 24 03:56:14.778: INFO: Got endpoints: latency-svc-j8rjd [774.802436ms]
Dec 24 03:56:14.792: INFO: Got endpoints: latency-svc-pgmhb [751.039421ms]
Dec 24 03:56:14.792: INFO: Created: latency-svc-86p98
Dec 24 03:56:14.815: INFO: Created: latency-svc-hz5bz
Dec 24 03:56:14.842: INFO: Got endpoints: latency-svc-lgthp [726.156243ms]
Dec 24 03:56:14.867: INFO: Created: latency-svc-vb67v
Dec 24 03:56:14.893: INFO: Got endpoints: latency-svc-75k5w [745.573946ms]
Dec 24 03:56:14.918: INFO: Created: latency-svc-cngbt
Dec 24 03:56:14.952: INFO: Got endpoints: latency-svc-qnzpn [759.496736ms]
Dec 24 03:56:14.975: INFO: Created: latency-svc-4h4sn
Dec 24 03:56:14.992: INFO: Got endpoints: latency-svc-wqpxf [749.535401ms]
Dec 24 03:56:15.027: INFO: Created: latency-svc-kmm5h
Dec 24 03:56:15.042: INFO: Got endpoints: latency-svc-6wxmp [741.110983ms]
Dec 24 03:56:15.066: INFO: Created: latency-svc-cv4pq
Dec 24 03:56:15.092: INFO: Got endpoints: latency-svc-lgscm [724.845686ms]
Dec 24 03:56:15.127: INFO: Created: latency-svc-zs4nm
Dec 24 03:56:15.142: INFO: Got endpoints: latency-svc-5x5xk [749.962881ms]
Dec 24 03:56:15.170: INFO: Created: latency-svc-rvhwh
Dec 24 03:56:15.198: INFO: Got endpoints: latency-svc-bz5w4 [756.70074ms]
Dec 24 03:56:15.221: INFO: Created: latency-svc-6mk4c
Dec 24 03:56:15.242: INFO: Got endpoints: latency-svc-5cv27 [731.576181ms]
Dec 24 03:56:15.272: INFO: Created: latency-svc-8tqjz
Dec 24 03:56:15.292: INFO: Got endpoints: latency-svc-6fcgl [743.964681ms]
Dec 24 03:56:15.324: INFO: Created: latency-svc-2lgtt
Dec 24 03:56:15.367: INFO: Got endpoints: latency-svc-rj74h [774.461668ms]
Dec 24 03:56:15.381: INFO: Created: latency-svc-npfnp
Dec 24 03:56:15.393: INFO: Got endpoints: latency-svc-kqnwr [737.485782ms]
Dec 24 03:56:15.415: INFO: Created: latency-svc-nzqpb
Dec 24 03:56:15.449: INFO: Got endpoints: latency-svc-bgb2s [757.699348ms]
Dec 24 03:56:15.484: INFO: Created: latency-svc-ngdsl
Dec 24 03:56:15.492: INFO: Got endpoints: latency-svc-86p98 [713.713164ms]
Dec 24 03:56:15.518: INFO: Created: latency-svc-9lqcz
Dec 24 03:56:15.543: INFO: Got endpoints: latency-svc-hz5bz [750.889091ms]
Dec 24 03:56:15.570: INFO: Created: latency-svc-xbmrx
Dec 24 03:56:15.595: INFO: Got endpoints: latency-svc-vb67v [753.476777ms]
Dec 24 03:56:15.621: INFO: Created: latency-svc-nwxbg
Dec 24 03:56:15.649: INFO: Got endpoints: latency-svc-cngbt [756.706673ms]
Dec 24 03:56:15.672: INFO: Created: latency-svc-hql6b
Dec 24 03:56:15.709: INFO: Got endpoints: latency-svc-4h4sn [757.23719ms]
Dec 24 03:56:15.730: INFO: Created: latency-svc-cw4wn
Dec 24 03:56:15.741: INFO: Got endpoints: latency-svc-kmm5h [749.370604ms]
Dec 24 03:56:15.787: INFO: Created: latency-svc-95dkx
Dec 24 03:56:15.795: INFO: Got endpoints: latency-svc-cv4pq [753.529999ms]
Dec 24 03:56:15.844: INFO: Created: latency-svc-p2shl
Dec 24 03:56:15.844: INFO: Got endpoints: latency-svc-zs4nm [752.503465ms]
Dec 24 03:56:15.878: INFO: Created: latency-svc-9p2v6
Dec 24 03:56:15.893: INFO: Got endpoints: latency-svc-rvhwh [751.046207ms]
Dec 24 03:56:15.918: INFO: Created: latency-svc-5f4m7
Dec 24 03:56:15.950: INFO: Got endpoints: latency-svc-6mk4c [751.647184ms]
Dec 24 03:56:15.975: INFO: Created: latency-svc-kw7bp
Dec 24 03:56:15.992: INFO: Got endpoints: latency-svc-8tqjz [750.620426ms]
Dec 24 03:56:16.021: INFO: Created: latency-svc-fnfqn
Dec 24 03:56:16.058: INFO: Got endpoints: latency-svc-2lgtt [766.534207ms]
Dec 24 03:56:16.096: INFO: Got endpoints: latency-svc-npfnp [729.063651ms]
Dec 24 03:56:16.096: INFO: Created: latency-svc-7nvsx
Dec 24 03:56:16.130: INFO: Created: latency-svc-rb2kc
Dec 24 03:56:16.144: INFO: Got endpoints: latency-svc-nzqpb [751.76656ms]
Dec 24 03:56:16.193: INFO: Created: latency-svc-8w4hk
Dec 24 03:56:16.193: INFO: Got endpoints: latency-svc-ngdsl [743.508864ms]
Dec 24 03:56:16.227: INFO: Created: latency-svc-r22jh
Dec 24 03:56:16.241: INFO: Got endpoints: latency-svc-9lqcz [749.146583ms]
Dec 24 03:56:16.293: INFO: Created: latency-svc-6cflj
Dec 24 03:56:16.299: INFO: Got endpoints: latency-svc-xbmrx [755.638387ms]
Dec 24 03:56:16.319: INFO: Created: latency-svc-bmhgh
Dec 24 03:56:16.342: INFO: Got endpoints: latency-svc-nwxbg [746.702273ms]
Dec 24 03:56:16.370: INFO: Created: latency-svc-rvjmf
Dec 24 03:56:16.392: INFO: Got endpoints: latency-svc-hql6b [742.208314ms]
Dec 24 03:56:16.427: INFO: Created: latency-svc-ctbn2
Dec 24 03:56:16.450: INFO: Got endpoints: latency-svc-cw4wn [740.637958ms]
Dec 24 03:56:16.473: INFO: Created: latency-svc-qrzpx
Dec 24 03:56:16.527: INFO: Got endpoints: latency-svc-95dkx [785.555857ms]
Dec 24 03:56:16.549: INFO: Created: latency-svc-8stgh
Dec 24 03:56:16.552: INFO: Got endpoints: latency-svc-p2shl [756.958366ms]
Dec 24 03:56:16.576: INFO: Created: latency-svc-598tm
Dec 24 03:56:16.591: INFO: Got endpoints: latency-svc-9p2v6 [747.074799ms]
Dec 24 03:56:16.641: INFO: Created: latency-svc-7fvtl
Dec 24 03:56:16.642: INFO: Got endpoints: latency-svc-5f4m7 [749.001753ms]
Dec 24 03:56:16.673: INFO: Created: latency-svc-gcgkc
Dec 24 03:56:16.691: INFO: Got endpoints: latency-svc-kw7bp [741.49031ms]
Dec 24 03:56:16.719: INFO: Created: latency-svc-qbznc
Dec 24 03:56:16.755: INFO: Got endpoints: latency-svc-fnfqn [762.749488ms]
Dec 24 03:56:16.776: INFO: Created: latency-svc-l2q5m
Dec 24 03:56:16.791: INFO: Got endpoints: latency-svc-7nvsx [733.156834ms]
Dec 24 03:56:16.821: INFO: Created: latency-svc-s9f5b
Dec 24 03:56:16.841: INFO: Got endpoints: latency-svc-rb2kc [745.617879ms]
Dec 24 03:56:16.892: INFO: Got endpoints: latency-svc-8w4hk [747.424157ms]
Dec 24 03:56:16.942: INFO: Got endpoints: latency-svc-r22jh [748.834876ms]
Dec 24 03:56:16.995: INFO: Got endpoints: latency-svc-6cflj [753.825134ms]
Dec 24 03:56:17.041: INFO: Got endpoints: latency-svc-bmhgh [742.307184ms]
Dec 24 03:56:17.092: INFO: Got endpoints: latency-svc-rvjmf [749.668011ms]
Dec 24 03:56:17.142: INFO: Got endpoints: latency-svc-ctbn2 [750.29091ms]
Dec 24 03:56:17.192: INFO: Got endpoints: latency-svc-qrzpx [741.787536ms]
Dec 24 03:56:17.242: INFO: Got endpoints: latency-svc-8stgh [715.236074ms]
Dec 24 03:56:17.296: INFO: Got endpoints: latency-svc-598tm [743.458436ms]
Dec 24 03:56:17.344: INFO: Got endpoints: latency-svc-7fvtl [752.273477ms]
Dec 24 03:56:17.392: INFO: Got endpoints: latency-svc-gcgkc [750.208208ms]
Dec 24 03:56:17.442: INFO: Got endpoints: latency-svc-qbznc [750.876641ms]
Dec 24 03:56:17.492: INFO: Got endpoints: latency-svc-l2q5m [736.363832ms]
Dec 24 03:56:17.542: INFO: Got endpoints: latency-svc-s9f5b [750.412835ms]
Dec 24 03:56:17.542: INFO: Latencies: [36.377455ms 75.866678ms 95.554853ms 124.596576ms 161.773595ms 193.434288ms 210.279866ms 233.338567ms 256.429119ms 279.181788ms 319.54637ms 336.751296ms 364.827216ms 366.778431ms 367.347594ms 370.273963ms 372.546988ms 372.63945ms 372.988076ms 373.592916ms 373.979484ms 374.138015ms 378.206872ms 379.428219ms 382.239651ms 382.668961ms 382.97978ms 383.757509ms 383.886058ms 384.753657ms 388.296551ms 388.405158ms 388.862815ms 389.345393ms 389.350205ms 389.54501ms 390.145939ms 390.147796ms 391.482192ms 394.143646ms 394.723405ms 394.936961ms 395.575602ms 396.576246ms 400.065573ms 401.279453ms 403.534166ms 405.384992ms 405.70361ms 405.814234ms 406.171009ms 406.434966ms 406.441626ms 406.836079ms 411.728877ms 412.862111ms 417.9879ms 418.215948ms 423.228638ms 423.962958ms 424.038082ms 428.208595ms 434.734734ms 445.70254ms 474.676181ms 497.165409ms 514.518001ms 521.372183ms 559.833575ms 581.724557ms 607.219619ms 647.452525ms 673.082368ms 681.729042ms 703.037123ms 707.702697ms 708.310034ms 713.713164ms 715.236074ms 720.231814ms 724.845686ms 725.146374ms 726.156243ms 729.063651ms 731.576181ms 733.156834ms 734.98058ms 736.363832ms 736.916243ms 737.202945ms 737.485782ms 738.17306ms 738.972857ms 740.637958ms 740.641369ms 741.110983ms 741.49031ms 741.787536ms 742.208314ms 742.307184ms 742.811479ms 743.005178ms 743.458436ms 743.508864ms 743.523867ms 743.964681ms 744.291222ms 744.985703ms 745.457519ms 745.466998ms 745.573946ms 745.617879ms 746.279311ms 746.567308ms 746.702273ms 746.816253ms 747.074799ms 747.189473ms 747.285234ms 747.290969ms 747.424157ms 748.020878ms 748.14034ms 748.3643ms 748.386002ms 748.577957ms 748.791263ms 748.834876ms 749.001753ms 749.146583ms 749.370604ms 749.535401ms 749.539692ms 749.632272ms 749.668011ms 749.915694ms 749.962881ms 749.992335ms 750.208208ms 750.29091ms 750.36765ms 750.412835ms 750.620426ms 750.876641ms 750.889091ms 751.022407ms 751.039421ms 751.046207ms 751.081776ms 751.647184ms 751.76656ms 752.266185ms 752.273477ms 752.503465ms 753.020201ms 753.476777ms 753.529999ms 753.825134ms 753.933775ms 754.087545ms 754.349783ms 754.487315ms 754.514617ms 754.551571ms 755.301322ms 755.638387ms 755.742529ms 756.70074ms 756.706673ms 756.958366ms 757.23719ms 757.699348ms 759.496736ms 759.875504ms 760.802134ms 762.749488ms 763.2034ms 763.815135ms 766.534207ms 768.460379ms 774.461668ms 774.802436ms 776.029026ms 776.044383ms 779.746442ms 785.555857ms 789.573784ms 794.237382ms 796.156044ms 796.303396ms 797.280114ms 797.939062ms 800.410128ms 800.599195ms 802.284082ms 805.425901ms 811.94778ms 821.115381ms 823.948281ms 838.38036ms]
Dec 24 03:56:17.542: INFO: 50 %ile: 742.811479ms
Dec 24 03:56:17.542: INFO: 90 %ile: 774.461668ms
Dec 24 03:56:17.542: INFO: 99 %ile: 823.948281ms
Dec 24 03:56:17.542: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:56:17.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-783" for this suite.

• [SLOW TEST:10.837 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":305,"completed":288,"skipped":4571,"failed":0}
SSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:56:17.554: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Dec 24 03:56:17.612: INFO: Waiting up to 5m0s for pod "downward-api-2094fc7d-a61a-4f64-91b6-7ed141e21c01" in namespace "downward-api-8479" to be "Succeeded or Failed"
Dec 24 03:56:17.627: INFO: Pod "downward-api-2094fc7d-a61a-4f64-91b6-7ed141e21c01": Phase="Pending", Reason="", readiness=false. Elapsed: 15.719324ms
Dec 24 03:56:19.631: INFO: Pod "downward-api-2094fc7d-a61a-4f64-91b6-7ed141e21c01": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019066214s
Dec 24 03:56:21.633: INFO: Pod "downward-api-2094fc7d-a61a-4f64-91b6-7ed141e21c01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021928186s
STEP: Saw pod success
Dec 24 03:56:21.634: INFO: Pod "downward-api-2094fc7d-a61a-4f64-91b6-7ed141e21c01" satisfied condition "Succeeded or Failed"
Dec 24 03:56:21.636: INFO: Trying to get logs from node c1-4 pod downward-api-2094fc7d-a61a-4f64-91b6-7ed141e21c01 container dapi-container: <nil>
STEP: delete the pod
Dec 24 03:56:21.697: INFO: Waiting for pod downward-api-2094fc7d-a61a-4f64-91b6-7ed141e21c01 to disappear
Dec 24 03:56:21.706: INFO: Pod downward-api-2094fc7d-a61a-4f64-91b6-7ed141e21c01 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:56:21.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8479" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":305,"completed":289,"skipped":4575,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:56:21.714: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's args
Dec 24 03:56:21.787: INFO: Waiting up to 5m0s for pod "var-expansion-46b4161e-dd0f-4e83-a969-5082880733e9" in namespace "var-expansion-9436" to be "Succeeded or Failed"
Dec 24 03:56:21.799: INFO: Pod "var-expansion-46b4161e-dd0f-4e83-a969-5082880733e9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.541206ms
Dec 24 03:56:23.810: INFO: Pod "var-expansion-46b4161e-dd0f-4e83-a969-5082880733e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022990322s
STEP: Saw pod success
Dec 24 03:56:23.810: INFO: Pod "var-expansion-46b4161e-dd0f-4e83-a969-5082880733e9" satisfied condition "Succeeded or Failed"
Dec 24 03:56:23.815: INFO: Trying to get logs from node c1-4 pod var-expansion-46b4161e-dd0f-4e83-a969-5082880733e9 container dapi-container: <nil>
STEP: delete the pod
Dec 24 03:56:23.855: INFO: Waiting for pod var-expansion-46b4161e-dd0f-4e83-a969-5082880733e9 to disappear
Dec 24 03:56:23.872: INFO: Pod var-expansion-46b4161e-dd0f-4e83-a969-5082880733e9 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:56:23.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9436" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":305,"completed":290,"skipped":4603,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:56:23.909: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:56:29.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4654" for this suite.

• [SLOW TEST:6.030 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":305,"completed":291,"skipped":4611,"failed":0}
S
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:56:29.939: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Dec 24 03:56:36.093: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8998 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 24 03:56:36.093: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
Dec 24 03:56:36.196: INFO: Exec stderr: ""
Dec 24 03:56:36.196: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8998 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 24 03:56:36.196: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
Dec 24 03:56:36.274: INFO: Exec stderr: ""
Dec 24 03:56:36.274: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8998 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 24 03:56:36.274: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
Dec 24 03:56:36.353: INFO: Exec stderr: ""
Dec 24 03:56:36.353: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8998 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 24 03:56:36.353: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
Dec 24 03:56:36.431: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Dec 24 03:56:36.431: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8998 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 24 03:56:36.431: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
Dec 24 03:56:36.507: INFO: Exec stderr: ""
Dec 24 03:56:36.507: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8998 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 24 03:56:36.507: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
Dec 24 03:56:36.595: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Dec 24 03:56:36.595: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8998 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 24 03:56:36.595: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
Dec 24 03:56:36.680: INFO: Exec stderr: ""
Dec 24 03:56:36.680: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8998 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 24 03:56:36.680: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
Dec 24 03:56:36.763: INFO: Exec stderr: ""
Dec 24 03:56:36.763: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8998 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 24 03:56:36.763: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
Dec 24 03:56:36.837: INFO: Exec stderr: ""
Dec 24 03:56:36.837: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8998 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 24 03:56:36.837: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
Dec 24 03:56:36.917: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:56:36.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-8998" for this suite.

• [SLOW TEST:7.004 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":292,"skipped":4612,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:56:36.944: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W1224 03:56:38.548562      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Dec 24 03:57:40.562: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:57:40.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8502" for this suite.

• [SLOW TEST:63.628 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":305,"completed":293,"skipped":4627,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:57:40.572: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Dec 24 03:57:40.645: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6f7d6d92-d256-4d6c-8e1c-211a755b52ae" in namespace "downward-api-919" to be "Succeeded or Failed"
Dec 24 03:57:40.647: INFO: Pod "downwardapi-volume-6f7d6d92-d256-4d6c-8e1c-211a755b52ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.148596ms
Dec 24 03:57:42.650: INFO: Pod "downwardapi-volume-6f7d6d92-d256-4d6c-8e1c-211a755b52ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005074977s
STEP: Saw pod success
Dec 24 03:57:42.650: INFO: Pod "downwardapi-volume-6f7d6d92-d256-4d6c-8e1c-211a755b52ae" satisfied condition "Succeeded or Failed"
Dec 24 03:57:42.652: INFO: Trying to get logs from node c1-4 pod downwardapi-volume-6f7d6d92-d256-4d6c-8e1c-211a755b52ae container client-container: <nil>
STEP: delete the pod
Dec 24 03:57:42.679: INFO: Waiting for pod downwardapi-volume-6f7d6d92-d256-4d6c-8e1c-211a755b52ae to disappear
Dec 24 03:57:42.687: INFO: Pod downwardapi-volume-6f7d6d92-d256-4d6c-8e1c-211a755b52ae no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:57:42.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-919" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":294,"skipped":4657,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:57:42.695: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating cluster-info
Dec 24 03:57:42.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 cluster-info'
Dec 24 03:57:42.841: INFO: stderr: ""
Dec 24 03:57:42.841: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:57:42.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2189" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":305,"completed":295,"skipped":4734,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:57:42.847: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 24 03:57:43.485: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 24 03:57:45.493: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744379063, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744379063, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744379063, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744379063, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 24 03:57:48.518: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:57:48.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2262" for this suite.
STEP: Destroying namespace "webhook-2262-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.865 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":305,"completed":296,"skipped":4735,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:57:48.712: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:57:52.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5658" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":305,"completed":297,"skipped":4757,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:57:52.817: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 24 03:57:53.627: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 24 03:57:56.662: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 03:57:56.665: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-414-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:57:57.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1733" for this suite.
STEP: Destroying namespace "webhook-1733-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.118 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":305,"completed":298,"skipped":4768,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:57:57.935: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service nodeport-service with the type=NodePort in namespace services-9854
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-9854
STEP: creating replication controller externalsvc in namespace services-9854
I1224 03:57:58.067067      24 runners.go:190] Created replication controller with name: externalsvc, namespace: services-9854, replica count: 2
I1224 03:58:01.117376      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Dec 24 03:58:01.164: INFO: Creating new exec pod
Dec 24 03:58:03.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-303221030 exec --namespace=services-9854 execpodfdjgb -- /bin/sh -x -c nslookup nodeport-service.services-9854.svc.cluster.local'
Dec 24 03:58:03.402: INFO: stderr: "+ nslookup nodeport-service.services-9854.svc.cluster.local\n"
Dec 24 03:58:03.402: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-9854.svc.cluster.local\tcanonical name = externalsvc.services-9854.svc.cluster.local.\nName:\texternalsvc.services-9854.svc.cluster.local\nAddress: 10.96.73.155\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9854, will wait for the garbage collector to delete the pods
Dec 24 03:58:03.460: INFO: Deleting ReplicationController externalsvc took: 5.255033ms
Dec 24 03:58:03.560: INFO: Terminating ReplicationController externalsvc pods took: 100.182447ms
Dec 24 03:58:07.989: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:58:08.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9854" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:10.107 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":305,"completed":299,"skipped":4778,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:58:08.043: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 03:58:08.081: INFO: Creating deployment "test-recreate-deployment"
Dec 24 03:58:08.085: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec 24 03:58:08.102: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Dec 24 03:58:10.107: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec 24 03:58:10.109: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec 24 03:58:10.114: INFO: Updating deployment test-recreate-deployment
Dec 24 03:58:10.115: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Dec 24 03:58:10.303: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-6198 /apis/apps/v1/namespaces/deployment-6198/deployments/test-recreate-deployment 23e303f7-104f-4855-9370-c5db374268f3 3485337 2 2020-12-24 03:58:08 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-12-24 03:58:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-12-24 03:58:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009671708 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-12-24 03:58:10 +0000 UTC,LastTransitionTime:2020-12-24 03:58:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2020-12-24 03:58:10 +0000 UTC,LastTransitionTime:2020-12-24 03:58:08 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Dec 24 03:58:10.320: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-6198 /apis/apps/v1/namespaces/deployment-6198/replicasets/test-recreate-deployment-f79dd4667 a3fe8edf-06bd-47da-a3e8-67086cfe1ec5 3485335 1 2020-12-24 03:58:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 23e303f7-104f-4855-9370-c5db374268f3 0xc009671c20 0xc009671c21}] []  [{kube-controller-manager Update apps/v1 2020-12-24 03:58:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"23e303f7-104f-4855-9370-c5db374268f3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009671c98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 24 03:58:10.320: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec 24 03:58:10.320: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-c96cf48f  deployment-6198 /apis/apps/v1/namespaces/deployment-6198/replicasets/test-recreate-deployment-c96cf48f 9784c836-1819-4b30-a41b-c898387c4b28 3485326 2 2020-12-24 03:58:08 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 23e303f7-104f-4855-9370-c5db374268f3 0xc009671b1f 0xc009671b30}] []  [{kube-controller-manager Update apps/v1 2020-12-24 03:58:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"23e303f7-104f-4855-9370-c5db374268f3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c96cf48f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009671ba8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 24 03:58:10.322: INFO: Pod "test-recreate-deployment-f79dd4667-4f6bk" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-4f6bk test-recreate-deployment-f79dd4667- deployment-6198 /api/v1/namespaces/deployment-6198/pods/test-recreate-deployment-f79dd4667-4f6bk 9dc7c53a-fb27-4123-bf28-5484cb6a89f2 3485338 0 2020-12-24 03:58:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 a3fe8edf-06bd-47da-a3e8-67086cfe1ec5 0xc0026bf500 0xc0026bf501}] []  [{kube-controller-manager Update v1 2020-12-24 03:58:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a3fe8edf-06bd-47da-a3e8-67086cfe1ec5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-24 03:58:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5swhl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5swhl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5swhl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c1-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:58:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:58:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:58:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-24 03:58:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.3.5,PodIP:,StartTime:2020-12-24 03:58:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:58:10.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6198" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":300,"skipped":4781,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:58:10.328: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 03:58:10.378: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:58:11.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8218" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":305,"completed":301,"skipped":4803,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:58:11.399: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Dec 24 03:58:12.114: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Dec 24 03:58:14.122: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744379092, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744379092, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744379092, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744379092, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 24 03:58:17.143: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 03:58:17.146: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:58:18.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-659" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.918 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":305,"completed":302,"skipped":4864,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:58:18.318: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Dec 24 03:58:18.385: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Creating first CR 
Dec 24 03:58:18.961: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-12-24T03:58:18Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-12-24T03:58:18Z]] name:name1 resourceVersion:3485542 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:04622d64-1214-4261-b628-f689f73a4f17] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Dec 24 03:58:28.966: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-12-24T03:58:28Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-12-24T03:58:28Z]] name:name2 resourceVersion:3485617 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:e3c9dfd2-1395-4f1a-a74e-46054807520b] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Dec 24 03:58:38.971: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-12-24T03:58:18Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-12-24T03:58:38Z]] name:name1 resourceVersion:3485679 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:04622d64-1214-4261-b628-f689f73a4f17] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Dec 24 03:58:48.977: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-12-24T03:58:28Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-12-24T03:58:48Z]] name:name2 resourceVersion:3485741 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:e3c9dfd2-1395-4f1a-a74e-46054807520b] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Dec 24 03:58:58.984: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-12-24T03:58:18Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-12-24T03:58:38Z]] name:name1 resourceVersion:3485804 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:04622d64-1214-4261-b628-f689f73a4f17] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Dec 24 03:59:08.990: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-12-24T03:58:28Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-12-24T03:58:48Z]] name:name2 resourceVersion:3485867 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:e3c9dfd2-1395-4f1a-a74e-46054807520b] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:59:19.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-5671" for this suite.

• [SLOW TEST:61.190 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":305,"completed":303,"skipped":4906,"failed":0}
SS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:59:19.507: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:59:19.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-555" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":305,"completed":304,"skipped":4908,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 24 03:59:19.586: INFO: >>> kubeConfig: /tmp/kubeconfig-303221030
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-downwardapi-ffvh
STEP: Creating a pod to test atomic-volume-subpath
Dec 24 03:59:19.672: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-ffvh" in namespace "subpath-9936" to be "Succeeded or Failed"
Dec 24 03:59:19.677: INFO: Pod "pod-subpath-test-downwardapi-ffvh": Phase="Pending", Reason="", readiness=false. Elapsed: 4.199191ms
Dec 24 03:59:21.679: INFO: Pod "pod-subpath-test-downwardapi-ffvh": Phase="Running", Reason="", readiness=true. Elapsed: 2.006600642s
Dec 24 03:59:23.682: INFO: Pod "pod-subpath-test-downwardapi-ffvh": Phase="Running", Reason="", readiness=true. Elapsed: 4.009634934s
Dec 24 03:59:25.685: INFO: Pod "pod-subpath-test-downwardapi-ffvh": Phase="Running", Reason="", readiness=true. Elapsed: 6.01247583s
Dec 24 03:59:27.688: INFO: Pod "pod-subpath-test-downwardapi-ffvh": Phase="Running", Reason="", readiness=true. Elapsed: 8.015363866s
Dec 24 03:59:29.691: INFO: Pod "pod-subpath-test-downwardapi-ffvh": Phase="Running", Reason="", readiness=true. Elapsed: 10.018110421s
Dec 24 03:59:31.693: INFO: Pod "pod-subpath-test-downwardapi-ffvh": Phase="Running", Reason="", readiness=true. Elapsed: 12.020931972s
Dec 24 03:59:33.704: INFO: Pod "pod-subpath-test-downwardapi-ffvh": Phase="Running", Reason="", readiness=true. Elapsed: 14.031719873s
Dec 24 03:59:35.707: INFO: Pod "pod-subpath-test-downwardapi-ffvh": Phase="Running", Reason="", readiness=true. Elapsed: 16.034336021s
Dec 24 03:59:37.741: INFO: Pod "pod-subpath-test-downwardapi-ffvh": Phase="Running", Reason="", readiness=true. Elapsed: 18.06830388s
Dec 24 03:59:39.743: INFO: Pod "pod-subpath-test-downwardapi-ffvh": Phase="Running", Reason="", readiness=true. Elapsed: 20.070819078s
Dec 24 03:59:41.749: INFO: Pod "pod-subpath-test-downwardapi-ffvh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.076508851s
STEP: Saw pod success
Dec 24 03:59:41.749: INFO: Pod "pod-subpath-test-downwardapi-ffvh" satisfied condition "Succeeded or Failed"
Dec 24 03:59:41.751: INFO: Trying to get logs from node c1-4 pod pod-subpath-test-downwardapi-ffvh container test-container-subpath-downwardapi-ffvh: <nil>
STEP: delete the pod
Dec 24 03:59:41.781: INFO: Waiting for pod pod-subpath-test-downwardapi-ffvh to disappear
Dec 24 03:59:41.786: INFO: Pod pod-subpath-test-downwardapi-ffvh no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-ffvh
Dec 24 03:59:41.786: INFO: Deleting pod "pod-subpath-test-downwardapi-ffvh" in namespace "subpath-9936"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 24 03:59:41.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9936" for this suite.

• [SLOW TEST:22.209 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":305,"completed":305,"skipped":4925,"failed":0}
SSSSDec 24 03:59:41.796: INFO: Running AfterSuite actions on all nodes
Dec 24 03:59:41.796: INFO: Running AfterSuite actions on node 1
Dec 24 03:59:41.796: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":305,"completed":305,"skipped":4929,"failed":0}

Ran 305 of 5234 Specs in 6096.285 seconds
SUCCESS! -- 305 Passed | 0 Failed | 0 Pending | 4929 Skipped
PASS

Ginkgo ran 1 suite in 1h41m37.900662668s
Test Suite Passed
