I1119 16:56:45.475575      22 test_context.go:416] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-898325863
I1119 16:56:45.475600      22 test_context.go:429] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I1119 16:56:45.475677      22 e2e.go:129] Starting e2e run "59be4760-c7ea-499c-b9de-f3dc25720742" on Ginkgo node 1
{"msg":"Test Suite starting","total":305,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1605805004 - Will randomize all specs
Will run 305 of 5234 specs

Nov 19 16:56:45.485: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
Nov 19 16:56:45.487: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Nov 19 16:56:45.498: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Nov 19 16:56:45.528: INFO: 3 / 3 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Nov 19 16:56:45.528: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Nov 19 16:56:45.528: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Nov 19 16:56:45.535: INFO: e2e test version: v1.19.4
Nov 19 16:56:45.536: INFO: kube-apiserver version: v1.19.4
Nov 19 16:56:45.536: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
Nov 19 16:56:45.540: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 16:56:45.540: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename resourcequota
Nov 19 16:56:45.567: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Nov 19 16:56:45.575: INFO: PSP annotation exists on dry run pod: "privileged"; assuming PodSecurityPolicy is enabled
Nov 19 16:56:45.588: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-708
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 16:57:01.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-708" for this suite.

â€¢ [SLOW TEST:16.265 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":305,"completed":1,"skipped":22,"failed":0}
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 16:57:01.805: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8146
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov 19 16:57:01.937: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 19 16:57:01.942: INFO: Waiting for terminating namespaces to be deleted...
Nov 19 16:57:01.945: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-20-145 before test
Nov 19 16:57:01.949: INFO: nginx-ingress-controller-kubernetes-worker-cshw5 from ingress-nginx-kubernetes-worker started at 2020-11-19 16:40:07 +0000 UTC (1 container statuses recorded)
Nov 19 16:57:01.949: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Nov 19 16:57:01.949: INFO: kube-state-metrics-6f586bb967-8gvg6 from kube-system started at 2020-11-19 16:40:12 +0000 UTC (1 container statuses recorded)
Nov 19 16:57:01.949: INFO: 	Container kube-state-metrics ready: true, restart count 0
Nov 19 16:57:01.949: INFO: kubernetes-dashboard-64f87676d4-ggs4x from kubernetes-dashboard started at 2020-11-19 16:40:12 +0000 UTC (1 container statuses recorded)
Nov 19 16:57:01.949: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Nov 19 16:57:01.949: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-68-240 before test
Nov 19 16:57:01.952: INFO: default-http-backend-kubernetes-worker-6494cbc7fd-vtwt8 from ingress-nginx-kubernetes-worker started at 2020-11-19 16:40:15 +0000 UTC (1 container statuses recorded)
Nov 19 16:57:01.952: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
Nov 19 16:57:01.952: INFO: nginx-ingress-controller-kubernetes-worker-qkjj5 from ingress-nginx-kubernetes-worker started at 2020-11-19 16:40:11 +0000 UTC (1 container statuses recorded)
Nov 19 16:57:01.952: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Nov 19 16:57:01.952: INFO: metrics-server-v0.3.6-7685c8469-xspvp from kube-system started at 2020-11-19 16:40:36 +0000 UTC (2 container statuses recorded)
Nov 19 16:57:01.952: INFO: 	Container metrics-server ready: true, restart count 0
Nov 19 16:57:01.952: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Nov 19 16:57:01.952: INFO: sonobuoy from sonobuoy started at 2020-11-19 16:56:31 +0000 UTC (1 container statuses recorded)
Nov 19 16:57:01.952: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 19 16:57:01.952: INFO: sonobuoy-e2e-job-51d1badd77b04798 from sonobuoy started at 2020-11-19 16:56:36 +0000 UTC (2 container statuses recorded)
Nov 19 16:57:01.952: INFO: 	Container e2e ready: true, restart count 0
Nov 19 16:57:01.952: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 19 16:57:01.952: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-9-169 before test
Nov 19 16:57:01.955: INFO: nginx-ingress-controller-kubernetes-worker-47fcn from ingress-nginx-kubernetes-worker started at 2020-11-19 16:40:06 +0000 UTC (1 container statuses recorded)
Nov 19 16:57:01.955: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Nov 19 16:57:01.955: INFO: coredns-7bb4d77796-wgc4n from kube-system started at 2020-11-19 16:40:12 +0000 UTC (1 container statuses recorded)
Nov 19 16:57:01.955: INFO: 	Container coredns ready: true, restart count 0
Nov 19 16:57:01.955: INFO: dashboard-metrics-scraper-74757fb5b7-qfnkc from kubernetes-dashboard started at 2020-11-19 16:40:12 +0000 UTC (1 container statuses recorded)
Nov 19 16:57:01.955: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: verifying the node has the label node ip-172-31-20-145
STEP: verifying the node has the label node ip-172-31-68-240
STEP: verifying the node has the label node ip-172-31-9-169
Nov 19 16:57:01.990: INFO: Pod default-http-backend-kubernetes-worker-6494cbc7fd-vtwt8 requesting resource cpu=10m on Node ip-172-31-68-240
Nov 19 16:57:01.990: INFO: Pod nginx-ingress-controller-kubernetes-worker-47fcn requesting resource cpu=0m on Node ip-172-31-9-169
Nov 19 16:57:01.990: INFO: Pod nginx-ingress-controller-kubernetes-worker-cshw5 requesting resource cpu=0m on Node ip-172-31-20-145
Nov 19 16:57:01.990: INFO: Pod nginx-ingress-controller-kubernetes-worker-qkjj5 requesting resource cpu=0m on Node ip-172-31-68-240
Nov 19 16:57:01.990: INFO: Pod coredns-7bb4d77796-wgc4n requesting resource cpu=100m on Node ip-172-31-9-169
Nov 19 16:57:01.990: INFO: Pod kube-state-metrics-6f586bb967-8gvg6 requesting resource cpu=0m on Node ip-172-31-20-145
Nov 19 16:57:01.990: INFO: Pod metrics-server-v0.3.6-7685c8469-xspvp requesting resource cpu=53m on Node ip-172-31-68-240
Nov 19 16:57:01.990: INFO: Pod dashboard-metrics-scraper-74757fb5b7-qfnkc requesting resource cpu=0m on Node ip-172-31-9-169
Nov 19 16:57:01.990: INFO: Pod kubernetes-dashboard-64f87676d4-ggs4x requesting resource cpu=0m on Node ip-172-31-20-145
Nov 19 16:57:01.990: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-68-240
Nov 19 16:57:01.990: INFO: Pod sonobuoy-e2e-job-51d1badd77b04798 requesting resource cpu=0m on Node ip-172-31-68-240
STEP: Starting Pods to consume most of the cluster CPU.
Nov 19 16:57:01.990: INFO: Creating a pod which consumes cpu=2800m on Node ip-172-31-20-145
Nov 19 16:57:01.997: INFO: Creating a pod which consumes cpu=2755m on Node ip-172-31-68-240
Nov 19 16:57:02.003: INFO: Creating a pod which consumes cpu=2730m on Node ip-172-31-9-169
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4637ad85-c874-4b10-812c-d980d58c409b.1648f728dc253706], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8146/filler-pod-4637ad85-c874-4b10-812c-d980d58c409b to ip-172-31-9-169]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4637ad85-c874-4b10-812c-d980d58c409b.1648f728f9e632af], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.2"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4637ad85-c874-4b10-812c-d980d58c409b.1648f729078d8ad1], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.2" in 229.051757ms]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4637ad85-c874-4b10-812c-d980d58c409b.1648f7290bf74908], Reason = [Created], Message = [Created container filler-pod-4637ad85-c874-4b10-812c-d980d58c409b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4637ad85-c874-4b10-812c-d980d58c409b.1648f7291046e76f], Reason = [Started], Message = [Started container filler-pod-4637ad85-c874-4b10-812c-d980d58c409b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d5a6cfd6-950a-4b66-8b52-aa36300c0a46.1648f728db75cc41], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8146/filler-pod-d5a6cfd6-950a-4b66-8b52-aa36300c0a46 to ip-172-31-20-145]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d5a6cfd6-950a-4b66-8b52-aa36300c0a46.1648f728f82d1908], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.2"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d5a6cfd6-950a-4b66-8b52-aa36300c0a46.1648f729022d3409], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.2" in 167.75421ms]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d5a6cfd6-950a-4b66-8b52-aa36300c0a46.1648f72905d73180], Reason = [Created], Message = [Created container filler-pod-d5a6cfd6-950a-4b66-8b52-aa36300c0a46]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d5a6cfd6-950a-4b66-8b52-aa36300c0a46.1648f72908e8960a], Reason = [Started], Message = [Started container filler-pod-d5a6cfd6-950a-4b66-8b52-aa36300c0a46]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f9da8c27-0762-4f88-be6c-190ee2b9c1aa.1648f728dbcdb5ed], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8146/filler-pod-f9da8c27-0762-4f88-be6c-190ee2b9c1aa to ip-172-31-68-240]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f9da8c27-0762-4f88-be6c-190ee2b9c1aa.1648f728f8c151d2], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.2"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f9da8c27-0762-4f88-be6c-190ee2b9c1aa.1648f7290326dd94], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.2" in 174.403603ms]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f9da8c27-0762-4f88-be6c-190ee2b9c1aa.1648f72906d0b918], Reason = [Created], Message = [Created container filler-pod-f9da8c27-0762-4f88-be6c-190ee2b9c1aa]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f9da8c27-0762-4f88-be6c-190ee2b9c1aa.1648f7290a71db08], Reason = [Started], Message = [Started container filler-pod-f9da8c27-0762-4f88-be6c-190ee2b9c1aa]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1648f72954c0f147], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1648f7295549702b], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node ip-172-31-9-169
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-31-20-145
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-31-68-240
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 16:57:05.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8146" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
â€¢{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":305,"completed":2,"skipped":22,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 16:57:05.085: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8976
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 16:57:12.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8976" for this suite.

â€¢ [SLOW TEST:7.168 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":305,"completed":3,"skipped":29,"failed":0}
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 16:57:12.253: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-572
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 16:57:12.419: INFO: Create a RollingUpdate DaemonSet
Nov 19 16:57:12.426: INFO: Check that daemon pods launch on every node of the cluster
Nov 19 16:57:12.432: INFO: Number of nodes with available pods: 0
Nov 19 16:57:12.432: INFO: Node ip-172-31-20-145 is running more than one daemon pod
Nov 19 16:57:13.438: INFO: Number of nodes with available pods: 1
Nov 19 16:57:13.438: INFO: Node ip-172-31-20-145 is running more than one daemon pod
Nov 19 16:57:14.441: INFO: Number of nodes with available pods: 1
Nov 19 16:57:14.441: INFO: Node ip-172-31-20-145 is running more than one daemon pod
Nov 19 16:57:15.440: INFO: Number of nodes with available pods: 1
Nov 19 16:57:15.440: INFO: Node ip-172-31-20-145 is running more than one daemon pod
Nov 19 16:57:16.439: INFO: Number of nodes with available pods: 1
Nov 19 16:57:16.439: INFO: Node ip-172-31-20-145 is running more than one daemon pod
Nov 19 16:57:17.437: INFO: Number of nodes with available pods: 1
Nov 19 16:57:17.437: INFO: Node ip-172-31-20-145 is running more than one daemon pod
Nov 19 16:57:18.440: INFO: Number of nodes with available pods: 2
Nov 19 16:57:18.440: INFO: Node ip-172-31-68-240 is running more than one daemon pod
Nov 19 16:57:19.439: INFO: Number of nodes with available pods: 3
Nov 19 16:57:19.439: INFO: Number of running nodes: 3, number of available pods: 3
Nov 19 16:57:19.439: INFO: Update the DaemonSet to trigger a rollout
Nov 19 16:57:19.447: INFO: Updating DaemonSet daemon-set
Nov 19 16:57:25.460: INFO: Roll back the DaemonSet before rollout is complete
Nov 19 16:57:25.468: INFO: Updating DaemonSet daemon-set
Nov 19 16:57:25.468: INFO: Make sure DaemonSet rollback is complete
Nov 19 16:57:25.471: INFO: Wrong image for pod: daemon-set-dbjx6. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Nov 19 16:57:25.471: INFO: Pod daemon-set-dbjx6 is not available
Nov 19 16:57:26.478: INFO: Wrong image for pod: daemon-set-dbjx6. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Nov 19 16:57:26.478: INFO: Pod daemon-set-dbjx6 is not available
Nov 19 16:57:27.478: INFO: Wrong image for pod: daemon-set-dbjx6. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Nov 19 16:57:27.478: INFO: Pod daemon-set-dbjx6 is not available
Nov 19 16:57:28.479: INFO: Pod daemon-set-7cg5t is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-572, will wait for the garbage collector to delete the pods
Nov 19 16:57:28.549: INFO: Deleting DaemonSet.extensions daemon-set took: 8.365309ms
Nov 19 16:57:28.649: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.141508ms
Nov 19 16:57:35.052: INFO: Number of nodes with available pods: 0
Nov 19 16:57:35.052: INFO: Number of running nodes: 0, number of available pods: 0
Nov 19 16:57:35.056: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-572/daemonsets","resourceVersion":"3974"},"items":null}

Nov 19 16:57:35.060: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-572/pods","resourceVersion":"3974"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 16:57:35.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-572" for this suite.

â€¢ [SLOW TEST:22.827 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":305,"completed":4,"skipped":34,"failed":0}
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 16:57:35.079: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4902
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 16:57:35.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 create -f - --namespace=kubectl-4902'
Nov 19 16:57:35.697: INFO: stderr: ""
Nov 19 16:57:35.697: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Nov 19 16:57:35.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 create -f - --namespace=kubectl-4902'
Nov 19 16:57:35.908: INFO: stderr: ""
Nov 19 16:57:35.908: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Nov 19 16:57:36.911: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 19 16:57:36.911: INFO: Found 0 / 1
Nov 19 16:57:37.911: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 19 16:57:37.911: INFO: Found 0 / 1
Nov 19 16:57:38.914: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 19 16:57:38.914: INFO: Found 0 / 1
Nov 19 16:57:39.917: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 19 16:57:39.917: INFO: Found 1 / 1
Nov 19 16:57:39.917: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 19 16:57:39.919: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 19 16:57:39.919: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 19 16:57:39.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 describe pod agnhost-primary-npmgq --namespace=kubectl-4902'
Nov 19 16:57:39.990: INFO: stderr: ""
Nov 19 16:57:39.990: INFO: stdout: "Name:         agnhost-primary-npmgq\nNamespace:    kubectl-4902\nPriority:     0\nNode:         ip-172-31-20-145/172.31.20.145\nStart Time:   Thu, 19 Nov 2020 16:57:35 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           10.1.33.6\nIPs:\n  IP:           10.1.33.6\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://ad374a23fed2c1daa58d486d99dae3411e0fb125bd1214bc7e48d928159ccbf8\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 19 Nov 2020 16:57:39 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-sqhb8 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-sqhb8:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-sqhb8\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  4s    default-scheduler  Successfully assigned kubectl-4902/agnhost-primary-npmgq to ip-172-31-20-145\n  Normal  Pulling    3s    kubelet            Pulling image \"k8s.gcr.io/e2e-test-images/agnhost:2.20\"\n  Normal  Pulled     1s    kubelet            Successfully pulled image \"k8s.gcr.io/e2e-test-images/agnhost:2.20\" in 2.245350683s\n  Normal  Created    0s    kubelet            Created container agnhost-primary\n  Normal  Started    0s    kubelet            Started container agnhost-primary\n"
Nov 19 16:57:39.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 describe rc agnhost-primary --namespace=kubectl-4902'
Nov 19 16:57:40.062: INFO: stderr: ""
Nov 19 16:57:40.062: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-4902\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  5s    replication-controller  Created pod: agnhost-primary-npmgq\n"
Nov 19 16:57:40.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 describe service agnhost-primary --namespace=kubectl-4902'
Nov 19 16:57:40.126: INFO: stderr: ""
Nov 19 16:57:40.126: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-4902\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP:                10.152.183.6\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.1.33.6:6379\nSession Affinity:  None\nEvents:            <none>\n"
Nov 19 16:57:40.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 describe node ip-172-31-20-145'
Nov 19 16:57:40.211: INFO: stderr: ""
Nov 19 16:57:40.211: INFO: stdout: "Name:               ip-172-31-20-145\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    juju-application=kubernetes-worker\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-20-145\n                    kubernetes.io/os=linux\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 19 Nov 2020 16:39:52 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-20-145\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 19 Nov 2020 16:57:39 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 19 Nov 2020 16:57:38 +0000   Thu, 19 Nov 2020 16:39:52 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 19 Nov 2020 16:57:38 +0000   Thu, 19 Nov 2020 16:39:52 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 19 Nov 2020 16:57:38 +0000   Thu, 19 Nov 2020 16:39:52 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 19 Nov 2020 16:57:38 +0000   Thu, 19 Nov 2020 16:40:07 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.31.20.145\n  Hostname:    ip-172-31-20-145\nCapacity:\n  cpu:                4\n  ephemeral-storage:  16197480Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             10365012Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  14927597544\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             10262612Ki\n  pods:               110\nSystem Info:\n  Machine ID:                      ec2a75327264b69ca694a66a8f396c7c\n  System UUID:                     ec2a7532-7264-b69c-a694-a66a8f396c7c\n  Boot ID:                         0ee35574-2808-4a96-9e5c-35988d5b7363\n  Kernel Version:                  5.4.0-1029-aws\n  OS Image:                        Ubuntu 20.04.1 LTS\n  Operating System:                linux\n  Architecture:                    amd64\n  Container Runtime Version:       containerd://1.3.3-0ubuntu2\n  Kubelet Version:                 v1.19.4\n  Kube-Proxy Version:              v1.19.4\nNon-terminated Pods:               (4 in total)\n  Namespace                        Name                                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                        ----                                                ------------  ----------  ---------------  -------------  ---\n  ingress-nginx-kubernetes-worker  nginx-ingress-controller-kubernetes-worker-cshw5    0 (0%)        0 (0%)      0 (0%)           0 (0%)         17m\n  kube-system                      kube-state-metrics-6f586bb967-8gvg6                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         18m\n  kubectl-4902                     agnhost-primary-npmgq                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         5s\n  kubernetes-dashboard             kubernetes-dashboard-64f87676d4-ggs4x               0 (0%)        0 (0%)      0 (0%)           0 (0%)         18m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests  Limits\n  --------           --------  ------\n  cpu                0 (0%)    0 (0%)\n  memory             0 (0%)    0 (0%)\n  ephemeral-storage  0 (0%)    0 (0%)\n  hugepages-1Gi      0 (0%)    0 (0%)\n  hugepages-2Mi      0 (0%)    0 (0%)\nEvents:\n  Type     Reason                   Age                From        Message\n  ----     ------                   ----               ----        -------\n  Normal   Starting                 17m                kubelet     Starting kubelet.\n  Warning  InvalidDiskCapacity      17m                kubelet     invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  17m (x2 over 17m)  kubelet     Node ip-172-31-20-145 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    17m (x2 over 17m)  kubelet     Node ip-172-31-20-145 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     17m (x2 over 17m)  kubelet     Node ip-172-31-20-145 status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  17m                kubelet     Updated Node Allocatable limit across pods\n  Normal   Starting                 17m                kube-proxy  Starting kube-proxy.\n  Normal   Starting                 17m                kubelet     Starting kubelet.\n  Warning  InvalidDiskCapacity      17m                kubelet     invalid capacity 0 on image filesystem\n  Normal   NodeAllocatableEnforced  17m                kubelet     Updated Node Allocatable limit across pods\n  Normal   NodeHasSufficientMemory  17m                kubelet     Node ip-172-31-20-145 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    17m                kubelet     Node ip-172-31-20-145 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     17m                kubelet     Node ip-172-31-20-145 status is now: NodeHasSufficientPID\n  Normal   NodeReady                17m                kubelet     Node ip-172-31-20-145 status is now: NodeReady\n  Normal   Starting                 13m                kube-proxy  Starting kube-proxy.\n  Normal   Starting                 13m                kubelet     Starting kubelet.\n  Warning  InvalidDiskCapacity      13m                kubelet     invalid capacity 0 on image filesystem\n  Normal   NodeAllocatableEnforced  13m                kubelet     Updated Node Allocatable limit across pods\n  Normal   NodeHasSufficientMemory  13m                kubelet     Node ip-172-31-20-145 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    13m                kubelet     Node ip-172-31-20-145 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     13m                kubelet     Node ip-172-31-20-145 status is now: NodeHasSufficientPID\n  Normal   Starting                 8m8s               kube-proxy  Starting kube-proxy.\n  Normal   Starting                 8m3s               kubelet     Starting kubelet.\n  Warning  InvalidDiskCapacity      8m3s               kubelet     invalid capacity 0 on image filesystem\n  Normal   NodeAllocatableEnforced  8m3s               kubelet     Updated Node Allocatable limit across pods\n  Normal   NodeHasSufficientMemory  8m3s               kubelet     Node ip-172-31-20-145 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    8m3s               kubelet     Node ip-172-31-20-145 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     8m3s               kubelet     Node ip-172-31-20-145 status is now: NodeHasSufficientPID\n"
Nov 19 16:57:40.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 describe namespace kubectl-4902'
Nov 19 16:57:40.278: INFO: stderr: ""
Nov 19 16:57:40.278: INFO: stdout: "Name:         kubectl-4902\nLabels:       e2e-framework=kubectl\n              e2e-run=59be4760-c7ea-499c-b9de-f3dc25720742\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 16:57:40.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4902" for this suite.

â€¢ [SLOW TEST:5.207 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":305,"completed":5,"skipped":34,"failed":0}
SS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 16:57:40.287: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1553
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name secret-emptykey-test-6dc1c36e-e722-436d-8b1d-30b8eec890f8
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 16:57:40.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1553" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":305,"completed":6,"skipped":36,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 16:57:40.427: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5350
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-d4f972f7-fc84-4420-8c8a-2f1b96bbdf4b
STEP: Creating a pod to test consume secrets
Nov 19 16:57:40.572: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1fe365d9-d798-43f4-b915-2798367b24de" in namespace "projected-5350" to be "Succeeded or Failed"
Nov 19 16:57:40.576: INFO: Pod "pod-projected-secrets-1fe365d9-d798-43f4-b915-2798367b24de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.752175ms
Nov 19 16:57:42.580: INFO: Pod "pod-projected-secrets-1fe365d9-d798-43f4-b915-2798367b24de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008187053s
Nov 19 16:57:44.584: INFO: Pod "pod-projected-secrets-1fe365d9-d798-43f4-b915-2798367b24de": Phase="Running", Reason="", readiness=true. Elapsed: 4.012095664s
Nov 19 16:57:46.587: INFO: Pod "pod-projected-secrets-1fe365d9-d798-43f4-b915-2798367b24de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015659612s
STEP: Saw pod success
Nov 19 16:57:46.587: INFO: Pod "pod-projected-secrets-1fe365d9-d798-43f4-b915-2798367b24de" satisfied condition "Succeeded or Failed"
Nov 19 16:57:46.591: INFO: Trying to get logs from node ip-172-31-9-169 pod pod-projected-secrets-1fe365d9-d798-43f4-b915-2798367b24de container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 19 16:57:46.619: INFO: Waiting for pod pod-projected-secrets-1fe365d9-d798-43f4-b915-2798367b24de to disappear
Nov 19 16:57:46.622: INFO: Pod pod-projected-secrets-1fe365d9-d798-43f4-b915-2798367b24de no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 16:57:46.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5350" for this suite.

â€¢ [SLOW TEST:6.203 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":7,"skipped":48,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 16:57:46.630: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6325
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Nov 19 16:57:46.771: INFO: Waiting up to 5m0s for pod "pod-63e853b4-b09f-40f5-b44d-dcdea593d656" in namespace "emptydir-6325" to be "Succeeded or Failed"
Nov 19 16:57:46.773: INFO: Pod "pod-63e853b4-b09f-40f5-b44d-dcdea593d656": Phase="Pending", Reason="", readiness=false. Elapsed: 2.277409ms
Nov 19 16:57:48.777: INFO: Pod "pod-63e853b4-b09f-40f5-b44d-dcdea593d656": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005822209s
STEP: Saw pod success
Nov 19 16:57:48.777: INFO: Pod "pod-63e853b4-b09f-40f5-b44d-dcdea593d656" satisfied condition "Succeeded or Failed"
Nov 19 16:57:48.780: INFO: Trying to get logs from node ip-172-31-9-169 pod pod-63e853b4-b09f-40f5-b44d-dcdea593d656 container test-container: <nil>
STEP: delete the pod
Nov 19 16:57:48.796: INFO: Waiting for pod pod-63e853b4-b09f-40f5-b44d-dcdea593d656 to disappear
Nov 19 16:57:48.799: INFO: Pod pod-63e853b4-b09f-40f5-b44d-dcdea593d656 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 16:57:48.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6325" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":8,"skipped":79,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 16:57:48.806: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2906
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Nov 19 16:57:51.476: INFO: Successfully updated pod "labelsupdate4c646e7f-78ca-4117-b15a-f747b018ef28"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 16:57:55.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2906" for this suite.

â€¢ [SLOW TEST:6.697 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":9,"skipped":96,"failed":0}
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 16:57:55.504: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6804
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 16:57:55.649: INFO: The status of Pod test-webserver-a3f58f1e-2554-4e2c-89b1-5bc619572a16 is Pending, waiting for it to be Running (with Ready = true)
Nov 19 16:57:57.654: INFO: The status of Pod test-webserver-a3f58f1e-2554-4e2c-89b1-5bc619572a16 is Running (Ready = false)
Nov 19 16:57:59.653: INFO: The status of Pod test-webserver-a3f58f1e-2554-4e2c-89b1-5bc619572a16 is Running (Ready = false)
Nov 19 16:58:01.652: INFO: The status of Pod test-webserver-a3f58f1e-2554-4e2c-89b1-5bc619572a16 is Running (Ready = false)
Nov 19 16:58:03.654: INFO: The status of Pod test-webserver-a3f58f1e-2554-4e2c-89b1-5bc619572a16 is Running (Ready = false)
Nov 19 16:58:05.654: INFO: The status of Pod test-webserver-a3f58f1e-2554-4e2c-89b1-5bc619572a16 is Running (Ready = false)
Nov 19 16:58:07.652: INFO: The status of Pod test-webserver-a3f58f1e-2554-4e2c-89b1-5bc619572a16 is Running (Ready = false)
Nov 19 16:58:09.653: INFO: The status of Pod test-webserver-a3f58f1e-2554-4e2c-89b1-5bc619572a16 is Running (Ready = false)
Nov 19 16:58:11.652: INFO: The status of Pod test-webserver-a3f58f1e-2554-4e2c-89b1-5bc619572a16 is Running (Ready = false)
Nov 19 16:58:13.653: INFO: The status of Pod test-webserver-a3f58f1e-2554-4e2c-89b1-5bc619572a16 is Running (Ready = false)
Nov 19 16:58:15.653: INFO: The status of Pod test-webserver-a3f58f1e-2554-4e2c-89b1-5bc619572a16 is Running (Ready = true)
Nov 19 16:58:15.657: INFO: Container started at 2020-11-19 16:57:56 +0000 UTC, pod became ready at 2020-11-19 16:58:14 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 16:58:15.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6804" for this suite.

â€¢ [SLOW TEST:20.162 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":305,"completed":10,"skipped":96,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 16:58:15.666: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4689
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Nov 19 16:58:15.797: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 16:58:29.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4689" for this suite.

â€¢ [SLOW TEST:13.794 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":305,"completed":11,"skipped":111,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 16:58:29.460: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1551
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 19 16:58:29.603: INFO: Waiting up to 5m0s for pod "downwardapi-volume-20eb8d28-6031-424c-a81a-7dc255e35dff" in namespace "projected-1551" to be "Succeeded or Failed"
Nov 19 16:58:29.606: INFO: Pod "downwardapi-volume-20eb8d28-6031-424c-a81a-7dc255e35dff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.749752ms
Nov 19 16:58:31.610: INFO: Pod "downwardapi-volume-20eb8d28-6031-424c-a81a-7dc255e35dff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006626947s
STEP: Saw pod success
Nov 19 16:58:31.610: INFO: Pod "downwardapi-volume-20eb8d28-6031-424c-a81a-7dc255e35dff" satisfied condition "Succeeded or Failed"
Nov 19 16:58:31.613: INFO: Trying to get logs from node ip-172-31-20-145 pod downwardapi-volume-20eb8d28-6031-424c-a81a-7dc255e35dff container client-container: <nil>
STEP: delete the pod
Nov 19 16:58:31.640: INFO: Waiting for pod downwardapi-volume-20eb8d28-6031-424c-a81a-7dc255e35dff to disappear
Nov 19 16:58:31.642: INFO: Pod downwardapi-volume-20eb8d28-6031-424c-a81a-7dc255e35dff no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 16:58:31.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1551" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":12,"skipped":113,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 16:58:31.651: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3022
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service nodeport-service with the type=NodePort in namespace services-3022
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3022
STEP: creating replication controller externalsvc in namespace services-3022
I1119 16:58:31.818946      22 runners.go:190] Created replication controller with name: externalsvc, namespace: services-3022, replica count: 2
I1119 16:58:34.869203      22 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Nov 19 16:58:34.888: INFO: Creating new exec pod
Nov 19 16:58:36.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-3022 execpod5w76b -- /bin/sh -x -c nslookup nodeport-service.services-3022.svc.cluster.local'
Nov 19 16:58:37.155: INFO: stderr: "+ nslookup nodeport-service.services-3022.svc.cluster.local\n"
Nov 19 16:58:37.155: INFO: stdout: "Server:\t\t10.152.183.127\nAddress:\t10.152.183.127#53\n\nnodeport-service.services-3022.svc.cluster.local\tcanonical name = externalsvc.services-3022.svc.cluster.local.\nName:\texternalsvc.services-3022.svc.cluster.local\nAddress: 10.152.183.143\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3022, will wait for the garbage collector to delete the pods
Nov 19 16:58:37.217: INFO: Deleting ReplicationController externalsvc took: 8.333445ms
Nov 19 16:58:37.717: INFO: Terminating ReplicationController externalsvc pods took: 500.12797ms
Nov 19 16:58:47.640: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 16:58:47.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3022" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

â€¢ [SLOW TEST:16.010 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":305,"completed":13,"skipped":146,"failed":0}
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 16:58:47.661: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9327
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 16:58:47.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9327" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":305,"completed":14,"skipped":147,"failed":0}
SSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 16:58:47.843: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5286
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-c9db937a-a352-4c9c-a39b-60b2cff35359 in namespace container-probe-5286
Nov 19 16:58:49.997: INFO: Started pod busybox-c9db937a-a352-4c9c-a39b-60b2cff35359 in namespace container-probe-5286
STEP: checking the pod's current state and verifying that restartCount is present
Nov 19 16:58:50.000: INFO: Initial restart count of pod busybox-c9db937a-a352-4c9c-a39b-60b2cff35359 is 0
Nov 19 16:59:44.121: INFO: Restart count of pod container-probe-5286/busybox-c9db937a-a352-4c9c-a39b-60b2cff35359 is now 1 (54.120783862s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 16:59:44.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5286" for this suite.

â€¢ [SLOW TEST:56.296 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":15,"skipped":153,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 16:59:44.139: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8387
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-lvsn
STEP: Creating a pod to test atomic-volume-subpath
Nov 19 16:59:44.293: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-lvsn" in namespace "subpath-8387" to be "Succeeded or Failed"
Nov 19 16:59:44.298: INFO: Pod "pod-subpath-test-configmap-lvsn": Phase="Pending", Reason="", readiness=false. Elapsed: 5.302977ms
Nov 19 16:59:46.303: INFO: Pod "pod-subpath-test-configmap-lvsn": Phase="Running", Reason="", readiness=true. Elapsed: 2.010219166s
Nov 19 16:59:48.307: INFO: Pod "pod-subpath-test-configmap-lvsn": Phase="Running", Reason="", readiness=true. Elapsed: 4.013988575s
Nov 19 16:59:50.312: INFO: Pod "pod-subpath-test-configmap-lvsn": Phase="Running", Reason="", readiness=true. Elapsed: 6.018619298s
Nov 19 16:59:52.317: INFO: Pod "pod-subpath-test-configmap-lvsn": Phase="Running", Reason="", readiness=true. Elapsed: 8.023632061s
Nov 19 16:59:54.321: INFO: Pod "pod-subpath-test-configmap-lvsn": Phase="Running", Reason="", readiness=true. Elapsed: 10.02787963s
Nov 19 16:59:56.325: INFO: Pod "pod-subpath-test-configmap-lvsn": Phase="Running", Reason="", readiness=true. Elapsed: 12.031955276s
Nov 19 16:59:58.330: INFO: Pod "pod-subpath-test-configmap-lvsn": Phase="Running", Reason="", readiness=true. Elapsed: 14.036648277s
Nov 19 17:00:00.334: INFO: Pod "pod-subpath-test-configmap-lvsn": Phase="Running", Reason="", readiness=true. Elapsed: 16.040866773s
Nov 19 17:00:02.339: INFO: Pod "pod-subpath-test-configmap-lvsn": Phase="Running", Reason="", readiness=true. Elapsed: 18.045761166s
Nov 19 17:00:04.343: INFO: Pod "pod-subpath-test-configmap-lvsn": Phase="Running", Reason="", readiness=true. Elapsed: 20.050099156s
Nov 19 17:00:06.347: INFO: Pod "pod-subpath-test-configmap-lvsn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.053748494s
STEP: Saw pod success
Nov 19 17:00:06.347: INFO: Pod "pod-subpath-test-configmap-lvsn" satisfied condition "Succeeded or Failed"
Nov 19 17:00:06.349: INFO: Trying to get logs from node ip-172-31-20-145 pod pod-subpath-test-configmap-lvsn container test-container-subpath-configmap-lvsn: <nil>
STEP: delete the pod
Nov 19 17:00:06.373: INFO: Waiting for pod pod-subpath-test-configmap-lvsn to disappear
Nov 19 17:00:06.375: INFO: Pod pod-subpath-test-configmap-lvsn no longer exists
STEP: Deleting pod pod-subpath-test-configmap-lvsn
Nov 19 17:00:06.375: INFO: Deleting pod "pod-subpath-test-configmap-lvsn" in namespace "subpath-8387"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:00:06.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8387" for this suite.

â€¢ [SLOW TEST:22.246 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":305,"completed":16,"skipped":189,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:00:06.386: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2320
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2320
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-2320
I1119 17:00:06.542883      22 runners.go:190] Created replication controller with name: externalname-service, namespace: services-2320, replica count: 2
I1119 17:00:09.593134      22 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 19 17:00:09.593: INFO: Creating new exec pod
Nov 19 17:00:12.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-2320 execpodmfbrs -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Nov 19 17:00:12.757: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Nov 19 17:00:12.757: INFO: stdout: ""
Nov 19 17:00:12.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-2320 execpodmfbrs -- /bin/sh -x -c nc -zv -t -w 2 10.152.183.4 80'
Nov 19 17:00:12.879: INFO: stderr: "+ nc -zv -t -w 2 10.152.183.4 80\nConnection to 10.152.183.4 80 port [tcp/http] succeeded!\n"
Nov 19 17:00:12.879: INFO: stdout: ""
Nov 19 17:00:12.879: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:00:12.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2320" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

â€¢ [SLOW TEST:6.524 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":305,"completed":17,"skipped":203,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:00:12.911: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3346
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-90130dcc-57e9-465f-b053-7bf0901d4dfc
STEP: Creating a pod to test consume configMaps
Nov 19 17:00:13.059: INFO: Waiting up to 5m0s for pod "pod-configmaps-f4c1db25-f764-464b-9ae9-519e33993269" in namespace "configmap-3346" to be "Succeeded or Failed"
Nov 19 17:00:13.064: INFO: Pod "pod-configmaps-f4c1db25-f764-464b-9ae9-519e33993269": Phase="Pending", Reason="", readiness=false. Elapsed: 5.269224ms
Nov 19 17:00:15.069: INFO: Pod "pod-configmaps-f4c1db25-f764-464b-9ae9-519e33993269": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010209367s
STEP: Saw pod success
Nov 19 17:00:15.069: INFO: Pod "pod-configmaps-f4c1db25-f764-464b-9ae9-519e33993269" satisfied condition "Succeeded or Failed"
Nov 19 17:00:15.071: INFO: Trying to get logs from node ip-172-31-9-169 pod pod-configmaps-f4c1db25-f764-464b-9ae9-519e33993269 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 19 17:00:15.096: INFO: Waiting for pod pod-configmaps-f4c1db25-f764-464b-9ae9-519e33993269 to disappear
Nov 19 17:00:15.099: INFO: Pod pod-configmaps-f4c1db25-f764-464b-9ae9-519e33993269 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:00:15.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3346" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":18,"skipped":240,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:00:15.107: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2905
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Nov 19 17:00:15.249: INFO: Waiting up to 5m0s for pod "downward-api-4fdd6681-4de1-4570-b6b4-eacb54c7a4c2" in namespace "downward-api-2905" to be "Succeeded or Failed"
Nov 19 17:00:15.252: INFO: Pod "downward-api-4fdd6681-4de1-4570-b6b4-eacb54c7a4c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.613295ms
Nov 19 17:00:17.257: INFO: Pod "downward-api-4fdd6681-4de1-4570-b6b4-eacb54c7a4c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007236328s
STEP: Saw pod success
Nov 19 17:00:17.257: INFO: Pod "downward-api-4fdd6681-4de1-4570-b6b4-eacb54c7a4c2" satisfied condition "Succeeded or Failed"
Nov 19 17:00:17.260: INFO: Trying to get logs from node ip-172-31-68-240 pod downward-api-4fdd6681-4de1-4570-b6b4-eacb54c7a4c2 container dapi-container: <nil>
STEP: delete the pod
Nov 19 17:00:17.291: INFO: Waiting for pod downward-api-4fdd6681-4de1-4570-b6b4-eacb54c7a4c2 to disappear
Nov 19 17:00:17.294: INFO: Pod downward-api-4fdd6681-4de1-4570-b6b4-eacb54c7a4c2 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:00:17.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2905" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":305,"completed":19,"skipped":266,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:00:17.306: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-1280
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a Namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nspatchtest-7136305c-a841-4ce0-bab3-b94a6156c2dc-5111
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:00:17.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1280" for this suite.
STEP: Destroying namespace "nspatchtest-7136305c-a841-4ce0-bab3-b94a6156c2dc-5111" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":305,"completed":20,"skipped":270,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:00:17.598: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-8721
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 17:00:17.747: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:00:18.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8721" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":305,"completed":21,"skipped":301,"failed":0}
SSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:00:18.779: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1289
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 17:00:18.923: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Nov 19 17:00:23.928: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 19 17:00:23.928: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Nov 19 17:00:23.951: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1289 /apis/apps/v1/namespaces/deployment-1289/deployments/test-cleanup-deployment 9ee0a03f-03f9-4f0c-a5d1-071f4fef2de9 5078 1 2020-11-19 17:00:23 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2020-11-19 17:00:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000a9adb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Nov 19 17:00:23.956: INFO: New ReplicaSet "test-cleanup-deployment-5d446bdd47" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5d446bdd47  deployment-1289 /apis/apps/v1/namespaces/deployment-1289/replicasets/test-cleanup-deployment-5d446bdd47 c4b07c90-2617-423f-bcd2-7298835faf0c 5081 1 2020-11-19 17:00:23 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 9ee0a03f-03f9-4f0c-a5d1-071f4fef2de9 0xc000a9b2c7 0xc000a9b2c8}] []  [{kube-controller-manager Update apps/v1 2020-11-19 17:00:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9ee0a03f-03f9-4f0c-a5d1-071f4fef2de9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5d446bdd47,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000a9b358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 19 17:00:23.956: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Nov 19 17:00:23.956: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-1289 /apis/apps/v1/namespaces/deployment-1289/replicasets/test-cleanup-controller 70f604d1-810f-45e9-b1d8-f5c1f466a970 5080 1 2020-11-19 17:00:18 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 9ee0a03f-03f9-4f0c-a5d1-071f4fef2de9 0xc000a9b1bf 0xc000a9b1d0}] []  [{e2e.test Update apps/v1 2020-11-19 17:00:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-11-19 17:00:23 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"9ee0a03f-03f9-4f0c-a5d1-071f4fef2de9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000a9b268 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 19 17:00:23.963: INFO: Pod "test-cleanup-controller-n5ctl" is available:
&Pod{ObjectMeta:{test-cleanup-controller-n5ctl test-cleanup-controller- deployment-1289 /api/v1/namespaces/deployment-1289/pods/test-cleanup-controller-n5ctl efa52a77-f318-4c55-90a0-ab285d3d98cb 5031 0 2020-11-19 17:00:18 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-controller 70f604d1-810f-45e9-b1d8-f5c1f466a970 0xc000a9b97f 0xc000a9b990}] []  [{kube-controller-manager Update v1 2020-11-19 17:00:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70f604d1-810f-45e9-b1d8-f5c1f466a970\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-19 17:00:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.33.14\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6ptm6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6ptm6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6ptm6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-20-145,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.20.145,PodIP:10.1.33.14,StartTime:2020-11-19 17:00:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-19 17:00:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://f55c6fa54b8cf936cfcb7b39e7e15a974dcc64f7b8367a5f51e521fc20dd423b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.33.14,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:00:23.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1289" for this suite.

â€¢ [SLOW TEST:5.199 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":305,"completed":22,"skipped":307,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:00:23.979: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9941
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 17:00:24.117: INFO: Creating deployment "webserver-deployment"
Nov 19 17:00:24.123: INFO: Waiting for observed generation 1
Nov 19 17:00:26.130: INFO: Waiting for all required pods to come up
Nov 19 17:00:26.134: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Nov 19 17:00:26.134: INFO: Waiting for deployment "webserver-deployment" to complete
Nov 19 17:00:26.140: INFO: Updating deployment "webserver-deployment" with a non-existent image
Nov 19 17:00:26.147: INFO: Updating deployment webserver-deployment
Nov 19 17:00:26.147: INFO: Waiting for observed generation 2
Nov 19 17:00:28.154: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Nov 19 17:00:28.157: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Nov 19 17:00:28.159: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Nov 19 17:00:28.166: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Nov 19 17:00:28.166: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Nov 19 17:00:28.169: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Nov 19 17:00:28.173: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Nov 19 17:00:28.173: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Nov 19 17:00:28.182: INFO: Updating deployment webserver-deployment
Nov 19 17:00:28.182: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Nov 19 17:00:28.189: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Nov 19 17:00:28.191: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Nov 19 17:00:28.200: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-9941 /apis/apps/v1/namespaces/deployment-9941/deployments/webserver-deployment 3f25dd39-9279-4d0c-b35e-5c821d54e989 5319 3 2020-11-19 17:00:24 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{kube-controller-manager Update apps/v1 2020-11-19 17:00:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}} {e2e.test Update apps/v1 2020-11-19 17:00:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002fa4f58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-11-19 17:00:25 +0000 UTC,LastTransitionTime:2020-11-19 17:00:25 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2020-11-19 17:00:26 +0000 UTC,LastTransitionTime:2020-11-19 17:00:24 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Nov 19 17:00:28.205: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-9941 /apis/apps/v1/namespaces/deployment-9941/replicasets/webserver-deployment-795d758f88 e76c6bee-6102-4af5-9961-836556754c9c 5323 3 2020-11-19 17:00:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 3f25dd39-9279-4d0c-b35e-5c821d54e989 0xc002fa5717 0xc002fa5718}] []  [{kube-controller-manager Update apps/v1 2020-11-19 17:00:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3f25dd39-9279-4d0c-b35e-5c821d54e989\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002fa5838 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 19 17:00:28.205: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Nov 19 17:00:28.205: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-9941 /apis/apps/v1/namespaces/deployment-9941/replicasets/webserver-deployment-dd94f59b7 9d08fdde-2b9d-4599-80e6-2a8182beeccc 5320 3 2020-11-19 17:00:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 3f25dd39-9279-4d0c-b35e-5c821d54e989 0xc002fa58f7 0xc002fa58f8}] []  [{kube-controller-manager Update apps/v1 2020-11-19 17:00:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3f25dd39-9279-4d0c-b35e-5c821d54e989\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002fa5988 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Nov 19 17:00:28.209: INFO: Pod "webserver-deployment-795d758f88-5lqlp" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-5lqlp webserver-deployment-795d758f88- deployment-9941 /api/v1/namespaces/deployment-9941/pods/webserver-deployment-795d758f88-5lqlp 1d7bca13-04ef-4631-b694-88002be8f6f2 5308 0 2020-11-19 17:00:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e76c6bee-6102-4af5-9961-836556754c9c 0xc00395e1f0 0xc00395e1f1}] []  [{kube-controller-manager Update v1 2020-11-19 17:00:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e76c6bee-6102-4af5-9961-836556754c9c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-19 17:00:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.31.19\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rtxn5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rtxn5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rtxn5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-9-169,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.9.169,PodIP:10.1.31.19,StartTime:2020-11-19 17:00:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.31.19,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 19 17:00:28.209: INFO: Pod "webserver-deployment-795d758f88-bmx4b" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-bmx4b webserver-deployment-795d758f88- deployment-9941 /api/v1/namespaces/deployment-9941/pods/webserver-deployment-795d758f88-bmx4b 89da4dfc-c411-4d5f-9084-16c04ad2bacb 5305 0 2020-11-19 17:00:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e76c6bee-6102-4af5-9961-836556754c9c 0xc00395e3c0 0xc00395e3c1}] []  [{kube-controller-manager Update v1 2020-11-19 17:00:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e76c6bee-6102-4af5-9961-836556754c9c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-19 17:00:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.31.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rtxn5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rtxn5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rtxn5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-9-169,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.9.169,PodIP:10.1.31.20,StartTime:2020-11-19 17:00:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.31.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 19 17:00:28.209: INFO: Pod "webserver-deployment-795d758f88-ktfrt" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-ktfrt webserver-deployment-795d758f88- deployment-9941 /api/v1/namespaces/deployment-9941/pods/webserver-deployment-795d758f88-ktfrt ced1de5c-1d2b-4515-a413-dc2c841dd400 5302 0 2020-11-19 17:00:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e76c6bee-6102-4af5-9961-836556754c9c 0xc00395e590 0xc00395e591}] []  [{kube-controller-manager Update v1 2020-11-19 17:00:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e76c6bee-6102-4af5-9961-836556754c9c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-19 17:00:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.33.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rtxn5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rtxn5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rtxn5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-20-145,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.20.145,PodIP:10.1.33.20,StartTime:2020-11-19 17:00:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.33.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 19 17:00:28.210: INFO: Pod "webserver-deployment-795d758f88-ns98g" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-ns98g webserver-deployment-795d758f88- deployment-9941 /api/v1/namespaces/deployment-9941/pods/webserver-deployment-795d758f88-ns98g 92dcf117-62b3-48b1-9a90-9847a4881fbc 5311 0 2020-11-19 17:00:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e76c6bee-6102-4af5-9961-836556754c9c 0xc00395e760 0xc00395e761}] []  [{kube-controller-manager Update v1 2020-11-19 17:00:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e76c6bee-6102-4af5-9961-836556754c9c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-19 17:00:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.33.19\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rtxn5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rtxn5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rtxn5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-20-145,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.20.145,PodIP:10.1.33.19,StartTime:2020-11-19 17:00:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.33.19,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 19 17:00:28.210: INFO: Pod "webserver-deployment-795d758f88-rp9xk" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-rp9xk webserver-deployment-795d758f88- deployment-9941 /api/v1/namespaces/deployment-9941/pods/webserver-deployment-795d758f88-rp9xk 29fc5ca3-9adb-4c39-9dc4-613f0de616be 5297 0 2020-11-19 17:00:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e76c6bee-6102-4af5-9961-836556754c9c 0xc00395ea30 0xc00395ea31}] []  [{kube-controller-manager Update v1 2020-11-19 17:00:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e76c6bee-6102-4af5-9961-836556754c9c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-19 17:00:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.27.15\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rtxn5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rtxn5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rtxn5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-68-240,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.68.240,PodIP:10.1.27.15,StartTime:2020-11-19 17:00:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.27.15,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 19 17:00:28.210: INFO: Pod "webserver-deployment-dd94f59b7-728x7" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-728x7 webserver-deployment-dd94f59b7- deployment-9941 /api/v1/namespaces/deployment-9941/pods/webserver-deployment-dd94f59b7-728x7 f05c07e0-ca9d-45ce-82d3-4320e16a85fb 5227 0 2020-11-19 17:00:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 9d08fdde-2b9d-4599-80e6-2a8182beeccc 0xc00395ed00 0xc00395ed01}] []  [{kube-controller-manager Update v1 2020-11-19 17:00:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d08fdde-2b9d-4599-80e6-2a8182beeccc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-19 17:00:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.27.12\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rtxn5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rtxn5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rtxn5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-68-240,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.68.240,PodIP:10.1.27.12,StartTime:2020-11-19 17:00:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-19 17:00:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://4cc052c6b20f723c1c0670150cf6848ac2cf6374486c5b61cc6e764a693991a7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.27.12,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 19 17:00:28.210: INFO: Pod "webserver-deployment-dd94f59b7-7fqfx" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-7fqfx webserver-deployment-dd94f59b7- deployment-9941 /api/v1/namespaces/deployment-9941/pods/webserver-deployment-dd94f59b7-7fqfx 42024099-cf7c-43ce-bd13-1e4b735c8386 5202 0 2020-11-19 17:00:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 9d08fdde-2b9d-4599-80e6-2a8182beeccc 0xc00395eff0 0xc00395eff1}] []  [{kube-controller-manager Update v1 2020-11-19 17:00:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d08fdde-2b9d-4599-80e6-2a8182beeccc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-19 17:00:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.31.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rtxn5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rtxn5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rtxn5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-9-169,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.9.169,PodIP:10.1.31.17,StartTime:2020-11-19 17:00:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-19 17:00:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://8432ff52b8684a4efe734973f2a6ae3affd1ca7928395f31a1f6eba5c510406a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.31.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 19 17:00:28.210: INFO: Pod "webserver-deployment-dd94f59b7-bj6zf" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-bj6zf webserver-deployment-dd94f59b7- deployment-9941 /api/v1/namespaces/deployment-9941/pods/webserver-deployment-dd94f59b7-bj6zf bf7ccb9b-674d-42f8-9df3-2e5eb4830379 5224 0 2020-11-19 17:00:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 9d08fdde-2b9d-4599-80e6-2a8182beeccc 0xc00395f250 0xc00395f251}] []  [{kube-controller-manager Update v1 2020-11-19 17:00:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d08fdde-2b9d-4599-80e6-2a8182beeccc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-19 17:00:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.27.14\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rtxn5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rtxn5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rtxn5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-68-240,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.68.240,PodIP:10.1.27.14,StartTime:2020-11-19 17:00:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-19 17:00:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://e328935983aecf313198c7248ddf7a9ae48481554cc561e236f6426e760e9c72,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.27.14,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 19 17:00:28.210: INFO: Pod "webserver-deployment-dd94f59b7-cn8bq" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-cn8bq webserver-deployment-dd94f59b7- deployment-9941 /api/v1/namespaces/deployment-9941/pods/webserver-deployment-dd94f59b7-cn8bq 5f3744a7-7b21-4b75-ad9e-f9d55fde9044 5229 0 2020-11-19 17:00:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 9d08fdde-2b9d-4599-80e6-2a8182beeccc 0xc00395f580 0xc00395f581}] []  [{kube-controller-manager Update v1 2020-11-19 17:00:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d08fdde-2b9d-4599-80e6-2a8182beeccc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-19 17:00:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.27.13\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rtxn5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rtxn5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rtxn5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-68-240,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.68.240,PodIP:10.1.27.13,StartTime:2020-11-19 17:00:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-19 17:00:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://874db2d70eeba488f8ec9208a9c61a1ecb76a41d7cd24f0d3e2ca06a73cf5b2b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.27.13,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 19 17:00:28.210: INFO: Pod "webserver-deployment-dd94f59b7-d68bz" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-d68bz webserver-deployment-dd94f59b7- deployment-9941 /api/v1/namespaces/deployment-9941/pods/webserver-deployment-dd94f59b7-d68bz ab7ad1b1-b192-4952-8df4-fdbdb408e8b7 5218 0 2020-11-19 17:00:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 9d08fdde-2b9d-4599-80e6-2a8182beeccc 0xc00395f750 0xc00395f751}] []  [{kube-controller-manager Update v1 2020-11-19 17:00:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d08fdde-2b9d-4599-80e6-2a8182beeccc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-19 17:00:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.31.16\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rtxn5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rtxn5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rtxn5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-9-169,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.9.169,PodIP:10.1.31.16,StartTime:2020-11-19 17:00:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-19 17:00:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://a9f4af3c2411edbfa2ca12f2cb0c69b0a840e3acf34c6fef74510bf13da35952,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.31.16,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 19 17:00:28.211: INFO: Pod "webserver-deployment-dd94f59b7-db4hw" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-db4hw webserver-deployment-dd94f59b7- deployment-9941 /api/v1/namespaces/deployment-9941/pods/webserver-deployment-dd94f59b7-db4hw 7b71f523-92e6-4a60-bf0f-f33f95c5a6e8 5215 0 2020-11-19 17:00:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 9d08fdde-2b9d-4599-80e6-2a8182beeccc 0xc00395fa70 0xc00395fa71}] []  [{kube-controller-manager Update v1 2020-11-19 17:00:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d08fdde-2b9d-4599-80e6-2a8182beeccc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-19 17:00:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.33.15\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rtxn5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rtxn5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rtxn5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-20-145,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.20.145,PodIP:10.1.33.15,StartTime:2020-11-19 17:00:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-19 17:00:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://fbda3451f7dfc4452b68ce40c731e3dfebd967dc85d10ed8930793714701b81c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.33.15,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 19 17:00:28.211: INFO: Pod "webserver-deployment-dd94f59b7-f9fx7" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-f9fx7 webserver-deployment-dd94f59b7- deployment-9941 /api/v1/namespaces/deployment-9941/pods/webserver-deployment-dd94f59b7-f9fx7 b6eef3d7-269c-486a-b605-7aa540f084a9 5188 0 2020-11-19 17:00:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 9d08fdde-2b9d-4599-80e6-2a8182beeccc 0xc00395fc20 0xc00395fc21}] []  [{kube-controller-manager Update v1 2020-11-19 17:00:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d08fdde-2b9d-4599-80e6-2a8182beeccc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-19 17:00:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.33.16\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rtxn5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rtxn5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rtxn5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-20-145,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.20.145,PodIP:10.1.33.16,StartTime:2020-11-19 17:00:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-19 17:00:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://b6dc3efa777b9e4634d5a48b2484597514006fd1d720835de152095e1c736157,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.33.16,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 19 17:00:28.211: INFO: Pod "webserver-deployment-dd94f59b7-gnxsf" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-gnxsf webserver-deployment-dd94f59b7- deployment-9941 /api/v1/namespaces/deployment-9941/pods/webserver-deployment-dd94f59b7-gnxsf 2429be08-1ddd-4689-b4f1-2907640a47ca 5324 0 2020-11-19 17:00:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 9d08fdde-2b9d-4599-80e6-2a8182beeccc 0xc00395fdc0 0xc00395fdc1}] []  [{kube-controller-manager Update v1 2020-11-19 17:00:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d08fdde-2b9d-4599-80e6-2a8182beeccc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rtxn5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rtxn5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rtxn5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-20-145,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 19 17:00:28.211: INFO: Pod "webserver-deployment-dd94f59b7-v86f8" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-v86f8 webserver-deployment-dd94f59b7- deployment-9941 /api/v1/namespaces/deployment-9941/pods/webserver-deployment-dd94f59b7-v86f8 661450f2-a9e2-4bfb-ad7f-8d91da7479f6 5199 0 2020-11-19 17:00:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 9d08fdde-2b9d-4599-80e6-2a8182beeccc 0xc00395fef0 0xc00395fef1}] []  [{kube-controller-manager Update v1 2020-11-19 17:00:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d08fdde-2b9d-4599-80e6-2a8182beeccc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-19 17:00:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.31.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rtxn5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rtxn5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rtxn5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-9-169,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:00:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.9.169,PodIP:10.1.31.18,StartTime:2020-11-19 17:00:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-19 17:00:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://7aee180a77e1a4f8de956da9d6df6e1c44adf6625021182c11fb3a825a538dec,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.31.18,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:00:28.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9941" for this suite.
â€¢{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":305,"completed":23,"skipped":316,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:00:28.251: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7510
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 19 17:00:29.046: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 19 17:00:32.068: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:00:32.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7510" for this suite.
STEP: Destroying namespace "webhook-7510-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":305,"completed":24,"skipped":332,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:00:32.191: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4054
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 17:02:32.345: INFO: Deleting pod "var-expansion-394aa55b-6f64-4f34-ae95-9fe2d8456fa8" in namespace "var-expansion-4054"
Nov 19 17:02:32.355: INFO: Wait up to 5m0s for pod "var-expansion-394aa55b-6f64-4f34-ae95-9fe2d8456fa8" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:02:34.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4054" for this suite.

â€¢ [SLOW TEST:122.180 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":305,"completed":25,"skipped":360,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:02:34.371: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-5183
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Nov 19 17:02:34.520: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 19 17:03:34.533: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Nov 19 17:03:34.556: INFO: Created pod: pod0-sched-preemption-low-priority
Nov 19 17:03:34.570: INFO: Created pod: pod1-sched-preemption-medium-priority
Nov 19 17:03:34.601: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:03:54.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5183" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

â€¢ [SLOW TEST:80.318 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":305,"completed":26,"skipped":362,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:03:54.689: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3545
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:04:10.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3545" for this suite.

â€¢ [SLOW TEST:16.199 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":305,"completed":27,"skipped":380,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:04:10.888: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7414
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Nov 19 17:04:11.037: INFO: Waiting up to 5m0s for pod "pod-966e036e-5dd9-4a89-9237-245c2a15b081" in namespace "emptydir-7414" to be "Succeeded or Failed"
Nov 19 17:04:11.041: INFO: Pod "pod-966e036e-5dd9-4a89-9237-245c2a15b081": Phase="Pending", Reason="", readiness=false. Elapsed: 4.167304ms
Nov 19 17:04:13.045: INFO: Pod "pod-966e036e-5dd9-4a89-9237-245c2a15b081": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00803853s
STEP: Saw pod success
Nov 19 17:04:13.045: INFO: Pod "pod-966e036e-5dd9-4a89-9237-245c2a15b081" satisfied condition "Succeeded or Failed"
Nov 19 17:04:13.048: INFO: Trying to get logs from node ip-172-31-20-145 pod pod-966e036e-5dd9-4a89-9237-245c2a15b081 container test-container: <nil>
STEP: delete the pod
Nov 19 17:04:13.074: INFO: Waiting for pod pod-966e036e-5dd9-4a89-9237-245c2a15b081 to disappear
Nov 19 17:04:13.077: INFO: Pod pod-966e036e-5dd9-4a89-9237-245c2a15b081 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:04:13.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7414" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":28,"skipped":388,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:04:13.086: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-937
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Nov 19 17:04:13.255: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-937 /api/v1/namespaces/watch-937/configmaps/e2e-watch-test-resource-version d563e638-0f49-404a-bc8c-25d74f078ad4 6418 0 2020-11-19 17:04:13 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-11-19 17:04:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 19 17:04:13.255: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-937 /api/v1/namespaces/watch-937/configmaps/e2e-watch-test-resource-version d563e638-0f49-404a-bc8c-25d74f078ad4 6419 0 2020-11-19 17:04:13 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-11-19 17:04:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:04:13.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-937" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":305,"completed":29,"skipped":460,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:04:13.263: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6135
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 19 17:04:13.406: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eb45cb4f-79ff-4436-9e65-a37de0b9ff57" in namespace "downward-api-6135" to be "Succeeded or Failed"
Nov 19 17:04:13.408: INFO: Pod "downwardapi-volume-eb45cb4f-79ff-4436-9e65-a37de0b9ff57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.56463ms
Nov 19 17:04:15.413: INFO: Pod "downwardapi-volume-eb45cb4f-79ff-4436-9e65-a37de0b9ff57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00730545s
STEP: Saw pod success
Nov 19 17:04:15.413: INFO: Pod "downwardapi-volume-eb45cb4f-79ff-4436-9e65-a37de0b9ff57" satisfied condition "Succeeded or Failed"
Nov 19 17:04:15.417: INFO: Trying to get logs from node ip-172-31-20-145 pod downwardapi-volume-eb45cb4f-79ff-4436-9e65-a37de0b9ff57 container client-container: <nil>
STEP: delete the pod
Nov 19 17:04:15.436: INFO: Waiting for pod downwardapi-volume-eb45cb4f-79ff-4436-9e65-a37de0b9ff57 to disappear
Nov 19 17:04:15.439: INFO: Pod downwardapi-volume-eb45cb4f-79ff-4436-9e65-a37de0b9ff57 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:04:15.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6135" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":30,"skipped":477,"failed":0}
SSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:04:15.448: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3348
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-e33e27b0-0715-49af-a1fa-df5854bc4911 in namespace container-probe-3348
Nov 19 17:04:17.606: INFO: Started pod busybox-e33e27b0-0715-49af-a1fa-df5854bc4911 in namespace container-probe-3348
STEP: checking the pod's current state and verifying that restartCount is present
Nov 19 17:04:17.609: INFO: Initial restart count of pod busybox-e33e27b0-0715-49af-a1fa-df5854bc4911 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:08:18.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3348" for this suite.

â€¢ [SLOW TEST:242.729 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":31,"skipped":481,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:08:18.178: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-6706
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:08:18.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-6706" for this suite.
â€¢{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":305,"completed":32,"skipped":489,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:08:18.369: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9565
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:08:18.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9565" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":305,"completed":33,"skipped":496,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:08:18.532: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9194
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Nov 19 17:08:18.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 create -f - --namespace=kubectl-9194'
Nov 19 17:08:19.065: INFO: stderr: ""
Nov 19 17:08:19.065: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 19 17:08:19.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9194'
Nov 19 17:08:19.124: INFO: stderr: ""
Nov 19 17:08:19.124: INFO: stdout: "update-demo-nautilus-4cprm update-demo-nautilus-l77qv "
Nov 19 17:08:19.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get pods update-demo-nautilus-4cprm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9194'
Nov 19 17:08:19.181: INFO: stderr: ""
Nov 19 17:08:19.181: INFO: stdout: ""
Nov 19 17:08:19.181: INFO: update-demo-nautilus-4cprm is created but not running
Nov 19 17:08:24.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9194'
Nov 19 17:08:24.241: INFO: stderr: ""
Nov 19 17:08:24.241: INFO: stdout: "update-demo-nautilus-4cprm update-demo-nautilus-l77qv "
Nov 19 17:08:24.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get pods update-demo-nautilus-4cprm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9194'
Nov 19 17:08:24.297: INFO: stderr: ""
Nov 19 17:08:24.297: INFO: stdout: "true"
Nov 19 17:08:24.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get pods update-demo-nautilus-4cprm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9194'
Nov 19 17:08:24.352: INFO: stderr: ""
Nov 19 17:08:24.352: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 19 17:08:24.352: INFO: validating pod update-demo-nautilus-4cprm
Nov 19 17:08:24.356: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 19 17:08:24.356: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 19 17:08:24.356: INFO: update-demo-nautilus-4cprm is verified up and running
Nov 19 17:08:24.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get pods update-demo-nautilus-l77qv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9194'
Nov 19 17:08:24.411: INFO: stderr: ""
Nov 19 17:08:24.411: INFO: stdout: "true"
Nov 19 17:08:24.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get pods update-demo-nautilus-l77qv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9194'
Nov 19 17:08:24.467: INFO: stderr: ""
Nov 19 17:08:24.467: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 19 17:08:24.467: INFO: validating pod update-demo-nautilus-l77qv
Nov 19 17:08:24.472: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 19 17:08:24.472: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 19 17:08:24.472: INFO: update-demo-nautilus-l77qv is verified up and running
STEP: using delete to clean up resources
Nov 19 17:08:24.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 delete --grace-period=0 --force -f - --namespace=kubectl-9194'
Nov 19 17:08:24.534: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 19 17:08:24.534: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov 19 17:08:24.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9194'
Nov 19 17:08:24.595: INFO: stderr: "No resources found in kubectl-9194 namespace.\n"
Nov 19 17:08:24.595: INFO: stdout: ""
Nov 19 17:08:24.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get pods -l name=update-demo --namespace=kubectl-9194 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 19 17:08:24.653: INFO: stderr: ""
Nov 19 17:08:24.653: INFO: stdout: "update-demo-nautilus-4cprm\nupdate-demo-nautilus-l77qv\n"
Nov 19 17:08:25.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9194'
Nov 19 17:08:25.215: INFO: stderr: "No resources found in kubectl-9194 namespace.\n"
Nov 19 17:08:25.215: INFO: stdout: ""
Nov 19 17:08:25.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get pods -l name=update-demo --namespace=kubectl-9194 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 19 17:08:25.275: INFO: stderr: ""
Nov 19 17:08:25.275: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:08:25.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9194" for this suite.

â€¢ [SLOW TEST:6.751 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":305,"completed":34,"skipped":509,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:08:25.284: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-5132
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Nov 19 17:08:28.453: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:08:29.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5132" for this suite.
â€¢{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":305,"completed":35,"skipped":514,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:08:29.481: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-1930
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:08:29.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1930" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":36,"skipped":533,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:08:29.671: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-401
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-secret-srs5
STEP: Creating a pod to test atomic-volume-subpath
Nov 19 17:08:29.824: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-srs5" in namespace "subpath-401" to be "Succeeded or Failed"
Nov 19 17:08:29.826: INFO: Pod "pod-subpath-test-secret-srs5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.656614ms
Nov 19 17:08:31.831: INFO: Pod "pod-subpath-test-secret-srs5": Phase="Running", Reason="", readiness=true. Elapsed: 2.006874012s
Nov 19 17:08:33.836: INFO: Pod "pod-subpath-test-secret-srs5": Phase="Running", Reason="", readiness=true. Elapsed: 4.011883997s
Nov 19 17:08:35.840: INFO: Pod "pod-subpath-test-secret-srs5": Phase="Running", Reason="", readiness=true. Elapsed: 6.01612752s
Nov 19 17:08:37.844: INFO: Pod "pod-subpath-test-secret-srs5": Phase="Running", Reason="", readiness=true. Elapsed: 8.019844286s
Nov 19 17:08:39.848: INFO: Pod "pod-subpath-test-secret-srs5": Phase="Running", Reason="", readiness=true. Elapsed: 10.024218959s
Nov 19 17:08:41.852: INFO: Pod "pod-subpath-test-secret-srs5": Phase="Running", Reason="", readiness=true. Elapsed: 12.028516225s
Nov 19 17:08:43.856: INFO: Pod "pod-subpath-test-secret-srs5": Phase="Running", Reason="", readiness=true. Elapsed: 14.032528177s
Nov 19 17:08:45.861: INFO: Pod "pod-subpath-test-secret-srs5": Phase="Running", Reason="", readiness=true. Elapsed: 16.036945401s
Nov 19 17:08:47.864: INFO: Pod "pod-subpath-test-secret-srs5": Phase="Running", Reason="", readiness=true. Elapsed: 18.040554087s
Nov 19 17:08:49.868: INFO: Pod "pod-subpath-test-secret-srs5": Phase="Running", Reason="", readiness=true. Elapsed: 20.044358191s
Nov 19 17:08:51.872: INFO: Pod "pod-subpath-test-secret-srs5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.048651121s
STEP: Saw pod success
Nov 19 17:08:51.872: INFO: Pod "pod-subpath-test-secret-srs5" satisfied condition "Succeeded or Failed"
Nov 19 17:08:51.875: INFO: Trying to get logs from node ip-172-31-9-169 pod pod-subpath-test-secret-srs5 container test-container-subpath-secret-srs5: <nil>
STEP: delete the pod
Nov 19 17:08:51.906: INFO: Waiting for pod pod-subpath-test-secret-srs5 to disappear
Nov 19 17:08:51.908: INFO: Pod pod-subpath-test-secret-srs5 no longer exists
STEP: Deleting pod pod-subpath-test-secret-srs5
Nov 19 17:08:51.908: INFO: Deleting pod "pod-subpath-test-secret-srs5" in namespace "subpath-401"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:08:51.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-401" for this suite.

â€¢ [SLOW TEST:22.248 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":305,"completed":37,"skipped":549,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:08:51.919: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-6063
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Nov 19 17:08:52.323: INFO: Pod name wrapped-volume-race-0fd3f524-27bb-4e96-bd56-8157a32f54f5: Found 1 pods out of 5
Nov 19 17:08:57.329: INFO: Pod name wrapped-volume-race-0fd3f524-27bb-4e96-bd56-8157a32f54f5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-0fd3f524-27bb-4e96-bd56-8157a32f54f5 in namespace emptydir-wrapper-6063, will wait for the garbage collector to delete the pods
Nov 19 17:09:07.409: INFO: Deleting ReplicationController wrapped-volume-race-0fd3f524-27bb-4e96-bd56-8157a32f54f5 took: 8.800186ms
Nov 19 17:09:07.909: INFO: Terminating ReplicationController wrapped-volume-race-0fd3f524-27bb-4e96-bd56-8157a32f54f5 pods took: 500.154367ms
STEP: Creating RC which spawns configmap-volume pods
Nov 19 17:09:14.925: INFO: Pod name wrapped-volume-race-ebe5564a-7811-4b6b-9730-3e73de1bb3b6: Found 0 pods out of 5
Nov 19 17:09:19.931: INFO: Pod name wrapped-volume-race-ebe5564a-7811-4b6b-9730-3e73de1bb3b6: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-ebe5564a-7811-4b6b-9730-3e73de1bb3b6 in namespace emptydir-wrapper-6063, will wait for the garbage collector to delete the pods
Nov 19 17:09:30.010: INFO: Deleting ReplicationController wrapped-volume-race-ebe5564a-7811-4b6b-9730-3e73de1bb3b6 took: 8.303619ms
Nov 19 17:09:30.510: INFO: Terminating ReplicationController wrapped-volume-race-ebe5564a-7811-4b6b-9730-3e73de1bb3b6 pods took: 500.138405ms
STEP: Creating RC which spawns configmap-volume pods
Nov 19 17:09:37.728: INFO: Pod name wrapped-volume-race-c6b2b9b2-f1ba-4e74-a265-a7f03192b797: Found 0 pods out of 5
Nov 19 17:09:42.736: INFO: Pod name wrapped-volume-race-c6b2b9b2-f1ba-4e74-a265-a7f03192b797: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-c6b2b9b2-f1ba-4e74-a265-a7f03192b797 in namespace emptydir-wrapper-6063, will wait for the garbage collector to delete the pods
Nov 19 17:09:52.816: INFO: Deleting ReplicationController wrapped-volume-race-c6b2b9b2-f1ba-4e74-a265-a7f03192b797 took: 7.896375ms
Nov 19 17:09:52.916: INFO: Terminating ReplicationController wrapped-volume-race-c6b2b9b2-f1ba-4e74-a265-a7f03192b797 pods took: 100.15072ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:10:05.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6063" for this suite.

â€¢ [SLOW TEST:73.630 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":305,"completed":38,"skipped":568,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:10:05.549: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8880
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 19 17:10:05.692: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5e4c2f40-f62b-4411-9218-9f08999a109f" in namespace "downward-api-8880" to be "Succeeded or Failed"
Nov 19 17:10:05.695: INFO: Pod "downwardapi-volume-5e4c2f40-f62b-4411-9218-9f08999a109f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.733879ms
Nov 19 17:10:07.699: INFO: Pod "downwardapi-volume-5e4c2f40-f62b-4411-9218-9f08999a109f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006723109s
STEP: Saw pod success
Nov 19 17:10:07.699: INFO: Pod "downwardapi-volume-5e4c2f40-f62b-4411-9218-9f08999a109f" satisfied condition "Succeeded or Failed"
Nov 19 17:10:07.702: INFO: Trying to get logs from node ip-172-31-9-169 pod downwardapi-volume-5e4c2f40-f62b-4411-9218-9f08999a109f container client-container: <nil>
STEP: delete the pod
Nov 19 17:10:07.720: INFO: Waiting for pod downwardapi-volume-5e4c2f40-f62b-4411-9218-9f08999a109f to disappear
Nov 19 17:10:07.723: INFO: Pod downwardapi-volume-5e4c2f40-f62b-4411-9218-9f08999a109f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:10:07.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8880" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":39,"skipped":575,"failed":0}
SSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:10:07.730: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-199
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 19 17:10:08.891: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:10:08.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-199" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":305,"completed":40,"skipped":580,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:10:08.916: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1744
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-1744
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-1744
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1744
Nov 19 17:10:09.079: INFO: Found 0 stateful pods, waiting for 1
Nov 19 17:10:19.084: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Nov 19 17:10:19.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1744 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 19 17:10:19.228: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 19 17:10:19.228: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 19 17:10:19.228: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 19 17:10:19.231: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov 19 17:10:29.236: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 19 17:10:29.236: INFO: Waiting for statefulset status.replicas updated to 0
Nov 19 17:10:29.251: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999825s
Nov 19 17:10:30.256: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997174949s
Nov 19 17:10:31.260: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992041705s
Nov 19 17:10:32.264: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.987612055s
Nov 19 17:10:33.268: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.983213542s
Nov 19 17:10:34.273: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.979319141s
Nov 19 17:10:35.278: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.97444224s
Nov 19 17:10:36.281: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.969994991s
Nov 19 17:10:37.286: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.966330232s
Nov 19 17:10:38.290: INFO: Verifying statefulset ss doesn't scale past 1 for another 961.732145ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1744
Nov 19 17:10:39.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1744 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:10:39.425: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 19 17:10:39.425: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 19 17:10:39.425: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 19 17:10:39.430: INFO: Found 1 stateful pods, waiting for 3
Nov 19 17:10:49.434: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 19 17:10:49.434: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 19 17:10:49.434: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Nov 19 17:10:49.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1744 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 19 17:10:49.582: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 19 17:10:49.582: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 19 17:10:49.582: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 19 17:10:49.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1744 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 19 17:10:49.739: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 19 17:10:49.739: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 19 17:10:49.739: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 19 17:10:49.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1744 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 19 17:10:49.892: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 19 17:10:49.892: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 19 17:10:49.892: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 19 17:10:49.892: INFO: Waiting for statefulset status.replicas updated to 0
Nov 19 17:10:49.895: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Nov 19 17:10:59.902: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 19 17:10:59.902: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov 19 17:10:59.902: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov 19 17:10:59.914: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999816s
Nov 19 17:11:00.917: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997137612s
Nov 19 17:11:01.921: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993881246s
Nov 19 17:11:02.926: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.989429321s
Nov 19 17:11:03.929: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.985333228s
Nov 19 17:11:04.934: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.981312849s
Nov 19 17:11:05.939: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.976960644s
Nov 19 17:11:06.941: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.972358815s
Nov 19 17:11:07.945: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.969375814s
Nov 19 17:11:08.950: INFO: Verifying statefulset ss doesn't scale past 3 for another 965.596741ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1744
Nov 19 17:11:09.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1744 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:11:10.097: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 19 17:11:10.097: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 19 17:11:10.097: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 19 17:11:10.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1744 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:11:10.234: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 19 17:11:10.234: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 19 17:11:10.234: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 19 17:11:10.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1744 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:11:10.365: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 19 17:11:10.365: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 19 17:11:10.365: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 19 17:11:10.365: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Nov 19 17:11:30.378: INFO: Deleting all statefulset in ns statefulset-1744
Nov 19 17:11:30.382: INFO: Scaling statefulset ss to 0
Nov 19 17:11:30.397: INFO: Waiting for statefulset status.replicas updated to 0
Nov 19 17:11:30.400: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:11:30.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1744" for this suite.

â€¢ [SLOW TEST:81.504 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":305,"completed":41,"skipped":586,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] server version
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:11:30.420: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename server-version
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in server-version-3116
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Request ServerVersion
STEP: Confirm major version
Nov 19 17:11:30.556: INFO: Major version: 1
STEP: Confirm minor version
Nov 19 17:11:30.556: INFO: cleanMinorVersion: 19
Nov 19 17:11:30.556: INFO: Minor version: 19
[AfterEach] [sig-api-machinery] server version
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:11:30.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-3116" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":305,"completed":42,"skipped":591,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:11:30.564: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3990
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-3990
Nov 19 17:11:32.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-3990 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Nov 19 17:11:32.876: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Nov 19 17:11:32.876: INFO: stdout: "iptables"
Nov 19 17:11:32.876: INFO: proxyMode: iptables
Nov 19 17:11:32.884: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 19 17:11:32.887: INFO: Pod kube-proxy-mode-detector still exists
Nov 19 17:11:34.887: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 19 17:11:34.890: INFO: Pod kube-proxy-mode-detector still exists
Nov 19 17:11:36.887: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 19 17:11:36.891: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-3990
STEP: creating replication controller affinity-nodeport-timeout in namespace services-3990
I1119 17:11:36.911888      22 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-3990, replica count: 3
I1119 17:11:39.962150      22 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1119 17:11:42.962329      22 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 19 17:11:42.972: INFO: Creating new exec pod
Nov 19 17:11:45.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-3990 execpod-affinityz84hh -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Nov 19 17:11:46.142: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Nov 19 17:11:46.142: INFO: stdout: ""
Nov 19 17:11:46.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-3990 execpod-affinityz84hh -- /bin/sh -x -c nc -zv -t -w 2 10.152.183.73 80'
Nov 19 17:11:46.280: INFO: stderr: "+ nc -zv -t -w 2 10.152.183.73 80\nConnection to 10.152.183.73 80 port [tcp/http] succeeded!\n"
Nov 19 17:11:46.281: INFO: stdout: ""
Nov 19 17:11:46.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-3990 execpod-affinityz84hh -- /bin/sh -x -c nc -zv -t -w 2 172.31.20.145 30919'
Nov 19 17:11:46.397: INFO: stderr: "+ nc -zv -t -w 2 172.31.20.145 30919\nConnection to 172.31.20.145 30919 port [tcp/30919] succeeded!\n"
Nov 19 17:11:46.397: INFO: stdout: ""
Nov 19 17:11:46.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-3990 execpod-affinityz84hh -- /bin/sh -x -c nc -zv -t -w 2 172.31.68.240 30919'
Nov 19 17:11:46.525: INFO: stderr: "+ nc -zv -t -w 2 172.31.68.240 30919\nConnection to 172.31.68.240 30919 port [tcp/30919] succeeded!\n"
Nov 19 17:11:46.525: INFO: stdout: ""
Nov 19 17:11:46.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-3990 execpod-affinityz84hh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.20.145:30919/ ; done'
Nov 19 17:11:46.684: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:30919/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:30919/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:30919/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:30919/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:30919/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:30919/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:30919/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:30919/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:30919/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:30919/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:30919/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:30919/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:30919/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:30919/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:30919/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:30919/\n"
Nov 19 17:11:46.684: INFO: stdout: "\naffinity-nodeport-timeout-z4lcw\naffinity-nodeport-timeout-z4lcw\naffinity-nodeport-timeout-z4lcw\naffinity-nodeport-timeout-z4lcw\naffinity-nodeport-timeout-z4lcw\naffinity-nodeport-timeout-z4lcw\naffinity-nodeport-timeout-z4lcw\naffinity-nodeport-timeout-z4lcw\naffinity-nodeport-timeout-z4lcw\naffinity-nodeport-timeout-z4lcw\naffinity-nodeport-timeout-z4lcw\naffinity-nodeport-timeout-z4lcw\naffinity-nodeport-timeout-z4lcw\naffinity-nodeport-timeout-z4lcw\naffinity-nodeport-timeout-z4lcw\naffinity-nodeport-timeout-z4lcw"
Nov 19 17:11:46.684: INFO: Received response from host: affinity-nodeport-timeout-z4lcw
Nov 19 17:11:46.684: INFO: Received response from host: affinity-nodeport-timeout-z4lcw
Nov 19 17:11:46.684: INFO: Received response from host: affinity-nodeport-timeout-z4lcw
Nov 19 17:11:46.684: INFO: Received response from host: affinity-nodeport-timeout-z4lcw
Nov 19 17:11:46.684: INFO: Received response from host: affinity-nodeport-timeout-z4lcw
Nov 19 17:11:46.684: INFO: Received response from host: affinity-nodeport-timeout-z4lcw
Nov 19 17:11:46.684: INFO: Received response from host: affinity-nodeport-timeout-z4lcw
Nov 19 17:11:46.684: INFO: Received response from host: affinity-nodeport-timeout-z4lcw
Nov 19 17:11:46.684: INFO: Received response from host: affinity-nodeport-timeout-z4lcw
Nov 19 17:11:46.684: INFO: Received response from host: affinity-nodeport-timeout-z4lcw
Nov 19 17:11:46.684: INFO: Received response from host: affinity-nodeport-timeout-z4lcw
Nov 19 17:11:46.684: INFO: Received response from host: affinity-nodeport-timeout-z4lcw
Nov 19 17:11:46.684: INFO: Received response from host: affinity-nodeport-timeout-z4lcw
Nov 19 17:11:46.684: INFO: Received response from host: affinity-nodeport-timeout-z4lcw
Nov 19 17:11:46.684: INFO: Received response from host: affinity-nodeport-timeout-z4lcw
Nov 19 17:11:46.684: INFO: Received response from host: affinity-nodeport-timeout-z4lcw
Nov 19 17:11:46.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-3990 execpod-affinityz84hh -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.31.20.145:30919/'
Nov 19 17:11:46.813: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.31.20.145:30919/\n"
Nov 19 17:11:46.814: INFO: stdout: "affinity-nodeport-timeout-z4lcw"
Nov 19 17:12:01.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-3990 execpod-affinityz84hh -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.31.20.145:30919/'
Nov 19 17:12:01.954: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.31.20.145:30919/\n"
Nov 19 17:12:01.954: INFO: stdout: "affinity-nodeport-timeout-z4lcw"
Nov 19 17:12:16.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-3990 execpod-affinityz84hh -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.31.20.145:30919/'
Nov 19 17:12:17.103: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.31.20.145:30919/\n"
Nov 19 17:12:17.103: INFO: stdout: "affinity-nodeport-timeout-z4lcw"
Nov 19 17:12:32.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-3990 execpod-affinityz84hh -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.31.20.145:30919/'
Nov 19 17:12:32.250: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.31.20.145:30919/\n"
Nov 19 17:12:32.250: INFO: stdout: "affinity-nodeport-timeout-sxnkq"
Nov 19 17:12:32.250: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-3990, will wait for the garbage collector to delete the pods
Nov 19 17:12:32.324: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 7.247363ms
Nov 19 17:12:32.824: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 500.154169ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:12:47.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3990" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

â€¢ [SLOW TEST:76.992 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":43,"skipped":603,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:12:47.556: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-445
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Nov 19 17:12:47.700: INFO: Waiting up to 5m0s for pod "pod-55215aa9-f49a-41e0-8b5d-2fae5accd313" in namespace "emptydir-445" to be "Succeeded or Failed"
Nov 19 17:12:47.706: INFO: Pod "pod-55215aa9-f49a-41e0-8b5d-2fae5accd313": Phase="Pending", Reason="", readiness=false. Elapsed: 5.725103ms
Nov 19 17:12:49.710: INFO: Pod "pod-55215aa9-f49a-41e0-8b5d-2fae5accd313": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010183712s
STEP: Saw pod success
Nov 19 17:12:49.710: INFO: Pod "pod-55215aa9-f49a-41e0-8b5d-2fae5accd313" satisfied condition "Succeeded or Failed"
Nov 19 17:12:49.713: INFO: Trying to get logs from node ip-172-31-20-145 pod pod-55215aa9-f49a-41e0-8b5d-2fae5accd313 container test-container: <nil>
STEP: delete the pod
Nov 19 17:12:49.740: INFO: Waiting for pod pod-55215aa9-f49a-41e0-8b5d-2fae5accd313 to disappear
Nov 19 17:12:49.743: INFO: Pod pod-55215aa9-f49a-41e0-8b5d-2fae5accd313 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:12:49.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-445" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":44,"skipped":612,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:12:49.752: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6753
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 19 17:12:50.114: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 19 17:12:53.132: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:12:53.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6753" for this suite.
STEP: Destroying namespace "webhook-6753-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":305,"completed":45,"skipped":645,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:12:53.232: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4526
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Nov 19 17:12:53.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 create -f - --namespace=kubectl-4526'
Nov 19 17:12:53.724: INFO: stderr: ""
Nov 19 17:12:53.724: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 19 17:12:53.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4526'
Nov 19 17:12:53.783: INFO: stderr: ""
Nov 19 17:12:53.783: INFO: stdout: "update-demo-nautilus-cctdn update-demo-nautilus-w5948 "
Nov 19 17:12:53.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get pods update-demo-nautilus-cctdn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4526'
Nov 19 17:12:53.838: INFO: stderr: ""
Nov 19 17:12:53.838: INFO: stdout: ""
Nov 19 17:12:53.838: INFO: update-demo-nautilus-cctdn is created but not running
Nov 19 17:12:58.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4526'
Nov 19 17:12:58.898: INFO: stderr: ""
Nov 19 17:12:58.898: INFO: stdout: "update-demo-nautilus-cctdn update-demo-nautilus-w5948 "
Nov 19 17:12:58.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get pods update-demo-nautilus-cctdn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4526'
Nov 19 17:12:58.955: INFO: stderr: ""
Nov 19 17:12:58.955: INFO: stdout: "true"
Nov 19 17:12:58.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get pods update-demo-nautilus-cctdn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4526'
Nov 19 17:12:59.012: INFO: stderr: ""
Nov 19 17:12:59.012: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 19 17:12:59.012: INFO: validating pod update-demo-nautilus-cctdn
Nov 19 17:12:59.018: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 19 17:12:59.018: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 19 17:12:59.018: INFO: update-demo-nautilus-cctdn is verified up and running
Nov 19 17:12:59.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get pods update-demo-nautilus-w5948 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4526'
Nov 19 17:12:59.073: INFO: stderr: ""
Nov 19 17:12:59.073: INFO: stdout: "true"
Nov 19 17:12:59.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get pods update-demo-nautilus-w5948 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4526'
Nov 19 17:12:59.129: INFO: stderr: ""
Nov 19 17:12:59.129: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 19 17:12:59.129: INFO: validating pod update-demo-nautilus-w5948
Nov 19 17:12:59.133: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 19 17:12:59.133: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 19 17:12:59.133: INFO: update-demo-nautilus-w5948 is verified up and running
STEP: scaling down the replication controller
Nov 19 17:12:59.134: INFO: scanned /root for discovery docs: <nil>
Nov 19 17:12:59.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-4526'
Nov 19 17:13:00.220: INFO: stderr: ""
Nov 19 17:13:00.220: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 19 17:13:00.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4526'
Nov 19 17:13:00.281: INFO: stderr: ""
Nov 19 17:13:00.281: INFO: stdout: "update-demo-nautilus-cctdn update-demo-nautilus-w5948 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Nov 19 17:13:05.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4526'
Nov 19 17:13:05.340: INFO: stderr: ""
Nov 19 17:13:05.340: INFO: stdout: "update-demo-nautilus-w5948 "
Nov 19 17:13:05.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get pods update-demo-nautilus-w5948 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4526'
Nov 19 17:13:05.395: INFO: stderr: ""
Nov 19 17:13:05.395: INFO: stdout: "true"
Nov 19 17:13:05.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get pods update-demo-nautilus-w5948 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4526'
Nov 19 17:13:05.454: INFO: stderr: ""
Nov 19 17:13:05.454: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 19 17:13:05.454: INFO: validating pod update-demo-nautilus-w5948
Nov 19 17:13:05.458: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 19 17:13:05.458: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 19 17:13:05.458: INFO: update-demo-nautilus-w5948 is verified up and running
STEP: scaling up the replication controller
Nov 19 17:13:05.459: INFO: scanned /root for discovery docs: <nil>
Nov 19 17:13:05.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-4526'
Nov 19 17:13:06.534: INFO: stderr: ""
Nov 19 17:13:06.534: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 19 17:13:06.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4526'
Nov 19 17:13:06.594: INFO: stderr: ""
Nov 19 17:13:06.594: INFO: stdout: "update-demo-nautilus-jc6hj update-demo-nautilus-w5948 "
Nov 19 17:13:06.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get pods update-demo-nautilus-jc6hj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4526'
Nov 19 17:13:06.648: INFO: stderr: ""
Nov 19 17:13:06.648: INFO: stdout: "true"
Nov 19 17:13:06.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get pods update-demo-nautilus-jc6hj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4526'
Nov 19 17:13:06.704: INFO: stderr: ""
Nov 19 17:13:06.704: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 19 17:13:06.704: INFO: validating pod update-demo-nautilus-jc6hj
Nov 19 17:13:06.709: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 19 17:13:06.709: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 19 17:13:06.709: INFO: update-demo-nautilus-jc6hj is verified up and running
Nov 19 17:13:06.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get pods update-demo-nautilus-w5948 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4526'
Nov 19 17:13:06.765: INFO: stderr: ""
Nov 19 17:13:06.765: INFO: stdout: "true"
Nov 19 17:13:06.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get pods update-demo-nautilus-w5948 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4526'
Nov 19 17:13:06.820: INFO: stderr: ""
Nov 19 17:13:06.820: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 19 17:13:06.820: INFO: validating pod update-demo-nautilus-w5948
Nov 19 17:13:06.825: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 19 17:13:06.825: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 19 17:13:06.825: INFO: update-demo-nautilus-w5948 is verified up and running
STEP: using delete to clean up resources
Nov 19 17:13:06.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 delete --grace-period=0 --force -f - --namespace=kubectl-4526'
Nov 19 17:13:06.887: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 19 17:13:06.887: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov 19 17:13:06.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4526'
Nov 19 17:13:06.950: INFO: stderr: "No resources found in kubectl-4526 namespace.\n"
Nov 19 17:13:06.950: INFO: stdout: ""
Nov 19 17:13:06.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get pods -l name=update-demo --namespace=kubectl-4526 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 19 17:13:07.011: INFO: stderr: ""
Nov 19 17:13:07.011: INFO: stdout: "update-demo-nautilus-jc6hj\nupdate-demo-nautilus-w5948\n"
Nov 19 17:13:07.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4526'
Nov 19 17:13:07.572: INFO: stderr: "No resources found in kubectl-4526 namespace.\n"
Nov 19 17:13:07.572: INFO: stdout: ""
Nov 19 17:13:07.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get pods -l name=update-demo --namespace=kubectl-4526 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 19 17:13:07.632: INFO: stderr: ""
Nov 19 17:13:07.632: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:13:07.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4526" for this suite.

â€¢ [SLOW TEST:14.410 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":305,"completed":46,"skipped":651,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:13:07.643: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2729
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 19 17:13:08.352: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 19 17:13:11.368: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:13:11.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2729" for this suite.
STEP: Destroying namespace "webhook-2729-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":305,"completed":47,"skipped":682,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:13:11.484: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5484
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-bab79f38-7dee-470e-b857-c9dbe28735e8
STEP: Creating a pod to test consume secrets
Nov 19 17:13:11.635: INFO: Waiting up to 5m0s for pod "pod-secrets-ce20a4c0-2f7b-4474-80f9-61cd3af6e86b" in namespace "secrets-5484" to be "Succeeded or Failed"
Nov 19 17:13:11.637: INFO: Pod "pod-secrets-ce20a4c0-2f7b-4474-80f9-61cd3af6e86b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.313142ms
Nov 19 17:13:13.640: INFO: Pod "pod-secrets-ce20a4c0-2f7b-4474-80f9-61cd3af6e86b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00503638s
STEP: Saw pod success
Nov 19 17:13:13.640: INFO: Pod "pod-secrets-ce20a4c0-2f7b-4474-80f9-61cd3af6e86b" satisfied condition "Succeeded or Failed"
Nov 19 17:13:13.643: INFO: Trying to get logs from node ip-172-31-9-169 pod pod-secrets-ce20a4c0-2f7b-4474-80f9-61cd3af6e86b container secret-volume-test: <nil>
STEP: delete the pod
Nov 19 17:13:13.666: INFO: Waiting for pod pod-secrets-ce20a4c0-2f7b-4474-80f9-61cd3af6e86b to disappear
Nov 19 17:13:13.669: INFO: Pod pod-secrets-ce20a4c0-2f7b-4474-80f9-61cd3af6e86b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:13:13.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5484" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":48,"skipped":702,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:13:13.676: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2283
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 19 17:13:13.817: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0e4a118d-3c49-463e-82b0-002d3212f5e8" in namespace "projected-2283" to be "Succeeded or Failed"
Nov 19 17:13:13.819: INFO: Pod "downwardapi-volume-0e4a118d-3c49-463e-82b0-002d3212f5e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.432696ms
Nov 19 17:13:15.824: INFO: Pod "downwardapi-volume-0e4a118d-3c49-463e-82b0-002d3212f5e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007312755s
STEP: Saw pod success
Nov 19 17:13:15.824: INFO: Pod "downwardapi-volume-0e4a118d-3c49-463e-82b0-002d3212f5e8" satisfied condition "Succeeded or Failed"
Nov 19 17:13:15.828: INFO: Trying to get logs from node ip-172-31-20-145 pod downwardapi-volume-0e4a118d-3c49-463e-82b0-002d3212f5e8 container client-container: <nil>
STEP: delete the pod
Nov 19 17:13:15.846: INFO: Waiting for pod downwardapi-volume-0e4a118d-3c49-463e-82b0-002d3212f5e8 to disappear
Nov 19 17:13:15.848: INFO: Pod downwardapi-volume-0e4a118d-3c49-463e-82b0-002d3212f5e8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:13:15.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2283" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":49,"skipped":715,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:13:15.855: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2332
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 19 17:13:16.304: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 19 17:13:19.322: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 17:13:19.326: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3995-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:13:20.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2332" for this suite.
STEP: Destroying namespace "webhook-2332-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":305,"completed":50,"skipped":725,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:13:20.513: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename discovery
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in discovery-1779
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 17:13:20.956: INFO: Checking APIGroup: apiregistration.k8s.io
Nov 19 17:13:20.957: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Nov 19 17:13:20.957: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Nov 19 17:13:20.957: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Nov 19 17:13:20.957: INFO: Checking APIGroup: extensions
Nov 19 17:13:20.958: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Nov 19 17:13:20.958: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Nov 19 17:13:20.958: INFO: extensions/v1beta1 matches extensions/v1beta1
Nov 19 17:13:20.958: INFO: Checking APIGroup: apps
Nov 19 17:13:20.959: INFO: PreferredVersion.GroupVersion: apps/v1
Nov 19 17:13:20.959: INFO: Versions found [{apps/v1 v1}]
Nov 19 17:13:20.959: INFO: apps/v1 matches apps/v1
Nov 19 17:13:20.959: INFO: Checking APIGroup: events.k8s.io
Nov 19 17:13:20.960: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Nov 19 17:13:20.960: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Nov 19 17:13:20.960: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Nov 19 17:13:20.960: INFO: Checking APIGroup: authentication.k8s.io
Nov 19 17:13:20.960: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Nov 19 17:13:20.960: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Nov 19 17:13:20.960: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Nov 19 17:13:20.960: INFO: Checking APIGroup: authorization.k8s.io
Nov 19 17:13:20.961: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Nov 19 17:13:20.961: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Nov 19 17:13:20.961: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Nov 19 17:13:20.961: INFO: Checking APIGroup: autoscaling
Nov 19 17:13:20.962: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Nov 19 17:13:20.962: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Nov 19 17:13:20.962: INFO: autoscaling/v1 matches autoscaling/v1
Nov 19 17:13:20.962: INFO: Checking APIGroup: batch
Nov 19 17:13:20.963: INFO: PreferredVersion.GroupVersion: batch/v1
Nov 19 17:13:20.963: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Nov 19 17:13:20.963: INFO: batch/v1 matches batch/v1
Nov 19 17:13:20.963: INFO: Checking APIGroup: certificates.k8s.io
Nov 19 17:13:20.964: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Nov 19 17:13:20.964: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Nov 19 17:13:20.964: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Nov 19 17:13:20.964: INFO: Checking APIGroup: networking.k8s.io
Nov 19 17:13:20.964: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Nov 19 17:13:20.964: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Nov 19 17:13:20.964: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Nov 19 17:13:20.964: INFO: Checking APIGroup: policy
Nov 19 17:13:20.965: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Nov 19 17:13:20.965: INFO: Versions found [{policy/v1beta1 v1beta1}]
Nov 19 17:13:20.965: INFO: policy/v1beta1 matches policy/v1beta1
Nov 19 17:13:20.965: INFO: Checking APIGroup: rbac.authorization.k8s.io
Nov 19 17:13:20.966: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Nov 19 17:13:20.966: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Nov 19 17:13:20.966: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Nov 19 17:13:20.966: INFO: Checking APIGroup: storage.k8s.io
Nov 19 17:13:20.967: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Nov 19 17:13:20.967: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Nov 19 17:13:20.967: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Nov 19 17:13:20.967: INFO: Checking APIGroup: admissionregistration.k8s.io
Nov 19 17:13:20.967: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Nov 19 17:13:20.967: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Nov 19 17:13:20.967: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Nov 19 17:13:20.967: INFO: Checking APIGroup: apiextensions.k8s.io
Nov 19 17:13:20.968: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Nov 19 17:13:20.968: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Nov 19 17:13:20.968: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Nov 19 17:13:20.968: INFO: Checking APIGroup: scheduling.k8s.io
Nov 19 17:13:20.969: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Nov 19 17:13:20.969: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Nov 19 17:13:20.969: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Nov 19 17:13:20.969: INFO: Checking APIGroup: coordination.k8s.io
Nov 19 17:13:20.970: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Nov 19 17:13:20.970: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Nov 19 17:13:20.970: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Nov 19 17:13:20.970: INFO: Checking APIGroup: node.k8s.io
Nov 19 17:13:20.970: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1beta1
Nov 19 17:13:20.970: INFO: Versions found [{node.k8s.io/v1beta1 v1beta1}]
Nov 19 17:13:20.970: INFO: node.k8s.io/v1beta1 matches node.k8s.io/v1beta1
Nov 19 17:13:20.970: INFO: Checking APIGroup: discovery.k8s.io
Nov 19 17:13:20.971: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Nov 19 17:13:20.971: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Nov 19 17:13:20.971: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Nov 19 17:13:20.971: INFO: Checking APIGroup: metrics.k8s.io
Nov 19 17:13:20.972: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Nov 19 17:13:20.972: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Nov 19 17:13:20.972: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:13:20.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-1779" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":305,"completed":51,"skipped":729,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:13:20.981: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3351
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Starting the proxy
Nov 19 17:13:21.118: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-898325863 proxy --unix-socket=/tmp/kubectl-proxy-unix141317950/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:13:21.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3351" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":305,"completed":52,"skipped":737,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:13:21.171: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-4755
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Nov 19 17:13:21.318: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 19 17:14:21.331: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Nov 19 17:14:21.353: INFO: Created pod: pod0-sched-preemption-low-priority
Nov 19 17:14:21.365: INFO: Created pod: pod1-sched-preemption-medium-priority
Nov 19 17:14:21.377: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:14:39.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4755" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

â€¢ [SLOW TEST:78.315 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":305,"completed":53,"skipped":748,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:14:39.485: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6846
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:14:39.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6846" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
â€¢{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":305,"completed":54,"skipped":771,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:14:39.631: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9844
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-2cead2bc-e4af-44e5-8b90-3f72f7bf01d0
STEP: Creating a pod to test consume configMaps
Nov 19 17:14:39.787: INFO: Waiting up to 5m0s for pod "pod-configmaps-ae02e5ac-f2d6-40f0-afd6-e3bc3a09ddf6" in namespace "configmap-9844" to be "Succeeded or Failed"
Nov 19 17:14:39.789: INFO: Pod "pod-configmaps-ae02e5ac-f2d6-40f0-afd6-e3bc3a09ddf6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.28764ms
Nov 19 17:14:41.793: INFO: Pod "pod-configmaps-ae02e5ac-f2d6-40f0-afd6-e3bc3a09ddf6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006951211s
STEP: Saw pod success
Nov 19 17:14:41.794: INFO: Pod "pod-configmaps-ae02e5ac-f2d6-40f0-afd6-e3bc3a09ddf6" satisfied condition "Succeeded or Failed"
Nov 19 17:14:41.796: INFO: Trying to get logs from node ip-172-31-20-145 pod pod-configmaps-ae02e5ac-f2d6-40f0-afd6-e3bc3a09ddf6 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 19 17:14:41.813: INFO: Waiting for pod pod-configmaps-ae02e5ac-f2d6-40f0-afd6-e3bc3a09ddf6 to disappear
Nov 19 17:14:41.815: INFO: Pod pod-configmaps-ae02e5ac-f2d6-40f0-afd6-e3bc3a09ddf6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:14:41.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9844" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":55,"skipped":785,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:14:41.825: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7038
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Nov 19 17:14:44.505: INFO: Successfully updated pod "labelsupdate0bbe01a4-89f8-4274-93bb-73c447050415"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:14:48.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7038" for this suite.

â€¢ [SLOW TEST:6.708 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":56,"skipped":789,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:14:48.533: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3256
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Nov 19 17:14:51.207: INFO: Successfully updated pod "annotationupdate82dcdf6c-9881-4529-9860-6ae7edb9577a"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:14:55.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3256" for this suite.

â€¢ [SLOW TEST:6.701 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":57,"skipped":790,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:14:55.234: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8570
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating replication controller my-hostname-basic-59265d50-7151-4e49-b618-c6f412208f4c
Nov 19 17:14:55.387: INFO: Pod name my-hostname-basic-59265d50-7151-4e49-b618-c6f412208f4c: Found 0 pods out of 1
Nov 19 17:15:00.390: INFO: Pod name my-hostname-basic-59265d50-7151-4e49-b618-c6f412208f4c: Found 1 pods out of 1
Nov 19 17:15:00.391: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-59265d50-7151-4e49-b618-c6f412208f4c" are running
Nov 19 17:15:00.393: INFO: Pod "my-hostname-basic-59265d50-7151-4e49-b618-c6f412208f4c-hw5x5" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-19 17:14:55 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-19 17:14:56 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-19 17:14:56 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-19 17:14:55 +0000 UTC Reason: Message:}])
Nov 19 17:15:00.393: INFO: Trying to dial the pod
Nov 19 17:15:05.406: INFO: Controller my-hostname-basic-59265d50-7151-4e49-b618-c6f412208f4c: Got expected result from replica 1 [my-hostname-basic-59265d50-7151-4e49-b618-c6f412208f4c-hw5x5]: "my-hostname-basic-59265d50-7151-4e49-b618-c6f412208f4c-hw5x5", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:15:05.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8570" for this suite.

â€¢ [SLOW TEST:10.184 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":58,"skipped":808,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:15:05.419: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8410
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Nov 19 17:15:05.570: INFO: Waiting up to 5m0s for pod "pod-a7d9471c-d522-41cb-b50f-fe1217d94a94" in namespace "emptydir-8410" to be "Succeeded or Failed"
Nov 19 17:15:05.572: INFO: Pod "pod-a7d9471c-d522-41cb-b50f-fe1217d94a94": Phase="Pending", Reason="", readiness=false. Elapsed: 2.176197ms
Nov 19 17:15:07.576: INFO: Pod "pod-a7d9471c-d522-41cb-b50f-fe1217d94a94": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006020423s
STEP: Saw pod success
Nov 19 17:15:07.576: INFO: Pod "pod-a7d9471c-d522-41cb-b50f-fe1217d94a94" satisfied condition "Succeeded or Failed"
Nov 19 17:15:07.578: INFO: Trying to get logs from node ip-172-31-20-145 pod pod-a7d9471c-d522-41cb-b50f-fe1217d94a94 container test-container: <nil>
STEP: delete the pod
Nov 19 17:15:07.596: INFO: Waiting for pod pod-a7d9471c-d522-41cb-b50f-fe1217d94a94 to disappear
Nov 19 17:15:07.599: INFO: Pod pod-a7d9471c-d522-41cb-b50f-fe1217d94a94 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:15:07.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8410" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":59,"skipped":816,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:15:07.608: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4947
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 19 17:15:08.234: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 19 17:15:11.254: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:15:11.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4947" for this suite.
STEP: Destroying namespace "webhook-4947-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":305,"completed":60,"skipped":828,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:15:11.492: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3423
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Nov 19 17:15:15.694: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 19 17:15:15.698: INFO: Pod pod-with-poststart-http-hook still exists
Nov 19 17:15:17.698: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 19 17:15:17.701: INFO: Pod pod-with-poststart-http-hook still exists
Nov 19 17:15:19.698: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 19 17:15:19.702: INFO: Pod pod-with-poststart-http-hook still exists
Nov 19 17:15:21.698: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 19 17:15:21.703: INFO: Pod pod-with-poststart-http-hook still exists
Nov 19 17:15:23.698: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 19 17:15:23.701: INFO: Pod pod-with-poststart-http-hook still exists
Nov 19 17:15:25.698: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 19 17:15:25.703: INFO: Pod pod-with-poststart-http-hook still exists
Nov 19 17:15:27.698: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 19 17:15:27.703: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:15:27.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3423" for this suite.

â€¢ [SLOW TEST:16.219 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":305,"completed":61,"skipped":838,"failed":0}
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:15:27.711: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1236
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 19 17:15:27.854: INFO: Waiting up to 5m0s for pod "downwardapi-volume-672e2aaa-2aa3-4d49-8504-4338d8c8ed38" in namespace "downward-api-1236" to be "Succeeded or Failed"
Nov 19 17:15:27.861: INFO: Pod "downwardapi-volume-672e2aaa-2aa3-4d49-8504-4338d8c8ed38": Phase="Pending", Reason="", readiness=false. Elapsed: 7.258412ms
Nov 19 17:15:29.865: INFO: Pod "downwardapi-volume-672e2aaa-2aa3-4d49-8504-4338d8c8ed38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010777975s
STEP: Saw pod success
Nov 19 17:15:29.865: INFO: Pod "downwardapi-volume-672e2aaa-2aa3-4d49-8504-4338d8c8ed38" satisfied condition "Succeeded or Failed"
Nov 19 17:15:29.868: INFO: Trying to get logs from node ip-172-31-20-145 pod downwardapi-volume-672e2aaa-2aa3-4d49-8504-4338d8c8ed38 container client-container: <nil>
STEP: delete the pod
Nov 19 17:15:29.884: INFO: Waiting for pod downwardapi-volume-672e2aaa-2aa3-4d49-8504-4338d8c8ed38 to disappear
Nov 19 17:15:29.886: INFO: Pod downwardapi-volume-672e2aaa-2aa3-4d49-8504-4338d8c8ed38 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:15:29.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1236" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":62,"skipped":838,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:15:29.898: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1800
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name projected-secret-test-65d3ce76-040f-4313-817a-e60e980805c4
STEP: Creating a pod to test consume secrets
Nov 19 17:15:30.050: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e14d292e-2ff2-4294-a725-20bfa21f7965" in namespace "projected-1800" to be "Succeeded or Failed"
Nov 19 17:15:30.055: INFO: Pod "pod-projected-secrets-e14d292e-2ff2-4294-a725-20bfa21f7965": Phase="Pending", Reason="", readiness=false. Elapsed: 5.38551ms
Nov 19 17:15:32.060: INFO: Pod "pod-projected-secrets-e14d292e-2ff2-4294-a725-20bfa21f7965": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009665446s
STEP: Saw pod success
Nov 19 17:15:32.060: INFO: Pod "pod-projected-secrets-e14d292e-2ff2-4294-a725-20bfa21f7965" satisfied condition "Succeeded or Failed"
Nov 19 17:15:32.062: INFO: Trying to get logs from node ip-172-31-20-145 pod pod-projected-secrets-e14d292e-2ff2-4294-a725-20bfa21f7965 container secret-volume-test: <nil>
STEP: delete the pod
Nov 19 17:15:32.078: INFO: Waiting for pod pod-projected-secrets-e14d292e-2ff2-4294-a725-20bfa21f7965 to disappear
Nov 19 17:15:32.080: INFO: Pod pod-projected-secrets-e14d292e-2ff2-4294-a725-20bfa21f7965 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:15:32.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1800" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":63,"skipped":867,"failed":0}
SSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:15:32.087: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename certificates
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in certificates-9004
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Nov 19 17:15:32.580: INFO: starting watch
STEP: patching
STEP: updating
Nov 19 17:15:32.590: INFO: waiting for watch events with expected annotations
Nov 19 17:15:32.590: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:15:32.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-9004" for this suite.
â€¢{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":305,"completed":64,"skipped":872,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:15:32.652: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4639
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap that has name configmap-test-emptyKey-51749054-a7b5-49a6-8970-409866d9ff0c
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:15:32.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4639" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":305,"completed":65,"skipped":883,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:15:32.793: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2691
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-projected-5fb2
STEP: Creating a pod to test atomic-volume-subpath
Nov 19 17:15:32.954: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-5fb2" in namespace "subpath-2691" to be "Succeeded or Failed"
Nov 19 17:15:32.956: INFO: Pod "pod-subpath-test-projected-5fb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.081195ms
Nov 19 17:15:34.960: INFO: Pod "pod-subpath-test-projected-5fb2": Phase="Running", Reason="", readiness=true. Elapsed: 2.005783988s
Nov 19 17:15:36.965: INFO: Pod "pod-subpath-test-projected-5fb2": Phase="Running", Reason="", readiness=true. Elapsed: 4.010805319s
Nov 19 17:15:38.969: INFO: Pod "pod-subpath-test-projected-5fb2": Phase="Running", Reason="", readiness=true. Elapsed: 6.014597425s
Nov 19 17:15:40.973: INFO: Pod "pod-subpath-test-projected-5fb2": Phase="Running", Reason="", readiness=true. Elapsed: 8.018640481s
Nov 19 17:15:42.976: INFO: Pod "pod-subpath-test-projected-5fb2": Phase="Running", Reason="", readiness=true. Elapsed: 10.021918057s
Nov 19 17:15:44.981: INFO: Pod "pod-subpath-test-projected-5fb2": Phase="Running", Reason="", readiness=true. Elapsed: 12.026899971s
Nov 19 17:15:46.985: INFO: Pod "pod-subpath-test-projected-5fb2": Phase="Running", Reason="", readiness=true. Elapsed: 14.031540109s
Nov 19 17:15:48.990: INFO: Pod "pod-subpath-test-projected-5fb2": Phase="Running", Reason="", readiness=true. Elapsed: 16.035976633s
Nov 19 17:15:50.994: INFO: Pod "pod-subpath-test-projected-5fb2": Phase="Running", Reason="", readiness=true. Elapsed: 18.040066302s
Nov 19 17:15:53.000: INFO: Pod "pod-subpath-test-projected-5fb2": Phase="Running", Reason="", readiness=true. Elapsed: 20.04593795s
Nov 19 17:15:55.004: INFO: Pod "pod-subpath-test-projected-5fb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.049602986s
STEP: Saw pod success
Nov 19 17:15:55.004: INFO: Pod "pod-subpath-test-projected-5fb2" satisfied condition "Succeeded or Failed"
Nov 19 17:15:55.007: INFO: Trying to get logs from node ip-172-31-20-145 pod pod-subpath-test-projected-5fb2 container test-container-subpath-projected-5fb2: <nil>
STEP: delete the pod
Nov 19 17:15:55.024: INFO: Waiting for pod pod-subpath-test-projected-5fb2 to disappear
Nov 19 17:15:55.026: INFO: Pod pod-subpath-test-projected-5fb2 no longer exists
STEP: Deleting pod pod-subpath-test-projected-5fb2
Nov 19 17:15:55.026: INFO: Deleting pod "pod-subpath-test-projected-5fb2" in namespace "subpath-2691"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:15:55.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2691" for this suite.

â€¢ [SLOW TEST:22.243 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":305,"completed":66,"skipped":895,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:15:55.037: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-3278
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Nov 19 17:15:55.445: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 19 17:15:58.462: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 17:15:58.467: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:15:59.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3278" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":305,"completed":67,"skipped":897,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:15:59.684: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5545
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service multi-endpoint-test in namespace services-5545
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5545 to expose endpoints map[]
Nov 19 17:15:59.829: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Nov 19 17:16:00.838: INFO: successfully validated that service multi-endpoint-test in namespace services-5545 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5545
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5545 to expose endpoints map[pod1:[100]]
Nov 19 17:16:02.863: INFO: successfully validated that service multi-endpoint-test in namespace services-5545 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-5545
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5545 to expose endpoints map[pod1:[100] pod2:[101]]
Nov 19 17:16:04.885: INFO: successfully validated that service multi-endpoint-test in namespace services-5545 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-5545
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5545 to expose endpoints map[pod2:[101]]
Nov 19 17:16:04.903: INFO: successfully validated that service multi-endpoint-test in namespace services-5545 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-5545
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5545 to expose endpoints map[]
Nov 19 17:16:05.922: INFO: successfully validated that service multi-endpoint-test in namespace services-5545 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:16:05.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5545" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

â€¢ [SLOW TEST:6.265 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":305,"completed":68,"skipped":902,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:16:05.949: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6482
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 17:16:06.099: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Nov 19 17:16:06.107: INFO: Number of nodes with available pods: 0
Nov 19 17:16:06.107: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Nov 19 17:16:06.122: INFO: Number of nodes with available pods: 0
Nov 19 17:16:06.122: INFO: Node ip-172-31-20-145 is running more than one daemon pod
Nov 19 17:16:07.126: INFO: Number of nodes with available pods: 1
Nov 19 17:16:07.126: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Nov 19 17:16:07.140: INFO: Number of nodes with available pods: 1
Nov 19 17:16:07.140: INFO: Number of running nodes: 0, number of available pods: 1
Nov 19 17:16:08.144: INFO: Number of nodes with available pods: 0
Nov 19 17:16:08.144: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Nov 19 17:16:08.156: INFO: Number of nodes with available pods: 0
Nov 19 17:16:08.156: INFO: Node ip-172-31-20-145 is running more than one daemon pod
Nov 19 17:16:09.160: INFO: Number of nodes with available pods: 0
Nov 19 17:16:09.160: INFO: Node ip-172-31-20-145 is running more than one daemon pod
Nov 19 17:16:10.160: INFO: Number of nodes with available pods: 0
Nov 19 17:16:10.160: INFO: Node ip-172-31-20-145 is running more than one daemon pod
Nov 19 17:16:11.160: INFO: Number of nodes with available pods: 0
Nov 19 17:16:11.160: INFO: Node ip-172-31-20-145 is running more than one daemon pod
Nov 19 17:16:12.160: INFO: Number of nodes with available pods: 0
Nov 19 17:16:12.160: INFO: Node ip-172-31-20-145 is running more than one daemon pod
Nov 19 17:16:13.160: INFO: Number of nodes with available pods: 0
Nov 19 17:16:13.161: INFO: Node ip-172-31-20-145 is running more than one daemon pod
Nov 19 17:16:14.163: INFO: Number of nodes with available pods: 0
Nov 19 17:16:14.163: INFO: Node ip-172-31-20-145 is running more than one daemon pod
Nov 19 17:16:15.160: INFO: Number of nodes with available pods: 0
Nov 19 17:16:15.160: INFO: Node ip-172-31-20-145 is running more than one daemon pod
Nov 19 17:16:16.161: INFO: Number of nodes with available pods: 0
Nov 19 17:16:16.161: INFO: Node ip-172-31-20-145 is running more than one daemon pod
Nov 19 17:16:17.160: INFO: Number of nodes with available pods: 0
Nov 19 17:16:17.160: INFO: Node ip-172-31-20-145 is running more than one daemon pod
Nov 19 17:16:18.160: INFO: Number of nodes with available pods: 0
Nov 19 17:16:18.160: INFO: Node ip-172-31-20-145 is running more than one daemon pod
Nov 19 17:16:19.161: INFO: Number of nodes with available pods: 1
Nov 19 17:16:19.161: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6482, will wait for the garbage collector to delete the pods
Nov 19 17:16:19.227: INFO: Deleting DaemonSet.extensions daemon-set took: 7.555249ms
Nov 19 17:16:19.327: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.141827ms
Nov 19 17:16:27.530: INFO: Number of nodes with available pods: 0
Nov 19 17:16:27.530: INFO: Number of running nodes: 0, number of available pods: 0
Nov 19 17:16:27.533: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6482/daemonsets","resourceVersion":"11017"},"items":null}

Nov 19 17:16:27.535: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6482/pods","resourceVersion":"11017"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:16:27.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6482" for this suite.

â€¢ [SLOW TEST:21.614 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":305,"completed":69,"skipped":935,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:16:27.564: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-53
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Nov 19 17:16:29.716: INFO: &Pod{ObjectMeta:{send-events-de961b18-921e-4452-8dff-33aa6240e0f5  events-53 /api/v1/namespaces/events-53/pods/send-events-de961b18-921e-4452-8dff-33aa6240e0f5 afb8a19f-e2b0-49d1-9631-ff2379c7d559 11035 0 2020-11-19 17:16:27 +0000 UTC <nil> <nil> map[name:foo time:695757165] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2020-11-19 17:16:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-19 17:16:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.33.61\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f89zv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f89zv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f89zv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-20-145,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:16:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:16:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:16:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 17:16:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.20.145,PodIP:10.1.33.61,StartTime:2020-11-19 17:16:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-19 17:16:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:containerd://aee81b6cf107cf0254159a273e425c8b478fed540aa507c1522dfefae93cc9af,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.33.61,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Nov 19 17:16:31.721: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Nov 19 17:16:33.725: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:16:33.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-53" for this suite.

â€¢ [SLOW TEST:6.176 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":305,"completed":70,"skipped":947,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:16:33.740: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-527
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
STEP: reading a file in the container
Nov 19 17:16:38.407: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-527 pod-service-account-7d903702-ac8b-4d57-bb84-a1ed75c34635 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Nov 19 17:16:38.570: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-527 pod-service-account-7d903702-ac8b-4d57-bb84-a1ed75c34635 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Nov 19 17:16:38.700: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-527 pod-service-account-7d903702-ac8b-4d57-bb84-a1ed75c34635 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:16:38.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-527" for this suite.

â€¢ [SLOW TEST:5.095 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":305,"completed":71,"skipped":962,"failed":0}
SSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:16:38.834: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8747
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Nov 19 17:16:38.981: INFO: Waiting up to 5m0s for pod "downward-api-c759e640-4665-449d-8d25-90be319579fc" in namespace "downward-api-8747" to be "Succeeded or Failed"
Nov 19 17:16:38.983: INFO: Pod "downward-api-c759e640-4665-449d-8d25-90be319579fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.450301ms
Nov 19 17:16:40.987: INFO: Pod "downward-api-c759e640-4665-449d-8d25-90be319579fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00672603s
STEP: Saw pod success
Nov 19 17:16:40.988: INFO: Pod "downward-api-c759e640-4665-449d-8d25-90be319579fc" satisfied condition "Succeeded or Failed"
Nov 19 17:16:40.990: INFO: Trying to get logs from node ip-172-31-9-169 pod downward-api-c759e640-4665-449d-8d25-90be319579fc container dapi-container: <nil>
STEP: delete the pod
Nov 19 17:16:41.007: INFO: Waiting for pod downward-api-c759e640-4665-449d-8d25-90be319579fc to disappear
Nov 19 17:16:41.009: INFO: Pod downward-api-c759e640-4665-449d-8d25-90be319579fc no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:16:41.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8747" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":305,"completed":72,"skipped":965,"failed":0}
S
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:16:41.017: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-1465
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
Nov 19 17:16:41.678: INFO: created pod pod-service-account-defaultsa
Nov 19 17:16:41.678: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Nov 19 17:16:41.683: INFO: created pod pod-service-account-mountsa
Nov 19 17:16:41.683: INFO: pod pod-service-account-mountsa service account token volume mount: true
Nov 19 17:16:41.689: INFO: created pod pod-service-account-nomountsa
Nov 19 17:16:41.689: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Nov 19 17:16:41.697: INFO: created pod pod-service-account-defaultsa-mountspec
Nov 19 17:16:41.697: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Nov 19 17:16:41.703: INFO: created pod pod-service-account-mountsa-mountspec
Nov 19 17:16:41.703: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Nov 19 17:16:41.710: INFO: created pod pod-service-account-nomountsa-mountspec
Nov 19 17:16:41.710: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Nov 19 17:16:41.716: INFO: created pod pod-service-account-defaultsa-nomountspec
Nov 19 17:16:41.716: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Nov 19 17:16:41.722: INFO: created pod pod-service-account-mountsa-nomountspec
Nov 19 17:16:41.722: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Nov 19 17:16:41.728: INFO: created pod pod-service-account-nomountsa-nomountspec
Nov 19 17:16:41.728: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:16:41.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1465" for this suite.
â€¢{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":305,"completed":73,"skipped":966,"failed":0}
SSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:16:41.736: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9376
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 17:16:41.987: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:16:44.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9376" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":305,"completed":74,"skipped":972,"failed":0}
SS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:16:44.073: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3798
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating pod
Nov 19 17:16:46.234: INFO: Pod pod-hostip-58436cd4-5dfc-4d26-bcca-da906fb1baa1 has hostIP: 172.31.9.169
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:16:46.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3798" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":305,"completed":75,"skipped":974,"failed":0}
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:16:46.242: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8818
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-8818
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 19 17:16:46.381: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 19 17:16:46.409: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 19 17:16:48.413: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 19 17:16:50.414: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 19 17:16:52.413: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 19 17:16:54.413: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 19 17:16:56.414: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 19 17:16:58.414: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 19 17:17:00.413: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 19 17:17:02.415: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 19 17:17:04.414: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov 19 17:17:04.420: INFO: The status of Pod netserver-1 is Running (Ready = true)
Nov 19 17:17:04.424: INFO: The status of Pod netserver-2 is Running (Ready = false)
Nov 19 17:17:06.429: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Nov 19 17:17:08.453: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.31.49:8080/dial?request=hostname&protocol=udp&host=10.1.33.64&port=8081&tries=1'] Namespace:pod-network-test-8818 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 19 17:17:08.453: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
Nov 19 17:17:08.535: INFO: Waiting for responses: map[]
Nov 19 17:17:08.539: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.31.49:8080/dial?request=hostname&protocol=udp&host=10.1.27.28&port=8081&tries=1'] Namespace:pod-network-test-8818 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 19 17:17:08.539: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
Nov 19 17:17:08.584: INFO: Waiting for responses: map[]
Nov 19 17:17:08.588: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.31.49:8080/dial?request=hostname&protocol=udp&host=10.1.31.48&port=8081&tries=1'] Namespace:pod-network-test-8818 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 19 17:17:08.588: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
Nov 19 17:17:08.661: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:17:08.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8818" for this suite.

â€¢ [SLOW TEST:22.429 seconds]
[sig-network] Networking
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":305,"completed":76,"skipped":976,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:17:08.672: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9426
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W1119 17:17:18.832631      22 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1119 17:17:18.832669      22 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1119 17:17:18.832673      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Nov 19 17:17:18.832: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:17:18.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9426" for this suite.

â€¢ [SLOW TEST:10.172 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":305,"completed":77,"skipped":1028,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:17:18.844: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-7258
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:17:18.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7258" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":305,"completed":78,"skipped":1029,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:17:18.990: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8084
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 19 17:17:19.137: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f70e8f32-c181-4113-a13b-4f965045df92" in namespace "projected-8084" to be "Succeeded or Failed"
Nov 19 17:17:19.139: INFO: Pod "downwardapi-volume-f70e8f32-c181-4113-a13b-4f965045df92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.524275ms
Nov 19 17:17:21.143: INFO: Pod "downwardapi-volume-f70e8f32-c181-4113-a13b-4f965045df92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006125641s
STEP: Saw pod success
Nov 19 17:17:21.143: INFO: Pod "downwardapi-volume-f70e8f32-c181-4113-a13b-4f965045df92" satisfied condition "Succeeded or Failed"
Nov 19 17:17:21.146: INFO: Trying to get logs from node ip-172-31-20-145 pod downwardapi-volume-f70e8f32-c181-4113-a13b-4f965045df92 container client-container: <nil>
STEP: delete the pod
Nov 19 17:17:21.162: INFO: Waiting for pod downwardapi-volume-f70e8f32-c181-4113-a13b-4f965045df92 to disappear
Nov 19 17:17:21.165: INFO: Pod downwardapi-volume-f70e8f32-c181-4113-a13b-4f965045df92 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:17:21.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8084" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":79,"skipped":1040,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:17:21.172: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8244
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1119 17:17:31.372069      22 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1119 17:17:31.372085      22 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1119 17:17:31.372090      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Nov 19 17:17:31.372: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Nov 19 17:17:31.372: INFO: Deleting pod "simpletest-rc-to-be-deleted-6b688" in namespace "gc-8244"
Nov 19 17:17:31.383: INFO: Deleting pod "simpletest-rc-to-be-deleted-6l689" in namespace "gc-8244"
Nov 19 17:17:31.393: INFO: Deleting pod "simpletest-rc-to-be-deleted-dx6cp" in namespace "gc-8244"
Nov 19 17:17:31.405: INFO: Deleting pod "simpletest-rc-to-be-deleted-gvtz4" in namespace "gc-8244"
Nov 19 17:17:31.419: INFO: Deleting pod "simpletest-rc-to-be-deleted-j7r76" in namespace "gc-8244"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:17:31.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8244" for this suite.

â€¢ [SLOW TEST:10.272 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":305,"completed":80,"skipped":1057,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:17:31.444: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5771
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5771.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5771.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5771.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5771.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5771.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5771.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 19 17:17:39.632: INFO: DNS probes using dns-5771/dns-test-720e5e1b-e572-4ec1-97f1-31a51b2a623e succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:17:39.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5771" for this suite.

â€¢ [SLOW TEST:8.224 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":305,"completed":81,"skipped":1081,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:17:39.668: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1861
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1861.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1861.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1861.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1861.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 19 17:17:41.860: INFO: DNS probes using dns-test-0d05e684-18c2-45ae-a38f-52f7465a8072 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1861.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1861.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1861.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1861.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 19 17:17:43.911: INFO: File wheezy_udp@dns-test-service-3.dns-1861.svc.cluster.local from pod  dns-1861/dns-test-bc27524d-0077-40ee-a047-20bfd8fb0ff5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 19 17:17:43.915: INFO: File jessie_udp@dns-test-service-3.dns-1861.svc.cluster.local from pod  dns-1861/dns-test-bc27524d-0077-40ee-a047-20bfd8fb0ff5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 19 17:17:43.915: INFO: Lookups using dns-1861/dns-test-bc27524d-0077-40ee-a047-20bfd8fb0ff5 failed for: [wheezy_udp@dns-test-service-3.dns-1861.svc.cluster.local jessie_udp@dns-test-service-3.dns-1861.svc.cluster.local]

Nov 19 17:17:48.924: INFO: DNS probes using dns-test-bc27524d-0077-40ee-a047-20bfd8fb0ff5 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1861.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1861.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1861.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1861.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 19 17:17:50.981: INFO: DNS probes using dns-test-65d3d16d-91b0-4275-bf8a-b99c5b8395f6 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:17:51.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1861" for this suite.

â€¢ [SLOW TEST:11.348 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":305,"completed":82,"skipped":1084,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:17:51.017: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2757
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-2757
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 19 17:17:51.213: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 19 17:17:51.243: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 19 17:17:53.248: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 19 17:17:55.248: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 19 17:17:57.248: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 19 17:17:59.247: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 19 17:18:01.247: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 19 17:18:03.247: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 19 17:18:05.248: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov 19 17:18:05.254: INFO: The status of Pod netserver-1 is Running (Ready = false)
Nov 19 17:18:07.258: INFO: The status of Pod netserver-1 is Running (Ready = true)
Nov 19 17:18:07.264: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Nov 19 17:18:09.303: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.1.33.71 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2757 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 19 17:18:09.303: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
Nov 19 17:18:10.391: INFO: Found all expected endpoints: [netserver-0]
Nov 19 17:18:10.395: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.1.27.33 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2757 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 19 17:18:10.395: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
Nov 19 17:18:11.475: INFO: Found all expected endpoints: [netserver-1]
Nov 19 17:18:11.479: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.1.31.57 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2757 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 19 17:18:11.479: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
Nov 19 17:18:12.560: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:18:12.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2757" for this suite.

â€¢ [SLOW TEST:21.554 seconds]
[sig-network] Networking
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":83,"skipped":1125,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:18:12.571: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1178
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 19 17:18:13.133: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 19 17:18:15.144: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741403093, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741403093, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741403093, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741403093, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 19 17:18:18.159: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:18:18.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1178" for this suite.
STEP: Destroying namespace "webhook-1178-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:5.710 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":305,"completed":84,"skipped":1125,"failed":0}
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:18:18.281: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-95
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-95
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 19 17:18:18.429: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 19 17:18:18.463: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 19 17:18:20.468: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 19 17:18:22.467: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 19 17:18:24.467: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 19 17:18:26.468: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 19 17:18:28.467: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 19 17:18:30.468: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 19 17:18:32.467: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov 19 17:18:32.473: INFO: The status of Pod netserver-1 is Running (Ready = false)
Nov 19 17:18:34.477: INFO: The status of Pod netserver-1 is Running (Ready = false)
Nov 19 17:18:36.479: INFO: The status of Pod netserver-1 is Running (Ready = false)
Nov 19 17:18:38.478: INFO: The status of Pod netserver-1 is Running (Ready = false)
Nov 19 17:18:40.477: INFO: The status of Pod netserver-1 is Running (Ready = true)
Nov 19 17:18:40.483: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Nov 19 17:18:42.504: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.31.61:8080/dial?request=hostname&protocol=http&host=10.1.33.73&port=8080&tries=1'] Namespace:pod-network-test-95 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 19 17:18:42.504: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
Nov 19 17:18:42.581: INFO: Waiting for responses: map[]
Nov 19 17:18:42.585: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.31.61:8080/dial?request=hostname&protocol=http&host=10.1.27.34&port=8080&tries=1'] Namespace:pod-network-test-95 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 19 17:18:42.585: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
Nov 19 17:18:42.662: INFO: Waiting for responses: map[]
Nov 19 17:18:42.665: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.31.61:8080/dial?request=hostname&protocol=http&host=10.1.31.60&port=8080&tries=1'] Namespace:pod-network-test-95 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 19 17:18:42.665: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
Nov 19 17:18:42.707: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:18:42.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-95" for this suite.

â€¢ [SLOW TEST:24.436 seconds]
[sig-network] Networking
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":305,"completed":85,"skipped":1126,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:18:42.717: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4762
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Nov 19 17:18:42.864: INFO: Waiting up to 5m0s for pod "pod-46e9b277-cb89-4a44-a819-b61eccb37509" in namespace "emptydir-4762" to be "Succeeded or Failed"
Nov 19 17:18:42.870: INFO: Pod "pod-46e9b277-cb89-4a44-a819-b61eccb37509": Phase="Pending", Reason="", readiness=false. Elapsed: 5.207196ms
Nov 19 17:18:44.874: INFO: Pod "pod-46e9b277-cb89-4a44-a819-b61eccb37509": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009967539s
STEP: Saw pod success
Nov 19 17:18:44.874: INFO: Pod "pod-46e9b277-cb89-4a44-a819-b61eccb37509" satisfied condition "Succeeded or Failed"
Nov 19 17:18:44.877: INFO: Trying to get logs from node ip-172-31-20-145 pod pod-46e9b277-cb89-4a44-a819-b61eccb37509 container test-container: <nil>
STEP: delete the pod
Nov 19 17:18:44.899: INFO: Waiting for pod pod-46e9b277-cb89-4a44-a819-b61eccb37509 to disappear
Nov 19 17:18:44.902: INFO: Pod pod-46e9b277-cb89-4a44-a819-b61eccb37509 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:18:44.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4762" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":86,"skipped":1131,"failed":0}

------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:18:44.909: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1844
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 17:18:47.081: INFO: Waiting up to 5m0s for pod "client-envvars-00c76096-890b-4bb4-b4ab-a51497bff2fe" in namespace "pods-1844" to be "Succeeded or Failed"
Nov 19 17:18:47.083: INFO: Pod "client-envvars-00c76096-890b-4bb4-b4ab-a51497bff2fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.765052ms
Nov 19 17:18:49.087: INFO: Pod "client-envvars-00c76096-890b-4bb4-b4ab-a51497bff2fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006740011s
STEP: Saw pod success
Nov 19 17:18:49.087: INFO: Pod "client-envvars-00c76096-890b-4bb4-b4ab-a51497bff2fe" satisfied condition "Succeeded or Failed"
Nov 19 17:18:49.090: INFO: Trying to get logs from node ip-172-31-9-169 pod client-envvars-00c76096-890b-4bb4-b4ab-a51497bff2fe container env3cont: <nil>
STEP: delete the pod
Nov 19 17:18:49.116: INFO: Waiting for pod client-envvars-00c76096-890b-4bb4-b4ab-a51497bff2fe to disappear
Nov 19 17:18:49.118: INFO: Pod client-envvars-00c76096-890b-4bb4-b4ab-a51497bff2fe no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:18:49.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1844" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":305,"completed":87,"skipped":1131,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:18:49.126: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5723
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 19 17:18:49.272: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d7664d96-fc14-4561-afec-95c2d812c070" in namespace "projected-5723" to be "Succeeded or Failed"
Nov 19 17:18:49.275: INFO: Pod "downwardapi-volume-d7664d96-fc14-4561-afec-95c2d812c070": Phase="Pending", Reason="", readiness=false. Elapsed: 2.765433ms
Nov 19 17:18:51.279: INFO: Pod "downwardapi-volume-d7664d96-fc14-4561-afec-95c2d812c070": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006155303s
STEP: Saw pod success
Nov 19 17:18:51.279: INFO: Pod "downwardapi-volume-d7664d96-fc14-4561-afec-95c2d812c070" satisfied condition "Succeeded or Failed"
Nov 19 17:18:51.281: INFO: Trying to get logs from node ip-172-31-9-169 pod downwardapi-volume-d7664d96-fc14-4561-afec-95c2d812c070 container client-container: <nil>
STEP: delete the pod
Nov 19 17:18:51.297: INFO: Waiting for pod downwardapi-volume-d7664d96-fc14-4561-afec-95c2d812c070 to disappear
Nov 19 17:18:51.299: INFO: Pod downwardapi-volume-d7664d96-fc14-4561-afec-95c2d812c070 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:18:51.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5723" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":305,"completed":88,"skipped":1180,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:18:51.306: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4326
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Nov 19 17:18:53.973: INFO: Successfully updated pod "pod-update-activedeadlineseconds-76c4121d-98ec-44eb-bdcd-c6cceb800130"
Nov 19 17:18:53.973: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-76c4121d-98ec-44eb-bdcd-c6cceb800130" in namespace "pods-4326" to be "terminated due to deadline exceeded"
Nov 19 17:18:53.976: INFO: Pod "pod-update-activedeadlineseconds-76c4121d-98ec-44eb-bdcd-c6cceb800130": Phase="Running", Reason="", readiness=true. Elapsed: 2.890985ms
Nov 19 17:18:55.981: INFO: Pod "pod-update-activedeadlineseconds-76c4121d-98ec-44eb-bdcd-c6cceb800130": Phase="Running", Reason="", readiness=true. Elapsed: 2.007234411s
Nov 19 17:18:57.985: INFO: Pod "pod-update-activedeadlineseconds-76c4121d-98ec-44eb-bdcd-c6cceb800130": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.011591216s
Nov 19 17:18:57.985: INFO: Pod "pod-update-activedeadlineseconds-76c4121d-98ec-44eb-bdcd-c6cceb800130" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:18:57.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4326" for this suite.

â€¢ [SLOW TEST:6.687 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":305,"completed":89,"skipped":1206,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:18:57.994: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6332
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-beeff45d-2183-41ec-9c7f-52a9f28edd82
STEP: Creating a pod to test consume secrets
Nov 19 17:18:58.139: INFO: Waiting up to 5m0s for pod "pod-secrets-017ddf60-5ad9-4aa1-9e85-fb161f510f4f" in namespace "secrets-6332" to be "Succeeded or Failed"
Nov 19 17:18:58.141: INFO: Pod "pod-secrets-017ddf60-5ad9-4aa1-9e85-fb161f510f4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.36823ms
Nov 19 17:19:00.146: INFO: Pod "pod-secrets-017ddf60-5ad9-4aa1-9e85-fb161f510f4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007272361s
STEP: Saw pod success
Nov 19 17:19:00.146: INFO: Pod "pod-secrets-017ddf60-5ad9-4aa1-9e85-fb161f510f4f" satisfied condition "Succeeded or Failed"
Nov 19 17:19:00.149: INFO: Trying to get logs from node ip-172-31-20-145 pod pod-secrets-017ddf60-5ad9-4aa1-9e85-fb161f510f4f container secret-volume-test: <nil>
STEP: delete the pod
Nov 19 17:19:00.166: INFO: Waiting for pod pod-secrets-017ddf60-5ad9-4aa1-9e85-fb161f510f4f to disappear
Nov 19 17:19:00.168: INFO: Pod pod-secrets-017ddf60-5ad9-4aa1-9e85-fb161f510f4f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:19:00.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6332" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":90,"skipped":1289,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:19:00.177: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-6582
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 17:19:00.311: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6582
I1119 17:19:00.320522      22 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6582, replica count: 1
I1119 17:19:01.370800      22 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 19 17:19:01.480: INFO: Created: latency-svc-vhq94
Nov 19 17:19:01.489: INFO: Got endpoints: latency-svc-vhq94 [18.340422ms]
Nov 19 17:19:01.500: INFO: Created: latency-svc-p6tqn
Nov 19 17:19:01.509: INFO: Created: latency-svc-xppkz
Nov 19 17:19:01.510: INFO: Got endpoints: latency-svc-p6tqn [21.170872ms]
Nov 19 17:19:01.524: INFO: Got endpoints: latency-svc-xppkz [34.455702ms]
Nov 19 17:19:01.524: INFO: Created: latency-svc-2fq6s
Nov 19 17:19:01.532: INFO: Got endpoints: latency-svc-2fq6s [43.056397ms]
Nov 19 17:19:01.534: INFO: Created: latency-svc-pd8c7
Nov 19 17:19:01.541: INFO: Got endpoints: latency-svc-pd8c7 [52.10305ms]
Nov 19 17:19:01.543: INFO: Created: latency-svc-bbhtq
Nov 19 17:19:01.546: INFO: Created: latency-svc-29kk8
Nov 19 17:19:01.551: INFO: Got endpoints: latency-svc-29kk8 [62.150128ms]
Nov 19 17:19:01.552: INFO: Got endpoints: latency-svc-bbhtq [62.967542ms]
Nov 19 17:19:01.556: INFO: Created: latency-svc-6p2p8
Nov 19 17:19:01.562: INFO: Got endpoints: latency-svc-6p2p8 [72.795068ms]
Nov 19 17:19:01.566: INFO: Created: latency-svc-fnhmd
Nov 19 17:19:01.570: INFO: Created: latency-svc-c4lb7
Nov 19 17:19:01.570: INFO: Got endpoints: latency-svc-fnhmd [80.855064ms]
Nov 19 17:19:01.575: INFO: Created: latency-svc-9nsxm
Nov 19 17:19:01.576: INFO: Got endpoints: latency-svc-c4lb7 [87.120912ms]
Nov 19 17:19:01.581: INFO: Got endpoints: latency-svc-9nsxm [91.566756ms]
Nov 19 17:19:01.590: INFO: Created: latency-svc-6tl7z
Nov 19 17:19:01.591: INFO: Created: latency-svc-6bh87
Nov 19 17:19:01.595: INFO: Got endpoints: latency-svc-6bh87 [105.69379ms]
Nov 19 17:19:01.597: INFO: Created: latency-svc-4pp5x
Nov 19 17:19:01.600: INFO: Got endpoints: latency-svc-6tl7z [111.20226ms]
Nov 19 17:19:01.604: INFO: Got endpoints: latency-svc-4pp5x [114.587652ms]
Nov 19 17:19:01.608: INFO: Created: latency-svc-8c4vj
Nov 19 17:19:01.613: INFO: Created: latency-svc-fcnn9
Nov 19 17:19:01.617: INFO: Got endpoints: latency-svc-fcnn9 [128.363043ms]
Nov 19 17:19:01.619: INFO: Got endpoints: latency-svc-8c4vj [129.749655ms]
Nov 19 17:19:01.621: INFO: Created: latency-svc-5mtkt
Nov 19 17:19:01.626: INFO: Got endpoints: latency-svc-5mtkt [115.641473ms]
Nov 19 17:19:01.635: INFO: Created: latency-svc-ml6np
Nov 19 17:19:01.637: INFO: Created: latency-svc-s6skl
Nov 19 17:19:01.637: INFO: Got endpoints: latency-svc-ml6np [19.934127ms]
Nov 19 17:19:01.641: INFO: Got endpoints: latency-svc-s6skl [117.376347ms]
Nov 19 17:19:01.643: INFO: Created: latency-svc-n5xqn
Nov 19 17:19:01.648: INFO: Got endpoints: latency-svc-n5xqn [115.446093ms]
Nov 19 17:19:01.659: INFO: Created: latency-svc-q9jfg
Nov 19 17:19:01.659: INFO: Created: latency-svc-zv68s
Nov 19 17:19:01.664: INFO: Created: latency-svc-lpf9c
Nov 19 17:19:01.666: INFO: Got endpoints: latency-svc-q9jfg [125.245742ms]
Nov 19 17:19:01.667: INFO: Got endpoints: latency-svc-zv68s [115.740112ms]
Nov 19 17:19:01.672: INFO: Got endpoints: latency-svc-lpf9c [120.00541ms]
Nov 19 17:19:01.678: INFO: Created: latency-svc-h5t7p
Nov 19 17:19:01.682: INFO: Created: latency-svc-6fh6r
Nov 19 17:19:01.683: INFO: Got endpoints: latency-svc-h5t7p [120.79736ms]
Nov 19 17:19:01.688: INFO: Got endpoints: latency-svc-6fh6r [117.678685ms]
Nov 19 17:19:01.688: INFO: Created: latency-svc-mxprb
Nov 19 17:19:01.693: INFO: Got endpoints: latency-svc-mxprb [116.719689ms]
Nov 19 17:19:01.702: INFO: Created: latency-svc-ttqq7
Nov 19 17:19:01.706: INFO: Got endpoints: latency-svc-ttqq7 [111.395814ms]
Nov 19 17:19:01.736: INFO: Created: latency-svc-bv7w2
Nov 19 17:19:01.739: INFO: Got endpoints: latency-svc-bv7w2 [158.095547ms]
Nov 19 17:19:01.750: INFO: Created: latency-svc-csnrh
Nov 19 17:19:01.760: INFO: Got endpoints: latency-svc-csnrh [160.308276ms]
Nov 19 17:19:01.764: INFO: Created: latency-svc-7n62t
Nov 19 17:19:01.765: INFO: Created: latency-svc-5t8nz
Nov 19 17:19:01.770: INFO: Got endpoints: latency-svc-7n62t [151.037684ms]
Nov 19 17:19:01.770: INFO: Got endpoints: latency-svc-5t8nz [166.712682ms]
Nov 19 17:19:01.773: INFO: Created: latency-svc-6pvsl
Nov 19 17:19:01.777: INFO: Got endpoints: latency-svc-6pvsl [151.156736ms]
Nov 19 17:19:01.783: INFO: Created: latency-svc-mv94n
Nov 19 17:19:01.788: INFO: Created: latency-svc-srxcj
Nov 19 17:19:01.792: INFO: Got endpoints: latency-svc-srxcj [150.556725ms]
Nov 19 17:19:01.792: INFO: Got endpoints: latency-svc-mv94n [154.699625ms]
Nov 19 17:19:01.795: INFO: Created: latency-svc-r488j
Nov 19 17:19:01.800: INFO: Got endpoints: latency-svc-r488j [151.801327ms]
Nov 19 17:19:01.806: INFO: Created: latency-svc-vr67x
Nov 19 17:19:01.807: INFO: Created: latency-svc-vd8d8
Nov 19 17:19:01.808: INFO: Got endpoints: latency-svc-vr67x [141.75045ms]
Nov 19 17:19:01.813: INFO: Created: latency-svc-h2chb
Nov 19 17:19:01.821: INFO: Created: latency-svc-mpsvs
Nov 19 17:19:01.824: INFO: Created: latency-svc-gxw7v
Nov 19 17:19:01.829: INFO: Created: latency-svc-gd56d
Nov 19 17:19:01.835: INFO: Got endpoints: latency-svc-vd8d8 [167.920417ms]
Nov 19 17:19:01.841: INFO: Created: latency-svc-xv6lb
Nov 19 17:19:01.844: INFO: Created: latency-svc-j8hrr
Nov 19 17:19:01.853: INFO: Created: latency-svc-p5tg4
Nov 19 17:19:01.861: INFO: Created: latency-svc-7c497
Nov 19 17:19:01.862: INFO: Created: latency-svc-n2vq4
Nov 19 17:19:01.866: INFO: Created: latency-svc-tzk54
Nov 19 17:19:01.889: INFO: Got endpoints: latency-svc-h2chb [216.231856ms]
Nov 19 17:19:01.932: INFO: Created: latency-svc-84cpg
Nov 19 17:19:01.935: INFO: Created: latency-svc-vq92d
Nov 19 17:19:01.936: INFO: Got endpoints: latency-svc-mpsvs [253.112104ms]
Nov 19 17:19:01.941: INFO: Created: latency-svc-8q7xh
Nov 19 17:19:01.950: INFO: Created: latency-svc-qwnps
Nov 19 17:19:01.951: INFO: Created: latency-svc-c9bf7
Nov 19 17:19:01.956: INFO: Created: latency-svc-74wb2
Nov 19 17:19:01.961: INFO: Created: latency-svc-kccnd
Nov 19 17:19:01.985: INFO: Got endpoints: latency-svc-gxw7v [297.501089ms]
Nov 19 17:19:01.995: INFO: Created: latency-svc-snsfv
Nov 19 17:19:02.035: INFO: Got endpoints: latency-svc-gd56d [342.137269ms]
Nov 19 17:19:02.044: INFO: Created: latency-svc-7k7fv
Nov 19 17:19:02.085: INFO: Got endpoints: latency-svc-xv6lb [378.804656ms]
Nov 19 17:19:02.095: INFO: Created: latency-svc-lszsx
Nov 19 17:19:02.134: INFO: Got endpoints: latency-svc-j8hrr [395.150184ms]
Nov 19 17:19:02.142: INFO: Created: latency-svc-lqwzs
Nov 19 17:19:02.185: INFO: Got endpoints: latency-svc-p5tg4 [424.401537ms]
Nov 19 17:19:02.192: INFO: Created: latency-svc-jzf9n
Nov 19 17:19:02.234: INFO: Got endpoints: latency-svc-7c497 [464.157287ms]
Nov 19 17:19:02.250: INFO: Created: latency-svc-jgcsc
Nov 19 17:19:02.285: INFO: Got endpoints: latency-svc-n2vq4 [514.987605ms]
Nov 19 17:19:02.292: INFO: Created: latency-svc-7sbqk
Nov 19 17:19:02.336: INFO: Got endpoints: latency-svc-tzk54 [559.093363ms]
Nov 19 17:19:02.344: INFO: Created: latency-svc-8wzgr
Nov 19 17:19:02.385: INFO: Got endpoints: latency-svc-84cpg [593.470106ms]
Nov 19 17:19:02.395: INFO: Created: latency-svc-8cfv5
Nov 19 17:19:02.435: INFO: Got endpoints: latency-svc-vq92d [642.980182ms]
Nov 19 17:19:02.443: INFO: Created: latency-svc-54slx
Nov 19 17:19:02.486: INFO: Got endpoints: latency-svc-8q7xh [686.48331ms]
Nov 19 17:19:02.495: INFO: Created: latency-svc-6lwqc
Nov 19 17:19:02.536: INFO: Got endpoints: latency-svc-qwnps [727.959682ms]
Nov 19 17:19:02.545: INFO: Created: latency-svc-7b5w2
Nov 19 17:19:02.586: INFO: Got endpoints: latency-svc-c9bf7 [750.937466ms]
Nov 19 17:19:02.593: INFO: Created: latency-svc-zxpbp
Nov 19 17:19:02.635: INFO: Got endpoints: latency-svc-74wb2 [746.362865ms]
Nov 19 17:19:02.644: INFO: Created: latency-svc-8gd4d
Nov 19 17:19:02.684: INFO: Got endpoints: latency-svc-kccnd [748.062027ms]
Nov 19 17:19:02.695: INFO: Created: latency-svc-9fl48
Nov 19 17:19:02.735: INFO: Got endpoints: latency-svc-snsfv [749.637233ms]
Nov 19 17:19:02.742: INFO: Created: latency-svc-hflxz
Nov 19 17:19:02.785: INFO: Got endpoints: latency-svc-7k7fv [749.536802ms]
Nov 19 17:19:02.795: INFO: Created: latency-svc-5cczm
Nov 19 17:19:02.836: INFO: Got endpoints: latency-svc-lszsx [750.652814ms]
Nov 19 17:19:02.845: INFO: Created: latency-svc-vtzkz
Nov 19 17:19:02.885: INFO: Got endpoints: latency-svc-lqwzs [751.26958ms]
Nov 19 17:19:02.893: INFO: Created: latency-svc-8z58w
Nov 19 17:19:02.934: INFO: Got endpoints: latency-svc-jzf9n [748.900742ms]
Nov 19 17:19:02.942: INFO: Created: latency-svc-psz2m
Nov 19 17:19:02.985: INFO: Got endpoints: latency-svc-jgcsc [750.30645ms]
Nov 19 17:19:02.994: INFO: Created: latency-svc-z6w6p
Nov 19 17:19:03.035: INFO: Got endpoints: latency-svc-7sbqk [750.071263ms]
Nov 19 17:19:03.043: INFO: Created: latency-svc-89hcs
Nov 19 17:19:03.086: INFO: Got endpoints: latency-svc-8wzgr [749.079275ms]
Nov 19 17:19:03.095: INFO: Created: latency-svc-76dt9
Nov 19 17:19:03.134: INFO: Got endpoints: latency-svc-8cfv5 [748.582773ms]
Nov 19 17:19:03.145: INFO: Created: latency-svc-8p5zn
Nov 19 17:19:03.189: INFO: Got endpoints: latency-svc-54slx [754.584957ms]
Nov 19 17:19:03.197: INFO: Created: latency-svc-b77xw
Nov 19 17:19:03.236: INFO: Got endpoints: latency-svc-6lwqc [749.545313ms]
Nov 19 17:19:03.245: INFO: Created: latency-svc-4cjxs
Nov 19 17:19:03.285: INFO: Got endpoints: latency-svc-7b5w2 [749.045692ms]
Nov 19 17:19:03.294: INFO: Created: latency-svc-x7njh
Nov 19 17:19:03.334: INFO: Got endpoints: latency-svc-zxpbp [748.405555ms]
Nov 19 17:19:03.349: INFO: Created: latency-svc-bzzr4
Nov 19 17:19:03.384: INFO: Got endpoints: latency-svc-8gd4d [749.471953ms]
Nov 19 17:19:03.393: INFO: Created: latency-svc-8f7gc
Nov 19 17:19:03.435: INFO: Got endpoints: latency-svc-9fl48 [751.055696ms]
Nov 19 17:19:03.444: INFO: Created: latency-svc-r4r92
Nov 19 17:19:03.486: INFO: Got endpoints: latency-svc-hflxz [750.586722ms]
Nov 19 17:19:03.495: INFO: Created: latency-svc-qgqmx
Nov 19 17:19:03.535: INFO: Got endpoints: latency-svc-5cczm [750.663646ms]
Nov 19 17:19:03.545: INFO: Created: latency-svc-v4xnt
Nov 19 17:19:03.585: INFO: Got endpoints: latency-svc-vtzkz [749.537514ms]
Nov 19 17:19:03.593: INFO: Created: latency-svc-wqfnq
Nov 19 17:19:03.637: INFO: Got endpoints: latency-svc-8z58w [751.81501ms]
Nov 19 17:19:03.646: INFO: Created: latency-svc-2d6wr
Nov 19 17:19:03.684: INFO: Got endpoints: latency-svc-psz2m [750.080603ms]
Nov 19 17:19:03.692: INFO: Created: latency-svc-9mn7l
Nov 19 17:19:03.740: INFO: Got endpoints: latency-svc-z6w6p [755.631759ms]
Nov 19 17:19:03.748: INFO: Created: latency-svc-thrlc
Nov 19 17:19:03.785: INFO: Got endpoints: latency-svc-89hcs [749.572583ms]
Nov 19 17:19:03.796: INFO: Created: latency-svc-sw2gg
Nov 19 17:19:03.835: INFO: Got endpoints: latency-svc-76dt9 [749.662027ms]
Nov 19 17:19:03.844: INFO: Created: latency-svc-m9hx7
Nov 19 17:19:03.885: INFO: Got endpoints: latency-svc-8p5zn [751.163566ms]
Nov 19 17:19:03.895: INFO: Created: latency-svc-qx4dw
Nov 19 17:19:03.934: INFO: Got endpoints: latency-svc-b77xw [744.819834ms]
Nov 19 17:19:03.942: INFO: Created: latency-svc-98cln
Nov 19 17:19:03.987: INFO: Got endpoints: latency-svc-4cjxs [751.010447ms]
Nov 19 17:19:03.997: INFO: Created: latency-svc-s7bqr
Nov 19 17:19:04.035: INFO: Got endpoints: latency-svc-x7njh [750.071539ms]
Nov 19 17:19:04.043: INFO: Created: latency-svc-2p89g
Nov 19 17:19:04.084: INFO: Got endpoints: latency-svc-bzzr4 [749.63638ms]
Nov 19 17:19:04.093: INFO: Created: latency-svc-8rjdl
Nov 19 17:19:04.134: INFO: Got endpoints: latency-svc-8f7gc [749.5409ms]
Nov 19 17:19:04.144: INFO: Created: latency-svc-r7wgb
Nov 19 17:19:04.185: INFO: Got endpoints: latency-svc-r4r92 [749.933599ms]
Nov 19 17:19:04.193: INFO: Created: latency-svc-5mzch
Nov 19 17:19:04.235: INFO: Got endpoints: latency-svc-qgqmx [749.164845ms]
Nov 19 17:19:04.244: INFO: Created: latency-svc-jvw2w
Nov 19 17:19:04.284: INFO: Got endpoints: latency-svc-v4xnt [748.826872ms]
Nov 19 17:19:04.292: INFO: Created: latency-svc-vvmpk
Nov 19 17:19:04.335: INFO: Got endpoints: latency-svc-wqfnq [750.018589ms]
Nov 19 17:19:04.344: INFO: Created: latency-svc-wvfjr
Nov 19 17:19:04.385: INFO: Got endpoints: latency-svc-2d6wr [748.216842ms]
Nov 19 17:19:04.393: INFO: Created: latency-svc-k9th9
Nov 19 17:19:04.434: INFO: Got endpoints: latency-svc-9mn7l [750.293452ms]
Nov 19 17:19:04.445: INFO: Created: latency-svc-9xrvz
Nov 19 17:19:04.485: INFO: Got endpoints: latency-svc-thrlc [745.207118ms]
Nov 19 17:19:04.495: INFO: Created: latency-svc-r2qx5
Nov 19 17:19:04.535: INFO: Got endpoints: latency-svc-sw2gg [750.189378ms]
Nov 19 17:19:04.544: INFO: Created: latency-svc-sc7bt
Nov 19 17:19:04.585: INFO: Got endpoints: latency-svc-m9hx7 [749.797419ms]
Nov 19 17:19:04.593: INFO: Created: latency-svc-m6hsc
Nov 19 17:19:04.635: INFO: Got endpoints: latency-svc-qx4dw [749.552433ms]
Nov 19 17:19:04.645: INFO: Created: latency-svc-hpkvm
Nov 19 17:19:04.685: INFO: Got endpoints: latency-svc-98cln [750.783499ms]
Nov 19 17:19:04.695: INFO: Created: latency-svc-mdjlj
Nov 19 17:19:04.734: INFO: Got endpoints: latency-svc-s7bqr [747.16581ms]
Nov 19 17:19:04.743: INFO: Created: latency-svc-qt7xl
Nov 19 17:19:04.785: INFO: Got endpoints: latency-svc-2p89g [750.045024ms]
Nov 19 17:19:04.794: INFO: Created: latency-svc-s624m
Nov 19 17:19:04.836: INFO: Got endpoints: latency-svc-8rjdl [751.435901ms]
Nov 19 17:19:04.844: INFO: Created: latency-svc-426xx
Nov 19 17:19:04.885: INFO: Got endpoints: latency-svc-r7wgb [751.039518ms]
Nov 19 17:19:04.896: INFO: Created: latency-svc-nkdww
Nov 19 17:19:04.935: INFO: Got endpoints: latency-svc-5mzch [749.642124ms]
Nov 19 17:19:04.943: INFO: Created: latency-svc-sh8f4
Nov 19 17:19:04.985: INFO: Got endpoints: latency-svc-jvw2w [750.4865ms]
Nov 19 17:19:04.993: INFO: Created: latency-svc-9pgj4
Nov 19 17:19:05.035: INFO: Got endpoints: latency-svc-vvmpk [750.898831ms]
Nov 19 17:19:05.044: INFO: Created: latency-svc-g945r
Nov 19 17:19:05.085: INFO: Got endpoints: latency-svc-wvfjr [749.664523ms]
Nov 19 17:19:05.093: INFO: Created: latency-svc-7d5kw
Nov 19 17:19:05.134: INFO: Got endpoints: latency-svc-k9th9 [748.905199ms]
Nov 19 17:19:05.142: INFO: Created: latency-svc-4m99h
Nov 19 17:19:05.186: INFO: Got endpoints: latency-svc-9xrvz [751.795396ms]
Nov 19 17:19:05.199: INFO: Created: latency-svc-r65fw
Nov 19 17:19:05.236: INFO: Got endpoints: latency-svc-r2qx5 [750.862213ms]
Nov 19 17:19:05.246: INFO: Created: latency-svc-98zk2
Nov 19 17:19:05.285: INFO: Got endpoints: latency-svc-sc7bt [749.978406ms]
Nov 19 17:19:05.293: INFO: Created: latency-svc-mfvdp
Nov 19 17:19:05.336: INFO: Got endpoints: latency-svc-m6hsc [750.498038ms]
Nov 19 17:19:05.347: INFO: Created: latency-svc-q5g7z
Nov 19 17:19:05.384: INFO: Got endpoints: latency-svc-hpkvm [749.371571ms]
Nov 19 17:19:05.392: INFO: Created: latency-svc-t52hg
Nov 19 17:19:05.435: INFO: Got endpoints: latency-svc-mdjlj [750.421591ms]
Nov 19 17:19:05.444: INFO: Created: latency-svc-thnfl
Nov 19 17:19:05.486: INFO: Got endpoints: latency-svc-qt7xl [751.956394ms]
Nov 19 17:19:05.496: INFO: Created: latency-svc-5z8mz
Nov 19 17:19:05.535: INFO: Got endpoints: latency-svc-s624m [749.821189ms]
Nov 19 17:19:05.545: INFO: Created: latency-svc-9cdrt
Nov 19 17:19:05.588: INFO: Got endpoints: latency-svc-426xx [752.532658ms]
Nov 19 17:19:05.596: INFO: Created: latency-svc-p8gz8
Nov 19 17:19:05.635: INFO: Got endpoints: latency-svc-nkdww [749.970485ms]
Nov 19 17:19:05.644: INFO: Created: latency-svc-pbfww
Nov 19 17:19:05.684: INFO: Got endpoints: latency-svc-sh8f4 [749.009985ms]
Nov 19 17:19:05.692: INFO: Created: latency-svc-lqnrt
Nov 19 17:19:05.736: INFO: Got endpoints: latency-svc-9pgj4 [750.557789ms]
Nov 19 17:19:05.744: INFO: Created: latency-svc-tdfbb
Nov 19 17:19:05.786: INFO: Got endpoints: latency-svc-g945r [750.413568ms]
Nov 19 17:19:05.794: INFO: Created: latency-svc-x7tfb
Nov 19 17:19:05.834: INFO: Got endpoints: latency-svc-7d5kw [748.717365ms]
Nov 19 17:19:05.843: INFO: Created: latency-svc-xvncw
Nov 19 17:19:05.885: INFO: Got endpoints: latency-svc-4m99h [750.457889ms]
Nov 19 17:19:05.894: INFO: Created: latency-svc-qlbml
Nov 19 17:19:05.937: INFO: Got endpoints: latency-svc-r65fw [750.427371ms]
Nov 19 17:19:05.945: INFO: Created: latency-svc-fcth4
Nov 19 17:19:05.985: INFO: Got endpoints: latency-svc-98zk2 [748.654975ms]
Nov 19 17:19:05.993: INFO: Created: latency-svc-dlqrl
Nov 19 17:19:06.035: INFO: Got endpoints: latency-svc-mfvdp [749.680534ms]
Nov 19 17:19:06.043: INFO: Created: latency-svc-t97bk
Nov 19 17:19:06.086: INFO: Got endpoints: latency-svc-q5g7z [749.90944ms]
Nov 19 17:19:06.097: INFO: Created: latency-svc-9wtxg
Nov 19 17:19:06.133: INFO: Got endpoints: latency-svc-t52hg [749.240643ms]
Nov 19 17:19:06.145: INFO: Created: latency-svc-5rscs
Nov 19 17:19:06.185: INFO: Got endpoints: latency-svc-thnfl [749.139442ms]
Nov 19 17:19:06.194: INFO: Created: latency-svc-7792g
Nov 19 17:19:06.237: INFO: Got endpoints: latency-svc-5z8mz [750.744056ms]
Nov 19 17:19:06.245: INFO: Created: latency-svc-cb4sm
Nov 19 17:19:06.285: INFO: Got endpoints: latency-svc-9cdrt [749.730794ms]
Nov 19 17:19:06.294: INFO: Created: latency-svc-qrtw5
Nov 19 17:19:06.335: INFO: Got endpoints: latency-svc-p8gz8 [746.898581ms]
Nov 19 17:19:06.343: INFO: Created: latency-svc-sv9gx
Nov 19 17:19:06.385: INFO: Got endpoints: latency-svc-pbfww [749.806161ms]
Nov 19 17:19:06.395: INFO: Created: latency-svc-svc97
Nov 19 17:19:06.434: INFO: Got endpoints: latency-svc-lqnrt [750.165404ms]
Nov 19 17:19:06.443: INFO: Created: latency-svc-scj4r
Nov 19 17:19:06.484: INFO: Got endpoints: latency-svc-tdfbb [748.011066ms]
Nov 19 17:19:06.495: INFO: Created: latency-svc-gsldk
Nov 19 17:19:06.535: INFO: Got endpoints: latency-svc-x7tfb [749.21792ms]
Nov 19 17:19:06.544: INFO: Created: latency-svc-lsfhq
Nov 19 17:19:06.586: INFO: Got endpoints: latency-svc-xvncw [752.430446ms]
Nov 19 17:19:06.595: INFO: Created: latency-svc-l2mtj
Nov 19 17:19:06.635: INFO: Got endpoints: latency-svc-qlbml [750.518206ms]
Nov 19 17:19:06.643: INFO: Created: latency-svc-94tcs
Nov 19 17:19:06.684: INFO: Got endpoints: latency-svc-fcth4 [747.809952ms]
Nov 19 17:19:06.694: INFO: Created: latency-svc-82n6n
Nov 19 17:19:06.735: INFO: Got endpoints: latency-svc-dlqrl [749.636828ms]
Nov 19 17:19:06.744: INFO: Created: latency-svc-q4rb5
Nov 19 17:19:06.786: INFO: Got endpoints: latency-svc-t97bk [750.436975ms]
Nov 19 17:19:06.793: INFO: Created: latency-svc-gqnst
Nov 19 17:19:06.835: INFO: Got endpoints: latency-svc-9wtxg [749.243203ms]
Nov 19 17:19:06.843: INFO: Created: latency-svc-j89hw
Nov 19 17:19:06.885: INFO: Got endpoints: latency-svc-5rscs [751.661886ms]
Nov 19 17:19:06.893: INFO: Created: latency-svc-z5vvk
Nov 19 17:19:06.934: INFO: Got endpoints: latency-svc-7792g [748.934058ms]
Nov 19 17:19:06.942: INFO: Created: latency-svc-5wnbz
Nov 19 17:19:06.984: INFO: Got endpoints: latency-svc-cb4sm [746.740522ms]
Nov 19 17:19:06.992: INFO: Created: latency-svc-g5jqg
Nov 19 17:19:07.035: INFO: Got endpoints: latency-svc-qrtw5 [749.905562ms]
Nov 19 17:19:07.043: INFO: Created: latency-svc-9lv7l
Nov 19 17:19:07.085: INFO: Got endpoints: latency-svc-sv9gx [750.032545ms]
Nov 19 17:19:07.093: INFO: Created: latency-svc-j6t7p
Nov 19 17:19:07.135: INFO: Got endpoints: latency-svc-svc97 [749.872866ms]
Nov 19 17:19:07.145: INFO: Created: latency-svc-89dbd
Nov 19 17:19:07.184: INFO: Got endpoints: latency-svc-scj4r [750.110289ms]
Nov 19 17:19:07.193: INFO: Created: latency-svc-zn4rz
Nov 19 17:19:07.235: INFO: Got endpoints: latency-svc-gsldk [751.036416ms]
Nov 19 17:19:07.244: INFO: Created: latency-svc-7lgv8
Nov 19 17:19:07.284: INFO: Got endpoints: latency-svc-lsfhq [748.737048ms]
Nov 19 17:19:07.293: INFO: Created: latency-svc-jd6vb
Nov 19 17:19:07.333: INFO: Got endpoints: latency-svc-l2mtj [747.047116ms]
Nov 19 17:19:07.341: INFO: Created: latency-svc-5vhvt
Nov 19 17:19:07.385: INFO: Got endpoints: latency-svc-94tcs [749.701895ms]
Nov 19 17:19:07.393: INFO: Created: latency-svc-2hwbt
Nov 19 17:19:07.436: INFO: Got endpoints: latency-svc-82n6n [751.190453ms]
Nov 19 17:19:07.448: INFO: Created: latency-svc-dc6w5
Nov 19 17:19:07.487: INFO: Got endpoints: latency-svc-q4rb5 [752.604164ms]
Nov 19 17:19:07.500: INFO: Created: latency-svc-xf9d5
Nov 19 17:19:07.536: INFO: Got endpoints: latency-svc-gqnst [750.915259ms]
Nov 19 17:19:07.544: INFO: Created: latency-svc-7l92z
Nov 19 17:19:07.586: INFO: Got endpoints: latency-svc-j89hw [750.683349ms]
Nov 19 17:19:07.593: INFO: Created: latency-svc-d2884
Nov 19 17:19:07.635: INFO: Got endpoints: latency-svc-z5vvk [750.108002ms]
Nov 19 17:19:07.644: INFO: Created: latency-svc-7pzcz
Nov 19 17:19:07.686: INFO: Got endpoints: latency-svc-5wnbz [752.279436ms]
Nov 19 17:19:07.695: INFO: Created: latency-svc-54g5d
Nov 19 17:19:07.734: INFO: Got endpoints: latency-svc-g5jqg [750.364955ms]
Nov 19 17:19:07.744: INFO: Created: latency-svc-5b7fq
Nov 19 17:19:07.784: INFO: Got endpoints: latency-svc-9lv7l [749.444892ms]
Nov 19 17:19:07.793: INFO: Created: latency-svc-bq22d
Nov 19 17:19:07.837: INFO: Got endpoints: latency-svc-j6t7p [751.648459ms]
Nov 19 17:19:07.846: INFO: Created: latency-svc-zqdd4
Nov 19 17:19:07.885: INFO: Got endpoints: latency-svc-89dbd [750.581279ms]
Nov 19 17:19:07.895: INFO: Created: latency-svc-w2kc5
Nov 19 17:19:07.936: INFO: Got endpoints: latency-svc-zn4rz [751.402077ms]
Nov 19 17:19:07.944: INFO: Created: latency-svc-zhrlb
Nov 19 17:19:07.985: INFO: Got endpoints: latency-svc-7lgv8 [750.392438ms]
Nov 19 17:19:07.994: INFO: Created: latency-svc-8mbr7
Nov 19 17:19:08.034: INFO: Got endpoints: latency-svc-jd6vb [750.038398ms]
Nov 19 17:19:08.041: INFO: Created: latency-svc-rxr5t
Nov 19 17:19:08.085: INFO: Got endpoints: latency-svc-5vhvt [751.206097ms]
Nov 19 17:19:08.093: INFO: Created: latency-svc-5xx2h
Nov 19 17:19:08.136: INFO: Got endpoints: latency-svc-2hwbt [750.805034ms]
Nov 19 17:19:08.144: INFO: Created: latency-svc-9ftvw
Nov 19 17:19:08.185: INFO: Got endpoints: latency-svc-dc6w5 [748.960906ms]
Nov 19 17:19:08.196: INFO: Created: latency-svc-mjw6d
Nov 19 17:19:08.235: INFO: Got endpoints: latency-svc-xf9d5 [747.32455ms]
Nov 19 17:19:08.245: INFO: Created: latency-svc-dpvcj
Nov 19 17:19:08.286: INFO: Got endpoints: latency-svc-7l92z [749.172907ms]
Nov 19 17:19:08.296: INFO: Created: latency-svc-6457g
Nov 19 17:19:08.335: INFO: Got endpoints: latency-svc-d2884 [749.733315ms]
Nov 19 17:19:08.371: INFO: Created: latency-svc-ldb4p
Nov 19 17:19:08.385: INFO: Got endpoints: latency-svc-7pzcz [749.639278ms]
Nov 19 17:19:08.393: INFO: Created: latency-svc-x2kfd
Nov 19 17:19:08.436: INFO: Got endpoints: latency-svc-54g5d [750.370089ms]
Nov 19 17:19:08.444: INFO: Created: latency-svc-fzcnm
Nov 19 17:19:08.490: INFO: Got endpoints: latency-svc-5b7fq [755.93449ms]
Nov 19 17:19:08.500: INFO: Created: latency-svc-whqz2
Nov 19 17:19:08.537: INFO: Got endpoints: latency-svc-bq22d [752.284987ms]
Nov 19 17:19:08.545: INFO: Created: latency-svc-7w74q
Nov 19 17:19:08.584: INFO: Got endpoints: latency-svc-zqdd4 [747.583159ms]
Nov 19 17:19:08.594: INFO: Created: latency-svc-nr67n
Nov 19 17:19:08.635: INFO: Got endpoints: latency-svc-w2kc5 [749.534509ms]
Nov 19 17:19:08.645: INFO: Created: latency-svc-v4mnv
Nov 19 17:19:08.684: INFO: Got endpoints: latency-svc-zhrlb [748.814444ms]
Nov 19 17:19:08.730: INFO: Created: latency-svc-n26gj
Nov 19 17:19:08.737: INFO: Got endpoints: latency-svc-8mbr7 [751.794138ms]
Nov 19 17:19:08.745: INFO: Created: latency-svc-lh4x4
Nov 19 17:19:08.786: INFO: Got endpoints: latency-svc-rxr5t [752.342976ms]
Nov 19 17:19:08.797: INFO: Created: latency-svc-6wqqh
Nov 19 17:19:08.834: INFO: Got endpoints: latency-svc-5xx2h [749.65169ms]
Nov 19 17:19:08.843: INFO: Created: latency-svc-bthmb
Nov 19 17:19:08.885: INFO: Got endpoints: latency-svc-9ftvw [749.272501ms]
Nov 19 17:19:08.895: INFO: Created: latency-svc-dpmz5
Nov 19 17:19:08.935: INFO: Got endpoints: latency-svc-mjw6d [750.11443ms]
Nov 19 17:19:08.943: INFO: Created: latency-svc-767gn
Nov 19 17:19:08.985: INFO: Got endpoints: latency-svc-dpvcj [749.986903ms]
Nov 19 17:19:08.992: INFO: Created: latency-svc-dfdf7
Nov 19 17:19:09.036: INFO: Got endpoints: latency-svc-6457g [749.994403ms]
Nov 19 17:19:09.045: INFO: Created: latency-svc-9j2ff
Nov 19 17:19:09.084: INFO: Got endpoints: latency-svc-ldb4p [748.87441ms]
Nov 19 17:19:09.095: INFO: Created: latency-svc-zqhjh
Nov 19 17:19:09.136: INFO: Got endpoints: latency-svc-x2kfd [750.840283ms]
Nov 19 17:19:09.148: INFO: Created: latency-svc-w6gmn
Nov 19 17:19:09.184: INFO: Got endpoints: latency-svc-fzcnm [747.296051ms]
Nov 19 17:19:09.191: INFO: Created: latency-svc-4xrz9
Nov 19 17:19:09.236: INFO: Got endpoints: latency-svc-whqz2 [745.962329ms]
Nov 19 17:19:09.247: INFO: Created: latency-svc-dk6r5
Nov 19 17:19:09.286: INFO: Got endpoints: latency-svc-7w74q [748.78074ms]
Nov 19 17:19:09.294: INFO: Created: latency-svc-ntjw2
Nov 19 17:19:09.333: INFO: Got endpoints: latency-svc-nr67n [748.934222ms]
Nov 19 17:19:09.385: INFO: Got endpoints: latency-svc-v4mnv [749.540739ms]
Nov 19 17:19:09.437: INFO: Got endpoints: latency-svc-n26gj [752.587169ms]
Nov 19 17:19:09.484: INFO: Got endpoints: latency-svc-lh4x4 [747.184356ms]
Nov 19 17:19:09.538: INFO: Got endpoints: latency-svc-6wqqh [752.010096ms]
Nov 19 17:19:09.585: INFO: Got endpoints: latency-svc-bthmb [750.242047ms]
Nov 19 17:19:09.634: INFO: Got endpoints: latency-svc-dpmz5 [749.072064ms]
Nov 19 17:19:09.685: INFO: Got endpoints: latency-svc-767gn [750.399183ms]
Nov 19 17:19:09.735: INFO: Got endpoints: latency-svc-dfdf7 [750.181971ms]
Nov 19 17:19:09.785: INFO: Got endpoints: latency-svc-9j2ff [749.045767ms]
Nov 19 17:19:09.835: INFO: Got endpoints: latency-svc-zqhjh [750.559318ms]
Nov 19 17:19:09.885: INFO: Got endpoints: latency-svc-w6gmn [748.709038ms]
Nov 19 17:19:09.935: INFO: Got endpoints: latency-svc-4xrz9 [751.071332ms]
Nov 19 17:19:09.986: INFO: Got endpoints: latency-svc-dk6r5 [750.226155ms]
Nov 19 17:19:10.034: INFO: Got endpoints: latency-svc-ntjw2 [748.844023ms]
Nov 19 17:19:10.034: INFO: Latencies: [19.934127ms 21.170872ms 34.455702ms 43.056397ms 52.10305ms 62.150128ms 62.967542ms 72.795068ms 80.855064ms 87.120912ms 91.566756ms 105.69379ms 111.20226ms 111.395814ms 114.587652ms 115.446093ms 115.641473ms 115.740112ms 116.719689ms 117.376347ms 117.678685ms 120.00541ms 120.79736ms 125.245742ms 128.363043ms 129.749655ms 141.75045ms 150.556725ms 151.037684ms 151.156736ms 151.801327ms 154.699625ms 158.095547ms 160.308276ms 166.712682ms 167.920417ms 216.231856ms 253.112104ms 297.501089ms 342.137269ms 378.804656ms 395.150184ms 424.401537ms 464.157287ms 514.987605ms 559.093363ms 593.470106ms 642.980182ms 686.48331ms 727.959682ms 744.819834ms 745.207118ms 745.962329ms 746.362865ms 746.740522ms 746.898581ms 747.047116ms 747.16581ms 747.184356ms 747.296051ms 747.32455ms 747.583159ms 747.809952ms 748.011066ms 748.062027ms 748.216842ms 748.405555ms 748.582773ms 748.654975ms 748.709038ms 748.717365ms 748.737048ms 748.78074ms 748.814444ms 748.826872ms 748.844023ms 748.87441ms 748.900742ms 748.905199ms 748.934058ms 748.934222ms 748.960906ms 749.009985ms 749.045692ms 749.045767ms 749.072064ms 749.079275ms 749.139442ms 749.164845ms 749.172907ms 749.21792ms 749.240643ms 749.243203ms 749.272501ms 749.371571ms 749.444892ms 749.471953ms 749.534509ms 749.536802ms 749.537514ms 749.540739ms 749.5409ms 749.545313ms 749.552433ms 749.572583ms 749.63638ms 749.636828ms 749.637233ms 749.639278ms 749.642124ms 749.65169ms 749.662027ms 749.664523ms 749.680534ms 749.701895ms 749.730794ms 749.733315ms 749.797419ms 749.806161ms 749.821189ms 749.872866ms 749.905562ms 749.90944ms 749.933599ms 749.970485ms 749.978406ms 749.986903ms 749.994403ms 750.018589ms 750.032545ms 750.038398ms 750.045024ms 750.071263ms 750.071539ms 750.080603ms 750.108002ms 750.110289ms 750.11443ms 750.165404ms 750.181971ms 750.189378ms 750.226155ms 750.242047ms 750.293452ms 750.30645ms 750.364955ms 750.370089ms 750.392438ms 750.399183ms 750.413568ms 750.421591ms 750.427371ms 750.436975ms 750.457889ms 750.4865ms 750.498038ms 750.518206ms 750.557789ms 750.559318ms 750.581279ms 750.586722ms 750.652814ms 750.663646ms 750.683349ms 750.744056ms 750.783499ms 750.805034ms 750.840283ms 750.862213ms 750.898831ms 750.915259ms 750.937466ms 751.010447ms 751.036416ms 751.039518ms 751.055696ms 751.071332ms 751.163566ms 751.190453ms 751.206097ms 751.26958ms 751.402077ms 751.435901ms 751.648459ms 751.661886ms 751.794138ms 751.795396ms 751.81501ms 751.956394ms 752.010096ms 752.279436ms 752.284987ms 752.342976ms 752.430446ms 752.532658ms 752.587169ms 752.604164ms 754.584957ms 755.631759ms 755.93449ms]
Nov 19 17:19:10.035: INFO: 50 %ile: 749.540739ms
Nov 19 17:19:10.035: INFO: 90 %ile: 751.26958ms
Nov 19 17:19:10.035: INFO: 99 %ile: 755.631759ms
Nov 19 17:19:10.035: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:19:10.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-6582" for this suite.

â€¢ [SLOW TEST:9.869 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":305,"completed":91,"skipped":1303,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:19:10.046: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8576
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 17:21:10.201: INFO: Deleting pod "var-expansion-005f3c3a-143a-44e9-8746-7bab0357407c" in namespace "var-expansion-8576"
Nov 19 17:21:10.212: INFO: Wait up to 5m0s for pod "var-expansion-005f3c3a-143a-44e9-8746-7bab0357407c" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:21:12.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8576" for this suite.

â€¢ [SLOW TEST:122.185 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":305,"completed":92,"skipped":1310,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:21:12.231: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-836
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-downwardapi-g2lt
STEP: Creating a pod to test atomic-volume-subpath
Nov 19 17:21:12.383: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-g2lt" in namespace "subpath-836" to be "Succeeded or Failed"
Nov 19 17:21:12.386: INFO: Pod "pod-subpath-test-downwardapi-g2lt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.578831ms
Nov 19 17:21:14.391: INFO: Pod "pod-subpath-test-downwardapi-g2lt": Phase="Running", Reason="", readiness=true. Elapsed: 2.008225243s
Nov 19 17:21:16.396: INFO: Pod "pod-subpath-test-downwardapi-g2lt": Phase="Running", Reason="", readiness=true. Elapsed: 4.013258259s
Nov 19 17:21:18.400: INFO: Pod "pod-subpath-test-downwardapi-g2lt": Phase="Running", Reason="", readiness=true. Elapsed: 6.01716899s
Nov 19 17:21:20.404: INFO: Pod "pod-subpath-test-downwardapi-g2lt": Phase="Running", Reason="", readiness=true. Elapsed: 8.020593198s
Nov 19 17:21:22.408: INFO: Pod "pod-subpath-test-downwardapi-g2lt": Phase="Running", Reason="", readiness=true. Elapsed: 10.024657928s
Nov 19 17:21:24.412: INFO: Pod "pod-subpath-test-downwardapi-g2lt": Phase="Running", Reason="", readiness=true. Elapsed: 12.028673993s
Nov 19 17:21:26.418: INFO: Pod "pod-subpath-test-downwardapi-g2lt": Phase="Running", Reason="", readiness=true. Elapsed: 14.034487717s
Nov 19 17:21:28.422: INFO: Pod "pod-subpath-test-downwardapi-g2lt": Phase="Running", Reason="", readiness=true. Elapsed: 16.038733005s
Nov 19 17:21:30.426: INFO: Pod "pod-subpath-test-downwardapi-g2lt": Phase="Running", Reason="", readiness=true. Elapsed: 18.042598894s
Nov 19 17:21:32.429: INFO: Pod "pod-subpath-test-downwardapi-g2lt": Phase="Running", Reason="", readiness=true. Elapsed: 20.046306751s
Nov 19 17:21:34.434: INFO: Pod "pod-subpath-test-downwardapi-g2lt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.051263393s
STEP: Saw pod success
Nov 19 17:21:34.434: INFO: Pod "pod-subpath-test-downwardapi-g2lt" satisfied condition "Succeeded or Failed"
Nov 19 17:21:34.437: INFO: Trying to get logs from node ip-172-31-20-145 pod pod-subpath-test-downwardapi-g2lt container test-container-subpath-downwardapi-g2lt: <nil>
STEP: delete the pod
Nov 19 17:21:34.464: INFO: Waiting for pod pod-subpath-test-downwardapi-g2lt to disappear
Nov 19 17:21:34.466: INFO: Pod pod-subpath-test-downwardapi-g2lt no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-g2lt
Nov 19 17:21:34.466: INFO: Deleting pod "pod-subpath-test-downwardapi-g2lt" in namespace "subpath-836"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:21:34.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-836" for this suite.

â€¢ [SLOW TEST:22.245 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":305,"completed":93,"skipped":1326,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:21:34.477: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8084
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:21:41.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8084" for this suite.

â€¢ [SLOW TEST:7.163 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":305,"completed":94,"skipped":1369,"failed":0}
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:21:41.639: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6471
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-6471
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6471
STEP: creating replication controller externalsvc in namespace services-6471
I1119 17:21:41.808319      22 runners.go:190] Created replication controller with name: externalsvc, namespace: services-6471, replica count: 2
I1119 17:21:44.858587      22 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Nov 19 17:21:44.880: INFO: Creating new exec pod
Nov 19 17:21:46.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-6471 execpod4m2d7 -- /bin/sh -x -c nslookup clusterip-service.services-6471.svc.cluster.local'
Nov 19 17:21:47.308: INFO: stderr: "+ nslookup clusterip-service.services-6471.svc.cluster.local\n"
Nov 19 17:21:47.308: INFO: stdout: "Server:\t\t10.152.183.127\nAddress:\t10.152.183.127#53\n\nclusterip-service.services-6471.svc.cluster.local\tcanonical name = externalsvc.services-6471.svc.cluster.local.\nName:\texternalsvc.services-6471.svc.cluster.local\nAddress: 10.152.183.74\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6471, will wait for the garbage collector to delete the pods
Nov 19 17:21:47.371: INFO: Deleting ReplicationController externalsvc took: 8.740142ms
Nov 19 17:21:47.871: INFO: Terminating ReplicationController externalsvc pods took: 500.101905ms
Nov 19 17:21:57.691: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:21:57.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6471" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

â€¢ [SLOW TEST:16.079 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":305,"completed":95,"skipped":1369,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:21:57.719: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-528
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 17:21:57.858: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:22:04.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-528" for this suite.

â€¢ [SLOW TEST:6.486 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":305,"completed":96,"skipped":1370,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:22:04.205: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6213
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Nov 19 17:22:04.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-6213'
Nov 19 17:22:04.416: INFO: stderr: ""
Nov 19 17:22:04.416: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1550
Nov 19 17:22:04.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 delete pods e2e-test-httpd-pod --namespace=kubectl-6213'
Nov 19 17:22:07.516: INFO: stderr: ""
Nov 19 17:22:07.516: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:22:07.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6213" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":305,"completed":97,"skipped":1372,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:22:07.526: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9553
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-30eff302-ebbd-4b71-a122-ddb653f7bc1e
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-30eff302-ebbd-4b71-a122-ddb653f7bc1e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:23:13.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9553" for this suite.

â€¢ [SLOW TEST:66.400 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":98,"skipped":1446,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:23:13.926: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6227
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 19 17:23:14.647: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 19 17:23:17.665: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:23:17.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6227" for this suite.
STEP: Destroying namespace "webhook-6227-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":305,"completed":99,"skipped":1452,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:23:17.724: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-9192
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
Nov 19 17:23:17.863: INFO: created test-event-1
Nov 19 17:23:17.868: INFO: created test-event-2
Nov 19 17:23:17.872: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Nov 19 17:23:17.875: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Nov 19 17:23:17.893: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:23:17.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9192" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":305,"completed":100,"skipped":1478,"failed":0}
SSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:23:17.904: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-7621
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Nov 19 17:23:18.056: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:23:18.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7621" for this suite.
â€¢{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":305,"completed":101,"skipped":1485,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:23:18.091: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9700
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:23:22.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9700" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":305,"completed":102,"skipped":1520,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:23:22.255: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2122
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-2122
STEP: creating service affinity-nodeport-transition in namespace services-2122
STEP: creating replication controller affinity-nodeport-transition in namespace services-2122
I1119 17:23:22.413230      22 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-2122, replica count: 3
I1119 17:23:25.463485      22 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 19 17:23:25.471: INFO: Creating new exec pod
Nov 19 17:23:28.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-2122 execpod-affinitykg2ct -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Nov 19 17:23:28.634: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Nov 19 17:23:28.634: INFO: stdout: ""
Nov 19 17:23:28.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-2122 execpod-affinitykg2ct -- /bin/sh -x -c nc -zv -t -w 2 10.152.183.227 80'
Nov 19 17:23:28.751: INFO: stderr: "+ nc -zv -t -w 2 10.152.183.227 80\nConnection to 10.152.183.227 80 port [tcp/http] succeeded!\n"
Nov 19 17:23:28.751: INFO: stdout: ""
Nov 19 17:23:28.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-2122 execpod-affinitykg2ct -- /bin/sh -x -c nc -zv -t -w 2 172.31.20.145 32580'
Nov 19 17:23:28.868: INFO: stderr: "+ nc -zv -t -w 2 172.31.20.145 32580\nConnection to 172.31.20.145 32580 port [tcp/32580] succeeded!\n"
Nov 19 17:23:28.868: INFO: stdout: ""
Nov 19 17:23:28.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-2122 execpod-affinitykg2ct -- /bin/sh -x -c nc -zv -t -w 2 172.31.68.240 32580'
Nov 19 17:23:28.980: INFO: stderr: "+ nc -zv -t -w 2 172.31.68.240 32580\nConnection to 172.31.68.240 32580 port [tcp/32580] succeeded!\n"
Nov 19 17:23:28.980: INFO: stdout: ""
Nov 19 17:23:28.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-2122 execpod-affinitykg2ct -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.20.145:32580/ ; done'
Nov 19 17:23:29.165: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32580/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32580/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32580/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32580/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32580/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32580/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32580/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32580/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32580/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32580/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32580/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32580/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32580/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32580/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32580/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32580/\n"
Nov 19 17:23:29.165: INFO: stdout: "\naffinity-nodeport-transition-2vtk8\naffinity-nodeport-transition-2vtk8\naffinity-nodeport-transition-262k8\naffinity-nodeport-transition-r96fz\naffinity-nodeport-transition-r96fz\naffinity-nodeport-transition-2vtk8\naffinity-nodeport-transition-r96fz\naffinity-nodeport-transition-2vtk8\naffinity-nodeport-transition-2vtk8\naffinity-nodeport-transition-r96fz\naffinity-nodeport-transition-262k8\naffinity-nodeport-transition-r96fz\naffinity-nodeport-transition-r96fz\naffinity-nodeport-transition-262k8\naffinity-nodeport-transition-2vtk8\naffinity-nodeport-transition-r96fz"
Nov 19 17:23:29.165: INFO: Received response from host: affinity-nodeport-transition-2vtk8
Nov 19 17:23:29.165: INFO: Received response from host: affinity-nodeport-transition-2vtk8
Nov 19 17:23:29.165: INFO: Received response from host: affinity-nodeport-transition-262k8
Nov 19 17:23:29.165: INFO: Received response from host: affinity-nodeport-transition-r96fz
Nov 19 17:23:29.165: INFO: Received response from host: affinity-nodeport-transition-r96fz
Nov 19 17:23:29.165: INFO: Received response from host: affinity-nodeport-transition-2vtk8
Nov 19 17:23:29.165: INFO: Received response from host: affinity-nodeport-transition-r96fz
Nov 19 17:23:29.165: INFO: Received response from host: affinity-nodeport-transition-2vtk8
Nov 19 17:23:29.165: INFO: Received response from host: affinity-nodeport-transition-2vtk8
Nov 19 17:23:29.165: INFO: Received response from host: affinity-nodeport-transition-r96fz
Nov 19 17:23:29.165: INFO: Received response from host: affinity-nodeport-transition-262k8
Nov 19 17:23:29.165: INFO: Received response from host: affinity-nodeport-transition-r96fz
Nov 19 17:23:29.165: INFO: Received response from host: affinity-nodeport-transition-r96fz
Nov 19 17:23:29.165: INFO: Received response from host: affinity-nodeport-transition-262k8
Nov 19 17:23:29.165: INFO: Received response from host: affinity-nodeport-transition-2vtk8
Nov 19 17:23:29.165: INFO: Received response from host: affinity-nodeport-transition-r96fz
Nov 19 17:23:29.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-2122 execpod-affinitykg2ct -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.20.145:32580/ ; done'
Nov 19 17:23:29.342: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32580/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32580/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32580/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32580/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32580/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32580/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32580/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32580/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32580/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32580/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32580/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32580/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32580/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32580/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32580/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32580/\n"
Nov 19 17:23:29.342: INFO: stdout: "\naffinity-nodeport-transition-262k8\naffinity-nodeport-transition-262k8\naffinity-nodeport-transition-262k8\naffinity-nodeport-transition-262k8\naffinity-nodeport-transition-262k8\naffinity-nodeport-transition-262k8\naffinity-nodeport-transition-262k8\naffinity-nodeport-transition-262k8\naffinity-nodeport-transition-262k8\naffinity-nodeport-transition-262k8\naffinity-nodeport-transition-262k8\naffinity-nodeport-transition-262k8\naffinity-nodeport-transition-262k8\naffinity-nodeport-transition-262k8\naffinity-nodeport-transition-262k8\naffinity-nodeport-transition-262k8"
Nov 19 17:23:29.342: INFO: Received response from host: affinity-nodeport-transition-262k8
Nov 19 17:23:29.342: INFO: Received response from host: affinity-nodeport-transition-262k8
Nov 19 17:23:29.342: INFO: Received response from host: affinity-nodeport-transition-262k8
Nov 19 17:23:29.342: INFO: Received response from host: affinity-nodeport-transition-262k8
Nov 19 17:23:29.342: INFO: Received response from host: affinity-nodeport-transition-262k8
Nov 19 17:23:29.342: INFO: Received response from host: affinity-nodeport-transition-262k8
Nov 19 17:23:29.342: INFO: Received response from host: affinity-nodeport-transition-262k8
Nov 19 17:23:29.342: INFO: Received response from host: affinity-nodeport-transition-262k8
Nov 19 17:23:29.342: INFO: Received response from host: affinity-nodeport-transition-262k8
Nov 19 17:23:29.342: INFO: Received response from host: affinity-nodeport-transition-262k8
Nov 19 17:23:29.342: INFO: Received response from host: affinity-nodeport-transition-262k8
Nov 19 17:23:29.342: INFO: Received response from host: affinity-nodeport-transition-262k8
Nov 19 17:23:29.342: INFO: Received response from host: affinity-nodeport-transition-262k8
Nov 19 17:23:29.342: INFO: Received response from host: affinity-nodeport-transition-262k8
Nov 19 17:23:29.342: INFO: Received response from host: affinity-nodeport-transition-262k8
Nov 19 17:23:29.342: INFO: Received response from host: affinity-nodeport-transition-262k8
Nov 19 17:23:29.342: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-2122, will wait for the garbage collector to delete the pods
Nov 19 17:23:29.415: INFO: Deleting ReplicationController affinity-nodeport-transition took: 7.064241ms
Nov 19 17:23:29.516: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.147235ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:23:37.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2122" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

â€¢ [SLOW TEST:15.291 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":103,"skipped":1558,"failed":0}
SSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:23:37.546: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-9796
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:171
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating server pod server in namespace prestop-9796
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-9796
STEP: Deleting pre-stop pod
Nov 19 17:23:46.722: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:23:46.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-9796" for this suite.

â€¢ [SLOW TEST:9.196 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":305,"completed":104,"skipped":1563,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:23:46.743: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3478
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-5c12a3e3-4763-48ac-a450-1b0b7d112c4d
STEP: Creating a pod to test consume configMaps
Nov 19 17:23:46.885: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2831667d-9674-4a4c-adb4-2cc8423bb114" in namespace "projected-3478" to be "Succeeded or Failed"
Nov 19 17:23:46.888: INFO: Pod "pod-projected-configmaps-2831667d-9674-4a4c-adb4-2cc8423bb114": Phase="Pending", Reason="", readiness=false. Elapsed: 2.680478ms
Nov 19 17:23:48.894: INFO: Pod "pod-projected-configmaps-2831667d-9674-4a4c-adb4-2cc8423bb114": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008288495s
STEP: Saw pod success
Nov 19 17:23:48.894: INFO: Pod "pod-projected-configmaps-2831667d-9674-4a4c-adb4-2cc8423bb114" satisfied condition "Succeeded or Failed"
Nov 19 17:23:48.897: INFO: Trying to get logs from node ip-172-31-9-169 pod pod-projected-configmaps-2831667d-9674-4a4c-adb4-2cc8423bb114 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 19 17:23:48.925: INFO: Waiting for pod pod-projected-configmaps-2831667d-9674-4a4c-adb4-2cc8423bb114 to disappear
Nov 19 17:23:48.927: INFO: Pod pod-projected-configmaps-2831667d-9674-4a4c-adb4-2cc8423bb114 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:23:48.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3478" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":105,"skipped":1632,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:23:48.936: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-6982
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-6982
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 19 17:23:49.077: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 19 17:23:49.106: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 19 17:23:51.111: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 19 17:23:53.110: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 19 17:23:55.110: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 19 17:23:57.110: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 19 17:23:59.111: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 19 17:24:01.110: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 19 17:24:03.110: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov 19 17:24:03.114: INFO: The status of Pod netserver-1 is Running (Ready = false)
Nov 19 17:24:05.118: INFO: The status of Pod netserver-1 is Running (Ready = false)
Nov 19 17:24:07.119: INFO: The status of Pod netserver-1 is Running (Ready = false)
Nov 19 17:24:09.118: INFO: The status of Pod netserver-1 is Running (Ready = true)
Nov 19 17:24:09.124: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Nov 19 17:24:11.159: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.1.33.85:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6982 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 19 17:24:11.159: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
Nov 19 17:24:11.257: INFO: Found all expected endpoints: [netserver-0]
Nov 19 17:24:11.260: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.1.27.36:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6982 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 19 17:24:11.260: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
Nov 19 17:24:11.305: INFO: Found all expected endpoints: [netserver-1]
Nov 19 17:24:11.308: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.1.31.72:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6982 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 19 17:24:11.308: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
Nov 19 17:24:11.382: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:24:11.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6982" for this suite.

â€¢ [SLOW TEST:22.458 seconds]
[sig-network] Networking
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":106,"skipped":1677,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:24:11.394: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1921
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-41014a5d-94ef-4861-8b73-dcf4ab72e4a4
STEP: Creating a pod to test consume secrets
Nov 19 17:24:11.543: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-325fcd26-d40b-474c-8ec5-1a014283f1d1" in namespace "projected-1921" to be "Succeeded or Failed"
Nov 19 17:24:11.548: INFO: Pod "pod-projected-secrets-325fcd26-d40b-474c-8ec5-1a014283f1d1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.29706ms
Nov 19 17:24:13.552: INFO: Pod "pod-projected-secrets-325fcd26-d40b-474c-8ec5-1a014283f1d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009278417s
STEP: Saw pod success
Nov 19 17:24:13.552: INFO: Pod "pod-projected-secrets-325fcd26-d40b-474c-8ec5-1a014283f1d1" satisfied condition "Succeeded or Failed"
Nov 19 17:24:13.555: INFO: Trying to get logs from node ip-172-31-20-145 pod pod-projected-secrets-325fcd26-d40b-474c-8ec5-1a014283f1d1 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 19 17:24:13.574: INFO: Waiting for pod pod-projected-secrets-325fcd26-d40b-474c-8ec5-1a014283f1d1 to disappear
Nov 19 17:24:13.576: INFO: Pod pod-projected-secrets-325fcd26-d40b-474c-8ec5-1a014283f1d1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:24:13.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1921" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":107,"skipped":1690,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:24:13.584: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8406
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod test-webserver-07bf13f0-4151-4657-ab21-4f9684d83d46 in namespace container-probe-8406
Nov 19 17:24:15.737: INFO: Started pod test-webserver-07bf13f0-4151-4657-ab21-4f9684d83d46 in namespace container-probe-8406
STEP: checking the pod's current state and verifying that restartCount is present
Nov 19 17:24:15.740: INFO: Initial restart count of pod test-webserver-07bf13f0-4151-4657-ab21-4f9684d83d46 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:28:16.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8406" for this suite.

â€¢ [SLOW TEST:242.691 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":108,"skipped":1692,"failed":0}
S
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:28:16.275: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-8471
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 17:28:16.414: INFO: Creating ReplicaSet my-hostname-basic-55f10d2a-e9c6-46e9-b8e9-90b7e5ced16b
Nov 19 17:28:16.422: INFO: Pod name my-hostname-basic-55f10d2a-e9c6-46e9-b8e9-90b7e5ced16b: Found 0 pods out of 1
Nov 19 17:28:21.426: INFO: Pod name my-hostname-basic-55f10d2a-e9c6-46e9-b8e9-90b7e5ced16b: Found 1 pods out of 1
Nov 19 17:28:21.426: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-55f10d2a-e9c6-46e9-b8e9-90b7e5ced16b" is running
Nov 19 17:28:21.430: INFO: Pod "my-hostname-basic-55f10d2a-e9c6-46e9-b8e9-90b7e5ced16b-wnwn9" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-19 17:28:16 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-19 17:28:17 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-19 17:28:17 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-19 17:28:16 +0000 UTC Reason: Message:}])
Nov 19 17:28:21.430: INFO: Trying to dial the pod
Nov 19 17:28:26.441: INFO: Controller my-hostname-basic-55f10d2a-e9c6-46e9-b8e9-90b7e5ced16b: Got expected result from replica 1 [my-hostname-basic-55f10d2a-e9c6-46e9-b8e9-90b7e5ced16b-wnwn9]: "my-hostname-basic-55f10d2a-e9c6-46e9-b8e9-90b7e5ced16b-wnwn9", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:28:26.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8471" for this suite.

â€¢ [SLOW TEST:10.178 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":109,"skipped":1693,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:28:26.453: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-113
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:28:28.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-113" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":305,"completed":110,"skipped":1719,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:28:28.634: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-322
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:28:39.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-322" for this suite.

â€¢ [SLOW TEST:11.204 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":305,"completed":111,"skipped":1740,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] version v1
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:28:39.838: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-9885
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-tx58w in namespace proxy-9885
I1119 17:28:39.994868      22 runners.go:190] Created replication controller with name: proxy-service-tx58w, namespace: proxy-9885, replica count: 1
I1119 17:28:41.045122      22 runners.go:190] proxy-service-tx58w Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1119 17:28:42.045271      22 runners.go:190] proxy-service-tx58w Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1119 17:28:43.045424      22 runners.go:190] proxy-service-tx58w Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1119 17:28:44.045592      22 runners.go:190] proxy-service-tx58w Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1119 17:28:45.045740      22 runners.go:190] proxy-service-tx58w Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1119 17:28:46.045881      22 runners.go:190] proxy-service-tx58w Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1119 17:28:47.046027      22 runners.go:190] proxy-service-tx58w Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 19 17:28:47.052: INFO: setup took 7.081918051s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Nov 19 17:28:47.058: INFO: (0) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/rewriteme">... (200; 6.561249ms)
Nov 19 17:28:47.058: INFO: (0) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:162/proxy/: bar (200; 6.585911ms)
Nov 19 17:28:47.060: INFO: (0) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/rewriteme">test<... (200; 7.946869ms)
Nov 19 17:28:47.060: INFO: (0) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:162/proxy/: bar (200; 8.058943ms)
Nov 19 17:28:47.064: INFO: (0) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/rewriteme">test</a> (200; 11.77126ms)
Nov 19 17:28:47.064: INFO: (0) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname1/proxy/: foo (200; 11.673916ms)
Nov 19 17:28:47.064: INFO: (0) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:160/proxy/: foo (200; 11.980052ms)
Nov 19 17:28:47.066: INFO: (0) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname2/proxy/: bar (200; 14.335812ms)
Nov 19 17:28:47.066: INFO: (0) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname2/proxy/: tls qux (200; 14.571256ms)
Nov 19 17:28:47.066: INFO: (0) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:460/proxy/: tls baz (200; 14.574519ms)
Nov 19 17:28:47.066: INFO: (0) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:160/proxy/: foo (200; 14.63964ms)
Nov 19 17:28:47.066: INFO: (0) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/tlsrewritem... (200; 14.445104ms)
Nov 19 17:28:47.066: INFO: (0) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname2/proxy/: bar (200; 14.637378ms)
Nov 19 17:28:47.066: INFO: (0) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:462/proxy/: tls qux (200; 14.450847ms)
Nov 19 17:28:47.066: INFO: (0) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname1/proxy/: foo (200; 14.678047ms)
Nov 19 17:28:47.066: INFO: (0) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname1/proxy/: tls baz (200; 14.560132ms)
Nov 19 17:28:47.074: INFO: (1) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:462/proxy/: tls qux (200; 7.718123ms)
Nov 19 17:28:47.075: INFO: (1) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/rewriteme">test<... (200; 8.357028ms)
Nov 19 17:28:47.075: INFO: (1) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/tlsrewritem... (200; 8.418325ms)
Nov 19 17:28:47.075: INFO: (1) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:160/proxy/: foo (200; 8.474058ms)
Nov 19 17:28:47.076: INFO: (1) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname1/proxy/: foo (200; 9.401799ms)
Nov 19 17:28:47.076: INFO: (1) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:460/proxy/: tls baz (200; 9.508321ms)
Nov 19 17:28:47.076: INFO: (1) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:162/proxy/: bar (200; 9.385327ms)
Nov 19 17:28:47.076: INFO: (1) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname2/proxy/: tls qux (200; 9.704066ms)
Nov 19 17:28:47.077: INFO: (1) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:162/proxy/: bar (200; 10.514618ms)
Nov 19 17:28:47.077: INFO: (1) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/rewriteme">test</a> (200; 10.627986ms)
Nov 19 17:28:47.077: INFO: (1) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:160/proxy/: foo (200; 10.834667ms)
Nov 19 17:28:47.077: INFO: (1) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/rewriteme">... (200; 10.915188ms)
Nov 19 17:28:47.078: INFO: (1) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname2/proxy/: bar (200; 10.886787ms)
Nov 19 17:28:47.078: INFO: (1) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname1/proxy/: foo (200; 11.382084ms)
Nov 19 17:28:47.078: INFO: (1) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname2/proxy/: bar (200; 11.338794ms)
Nov 19 17:28:47.078: INFO: (1) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname1/proxy/: tls baz (200; 11.459897ms)
Nov 19 17:28:47.082: INFO: (2) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:162/proxy/: bar (200; 3.345703ms)
Nov 19 17:28:47.082: INFO: (2) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:160/proxy/: foo (200; 3.914336ms)
Nov 19 17:28:47.084: INFO: (2) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:462/proxy/: tls qux (200; 5.45201ms)
Nov 19 17:28:47.084: INFO: (2) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/tlsrewritem... (200; 5.684238ms)
Nov 19 17:28:47.084: INFO: (2) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:162/proxy/: bar (200; 5.743499ms)
Nov 19 17:28:47.084: INFO: (2) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:460/proxy/: tls baz (200; 6.217302ms)
Nov 19 17:28:47.085: INFO: (2) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:160/proxy/: foo (200; 7.032752ms)
Nov 19 17:28:47.085: INFO: (2) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/rewriteme">... (200; 6.937367ms)
Nov 19 17:28:47.086: INFO: (2) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname1/proxy/: tls baz (200; 7.880942ms)
Nov 19 17:28:47.086: INFO: (2) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/rewriteme">test</a> (200; 7.893519ms)
Nov 19 17:28:47.086: INFO: (2) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname2/proxy/: bar (200; 7.968504ms)
Nov 19 17:28:47.087: INFO: (2) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/rewriteme">test<... (200; 8.905135ms)
Nov 19 17:28:47.087: INFO: (2) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname1/proxy/: foo (200; 8.85626ms)
Nov 19 17:28:47.088: INFO: (2) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname1/proxy/: foo (200; 9.583889ms)
Nov 19 17:28:47.089: INFO: (2) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname2/proxy/: tls qux (200; 10.390529ms)
Nov 19 17:28:47.089: INFO: (2) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname2/proxy/: bar (200; 10.225801ms)
Nov 19 17:28:47.092: INFO: (3) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:160/proxy/: foo (200; 3.188377ms)
Nov 19 17:28:47.094: INFO: (3) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/tlsrewritem... (200; 5.122805ms)
Nov 19 17:28:47.094: INFO: (3) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/rewriteme">... (200; 5.463072ms)
Nov 19 17:28:47.094: INFO: (3) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:160/proxy/: foo (200; 5.429388ms)
Nov 19 17:28:47.095: INFO: (3) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:162/proxy/: bar (200; 6.410096ms)
Nov 19 17:28:47.096: INFO: (3) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname2/proxy/: bar (200; 7.350152ms)
Nov 19 17:28:47.097: INFO: (3) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname1/proxy/: foo (200; 7.892884ms)
Nov 19 17:28:47.097: INFO: (3) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:162/proxy/: bar (200; 8.257947ms)
Nov 19 17:28:47.097: INFO: (3) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/rewriteme">test<... (200; 8.248226ms)
Nov 19 17:28:47.097: INFO: (3) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:462/proxy/: tls qux (200; 8.292886ms)
Nov 19 17:28:47.097: INFO: (3) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:460/proxy/: tls baz (200; 8.603919ms)
Nov 19 17:28:47.097: INFO: (3) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname2/proxy/: bar (200; 8.479662ms)
Nov 19 17:28:47.097: INFO: (3) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname1/proxy/: tls baz (200; 8.703913ms)
Nov 19 17:28:47.097: INFO: (3) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/rewriteme">test</a> (200; 8.848459ms)
Nov 19 17:28:47.097: INFO: (3) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname2/proxy/: tls qux (200; 8.743223ms)
Nov 19 17:28:47.098: INFO: (3) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname1/proxy/: foo (200; 9.123885ms)
Nov 19 17:28:47.101: INFO: (4) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:160/proxy/: foo (200; 3.459023ms)
Nov 19 17:28:47.101: INFO: (4) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:462/proxy/: tls qux (200; 3.563355ms)
Nov 19 17:28:47.103: INFO: (4) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:162/proxy/: bar (200; 4.640926ms)
Nov 19 17:28:47.103: INFO: (4) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:160/proxy/: foo (200; 4.848517ms)
Nov 19 17:28:47.103: INFO: (4) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/rewriteme">... (200; 5.070798ms)
Nov 19 17:28:47.104: INFO: (4) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname2/proxy/: bar (200; 5.81388ms)
Nov 19 17:28:47.104: INFO: (4) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:162/proxy/: bar (200; 6.134236ms)
Nov 19 17:28:47.104: INFO: (4) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/tlsrewritem... (200; 6.315273ms)
Nov 19 17:28:47.105: INFO: (4) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/rewriteme">test</a> (200; 6.531099ms)
Nov 19 17:28:47.105: INFO: (4) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname1/proxy/: foo (200; 7.339368ms)
Nov 19 17:28:47.106: INFO: (4) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname1/proxy/: tls baz (200; 7.556674ms)
Nov 19 17:28:47.107: INFO: (4) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:460/proxy/: tls baz (200; 8.973981ms)
Nov 19 17:28:47.107: INFO: (4) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/rewriteme">test<... (200; 9.244435ms)
Nov 19 17:28:47.108: INFO: (4) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname2/proxy/: bar (200; 10.502054ms)
Nov 19 17:28:47.109: INFO: (4) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname2/proxy/: tls qux (200; 11.093662ms)
Nov 19 17:28:47.109: INFO: (4) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname1/proxy/: foo (200; 11.328504ms)
Nov 19 17:28:47.113: INFO: (5) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/rewriteme">test</a> (200; 3.075682ms)
Nov 19 17:28:47.114: INFO: (5) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/rewriteme">... (200; 4.69598ms)
Nov 19 17:28:47.115: INFO: (5) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:460/proxy/: tls baz (200; 5.290642ms)
Nov 19 17:28:47.115: INFO: (5) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/rewriteme">test<... (200; 5.833933ms)
Nov 19 17:28:47.116: INFO: (5) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:462/proxy/: tls qux (200; 6.055252ms)
Nov 19 17:28:47.116: INFO: (5) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/tlsrewritem... (200; 6.647542ms)
Nov 19 17:28:47.116: INFO: (5) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:162/proxy/: bar (200; 6.641151ms)
Nov 19 17:28:47.117: INFO: (5) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname1/proxy/: tls baz (200; 7.182457ms)
Nov 19 17:28:47.118: INFO: (5) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname2/proxy/: bar (200; 8.079692ms)
Nov 19 17:28:47.118: INFO: (5) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:160/proxy/: foo (200; 8.361297ms)
Nov 19 17:28:47.118: INFO: (5) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:160/proxy/: foo (200; 8.456752ms)
Nov 19 17:28:47.118: INFO: (5) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:162/proxy/: bar (200; 8.542389ms)
Nov 19 17:28:47.118: INFO: (5) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname1/proxy/: foo (200; 8.561368ms)
Nov 19 17:28:47.119: INFO: (5) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname1/proxy/: foo (200; 9.019914ms)
Nov 19 17:28:47.119: INFO: (5) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname2/proxy/: tls qux (200; 9.215674ms)
Nov 19 17:28:47.119: INFO: (5) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname2/proxy/: bar (200; 9.276319ms)
Nov 19 17:28:47.123: INFO: (6) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:162/proxy/: bar (200; 3.67154ms)
Nov 19 17:28:47.123: INFO: (6) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:160/proxy/: foo (200; 3.706773ms)
Nov 19 17:28:47.123: INFO: (6) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/tlsrewritem... (200; 4.043065ms)
Nov 19 17:28:47.125: INFO: (6) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/rewriteme">... (200; 5.572322ms)
Nov 19 17:28:47.125: INFO: (6) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:160/proxy/: foo (200; 5.523713ms)
Nov 19 17:28:47.126: INFO: (6) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:460/proxy/: tls baz (200; 6.747885ms)
Nov 19 17:28:47.126: INFO: (6) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:462/proxy/: tls qux (200; 7.315724ms)
Nov 19 17:28:47.127: INFO: (6) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/rewriteme">test</a> (200; 7.929266ms)
Nov 19 17:28:47.127: INFO: (6) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:162/proxy/: bar (200; 7.870062ms)
Nov 19 17:28:47.128: INFO: (6) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/rewriteme">test<... (200; 8.318126ms)
Nov 19 17:28:47.128: INFO: (6) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname2/proxy/: tls qux (200; 8.912911ms)
Nov 19 17:28:47.128: INFO: (6) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname1/proxy/: foo (200; 9.078315ms)
Nov 19 17:28:47.128: INFO: (6) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname2/proxy/: bar (200; 9.099407ms)
Nov 19 17:28:47.129: INFO: (6) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname1/proxy/: tls baz (200; 10.062236ms)
Nov 19 17:28:47.129: INFO: (6) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname2/proxy/: bar (200; 10.014536ms)
Nov 19 17:28:47.129: INFO: (6) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname1/proxy/: foo (200; 10.310677ms)
Nov 19 17:28:47.134: INFO: (7) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/rewriteme">... (200; 4.47184ms)
Nov 19 17:28:47.134: INFO: (7) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:460/proxy/: tls baz (200; 4.686105ms)
Nov 19 17:28:47.135: INFO: (7) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/tlsrewritem... (200; 5.71988ms)
Nov 19 17:28:47.135: INFO: (7) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/rewriteme">test</a> (200; 5.781446ms)
Nov 19 17:28:47.136: INFO: (7) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:160/proxy/: foo (200; 6.86689ms)
Nov 19 17:28:47.136: INFO: (7) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:162/proxy/: bar (200; 6.776583ms)
Nov 19 17:28:47.137: INFO: (7) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname1/proxy/: foo (200; 7.87675ms)
Nov 19 17:28:47.138: INFO: (7) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname2/proxy/: bar (200; 7.980448ms)
Nov 19 17:28:47.138: INFO: (7) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:160/proxy/: foo (200; 7.978172ms)
Nov 19 17:28:47.138: INFO: (7) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname2/proxy/: tls qux (200; 8.114111ms)
Nov 19 17:28:47.138: INFO: (7) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:162/proxy/: bar (200; 8.304339ms)
Nov 19 17:28:47.138: INFO: (7) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/rewriteme">test<... (200; 8.22559ms)
Nov 19 17:28:47.138: INFO: (7) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname1/proxy/: tls baz (200; 8.974817ms)
Nov 19 17:28:47.138: INFO: (7) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:462/proxy/: tls qux (200; 8.923637ms)
Nov 19 17:28:47.139: INFO: (7) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname1/proxy/: foo (200; 9.444279ms)
Nov 19 17:28:47.139: INFO: (7) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname2/proxy/: bar (200; 9.410185ms)
Nov 19 17:28:47.142: INFO: (8) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/rewriteme">... (200; 2.950337ms)
Nov 19 17:28:47.143: INFO: (8) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/rewriteme">test<... (200; 3.526083ms)
Nov 19 17:28:47.143: INFO: (8) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:160/proxy/: foo (200; 4.335762ms)
Nov 19 17:28:47.144: INFO: (8) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/rewriteme">test</a> (200; 4.731919ms)
Nov 19 17:28:47.145: INFO: (8) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname1/proxy/: foo (200; 5.710563ms)
Nov 19 17:28:47.145: INFO: (8) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:162/proxy/: bar (200; 5.709855ms)
Nov 19 17:28:47.146: INFO: (8) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/tlsrewritem... (200; 6.502ms)
Nov 19 17:28:47.146: INFO: (8) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:160/proxy/: foo (200; 6.603255ms)
Nov 19 17:28:47.147: INFO: (8) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:460/proxy/: tls baz (200; 7.515366ms)
Nov 19 17:28:47.147: INFO: (8) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:462/proxy/: tls qux (200; 7.594746ms)
Nov 19 17:28:47.147: INFO: (8) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname2/proxy/: bar (200; 7.631399ms)
Nov 19 17:28:47.147: INFO: (8) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:162/proxy/: bar (200; 7.560083ms)
Nov 19 17:28:47.147: INFO: (8) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname2/proxy/: tls qux (200; 7.918415ms)
Nov 19 17:28:47.147: INFO: (8) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname2/proxy/: bar (200; 8.228495ms)
Nov 19 17:28:47.148: INFO: (8) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname1/proxy/: foo (200; 8.373623ms)
Nov 19 17:28:47.148: INFO: (8) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname1/proxy/: tls baz (200; 8.827269ms)
Nov 19 17:28:47.153: INFO: (9) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/rewriteme">test<... (200; 4.698326ms)
Nov 19 17:28:47.155: INFO: (9) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:460/proxy/: tls baz (200; 7.399442ms)
Nov 19 17:28:47.156: INFO: (9) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:462/proxy/: tls qux (200; 8.251719ms)
Nov 19 17:28:47.156: INFO: (9) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:160/proxy/: foo (200; 8.306684ms)
Nov 19 17:28:47.156: INFO: (9) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/tlsrewritem... (200; 8.318046ms)
Nov 19 17:28:47.156: INFO: (9) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/rewriteme">... (200; 8.259269ms)
Nov 19 17:28:47.156: INFO: (9) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/rewriteme">test</a> (200; 8.469749ms)
Nov 19 17:28:47.156: INFO: (9) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:160/proxy/: foo (200; 8.23242ms)
Nov 19 17:28:47.156: INFO: (9) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:162/proxy/: bar (200; 8.30987ms)
Nov 19 17:28:47.156: INFO: (9) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:162/proxy/: bar (200; 8.378592ms)
Nov 19 17:28:47.157: INFO: (9) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname1/proxy/: tls baz (200; 9.10608ms)
Nov 19 17:28:47.157: INFO: (9) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname1/proxy/: foo (200; 9.277304ms)
Nov 19 17:28:47.158: INFO: (9) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname1/proxy/: foo (200; 10.035306ms)
Nov 19 17:28:47.158: INFO: (9) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname2/proxy/: bar (200; 9.913601ms)
Nov 19 17:28:47.158: INFO: (9) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname2/proxy/: bar (200; 10.250724ms)
Nov 19 17:28:47.159: INFO: (9) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname2/proxy/: tls qux (200; 10.365876ms)
Nov 19 17:28:47.165: INFO: (10) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:162/proxy/: bar (200; 6.397567ms)
Nov 19 17:28:47.166: INFO: (10) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:162/proxy/: bar (200; 7.666573ms)
Nov 19 17:28:47.167: INFO: (10) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:460/proxy/: tls baz (200; 8.748231ms)
Nov 19 17:28:47.168: INFO: (10) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:160/proxy/: foo (200; 8.917088ms)
Nov 19 17:28:47.168: INFO: (10) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/tlsrewritem... (200; 8.815151ms)
Nov 19 17:28:47.168: INFO: (10) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:160/proxy/: foo (200; 8.880269ms)
Nov 19 17:28:47.168: INFO: (10) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/rewriteme">... (200; 8.85521ms)
Nov 19 17:28:47.168: INFO: (10) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/rewriteme">test</a> (200; 8.899228ms)
Nov 19 17:28:47.168: INFO: (10) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname1/proxy/: foo (200; 9.777777ms)
Nov 19 17:28:47.169: INFO: (10) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname1/proxy/: foo (200; 10.70199ms)
Nov 19 17:28:47.169: INFO: (10) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname2/proxy/: bar (200; 10.645173ms)
Nov 19 17:28:47.169: INFO: (10) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname2/proxy/: tls qux (200; 10.658956ms)
Nov 19 17:28:47.169: INFO: (10) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:462/proxy/: tls qux (200; 10.785575ms)
Nov 19 17:28:47.170: INFO: (10) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/rewriteme">test<... (200; 11.015309ms)
Nov 19 17:28:47.170: INFO: (10) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname2/proxy/: bar (200; 11.08162ms)
Nov 19 17:28:47.170: INFO: (10) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname1/proxy/: tls baz (200; 11.130136ms)
Nov 19 17:28:47.173: INFO: (11) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/rewriteme">test<... (200; 3.041241ms)
Nov 19 17:28:47.175: INFO: (11) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:162/proxy/: bar (200; 4.653399ms)
Nov 19 17:28:47.175: INFO: (11) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:462/proxy/: tls qux (200; 4.911429ms)
Nov 19 17:28:47.175: INFO: (11) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/rewriteme">... (200; 4.947841ms)
Nov 19 17:28:47.176: INFO: (11) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:160/proxy/: foo (200; 5.630766ms)
Nov 19 17:28:47.176: INFO: (11) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname1/proxy/: foo (200; 6.578749ms)
Nov 19 17:28:47.177: INFO: (11) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname1/proxy/: foo (200; 6.417356ms)
Nov 19 17:28:47.177: INFO: (11) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/tlsrewritem... (200; 6.612078ms)
Nov 19 17:28:47.177: INFO: (11) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:160/proxy/: foo (200; 6.996087ms)
Nov 19 17:28:47.178: INFO: (11) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname2/proxy/: bar (200; 8.009369ms)
Nov 19 17:28:47.178: INFO: (11) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:460/proxy/: tls baz (200; 8.259336ms)
Nov 19 17:28:47.178: INFO: (11) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname1/proxy/: tls baz (200; 8.283737ms)
Nov 19 17:28:47.179: INFO: (11) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/rewriteme">test</a> (200; 8.557436ms)
Nov 19 17:28:47.179: INFO: (11) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:162/proxy/: bar (200; 8.857912ms)
Nov 19 17:28:47.179: INFO: (11) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname2/proxy/: tls qux (200; 8.819021ms)
Nov 19 17:28:47.179: INFO: (11) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname2/proxy/: bar (200; 9.177828ms)
Nov 19 17:28:47.183: INFO: (12) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/rewriteme">... (200; 3.384816ms)
Nov 19 17:28:47.183: INFO: (12) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:162/proxy/: bar (200; 3.569866ms)
Nov 19 17:28:47.183: INFO: (12) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/rewriteme">test</a> (200; 3.681747ms)
Nov 19 17:28:47.185: INFO: (12) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/tlsrewritem... (200; 5.183901ms)
Nov 19 17:28:47.185: INFO: (12) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:162/proxy/: bar (200; 5.099166ms)
Nov 19 17:28:47.185: INFO: (12) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:160/proxy/: foo (200; 5.025114ms)
Nov 19 17:28:47.185: INFO: (12) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:160/proxy/: foo (200; 5.479166ms)
Nov 19 17:28:47.185: INFO: (12) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:462/proxy/: tls qux (200; 5.842354ms)
Nov 19 17:28:47.186: INFO: (12) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname1/proxy/: foo (200; 6.359558ms)
Nov 19 17:28:47.186: INFO: (12) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname1/proxy/: foo (200; 6.900935ms)
Nov 19 17:28:47.186: INFO: (12) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/rewriteme">test<... (200; 6.970921ms)
Nov 19 17:28:47.187: INFO: (12) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname2/proxy/: tls qux (200; 7.444007ms)
Nov 19 17:28:47.187: INFO: (12) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:460/proxy/: tls baz (200; 7.124003ms)
Nov 19 17:28:47.187: INFO: (12) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname2/proxy/: bar (200; 7.480186ms)
Nov 19 17:28:47.188: INFO: (12) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname1/proxy/: tls baz (200; 8.045662ms)
Nov 19 17:28:47.188: INFO: (12) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname2/proxy/: bar (200; 8.342703ms)
Nov 19 17:28:47.191: INFO: (13) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:162/proxy/: bar (200; 2.999486ms)
Nov 19 17:28:47.192: INFO: (13) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:160/proxy/: foo (200; 4.438989ms)
Nov 19 17:28:47.193: INFO: (13) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/tlsrewritem... (200; 5.299865ms)
Nov 19 17:28:47.195: INFO: (13) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/rewriteme">test<... (200; 6.982033ms)
Nov 19 17:28:47.195: INFO: (13) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:162/proxy/: bar (200; 6.967313ms)
Nov 19 17:28:47.195: INFO: (13) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:160/proxy/: foo (200; 7.039837ms)
Nov 19 17:28:47.195: INFO: (13) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/rewriteme">test</a> (200; 7.249106ms)
Nov 19 17:28:47.197: INFO: (13) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname2/proxy/: bar (200; 8.956399ms)
Nov 19 17:28:47.197: INFO: (13) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname2/proxy/: bar (200; 9.157834ms)
Nov 19 17:28:47.197: INFO: (13) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname1/proxy/: foo (200; 9.023237ms)
Nov 19 17:28:47.197: INFO: (13) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/rewriteme">... (200; 9.226702ms)
Nov 19 17:28:47.197: INFO: (13) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:462/proxy/: tls qux (200; 9.203965ms)
Nov 19 17:28:47.197: INFO: (13) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:460/proxy/: tls baz (200; 9.089523ms)
Nov 19 17:28:47.197: INFO: (13) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname1/proxy/: foo (200; 9.636815ms)
Nov 19 17:28:47.198: INFO: (13) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname2/proxy/: tls qux (200; 10.286662ms)
Nov 19 17:28:47.198: INFO: (13) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname1/proxy/: tls baz (200; 10.074073ms)
Nov 19 17:28:47.207: INFO: (14) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/rewriteme">test</a> (200; 8.389076ms)
Nov 19 17:28:47.207: INFO: (14) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/rewriteme">... (200; 8.412769ms)
Nov 19 17:28:47.207: INFO: (14) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:160/proxy/: foo (200; 8.280037ms)
Nov 19 17:28:47.207: INFO: (14) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:462/proxy/: tls qux (200; 8.212591ms)
Nov 19 17:28:47.207: INFO: (14) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname1/proxy/: tls baz (200; 8.524035ms)
Nov 19 17:28:47.207: INFO: (14) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:162/proxy/: bar (200; 8.321328ms)
Nov 19 17:28:47.207: INFO: (14) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/rewriteme">test<... (200; 8.153364ms)
Nov 19 17:28:47.207: INFO: (14) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname1/proxy/: foo (200; 8.210661ms)
Nov 19 17:28:47.207: INFO: (14) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname2/proxy/: bar (200; 8.379168ms)
Nov 19 17:28:47.207: INFO: (14) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:160/proxy/: foo (200; 8.55205ms)
Nov 19 17:28:47.208: INFO: (14) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname1/proxy/: foo (200; 9.391168ms)
Nov 19 17:28:47.208: INFO: (14) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname2/proxy/: tls qux (200; 9.69478ms)
Nov 19 17:28:47.208: INFO: (14) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/tlsrewritem... (200; 9.594072ms)
Nov 19 17:28:47.208: INFO: (14) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname2/proxy/: bar (200; 9.520748ms)
Nov 19 17:28:47.208: INFO: (14) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:460/proxy/: tls baz (200; 9.499676ms)
Nov 19 17:28:47.208: INFO: (14) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:162/proxy/: bar (200; 10.218037ms)
Nov 19 17:28:47.212: INFO: (15) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:160/proxy/: foo (200; 3.688344ms)
Nov 19 17:28:47.213: INFO: (15) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:160/proxy/: foo (200; 4.379117ms)
Nov 19 17:28:47.213: INFO: (15) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/tlsrewritem... (200; 4.853424ms)
Nov 19 17:28:47.213: INFO: (15) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:462/proxy/: tls qux (200; 4.863344ms)
Nov 19 17:28:47.213: INFO: (15) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:460/proxy/: tls baz (200; 5.104684ms)
Nov 19 17:28:47.214: INFO: (15) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/rewriteme">test</a> (200; 5.674523ms)
Nov 19 17:28:47.215: INFO: (15) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/rewriteme">... (200; 6.601838ms)
Nov 19 17:28:47.216: INFO: (15) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:162/proxy/: bar (200; 7.068049ms)
Nov 19 17:28:47.216: INFO: (15) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname2/proxy/: bar (200; 7.374851ms)
Nov 19 17:28:47.216: INFO: (15) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/rewriteme">test<... (200; 7.641988ms)
Nov 19 17:28:47.216: INFO: (15) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname2/proxy/: tls qux (200; 7.643737ms)
Nov 19 17:28:47.217: INFO: (15) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname1/proxy/: foo (200; 8.100033ms)
Nov 19 17:28:47.217: INFO: (15) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:162/proxy/: bar (200; 8.268689ms)
Nov 19 17:28:47.217: INFO: (15) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname1/proxy/: tls baz (200; 8.406584ms)
Nov 19 17:28:47.217: INFO: (15) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname1/proxy/: foo (200; 8.786624ms)
Nov 19 17:28:47.217: INFO: (15) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname2/proxy/: bar (200; 8.869874ms)
Nov 19 17:28:47.223: INFO: (16) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:162/proxy/: bar (200; 5.363198ms)
Nov 19 17:28:47.223: INFO: (16) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/rewriteme">... (200; 5.319514ms)
Nov 19 17:28:47.223: INFO: (16) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:162/proxy/: bar (200; 5.361376ms)
Nov 19 17:28:47.223: INFO: (16) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname2/proxy/: bar (200; 5.389392ms)
Nov 19 17:28:47.223: INFO: (16) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:460/proxy/: tls baz (200; 5.500313ms)
Nov 19 17:28:47.225: INFO: (16) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:160/proxy/: foo (200; 7.039388ms)
Nov 19 17:28:47.225: INFO: (16) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/rewriteme">test</a> (200; 6.950928ms)
Nov 19 17:28:47.225: INFO: (16) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/tlsrewritem... (200; 7.262321ms)
Nov 19 17:28:47.226: INFO: (16) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname1/proxy/: foo (200; 8.302851ms)
Nov 19 17:28:47.226: INFO: (16) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname1/proxy/: foo (200; 8.167897ms)
Nov 19 17:28:47.226: INFO: (16) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/rewriteme">test<... (200; 8.247654ms)
Nov 19 17:28:47.226: INFO: (16) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:160/proxy/: foo (200; 8.875835ms)
Nov 19 17:28:47.227: INFO: (16) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname2/proxy/: tls qux (200; 9.689039ms)
Nov 19 17:28:47.227: INFO: (16) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname2/proxy/: bar (200; 9.874472ms)
Nov 19 17:28:47.227: INFO: (16) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:462/proxy/: tls qux (200; 9.735439ms)
Nov 19 17:28:47.227: INFO: (16) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname1/proxy/: tls baz (200; 9.809588ms)
Nov 19 17:28:47.231: INFO: (17) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:160/proxy/: foo (200; 3.130887ms)
Nov 19 17:28:47.231: INFO: (17) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:460/proxy/: tls baz (200; 3.387902ms)
Nov 19 17:28:47.232: INFO: (17) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:162/proxy/: bar (200; 4.673078ms)
Nov 19 17:28:47.233: INFO: (17) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:160/proxy/: foo (200; 5.844481ms)
Nov 19 17:28:47.233: INFO: (17) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/tlsrewritem... (200; 5.944791ms)
Nov 19 17:28:47.234: INFO: (17) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/rewriteme">test<... (200; 6.322104ms)
Nov 19 17:28:47.234: INFO: (17) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/rewriteme">... (200; 6.496986ms)
Nov 19 17:28:47.234: INFO: (17) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/rewriteme">test</a> (200; 6.757696ms)
Nov 19 17:28:47.235: INFO: (17) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:162/proxy/: bar (200; 7.083865ms)
Nov 19 17:28:47.235: INFO: (17) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:462/proxy/: tls qux (200; 7.007346ms)
Nov 19 17:28:47.235: INFO: (17) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname2/proxy/: bar (200; 7.570113ms)
Nov 19 17:28:47.236: INFO: (17) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname1/proxy/: tls baz (200; 8.120859ms)
Nov 19 17:28:47.236: INFO: (17) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname1/proxy/: foo (200; 8.234682ms)
Nov 19 17:28:47.236: INFO: (17) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname2/proxy/: tls qux (200; 8.264979ms)
Nov 19 17:28:47.236: INFO: (17) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname1/proxy/: foo (200; 8.293178ms)
Nov 19 17:28:47.236: INFO: (17) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname2/proxy/: bar (200; 8.792622ms)
Nov 19 17:28:47.240: INFO: (18) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:462/proxy/: tls qux (200; 3.304344ms)
Nov 19 17:28:47.241: INFO: (18) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:160/proxy/: foo (200; 4.148407ms)
Nov 19 17:28:47.243: INFO: (18) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/rewriteme">test</a> (200; 5.912623ms)
Nov 19 17:28:47.243: INFO: (18) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:162/proxy/: bar (200; 6.002679ms)
Nov 19 17:28:47.243: INFO: (18) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:460/proxy/: tls baz (200; 6.402184ms)
Nov 19 17:28:47.244: INFO: (18) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname2/proxy/: bar (200; 7.419791ms)
Nov 19 17:28:47.244: INFO: (18) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:160/proxy/: foo (200; 7.576717ms)
Nov 19 17:28:47.244: INFO: (18) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:162/proxy/: bar (200; 7.804809ms)
Nov 19 17:28:47.245: INFO: (18) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/tlsrewritem... (200; 7.956233ms)
Nov 19 17:28:47.245: INFO: (18) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname2/proxy/: tls qux (200; 8.00831ms)
Nov 19 17:28:47.245: INFO: (18) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/rewriteme">test<... (200; 8.224528ms)
Nov 19 17:28:47.245: INFO: (18) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname1/proxy/: tls baz (200; 8.241ms)
Nov 19 17:28:47.245: INFO: (18) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/rewriteme">... (200; 8.184ms)
Nov 19 17:28:47.246: INFO: (18) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname1/proxy/: foo (200; 9.108824ms)
Nov 19 17:28:47.246: INFO: (18) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname1/proxy/: foo (200; 9.197113ms)
Nov 19 17:28:47.246: INFO: (18) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname2/proxy/: bar (200; 9.336701ms)
Nov 19 17:28:47.250: INFO: (19) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:160/proxy/: foo (200; 3.82579ms)
Nov 19 17:28:47.251: INFO: (19) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:1080/proxy/rewriteme">... (200; 4.938749ms)
Nov 19 17:28:47.252: INFO: (19) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:443/proxy/tlsrewritem... (200; 6.307271ms)
Nov 19 17:28:47.252: INFO: (19) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:462/proxy/: tls qux (200; 6.374611ms)
Nov 19 17:28:47.253: INFO: (19) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l/proxy/rewriteme">test</a> (200; 6.481705ms)
Nov 19 17:28:47.253: INFO: (19) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:160/proxy/: foo (200; 6.565155ms)
Nov 19 17:28:47.254: INFO: (19) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:162/proxy/: bar (200; 8.139601ms)
Nov 19 17:28:47.254: INFO: (19) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname1/proxy/: foo (200; 8.318464ms)
Nov 19 17:28:47.254: INFO: (19) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname2/proxy/: bar (200; 8.315126ms)
Nov 19 17:28:47.255: INFO: (19) /api/v1/namespaces/proxy-9885/services/http:proxy-service-tx58w:portname1/proxy/: foo (200; 8.926812ms)
Nov 19 17:28:47.255: INFO: (19) /api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/: <a href="/api/v1/namespaces/proxy-9885/pods/proxy-service-tx58w-knn4l:1080/proxy/rewriteme">test<... (200; 9.267666ms)
Nov 19 17:28:47.256: INFO: (19) /api/v1/namespaces/proxy-9885/pods/https:proxy-service-tx58w-knn4l:460/proxy/: tls baz (200; 9.255597ms)
Nov 19 17:28:47.256: INFO: (19) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname2/proxy/: tls qux (200; 9.409476ms)
Nov 19 17:28:47.256: INFO: (19) /api/v1/namespaces/proxy-9885/pods/http:proxy-service-tx58w-knn4l:162/proxy/: bar (200; 9.334965ms)
Nov 19 17:28:47.256: INFO: (19) /api/v1/namespaces/proxy-9885/services/https:proxy-service-tx58w:tlsportname1/proxy/: tls baz (200; 9.534165ms)
Nov 19 17:28:47.256: INFO: (19) /api/v1/namespaces/proxy-9885/services/proxy-service-tx58w:portname2/proxy/: bar (200; 10.025311ms)
STEP: deleting ReplicationController proxy-service-tx58w in namespace proxy-9885, will wait for the garbage collector to delete the pods
Nov 19 17:28:47.317: INFO: Deleting ReplicationController proxy-service-tx58w took: 8.211625ms
Nov 19 17:28:47.818: INFO: Terminating ReplicationController proxy-service-tx58w pods took: 500.141784ms
[AfterEach] version v1
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:28:49.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9885" for this suite.

â€¢ [SLOW TEST:9.989 seconds]
[sig-network] Proxy
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":305,"completed":112,"skipped":1771,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:28:49.827: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5650
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Nov 19 17:28:49.960: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Nov 19 17:28:59.198: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
Nov 19 17:29:01.913: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:29:11.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5650" for this suite.

â€¢ [SLOW TEST:22.154 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":305,"completed":113,"skipped":1778,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:29:11.981: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7161
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:29:12.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7161" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
â€¢{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":305,"completed":114,"skipped":1803,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:29:12.181: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9279
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Nov 19 17:29:12.324: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:29:17.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9279" for this suite.

â€¢ [SLOW TEST:5.585 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":305,"completed":115,"skipped":1855,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:29:17.767: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1183
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create deployment with httpd image
Nov 19 17:29:17.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 create -f -'
Nov 19 17:29:18.280: INFO: stderr: ""
Nov 19 17:29:18.280: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Nov 19 17:29:18.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 diff -f -'
Nov 19 17:29:18.768: INFO: rc: 1
Nov 19 17:29:18.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 delete -f -'
Nov 19 17:29:18.841: INFO: stderr: ""
Nov 19 17:29:18.841: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:29:18.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1183" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":305,"completed":116,"skipped":1876,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:29:18.849: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7578
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-268458b7-06c3-408a-bd5d-02bd9658d059
STEP: Creating a pod to test consume configMaps
Nov 19 17:29:18.996: INFO: Waiting up to 5m0s for pod "pod-configmaps-201aa209-8ed9-42a3-8612-3e8c1f56ee4e" in namespace "configmap-7578" to be "Succeeded or Failed"
Nov 19 17:29:19.000: INFO: Pod "pod-configmaps-201aa209-8ed9-42a3-8612-3e8c1f56ee4e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.505849ms
Nov 19 17:29:21.004: INFO: Pod "pod-configmaps-201aa209-8ed9-42a3-8612-3e8c1f56ee4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007793356s
STEP: Saw pod success
Nov 19 17:29:21.004: INFO: Pod "pod-configmaps-201aa209-8ed9-42a3-8612-3e8c1f56ee4e" satisfied condition "Succeeded or Failed"
Nov 19 17:29:21.008: INFO: Trying to get logs from node ip-172-31-9-169 pod pod-configmaps-201aa209-8ed9-42a3-8612-3e8c1f56ee4e container configmap-volume-test: <nil>
STEP: delete the pod
Nov 19 17:29:21.045: INFO: Waiting for pod pod-configmaps-201aa209-8ed9-42a3-8612-3e8c1f56ee4e to disappear
Nov 19 17:29:21.048: INFO: Pod pod-configmaps-201aa209-8ed9-42a3-8612-3e8c1f56ee4e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:29:21.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7578" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":117,"skipped":1887,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:29:21.057: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9143
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:29:37.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9143" for this suite.

â€¢ [SLOW TEST:16.239 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":305,"completed":118,"skipped":1897,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:29:37.296: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7225
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Nov 19 17:29:37.434: INFO: Waiting up to 5m0s for pod "downward-api-46f05f4b-e8aa-4e96-96a7-6683eebf1264" in namespace "downward-api-7225" to be "Succeeded or Failed"
Nov 19 17:29:37.439: INFO: Pod "downward-api-46f05f4b-e8aa-4e96-96a7-6683eebf1264": Phase="Pending", Reason="", readiness=false. Elapsed: 4.272182ms
Nov 19 17:29:39.441: INFO: Pod "downward-api-46f05f4b-e8aa-4e96-96a7-6683eebf1264": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007113594s
STEP: Saw pod success
Nov 19 17:29:39.441: INFO: Pod "downward-api-46f05f4b-e8aa-4e96-96a7-6683eebf1264" satisfied condition "Succeeded or Failed"
Nov 19 17:29:39.445: INFO: Trying to get logs from node ip-172-31-9-169 pod downward-api-46f05f4b-e8aa-4e96-96a7-6683eebf1264 container dapi-container: <nil>
STEP: delete the pod
Nov 19 17:29:39.472: INFO: Waiting for pod downward-api-46f05f4b-e8aa-4e96-96a7-6683eebf1264 to disappear
Nov 19 17:29:39.476: INFO: Pod downward-api-46f05f4b-e8aa-4e96-96a7-6683eebf1264 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:29:39.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7225" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":305,"completed":119,"skipped":1908,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:29:39.487: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8112
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov 19 17:29:39.646: INFO: Number of nodes with available pods: 0
Nov 19 17:29:39.646: INFO: Node ip-172-31-20-145 is running more than one daemon pod
Nov 19 17:29:40.653: INFO: Number of nodes with available pods: 2
Nov 19 17:29:40.653: INFO: Node ip-172-31-20-145 is running more than one daemon pod
Nov 19 17:29:41.653: INFO: Number of nodes with available pods: 3
Nov 19 17:29:41.653: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Nov 19 17:29:41.673: INFO: Number of nodes with available pods: 2
Nov 19 17:29:41.673: INFO: Node ip-172-31-68-240 is running more than one daemon pod
Nov 19 17:29:42.681: INFO: Number of nodes with available pods: 2
Nov 19 17:29:42.681: INFO: Node ip-172-31-68-240 is running more than one daemon pod
Nov 19 17:29:43.679: INFO: Number of nodes with available pods: 2
Nov 19 17:29:43.679: INFO: Node ip-172-31-68-240 is running more than one daemon pod
Nov 19 17:29:44.680: INFO: Number of nodes with available pods: 2
Nov 19 17:29:44.680: INFO: Node ip-172-31-68-240 is running more than one daemon pod
Nov 19 17:29:45.680: INFO: Number of nodes with available pods: 2
Nov 19 17:29:45.680: INFO: Node ip-172-31-68-240 is running more than one daemon pod
Nov 19 17:29:46.681: INFO: Number of nodes with available pods: 2
Nov 19 17:29:46.682: INFO: Node ip-172-31-68-240 is running more than one daemon pod
Nov 19 17:29:47.680: INFO: Number of nodes with available pods: 2
Nov 19 17:29:47.680: INFO: Node ip-172-31-68-240 is running more than one daemon pod
Nov 19 17:29:48.681: INFO: Number of nodes with available pods: 2
Nov 19 17:29:48.681: INFO: Node ip-172-31-68-240 is running more than one daemon pod
Nov 19 17:29:49.681: INFO: Number of nodes with available pods: 2
Nov 19 17:29:49.681: INFO: Node ip-172-31-68-240 is running more than one daemon pod
Nov 19 17:29:50.681: INFO: Number of nodes with available pods: 2
Nov 19 17:29:50.681: INFO: Node ip-172-31-68-240 is running more than one daemon pod
Nov 19 17:29:51.680: INFO: Number of nodes with available pods: 2
Nov 19 17:29:51.680: INFO: Node ip-172-31-68-240 is running more than one daemon pod
Nov 19 17:29:52.681: INFO: Number of nodes with available pods: 2
Nov 19 17:29:52.681: INFO: Node ip-172-31-68-240 is running more than one daemon pod
Nov 19 17:29:53.681: INFO: Number of nodes with available pods: 2
Nov 19 17:29:53.682: INFO: Node ip-172-31-68-240 is running more than one daemon pod
Nov 19 17:29:54.681: INFO: Number of nodes with available pods: 2
Nov 19 17:29:54.681: INFO: Node ip-172-31-68-240 is running more than one daemon pod
Nov 19 17:29:55.679: INFO: Number of nodes with available pods: 2
Nov 19 17:29:55.679: INFO: Node ip-172-31-68-240 is running more than one daemon pod
Nov 19 17:29:56.682: INFO: Number of nodes with available pods: 3
Nov 19 17:29:56.682: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8112, will wait for the garbage collector to delete the pods
Nov 19 17:29:56.746: INFO: Deleting DaemonSet.extensions daemon-set took: 7.928244ms
Nov 19 17:29:57.246: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.133709ms
Nov 19 17:30:07.550: INFO: Number of nodes with available pods: 0
Nov 19 17:30:07.550: INFO: Number of running nodes: 0, number of available pods: 0
Nov 19 17:30:07.553: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8112/daemonsets","resourceVersion":"17303"},"items":null}

Nov 19 17:30:07.554: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8112/pods","resourceVersion":"17303"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:30:07.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8112" for this suite.

â€¢ [SLOW TEST:28.085 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":305,"completed":120,"skipped":1909,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:30:07.573: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8549
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Nov 19 17:30:09.725: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8549 PodName:var-expansion-fe7f31ef-1aee-452b-9e86-f21e93c38fe0 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 19 17:30:09.725: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: test for file in mounted path
Nov 19 17:30:09.821: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8549 PodName:var-expansion-fe7f31ef-1aee-452b-9e86-f21e93c38fe0 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 19 17:30:09.821: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: updating the annotation value
Nov 19 17:30:10.391: INFO: Successfully updated pod "var-expansion-fe7f31ef-1aee-452b-9e86-f21e93c38fe0"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Nov 19 17:30:10.394: INFO: Deleting pod "var-expansion-fe7f31ef-1aee-452b-9e86-f21e93c38fe0" in namespace "var-expansion-8549"
Nov 19 17:30:10.402: INFO: Wait up to 5m0s for pod "var-expansion-fe7f31ef-1aee-452b-9e86-f21e93c38fe0" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:30:44.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8549" for this suite.

â€¢ [SLOW TEST:36.845 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":305,"completed":121,"skipped":1931,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:30:44.418: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5328
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov 19 17:30:44.549: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 19 17:30:44.554: INFO: Waiting for terminating namespaces to be deleted...
Nov 19 17:30:44.557: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-20-145 before test
Nov 19 17:30:44.560: INFO: nginx-ingress-controller-kubernetes-worker-cshw5 from ingress-nginx-kubernetes-worker started at 2020-11-19 16:40:07 +0000 UTC (1 container statuses recorded)
Nov 19 17:30:44.560: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Nov 19 17:30:44.560: INFO: kube-state-metrics-6f586bb967-8gvg6 from kube-system started at 2020-11-19 16:40:12 +0000 UTC (1 container statuses recorded)
Nov 19 17:30:44.560: INFO: 	Container kube-state-metrics ready: true, restart count 0
Nov 19 17:30:44.560: INFO: kubernetes-dashboard-64f87676d4-ggs4x from kubernetes-dashboard started at 2020-11-19 16:40:12 +0000 UTC (1 container statuses recorded)
Nov 19 17:30:44.560: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Nov 19 17:30:44.560: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-68-240 before test
Nov 19 17:30:44.564: INFO: default-http-backend-kubernetes-worker-6494cbc7fd-vtwt8 from ingress-nginx-kubernetes-worker started at 2020-11-19 16:40:15 +0000 UTC (1 container statuses recorded)
Nov 19 17:30:44.564: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
Nov 19 17:30:44.564: INFO: nginx-ingress-controller-kubernetes-worker-qkjj5 from ingress-nginx-kubernetes-worker started at 2020-11-19 16:40:11 +0000 UTC (1 container statuses recorded)
Nov 19 17:30:44.564: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Nov 19 17:30:44.564: INFO: metrics-server-v0.3.6-7685c8469-xspvp from kube-system started at 2020-11-19 16:40:36 +0000 UTC (2 container statuses recorded)
Nov 19 17:30:44.564: INFO: 	Container metrics-server ready: true, restart count 0
Nov 19 17:30:44.564: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Nov 19 17:30:44.564: INFO: sonobuoy from sonobuoy started at 2020-11-19 16:56:31 +0000 UTC (1 container statuses recorded)
Nov 19 17:30:44.564: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 19 17:30:44.564: INFO: sonobuoy-e2e-job-51d1badd77b04798 from sonobuoy started at 2020-11-19 16:56:36 +0000 UTC (2 container statuses recorded)
Nov 19 17:30:44.564: INFO: 	Container e2e ready: true, restart count 0
Nov 19 17:30:44.564: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 19 17:30:44.564: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-9-169 before test
Nov 19 17:30:44.567: INFO: nginx-ingress-controller-kubernetes-worker-47fcn from ingress-nginx-kubernetes-worker started at 2020-11-19 16:40:06 +0000 UTC (1 container statuses recorded)
Nov 19 17:30:44.567: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Nov 19 17:30:44.567: INFO: coredns-7bb4d77796-wgc4n from kube-system started at 2020-11-19 16:40:12 +0000 UTC (1 container statuses recorded)
Nov 19 17:30:44.567: INFO: 	Container coredns ready: true, restart count 0
Nov 19 17:30:44.567: INFO: dashboard-metrics-scraper-74757fb5b7-qfnkc from kubernetes-dashboard started at 2020-11-19 16:40:12 +0000 UTC (1 container statuses recorded)
Nov 19 17:30:44.567: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-0a8c3397-6164-45ae-a190-f19d67ac8274 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-0a8c3397-6164-45ae-a190-f19d67ac8274 off the node ip-172-31-20-145
STEP: verifying the node doesn't have the label kubernetes.io/e2e-0a8c3397-6164-45ae-a190-f19d67ac8274
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:30:52.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5328" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

â€¢ [SLOW TEST:8.263 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":305,"completed":122,"skipped":1932,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:30:52.682: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4718
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:163
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:30:52.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4718" for this suite.
â€¢{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":305,"completed":123,"skipped":1958,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:30:52.834: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8376
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-b1e54359-873b-463e-83a2-48c439a1323d
STEP: Creating a pod to test consume secrets
Nov 19 17:30:52.982: INFO: Waiting up to 5m0s for pod "pod-secrets-1ab5e96a-8203-4a42-a3e9-773fa384f85d" in namespace "secrets-8376" to be "Succeeded or Failed"
Nov 19 17:30:52.985: INFO: Pod "pod-secrets-1ab5e96a-8203-4a42-a3e9-773fa384f85d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.691621ms
Nov 19 17:30:54.989: INFO: Pod "pod-secrets-1ab5e96a-8203-4a42-a3e9-773fa384f85d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006701807s
STEP: Saw pod success
Nov 19 17:30:54.989: INFO: Pod "pod-secrets-1ab5e96a-8203-4a42-a3e9-773fa384f85d" satisfied condition "Succeeded or Failed"
Nov 19 17:30:54.992: INFO: Trying to get logs from node ip-172-31-9-169 pod pod-secrets-1ab5e96a-8203-4a42-a3e9-773fa384f85d container secret-volume-test: <nil>
STEP: delete the pod
Nov 19 17:30:55.009: INFO: Waiting for pod pod-secrets-1ab5e96a-8203-4a42-a3e9-773fa384f85d to disappear
Nov 19 17:30:55.011: INFO: Pod pod-secrets-1ab5e96a-8203-4a42-a3e9-773fa384f85d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:30:55.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8376" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":124,"skipped":1961,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:30:55.020: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4646
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov 19 17:30:55.150: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 19 17:30:55.156: INFO: Waiting for terminating namespaces to be deleted...
Nov 19 17:30:55.159: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-20-145 before test
Nov 19 17:30:55.163: INFO: nginx-ingress-controller-kubernetes-worker-cshw5 from ingress-nginx-kubernetes-worker started at 2020-11-19 16:40:07 +0000 UTC (1 container statuses recorded)
Nov 19 17:30:55.163: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Nov 19 17:30:55.163: INFO: kube-state-metrics-6f586bb967-8gvg6 from kube-system started at 2020-11-19 16:40:12 +0000 UTC (1 container statuses recorded)
Nov 19 17:30:55.163: INFO: 	Container kube-state-metrics ready: true, restart count 0
Nov 19 17:30:55.163: INFO: kubernetes-dashboard-64f87676d4-ggs4x from kubernetes-dashboard started at 2020-11-19 16:40:12 +0000 UTC (1 container statuses recorded)
Nov 19 17:30:55.163: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Nov 19 17:30:55.163: INFO: pod1 from sched-pred-5328 started at 2020-11-19 17:30:46 +0000 UTC (1 container statuses recorded)
Nov 19 17:30:55.163: INFO: 	Container pod1 ready: true, restart count 0
Nov 19 17:30:55.163: INFO: pod2 from sched-pred-5328 started at 2020-11-19 17:30:48 +0000 UTC (1 container statuses recorded)
Nov 19 17:30:55.163: INFO: 	Container pod2 ready: true, restart count 0
Nov 19 17:30:55.163: INFO: pod3 from sched-pred-5328 started at 2020-11-19 17:30:50 +0000 UTC (1 container statuses recorded)
Nov 19 17:30:55.163: INFO: 	Container pod3 ready: true, restart count 0
Nov 19 17:30:55.163: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-68-240 before test
Nov 19 17:30:55.166: INFO: default-http-backend-kubernetes-worker-6494cbc7fd-vtwt8 from ingress-nginx-kubernetes-worker started at 2020-11-19 16:40:15 +0000 UTC (1 container statuses recorded)
Nov 19 17:30:55.166: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
Nov 19 17:30:55.166: INFO: nginx-ingress-controller-kubernetes-worker-qkjj5 from ingress-nginx-kubernetes-worker started at 2020-11-19 16:40:11 +0000 UTC (1 container statuses recorded)
Nov 19 17:30:55.166: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Nov 19 17:30:55.167: INFO: metrics-server-v0.3.6-7685c8469-xspvp from kube-system started at 2020-11-19 16:40:36 +0000 UTC (2 container statuses recorded)
Nov 19 17:30:55.167: INFO: 	Container metrics-server ready: true, restart count 0
Nov 19 17:30:55.167: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Nov 19 17:30:55.167: INFO: sonobuoy from sonobuoy started at 2020-11-19 16:56:31 +0000 UTC (1 container statuses recorded)
Nov 19 17:30:55.167: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 19 17:30:55.167: INFO: sonobuoy-e2e-job-51d1badd77b04798 from sonobuoy started at 2020-11-19 16:56:36 +0000 UTC (2 container statuses recorded)
Nov 19 17:30:55.167: INFO: 	Container e2e ready: true, restart count 0
Nov 19 17:30:55.167: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 19 17:30:55.167: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-9-169 before test
Nov 19 17:30:55.172: INFO: nginx-ingress-controller-kubernetes-worker-47fcn from ingress-nginx-kubernetes-worker started at 2020-11-19 16:40:06 +0000 UTC (1 container statuses recorded)
Nov 19 17:30:55.172: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Nov 19 17:30:55.172: INFO: coredns-7bb4d77796-wgc4n from kube-system started at 2020-11-19 16:40:12 +0000 UTC (1 container statuses recorded)
Nov 19 17:30:55.172: INFO: 	Container coredns ready: true, restart count 0
Nov 19 17:30:55.172: INFO: dashboard-metrics-scraper-74757fb5b7-qfnkc from kubernetes-dashboard started at 2020-11-19 16:40:12 +0000 UTC (1 container statuses recorded)
Nov 19 17:30:55.172: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Nov 19 17:30:55.172: INFO: pod-qos-class-d61056de-63be-4b31-a904-80b88a0d8b18 from pods-4718 started at 2020-11-19 17:30:52 +0000 UTC (1 container statuses recorded)
Nov 19 17:30:55.172: INFO: 	Container agnhost ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1648f9023f6461a3], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1648f9023fdd962a], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:30:56.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4646" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
â€¢{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":305,"completed":125,"skipped":1979,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:30:56.198: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-620
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 19 17:30:56.591: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 19 17:30:59.614: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Nov 19 17:30:59.645: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:30:59.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-620" for this suite.
STEP: Destroying namespace "webhook-620-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":305,"completed":126,"skipped":2032,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:30:59.730: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4005
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:30:59.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4005" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":305,"completed":127,"skipped":2050,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:30:59.900: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1171
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Nov 19 17:31:04.106: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 19 17:31:04.110: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 19 17:31:06.110: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 19 17:31:06.116: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 19 17:31:08.110: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 19 17:31:08.113: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:31:08.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1171" for this suite.

â€¢ [SLOW TEST:8.223 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":305,"completed":128,"skipped":2063,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:31:08.123: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1358
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:32:08.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1358" for this suite.

â€¢ [SLOW TEST:60.171 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":305,"completed":129,"skipped":2085,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:32:08.295: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8643
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Nov 19 17:32:10.456: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8643 PodName:pod-sharedvolume-3f006e54-8148-479e-8e61-b63eed2262a6 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 19 17:32:10.456: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
Nov 19 17:32:10.544: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:32:10.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8643" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":305,"completed":130,"skipped":2115,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:32:10.553: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1254
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Nov 19 17:32:10.705: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1254 /api/v1/namespaces/watch-1254/configmaps/e2e-watch-test-label-changed fe83e3c8-79b6-44e3-8394-1e284d2f9127 18014 0 2020-11-19 17:32:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-19 17:32:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 19 17:32:10.705: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1254 /api/v1/namespaces/watch-1254/configmaps/e2e-watch-test-label-changed fe83e3c8-79b6-44e3-8394-1e284d2f9127 18015 0 2020-11-19 17:32:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-19 17:32:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 19 17:32:10.705: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1254 /api/v1/namespaces/watch-1254/configmaps/e2e-watch-test-label-changed fe83e3c8-79b6-44e3-8394-1e284d2f9127 18016 0 2020-11-19 17:32:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-19 17:32:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Nov 19 17:32:20.735: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1254 /api/v1/namespaces/watch-1254/configmaps/e2e-watch-test-label-changed fe83e3c8-79b6-44e3-8394-1e284d2f9127 18076 0 2020-11-19 17:32:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-19 17:32:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 19 17:32:20.735: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1254 /api/v1/namespaces/watch-1254/configmaps/e2e-watch-test-label-changed fe83e3c8-79b6-44e3-8394-1e284d2f9127 18077 0 2020-11-19 17:32:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-19 17:32:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 19 17:32:20.736: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1254 /api/v1/namespaces/watch-1254/configmaps/e2e-watch-test-label-changed fe83e3c8-79b6-44e3-8394-1e284d2f9127 18078 0 2020-11-19 17:32:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-19 17:32:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:32:20.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1254" for this suite.

â€¢ [SLOW TEST:10.192 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":305,"completed":131,"skipped":2118,"failed":0}
SSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:32:20.745: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-885
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Nov 19 17:32:23.410: INFO: Successfully updated pod "pod-update-c966489e-ff4f-4ba7-8b9c-c077998bfd9d"
STEP: verifying the updated pod is in kubernetes
Nov 19 17:32:23.417: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:32:23.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-885" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":305,"completed":132,"skipped":2121,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:32:23.427: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3213
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-fb8d2b10-5bab-47a9-b924-d771b6658e3f
STEP: Creating a pod to test consume configMaps
Nov 19 17:32:23.572: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e018a1fb-b713-40d9-9c5f-d8045be05f00" in namespace "projected-3213" to be "Succeeded or Failed"
Nov 19 17:32:23.574: INFO: Pod "pod-projected-configmaps-e018a1fb-b713-40d9-9c5f-d8045be05f00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.397018ms
Nov 19 17:32:25.578: INFO: Pod "pod-projected-configmaps-e018a1fb-b713-40d9-9c5f-d8045be05f00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005861769s
STEP: Saw pod success
Nov 19 17:32:25.578: INFO: Pod "pod-projected-configmaps-e018a1fb-b713-40d9-9c5f-d8045be05f00" satisfied condition "Succeeded or Failed"
Nov 19 17:32:25.581: INFO: Trying to get logs from node ip-172-31-9-169 pod pod-projected-configmaps-e018a1fb-b713-40d9-9c5f-d8045be05f00 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 19 17:32:25.603: INFO: Waiting for pod pod-projected-configmaps-e018a1fb-b713-40d9-9c5f-d8045be05f00 to disappear
Nov 19 17:32:25.606: INFO: Pod pod-projected-configmaps-e018a1fb-b713-40d9-9c5f-d8045be05f00 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:32:25.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3213" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":133,"skipped":2125,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:32:25.615: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8201
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-cbc58050-777d-4340-802f-2781409ab1f8
STEP: Creating a pod to test consume secrets
Nov 19 17:32:25.776: INFO: Waiting up to 5m0s for pod "pod-secrets-e9f1ec80-1f7e-4dcb-bda2-e1a95615faa1" in namespace "secrets-8201" to be "Succeeded or Failed"
Nov 19 17:32:25.783: INFO: Pod "pod-secrets-e9f1ec80-1f7e-4dcb-bda2-e1a95615faa1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.938442ms
Nov 19 17:32:27.790: INFO: Pod "pod-secrets-e9f1ec80-1f7e-4dcb-bda2-e1a95615faa1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014630285s
STEP: Saw pod success
Nov 19 17:32:27.790: INFO: Pod "pod-secrets-e9f1ec80-1f7e-4dcb-bda2-e1a95615faa1" satisfied condition "Succeeded or Failed"
Nov 19 17:32:27.793: INFO: Trying to get logs from node ip-172-31-9-169 pod pod-secrets-e9f1ec80-1f7e-4dcb-bda2-e1a95615faa1 container secret-volume-test: <nil>
STEP: delete the pod
Nov 19 17:32:27.809: INFO: Waiting for pod pod-secrets-e9f1ec80-1f7e-4dcb-bda2-e1a95615faa1 to disappear
Nov 19 17:32:27.811: INFO: Pod pod-secrets-e9f1ec80-1f7e-4dcb-bda2-e1a95615faa1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:32:27.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8201" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":134,"skipped":2128,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:32:27.820: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6616
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl label
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1333
STEP: creating the pod
Nov 19 17:32:27.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 create -f - --namespace=kubectl-6616'
Nov 19 17:32:28.504: INFO: stderr: ""
Nov 19 17:32:28.504: INFO: stdout: "pod/pause created\n"
Nov 19 17:32:28.504: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Nov 19 17:32:28.504: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6616" to be "running and ready"
Nov 19 17:32:28.506: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.392478ms
Nov 19 17:32:30.511: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.0068167s
Nov 19 17:32:30.511: INFO: Pod "pause" satisfied condition "running and ready"
Nov 19 17:32:30.511: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: adding the label testing-label with value testing-label-value to a pod
Nov 19 17:32:30.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 label pods pause testing-label=testing-label-value --namespace=kubectl-6616'
Nov 19 17:32:30.579: INFO: stderr: ""
Nov 19 17:32:30.579: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Nov 19 17:32:30.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get pod pause -L testing-label --namespace=kubectl-6616'
Nov 19 17:32:30.637: INFO: stderr: ""
Nov 19 17:32:30.637: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Nov 19 17:32:30.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 label pods pause testing-label- --namespace=kubectl-6616'
Nov 19 17:32:30.706: INFO: stderr: ""
Nov 19 17:32:30.706: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Nov 19 17:32:30.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get pod pause -L testing-label --namespace=kubectl-6616'
Nov 19 17:32:30.760: INFO: stderr: ""
Nov 19 17:32:30.761: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1340
STEP: using delete to clean up resources
Nov 19 17:32:30.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 delete --grace-period=0 --force -f - --namespace=kubectl-6616'
Nov 19 17:32:30.825: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 19 17:32:30.825: INFO: stdout: "pod \"pause\" force deleted\n"
Nov 19 17:32:30.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get rc,svc -l name=pause --no-headers --namespace=kubectl-6616'
Nov 19 17:32:30.884: INFO: stderr: "No resources found in kubectl-6616 namespace.\n"
Nov 19 17:32:30.884: INFO: stdout: ""
Nov 19 17:32:30.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get pods -l name=pause --namespace=kubectl-6616 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 19 17:32:30.938: INFO: stderr: ""
Nov 19 17:32:30.938: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:32:30.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6616" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":305,"completed":135,"skipped":2130,"failed":0}
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:32:30.947: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-8141
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3026
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-8445
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:32:37.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8141" for this suite.
STEP: Destroying namespace "nsdeletetest-3026" for this suite.
Nov 19 17:32:37.384: INFO: Namespace nsdeletetest-3026 was already deleted
STEP: Destroying namespace "nsdeletetest-8445" for this suite.

â€¢ [SLOW TEST:6.442 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":305,"completed":136,"skipped":2133,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:32:37.390: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2319
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:32:39.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2319" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":137,"skipped":2173,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:32:39.564: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5414
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 19 17:32:40.145: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 19 17:32:43.168: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 17:32:43.173: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6629-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:32:44.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5414" for this suite.
STEP: Destroying namespace "webhook-5414-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":305,"completed":138,"skipped":2177,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:32:44.344: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5498
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-58c3a923-37dd-42c7-b361-aea578d5d0f7
STEP: Creating a pod to test consume configMaps
Nov 19 17:32:44.491: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c528ac16-c468-4566-9920-7b487bc29324" in namespace "projected-5498" to be "Succeeded or Failed"
Nov 19 17:32:44.499: INFO: Pod "pod-projected-configmaps-c528ac16-c468-4566-9920-7b487bc29324": Phase="Pending", Reason="", readiness=false. Elapsed: 7.437823ms
Nov 19 17:32:46.503: INFO: Pod "pod-projected-configmaps-c528ac16-c468-4566-9920-7b487bc29324": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01142884s
STEP: Saw pod success
Nov 19 17:32:46.503: INFO: Pod "pod-projected-configmaps-c528ac16-c468-4566-9920-7b487bc29324" satisfied condition "Succeeded or Failed"
Nov 19 17:32:46.505: INFO: Trying to get logs from node ip-172-31-9-169 pod pod-projected-configmaps-c528ac16-c468-4566-9920-7b487bc29324 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 19 17:32:46.520: INFO: Waiting for pod pod-projected-configmaps-c528ac16-c468-4566-9920-7b487bc29324 to disappear
Nov 19 17:32:46.522: INFO: Pod pod-projected-configmaps-c528ac16-c468-4566-9920-7b487bc29324 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:32:46.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5498" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":139,"skipped":2195,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:32:46.530: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7957
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-9add414d-041d-4ce3-aba2-521b009ab7f4
STEP: Creating secret with name s-test-opt-upd-09799b59-f944-410d-9b36-999d7fea3e91
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-9add414d-041d-4ce3-aba2-521b009ab7f4
STEP: Updating secret s-test-opt-upd-09799b59-f944-410d-9b36-999d7fea3e91
STEP: Creating secret with name s-test-opt-create-e5bd8c32-b26e-47f1-84f0-38edd1b671c6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:32:52.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7957" for this suite.

â€¢ [SLOW TEST:6.299 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":140,"skipped":2270,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:32:52.829: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2646
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 19 17:32:52.968: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a261c859-3449-47a2-85f3-e9a68ed243ab" in namespace "downward-api-2646" to be "Succeeded or Failed"
Nov 19 17:32:52.971: INFO: Pod "downwardapi-volume-a261c859-3449-47a2-85f3-e9a68ed243ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.435032ms
Nov 19 17:32:54.974: INFO: Pod "downwardapi-volume-a261c859-3449-47a2-85f3-e9a68ed243ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005622549s
STEP: Saw pod success
Nov 19 17:32:54.974: INFO: Pod "downwardapi-volume-a261c859-3449-47a2-85f3-e9a68ed243ab" satisfied condition "Succeeded or Failed"
Nov 19 17:32:54.977: INFO: Trying to get logs from node ip-172-31-20-145 pod downwardapi-volume-a261c859-3449-47a2-85f3-e9a68ed243ab container client-container: <nil>
STEP: delete the pod
Nov 19 17:32:54.995: INFO: Waiting for pod downwardapi-volume-a261c859-3449-47a2-85f3-e9a68ed243ab to disappear
Nov 19 17:32:54.998: INFO: Pod downwardapi-volume-a261c859-3449-47a2-85f3-e9a68ed243ab no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:32:54.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2646" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":141,"skipped":2294,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:32:55.007: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-983
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-983
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-983
I1119 17:32:55.182888      22 runners.go:190] Created replication controller with name: externalname-service, namespace: services-983, replica count: 2
Nov 19 17:32:58.233: INFO: Creating new exec pod
I1119 17:32:58.233213      22 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 19 17:33:01.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-983 execpodhwch8 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Nov 19 17:33:01.382: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Nov 19 17:33:01.382: INFO: stdout: ""
Nov 19 17:33:01.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-983 execpodhwch8 -- /bin/sh -x -c nc -zv -t -w 2 10.152.183.231 80'
Nov 19 17:33:01.498: INFO: stderr: "+ nc -zv -t -w 2 10.152.183.231 80\nConnection to 10.152.183.231 80 port [tcp/http] succeeded!\n"
Nov 19 17:33:01.498: INFO: stdout: ""
Nov 19 17:33:01.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-983 execpodhwch8 -- /bin/sh -x -c nc -zv -t -w 2 172.31.68.240 31180'
Nov 19 17:33:01.621: INFO: stderr: "+ nc -zv -t -w 2 172.31.68.240 31180\nConnection to 172.31.68.240 31180 port [tcp/31180] succeeded!\n"
Nov 19 17:33:01.621: INFO: stdout: ""
Nov 19 17:33:01.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-983 execpodhwch8 -- /bin/sh -x -c nc -zv -t -w 2 172.31.20.145 31180'
Nov 19 17:33:01.746: INFO: stderr: "+ nc -zv -t -w 2 172.31.20.145 31180\nConnection to 172.31.20.145 31180 port [tcp/31180] succeeded!\n"
Nov 19 17:33:01.746: INFO: stdout: ""
Nov 19 17:33:01.746: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:33:01.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-983" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

â€¢ [SLOW TEST:6.783 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":305,"completed":142,"skipped":2302,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:33:01.790: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8655
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 19 17:33:02.277: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 19 17:33:05.293: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:33:05.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8655" for this suite.
STEP: Destroying namespace "webhook-8655-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":305,"completed":143,"skipped":2302,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:33:05.458: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1677
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Nov 19 17:33:05.620: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:33:19.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1677" for this suite.

â€¢ [SLOW TEST:13.966 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":305,"completed":144,"skipped":2321,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:33:19.424: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-340
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Nov 19 17:33:19.570: INFO: Waiting up to 5m0s for pod "pod-400c63b2-80a1-4089-9428-d29b168d391a" in namespace "emptydir-340" to be "Succeeded or Failed"
Nov 19 17:33:19.575: INFO: Pod "pod-400c63b2-80a1-4089-9428-d29b168d391a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.453282ms
Nov 19 17:33:21.579: INFO: Pod "pod-400c63b2-80a1-4089-9428-d29b168d391a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009063777s
STEP: Saw pod success
Nov 19 17:33:21.579: INFO: Pod "pod-400c63b2-80a1-4089-9428-d29b168d391a" satisfied condition "Succeeded or Failed"
Nov 19 17:33:21.582: INFO: Trying to get logs from node ip-172-31-20-145 pod pod-400c63b2-80a1-4089-9428-d29b168d391a container test-container: <nil>
STEP: delete the pod
Nov 19 17:33:21.600: INFO: Waiting for pod pod-400c63b2-80a1-4089-9428-d29b168d391a to disappear
Nov 19 17:33:21.602: INFO: Pod pod-400c63b2-80a1-4089-9428-d29b168d391a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:33:21.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-340" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":145,"skipped":2333,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:33:21.612: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-5745
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 17:33:21.746: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:33:22.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5745" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":305,"completed":146,"skipped":2401,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:33:22.968: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1888
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override arguments
Nov 19 17:33:23.110: INFO: Waiting up to 5m0s for pod "client-containers-5149e83b-bfba-40ed-b9ad-164c3e55de6d" in namespace "containers-1888" to be "Succeeded or Failed"
Nov 19 17:33:23.114: INFO: Pod "client-containers-5149e83b-bfba-40ed-b9ad-164c3e55de6d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033733ms
Nov 19 17:33:25.118: INFO: Pod "client-containers-5149e83b-bfba-40ed-b9ad-164c3e55de6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007630637s
STEP: Saw pod success
Nov 19 17:33:25.118: INFO: Pod "client-containers-5149e83b-bfba-40ed-b9ad-164c3e55de6d" satisfied condition "Succeeded or Failed"
Nov 19 17:33:25.121: INFO: Trying to get logs from node ip-172-31-20-145 pod client-containers-5149e83b-bfba-40ed-b9ad-164c3e55de6d container test-container: <nil>
STEP: delete the pod
Nov 19 17:33:25.137: INFO: Waiting for pod client-containers-5149e83b-bfba-40ed-b9ad-164c3e55de6d to disappear
Nov 19 17:33:25.140: INFO: Pod client-containers-5149e83b-bfba-40ed-b9ad-164c3e55de6d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:33:25.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1888" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":305,"completed":147,"skipped":2435,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:33:25.148: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7923
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override all
Nov 19 17:33:25.289: INFO: Waiting up to 5m0s for pod "client-containers-8a245555-3011-4e61-b02f-31485efda08c" in namespace "containers-7923" to be "Succeeded or Failed"
Nov 19 17:33:25.291: INFO: Pod "client-containers-8a245555-3011-4e61-b02f-31485efda08c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.465471ms
Nov 19 17:33:27.295: INFO: Pod "client-containers-8a245555-3011-4e61-b02f-31485efda08c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006135886s
STEP: Saw pod success
Nov 19 17:33:27.295: INFO: Pod "client-containers-8a245555-3011-4e61-b02f-31485efda08c" satisfied condition "Succeeded or Failed"
Nov 19 17:33:27.298: INFO: Trying to get logs from node ip-172-31-20-145 pod client-containers-8a245555-3011-4e61-b02f-31485efda08c container test-container: <nil>
STEP: delete the pod
Nov 19 17:33:27.316: INFO: Waiting for pod client-containers-8a245555-3011-4e61-b02f-31485efda08c to disappear
Nov 19 17:33:27.318: INFO: Pod client-containers-8a245555-3011-4e61-b02f-31485efda08c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:33:27.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7923" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":305,"completed":148,"skipped":2446,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:33:27.326: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6584
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 17:33:27.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 version'
Nov 19 17:33:27.512: INFO: stderr: ""
Nov 19 17:33:27.512: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.4\", GitCommit:\"d360454c9bcd1634cf4cc52d1867af5491dc9c5f\", GitTreeState:\"clean\", BuildDate:\"2020-11-11T13:17:17Z\", GoVersion:\"go1.15.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.4\", GitCommit:\"d360454c9bcd1634cf4cc52d1867af5491dc9c5f\", GitTreeState:\"clean\", BuildDate:\"2020-11-15T02:08:53Z\", GoVersion:\"go1.15.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:33:27.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6584" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":305,"completed":149,"skipped":2457,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:33:27.521: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-3226
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Nov 19 17:33:27.654: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the sample API server.
Nov 19 17:33:27.973: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Nov 19 17:33:30.023: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741404008, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741404008, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741404008, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741404007, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 19 17:33:32.027: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741404008, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741404008, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741404008, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741404007, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 19 17:33:34.028: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741404008, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741404008, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741404008, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741404007, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 19 17:33:36.028: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741404008, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741404008, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741404008, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741404007, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 19 17:33:41.444: INFO: Waited 3.41063504s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:33:42.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-3226" for this suite.

â€¢ [SLOW TEST:14.620 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":305,"completed":150,"skipped":2489,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:33:42.142: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-4680
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Nov 19 17:33:44.844: INFO: Successfully updated pod "adopt-release-2mms2"
STEP: Checking that the Job readopts the Pod
Nov 19 17:33:44.844: INFO: Waiting up to 15m0s for pod "adopt-release-2mms2" in namespace "job-4680" to be "adopted"
Nov 19 17:33:44.848: INFO: Pod "adopt-release-2mms2": Phase="Running", Reason="", readiness=true. Elapsed: 3.8855ms
Nov 19 17:33:46.851: INFO: Pod "adopt-release-2mms2": Phase="Running", Reason="", readiness=true. Elapsed: 2.0073725s
Nov 19 17:33:46.851: INFO: Pod "adopt-release-2mms2" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Nov 19 17:33:47.364: INFO: Successfully updated pod "adopt-release-2mms2"
STEP: Checking that the Job releases the Pod
Nov 19 17:33:47.364: INFO: Waiting up to 15m0s for pod "adopt-release-2mms2" in namespace "job-4680" to be "released"
Nov 19 17:33:47.371: INFO: Pod "adopt-release-2mms2": Phase="Running", Reason="", readiness=true. Elapsed: 6.759077ms
Nov 19 17:33:47.371: INFO: Pod "adopt-release-2mms2" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:33:47.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4680" for this suite.

â€¢ [SLOW TEST:5.240 seconds]
[sig-apps] Job
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":305,"completed":151,"skipped":2501,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:33:47.382: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-620
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-projected-all-test-volume-96906f17-a0f9-4c49-b94b-8996524c8de6
STEP: Creating secret with name secret-projected-all-test-volume-7cafdd5b-0bd8-4d66-97ff-0662c18cc765
STEP: Creating a pod to test Check all projections for projected volume plugin
Nov 19 17:33:47.539: INFO: Waiting up to 5m0s for pod "projected-volume-cd2b41ab-0066-4c9f-90a2-938e77d3023f" in namespace "projected-620" to be "Succeeded or Failed"
Nov 19 17:33:47.544: INFO: Pod "projected-volume-cd2b41ab-0066-4c9f-90a2-938e77d3023f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.255671ms
Nov 19 17:33:49.548: INFO: Pod "projected-volume-cd2b41ab-0066-4c9f-90a2-938e77d3023f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009150627s
STEP: Saw pod success
Nov 19 17:33:49.548: INFO: Pod "projected-volume-cd2b41ab-0066-4c9f-90a2-938e77d3023f" satisfied condition "Succeeded or Failed"
Nov 19 17:33:49.551: INFO: Trying to get logs from node ip-172-31-20-145 pod projected-volume-cd2b41ab-0066-4c9f-90a2-938e77d3023f container projected-all-volume-test: <nil>
STEP: delete the pod
Nov 19 17:33:49.567: INFO: Waiting for pod projected-volume-cd2b41ab-0066-4c9f-90a2-938e77d3023f to disappear
Nov 19 17:33:49.570: INFO: Pod projected-volume-cd2b41ab-0066-4c9f-90a2-938e77d3023f no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:33:49.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-620" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":305,"completed":152,"skipped":2506,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:33:49.579: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8172
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-276bd4fe-bfa4-45c8-a663-0962148179f7 in namespace container-probe-8172
Nov 19 17:33:51.728: INFO: Started pod liveness-276bd4fe-bfa4-45c8-a663-0962148179f7 in namespace container-probe-8172
STEP: checking the pod's current state and verifying that restartCount is present
Nov 19 17:33:51.730: INFO: Initial restart count of pod liveness-276bd4fe-bfa4-45c8-a663-0962148179f7 is 0
Nov 19 17:34:01.753: INFO: Restart count of pod container-probe-8172/liveness-276bd4fe-bfa4-45c8-a663-0962148179f7 is now 1 (10.022333216s elapsed)
Nov 19 17:34:21.792: INFO: Restart count of pod container-probe-8172/liveness-276bd4fe-bfa4-45c8-a663-0962148179f7 is now 2 (30.062009582s elapsed)
Nov 19 17:34:41.827: INFO: Restart count of pod container-probe-8172/liveness-276bd4fe-bfa4-45c8-a663-0962148179f7 is now 3 (50.096920198s elapsed)
Nov 19 17:35:01.862: INFO: Restart count of pod container-probe-8172/liveness-276bd4fe-bfa4-45c8-a663-0962148179f7 is now 4 (1m10.131863178s elapsed)
Nov 19 17:36:12.002: INFO: Restart count of pod container-probe-8172/liveness-276bd4fe-bfa4-45c8-a663-0962148179f7 is now 5 (2m20.271414478s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:36:12.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8172" for this suite.

â€¢ [SLOW TEST:142.441 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":305,"completed":153,"skipped":2533,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:36:12.020: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2544
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Nov 19 17:36:14.709: INFO: Successfully updated pod "annotationupdate25dcbb56-75fb-4ef3-b730-fc7bae22278a"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:36:18.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2544" for this suite.

â€¢ [SLOW TEST:6.719 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":154,"skipped":2535,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:36:18.739: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4486
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting the proxy server
Nov 19 17:36:18.869: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-898325863 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:36:18.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4486" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":305,"completed":155,"skipped":2554,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:36:18.933: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7141
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 19 17:36:20.082: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:36:20.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7141" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":156,"skipped":2569,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:36:20.105: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5160
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W1119 17:36:21.284244      22 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1119 17:36:21.284262      22 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1119 17:36:21.284268      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Nov 19 17:36:21.284: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:36:21.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5160" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":305,"completed":157,"skipped":2603,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:36:21.292: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5032
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service endpoint-test2 in namespace services-5032
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5032 to expose endpoints map[]
Nov 19 17:36:21.439: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Nov 19 17:36:22.447: INFO: successfully validated that service endpoint-test2 in namespace services-5032 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5032
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5032 to expose endpoints map[pod1:[80]]
Nov 19 17:36:24.472: INFO: successfully validated that service endpoint-test2 in namespace services-5032 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-5032
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5032 to expose endpoints map[pod1:[80] pod2:[80]]
Nov 19 17:36:25.496: INFO: successfully validated that service endpoint-test2 in namespace services-5032 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-5032
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5032 to expose endpoints map[pod2:[80]]
Nov 19 17:36:25.521: INFO: successfully validated that service endpoint-test2 in namespace services-5032 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-5032
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5032 to expose endpoints map[]
Nov 19 17:36:25.547: INFO: successfully validated that service endpoint-test2 in namespace services-5032 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:36:25.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5032" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
â€¢{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":305,"completed":158,"skipped":2617,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:36:25.575: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1734
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-70b05462-f254-4c63-8e35-4c47780d29f1
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:36:27.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1734" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":159,"skipped":2633,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:36:27.770: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4459
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test env composition
Nov 19 17:36:27.919: INFO: Waiting up to 5m0s for pod "var-expansion-bebf78a4-43c9-4a9c-8922-43a3243aa650" in namespace "var-expansion-4459" to be "Succeeded or Failed"
Nov 19 17:36:27.927: INFO: Pod "var-expansion-bebf78a4-43c9-4a9c-8922-43a3243aa650": Phase="Pending", Reason="", readiness=false. Elapsed: 8.340837ms
Nov 19 17:36:29.932: INFO: Pod "var-expansion-bebf78a4-43c9-4a9c-8922-43a3243aa650": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0131213s
STEP: Saw pod success
Nov 19 17:36:29.932: INFO: Pod "var-expansion-bebf78a4-43c9-4a9c-8922-43a3243aa650" satisfied condition "Succeeded or Failed"
Nov 19 17:36:29.934: INFO: Trying to get logs from node ip-172-31-20-145 pod var-expansion-bebf78a4-43c9-4a9c-8922-43a3243aa650 container dapi-container: <nil>
STEP: delete the pod
Nov 19 17:36:29.951: INFO: Waiting for pod var-expansion-bebf78a4-43c9-4a9c-8922-43a3243aa650 to disappear
Nov 19 17:36:29.953: INFO: Pod var-expansion-bebf78a4-43c9-4a9c-8922-43a3243aa650 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:36:29.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4459" for this suite.
â€¢{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":305,"completed":160,"skipped":2657,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:36:29.960: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-223
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating all guestbook components
Nov 19 17:36:30.094: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Nov 19 17:36:30.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 create -f - --namespace=kubectl-223'
Nov 19 17:36:30.304: INFO: stderr: ""
Nov 19 17:36:30.304: INFO: stdout: "service/agnhost-replica created\n"
Nov 19 17:36:30.304: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Nov 19 17:36:30.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 create -f - --namespace=kubectl-223'
Nov 19 17:36:30.475: INFO: stderr: ""
Nov 19 17:36:30.475: INFO: stdout: "service/agnhost-primary created\n"
Nov 19 17:36:30.475: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Nov 19 17:36:30.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 create -f - --namespace=kubectl-223'
Nov 19 17:36:30.695: INFO: stderr: ""
Nov 19 17:36:30.695: INFO: stdout: "service/frontend created\n"
Nov 19 17:36:30.695: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Nov 19 17:36:30.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 create -f - --namespace=kubectl-223'
Nov 19 17:36:30.914: INFO: stderr: ""
Nov 19 17:36:30.914: INFO: stdout: "deployment.apps/frontend created\n"
Nov 19 17:36:30.914: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Nov 19 17:36:30.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 create -f - --namespace=kubectl-223'
Nov 19 17:36:31.068: INFO: stderr: ""
Nov 19 17:36:31.068: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Nov 19 17:36:31.068: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Nov 19 17:36:31.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 create -f - --namespace=kubectl-223'
Nov 19 17:36:31.234: INFO: stderr: ""
Nov 19 17:36:31.234: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Nov 19 17:36:31.234: INFO: Waiting for all frontend pods to be Running.
Nov 19 17:36:36.284: INFO: Waiting for frontend to serve content.
Nov 19 17:36:36.293: INFO: Trying to add a new entry to the guestbook.
Nov 19 17:36:36.301: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Nov 19 17:36:36.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 delete --grace-period=0 --force -f - --namespace=kubectl-223'
Nov 19 17:36:36.385: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 19 17:36:36.385: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Nov 19 17:36:36.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 delete --grace-period=0 --force -f - --namespace=kubectl-223'
Nov 19 17:36:36.458: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 19 17:36:36.458: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Nov 19 17:36:36.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 delete --grace-period=0 --force -f - --namespace=kubectl-223'
Nov 19 17:36:36.532: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 19 17:36:36.532: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Nov 19 17:36:36.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 delete --grace-period=0 --force -f - --namespace=kubectl-223'
Nov 19 17:36:36.594: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 19 17:36:36.594: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Nov 19 17:36:36.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 delete --grace-period=0 --force -f - --namespace=kubectl-223'
Nov 19 17:36:36.656: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 19 17:36:36.656: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Nov 19 17:36:36.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 delete --grace-period=0 --force -f - --namespace=kubectl-223'
Nov 19 17:36:36.715: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 19 17:36:36.715: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:36:36.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-223" for this suite.

â€¢ [SLOW TEST:6.763 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:351
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":305,"completed":161,"skipped":2689,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:36:36.724: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5905
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-e292c3f5-5427-4335-ace9-4d900ae271c9
STEP: Creating a pod to test consume configMaps
Nov 19 17:36:36.872: INFO: Waiting up to 5m0s for pod "pod-configmaps-155dd13d-161d-4aa4-8f2a-b9e0e4e9cd9e" in namespace "configmap-5905" to be "Succeeded or Failed"
Nov 19 17:36:36.874: INFO: Pod "pod-configmaps-155dd13d-161d-4aa4-8f2a-b9e0e4e9cd9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.455042ms
Nov 19 17:36:38.877: INFO: Pod "pod-configmaps-155dd13d-161d-4aa4-8f2a-b9e0e4e9cd9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005566453s
STEP: Saw pod success
Nov 19 17:36:38.877: INFO: Pod "pod-configmaps-155dd13d-161d-4aa4-8f2a-b9e0e4e9cd9e" satisfied condition "Succeeded or Failed"
Nov 19 17:36:38.880: INFO: Trying to get logs from node ip-172-31-68-240 pod pod-configmaps-155dd13d-161d-4aa4-8f2a-b9e0e4e9cd9e container configmap-volume-test: <nil>
STEP: delete the pod
Nov 19 17:36:38.896: INFO: Waiting for pod pod-configmaps-155dd13d-161d-4aa4-8f2a-b9e0e4e9cd9e to disappear
Nov 19 17:36:38.899: INFO: Pod pod-configmaps-155dd13d-161d-4aa4-8f2a-b9e0e4e9cd9e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:36:38.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5905" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":162,"skipped":2699,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:36:38.908: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2509
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Nov 19 17:36:39.049: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:36:42.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2509" for this suite.
â€¢{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":305,"completed":163,"skipped":2712,"failed":0}

------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:36:42.837: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8219
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-9154a400-48b5-42f5-a778-8f55c28e7013
STEP: Creating a pod to test consume secrets
Nov 19 17:36:42.982: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3096bf1f-d811-4a9a-b6d6-d7dfe81e6bde" in namespace "projected-8219" to be "Succeeded or Failed"
Nov 19 17:36:42.985: INFO: Pod "pod-projected-secrets-3096bf1f-d811-4a9a-b6d6-d7dfe81e6bde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.635971ms
Nov 19 17:36:44.989: INFO: Pod "pod-projected-secrets-3096bf1f-d811-4a9a-b6d6-d7dfe81e6bde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00674984s
STEP: Saw pod success
Nov 19 17:36:44.989: INFO: Pod "pod-projected-secrets-3096bf1f-d811-4a9a-b6d6-d7dfe81e6bde" satisfied condition "Succeeded or Failed"
Nov 19 17:36:44.991: INFO: Trying to get logs from node ip-172-31-9-169 pod pod-projected-secrets-3096bf1f-d811-4a9a-b6d6-d7dfe81e6bde container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 19 17:36:45.023: INFO: Waiting for pod pod-projected-secrets-3096bf1f-d811-4a9a-b6d6-d7dfe81e6bde to disappear
Nov 19 17:36:45.027: INFO: Pod pod-projected-secrets-3096bf1f-d811-4a9a-b6d6-d7dfe81e6bde no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:36:45.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8219" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":164,"skipped":2712,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:36:45.038: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-7029
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Nov 19 17:36:49.226: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 19 17:36:49.229: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 19 17:36:51.229: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 19 17:36:51.234: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 19 17:36:53.229: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 19 17:36:53.233: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:36:53.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7029" for this suite.

â€¢ [SLOW TEST:8.207 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":305,"completed":165,"skipped":2731,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:36:53.245: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9303
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Nov 19 17:36:53.397: INFO: Waiting up to 5m0s for pod "downward-api-cbff8fea-de59-471b-9da6-315705f37376" in namespace "downward-api-9303" to be "Succeeded or Failed"
Nov 19 17:36:53.401: INFO: Pod "downward-api-cbff8fea-de59-471b-9da6-315705f37376": Phase="Pending", Reason="", readiness=false. Elapsed: 3.978398ms
Nov 19 17:36:55.405: INFO: Pod "downward-api-cbff8fea-de59-471b-9da6-315705f37376": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008073771s
STEP: Saw pod success
Nov 19 17:36:55.405: INFO: Pod "downward-api-cbff8fea-de59-471b-9da6-315705f37376" satisfied condition "Succeeded or Failed"
Nov 19 17:36:55.409: INFO: Trying to get logs from node ip-172-31-20-145 pod downward-api-cbff8fea-de59-471b-9da6-315705f37376 container dapi-container: <nil>
STEP: delete the pod
Nov 19 17:36:55.429: INFO: Waiting for pod downward-api-cbff8fea-de59-471b-9da6-315705f37376 to disappear
Nov 19 17:36:55.432: INFO: Pod downward-api-cbff8fea-de59-471b-9da6-315705f37376 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:36:55.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9303" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":305,"completed":166,"skipped":2758,"failed":0}
S
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:36:55.440: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2487
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should delete a collection of pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pods
Nov 19 17:36:55.583: INFO: created test-pod-1
Nov 19 17:36:55.590: INFO: created test-pod-2
Nov 19 17:36:55.598: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:36:55.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2487" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":305,"completed":167,"skipped":2759,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Ingress API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:36:55.655: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename ingress
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingress-8948
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Nov 19 17:36:55.819: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Nov 19 17:36:55.822: INFO: starting watch
STEP: patching
STEP: updating
Nov 19 17:36:55.837: INFO: waiting for watch events with expected annotations
Nov 19 17:36:55.837: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:36:55.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-8948" for this suite.
â€¢{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":305,"completed":168,"skipped":2777,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:36:55.910: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8082
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:36:56.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8082" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
â€¢{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":305,"completed":169,"skipped":2782,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:36:56.053: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9349
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:37:01.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9349" for this suite.

â€¢ [SLOW TEST:6.006 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":305,"completed":170,"skipped":2790,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:37:02.059: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3318
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Nov 19 17:37:02.203: INFO: Waiting up to 5m0s for pod "pod-fd63a67e-e17b-4793-86dd-1e7e08497f0f" in namespace "emptydir-3318" to be "Succeeded or Failed"
Nov 19 17:37:02.213: INFO: Pod "pod-fd63a67e-e17b-4793-86dd-1e7e08497f0f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.918465ms
Nov 19 17:37:04.217: INFO: Pod "pod-fd63a67e-e17b-4793-86dd-1e7e08497f0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013818633s
STEP: Saw pod success
Nov 19 17:37:04.217: INFO: Pod "pod-fd63a67e-e17b-4793-86dd-1e7e08497f0f" satisfied condition "Succeeded or Failed"
Nov 19 17:37:04.219: INFO: Trying to get logs from node ip-172-31-20-145 pod pod-fd63a67e-e17b-4793-86dd-1e7e08497f0f container test-container: <nil>
STEP: delete the pod
Nov 19 17:37:04.235: INFO: Waiting for pod pod-fd63a67e-e17b-4793-86dd-1e7e08497f0f to disappear
Nov 19 17:37:04.237: INFO: Pod pod-fd63a67e-e17b-4793-86dd-1e7e08497f0f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:37:04.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3318" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":171,"skipped":2799,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:37:04.247: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3679
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod with failed condition
STEP: updating the pod
Nov 19 17:39:04.918: INFO: Successfully updated pod "var-expansion-47fcc95b-8ad7-4c2c-8da9-192c87787225"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Nov 19 17:39:06.927: INFO: Deleting pod "var-expansion-47fcc95b-8ad7-4c2c-8da9-192c87787225" in namespace "var-expansion-3679"
Nov 19 17:39:06.934: INFO: Wait up to 5m0s for pod "var-expansion-47fcc95b-8ad7-4c2c-8da9-192c87787225" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:39:38.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3679" for this suite.

â€¢ [SLOW TEST:154.705 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":305,"completed":172,"skipped":2838,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:39:38.952: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3704
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-98cbdd36-3461-43e2-bd7a-7a93604bf46a
STEP: Creating a pod to test consume configMaps
Nov 19 17:39:39.099: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fb8d45f3-3a03-487b-9ec5-801806ed6dec" in namespace "projected-3704" to be "Succeeded or Failed"
Nov 19 17:39:39.103: INFO: Pod "pod-projected-configmaps-fb8d45f3-3a03-487b-9ec5-801806ed6dec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02266ms
Nov 19 17:39:41.108: INFO: Pod "pod-projected-configmaps-fb8d45f3-3a03-487b-9ec5-801806ed6dec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008192013s
STEP: Saw pod success
Nov 19 17:39:41.108: INFO: Pod "pod-projected-configmaps-fb8d45f3-3a03-487b-9ec5-801806ed6dec" satisfied condition "Succeeded or Failed"
Nov 19 17:39:41.110: INFO: Trying to get logs from node ip-172-31-20-145 pod pod-projected-configmaps-fb8d45f3-3a03-487b-9ec5-801806ed6dec container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 19 17:39:41.132: INFO: Waiting for pod pod-projected-configmaps-fb8d45f3-3a03-487b-9ec5-801806ed6dec to disappear
Nov 19 17:39:41.135: INFO: Pod pod-projected-configmaps-fb8d45f3-3a03-487b-9ec5-801806ed6dec no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:39:41.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3704" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":173,"skipped":2852,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:39:41.141: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9131
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9131.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9131.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 19 17:39:43.322: INFO: DNS probes using dns-9131/dns-test-2eadd5d5-aa88-49a4-94b7-c228b8290177 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:39:43.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9131" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":305,"completed":174,"skipped":2853,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:39:43.344: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2104
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on node default medium
Nov 19 17:39:43.484: INFO: Waiting up to 5m0s for pod "pod-0fe4e9ae-0190-435a-b5b0-202fc4658ab4" in namespace "emptydir-2104" to be "Succeeded or Failed"
Nov 19 17:39:43.488: INFO: Pod "pod-0fe4e9ae-0190-435a-b5b0-202fc4658ab4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.701325ms
Nov 19 17:39:45.491: INFO: Pod "pod-0fe4e9ae-0190-435a-b5b0-202fc4658ab4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007562036s
STEP: Saw pod success
Nov 19 17:39:45.491: INFO: Pod "pod-0fe4e9ae-0190-435a-b5b0-202fc4658ab4" satisfied condition "Succeeded or Failed"
Nov 19 17:39:45.495: INFO: Trying to get logs from node ip-172-31-20-145 pod pod-0fe4e9ae-0190-435a-b5b0-202fc4658ab4 container test-container: <nil>
STEP: delete the pod
Nov 19 17:39:45.509: INFO: Waiting for pod pod-0fe4e9ae-0190-435a-b5b0-202fc4658ab4 to disappear
Nov 19 17:39:45.512: INFO: Pod pod-0fe4e9ae-0190-435a-b5b0-202fc4658ab4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:39:45.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2104" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":175,"skipped":2895,"failed":0}
SSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:39:45.520: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5962
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5962 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5962;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5962 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5962;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5962.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5962.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5962.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5962.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5962.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5962.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5962.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5962.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5962.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5962.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5962.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5962.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5962.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 21.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.21_udp@PTR;check="$$(dig +tcp +noall +answer +search 21.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.21_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5962 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5962;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5962 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5962;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5962.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5962.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5962.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5962.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5962.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5962.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5962.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5962.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5962.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5962.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5962.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5962.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5962.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 21.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.21_udp@PTR;check="$$(dig +tcp +noall +answer +search 21.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.21_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 19 17:39:47.702: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5962/dns-test-11d49de4-98e7-4033-b456-bff57ed87e27: the server could not find the requested resource (get pods dns-test-11d49de4-98e7-4033-b456-bff57ed87e27)
Nov 19 17:39:47.706: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5962/dns-test-11d49de4-98e7-4033-b456-bff57ed87e27: the server could not find the requested resource (get pods dns-test-11d49de4-98e7-4033-b456-bff57ed87e27)
Nov 19 17:39:47.710: INFO: Unable to read wheezy_udp@dns-test-service.dns-5962 from pod dns-5962/dns-test-11d49de4-98e7-4033-b456-bff57ed87e27: the server could not find the requested resource (get pods dns-test-11d49de4-98e7-4033-b456-bff57ed87e27)
Nov 19 17:39:47.712: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5962 from pod dns-5962/dns-test-11d49de4-98e7-4033-b456-bff57ed87e27: the server could not find the requested resource (get pods dns-test-11d49de4-98e7-4033-b456-bff57ed87e27)
Nov 19 17:39:47.715: INFO: Unable to read wheezy_udp@dns-test-service.dns-5962.svc from pod dns-5962/dns-test-11d49de4-98e7-4033-b456-bff57ed87e27: the server could not find the requested resource (get pods dns-test-11d49de4-98e7-4033-b456-bff57ed87e27)
Nov 19 17:39:47.719: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5962.svc from pod dns-5962/dns-test-11d49de4-98e7-4033-b456-bff57ed87e27: the server could not find the requested resource (get pods dns-test-11d49de4-98e7-4033-b456-bff57ed87e27)
Nov 19 17:39:47.722: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5962.svc from pod dns-5962/dns-test-11d49de4-98e7-4033-b456-bff57ed87e27: the server could not find the requested resource (get pods dns-test-11d49de4-98e7-4033-b456-bff57ed87e27)
Nov 19 17:39:47.724: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5962.svc from pod dns-5962/dns-test-11d49de4-98e7-4033-b456-bff57ed87e27: the server could not find the requested resource (get pods dns-test-11d49de4-98e7-4033-b456-bff57ed87e27)
Nov 19 17:39:47.746: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5962/dns-test-11d49de4-98e7-4033-b456-bff57ed87e27: the server could not find the requested resource (get pods dns-test-11d49de4-98e7-4033-b456-bff57ed87e27)
Nov 19 17:39:47.748: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5962/dns-test-11d49de4-98e7-4033-b456-bff57ed87e27: the server could not find the requested resource (get pods dns-test-11d49de4-98e7-4033-b456-bff57ed87e27)
Nov 19 17:39:47.751: INFO: Unable to read jessie_udp@dns-test-service.dns-5962 from pod dns-5962/dns-test-11d49de4-98e7-4033-b456-bff57ed87e27: the server could not find the requested resource (get pods dns-test-11d49de4-98e7-4033-b456-bff57ed87e27)
Nov 19 17:39:47.754: INFO: Unable to read jessie_tcp@dns-test-service.dns-5962 from pod dns-5962/dns-test-11d49de4-98e7-4033-b456-bff57ed87e27: the server could not find the requested resource (get pods dns-test-11d49de4-98e7-4033-b456-bff57ed87e27)
Nov 19 17:39:47.757: INFO: Unable to read jessie_udp@dns-test-service.dns-5962.svc from pod dns-5962/dns-test-11d49de4-98e7-4033-b456-bff57ed87e27: the server could not find the requested resource (get pods dns-test-11d49de4-98e7-4033-b456-bff57ed87e27)
Nov 19 17:39:47.759: INFO: Unable to read jessie_tcp@dns-test-service.dns-5962.svc from pod dns-5962/dns-test-11d49de4-98e7-4033-b456-bff57ed87e27: the server could not find the requested resource (get pods dns-test-11d49de4-98e7-4033-b456-bff57ed87e27)
Nov 19 17:39:47.763: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5962.svc from pod dns-5962/dns-test-11d49de4-98e7-4033-b456-bff57ed87e27: the server could not find the requested resource (get pods dns-test-11d49de4-98e7-4033-b456-bff57ed87e27)
Nov 19 17:39:47.765: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5962.svc from pod dns-5962/dns-test-11d49de4-98e7-4033-b456-bff57ed87e27: the server could not find the requested resource (get pods dns-test-11d49de4-98e7-4033-b456-bff57ed87e27)
Nov 19 17:39:47.783: INFO: Lookups using dns-5962/dns-test-11d49de4-98e7-4033-b456-bff57ed87e27 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5962 wheezy_tcp@dns-test-service.dns-5962 wheezy_udp@dns-test-service.dns-5962.svc wheezy_tcp@dns-test-service.dns-5962.svc wheezy_udp@_http._tcp.dns-test-service.dns-5962.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5962.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5962 jessie_tcp@dns-test-service.dns-5962 jessie_udp@dns-test-service.dns-5962.svc jessie_tcp@dns-test-service.dns-5962.svc jessie_udp@_http._tcp.dns-test-service.dns-5962.svc jessie_tcp@_http._tcp.dns-test-service.dns-5962.svc]

Nov 19 17:39:52.868: INFO: DNS probes using dns-5962/dns-test-11d49de4-98e7-4033-b456-bff57ed87e27 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:39:52.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5962" for this suite.

â€¢ [SLOW TEST:7.426 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":305,"completed":176,"skipped":2903,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:39:52.946: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-3705
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:39:53.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-3705" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":305,"completed":177,"skipped":2913,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:39:53.087: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2696
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating api versions
Nov 19 17:39:53.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 api-versions'
Nov 19 17:39:53.277: INFO: stderr: ""
Nov 19 17:39:53.277: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:39:53.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2696" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":305,"completed":178,"skipped":2931,"failed":0}
SSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:39:53.288: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4881
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:39:53.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4881" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":305,"completed":179,"skipped":2934,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:39:53.455: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9693
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:40:04.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9693" for this suite.

â€¢ [SLOW TEST:11.243 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":305,"completed":180,"skipped":2947,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:40:04.698: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8589
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-8589
STEP: creating service affinity-clusterip-transition in namespace services-8589
STEP: creating replication controller affinity-clusterip-transition in namespace services-8589
I1119 17:40:04.850121      22 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-8589, replica count: 3
I1119 17:40:07.900395      22 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 19 17:40:07.905: INFO: Creating new exec pod
Nov 19 17:40:10.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-8589 execpod-affinity9wqmm -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Nov 19 17:40:11.078: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Nov 19 17:40:11.078: INFO: stdout: ""
Nov 19 17:40:11.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-8589 execpod-affinity9wqmm -- /bin/sh -x -c nc -zv -t -w 2 10.152.183.64 80'
Nov 19 17:40:11.196: INFO: stderr: "+ nc -zv -t -w 2 10.152.183.64 80\nConnection to 10.152.183.64 80 port [tcp/http] succeeded!\n"
Nov 19 17:40:11.196: INFO: stdout: ""
Nov 19 17:40:11.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-8589 execpod-affinity9wqmm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.64:80/ ; done'
Nov 19 17:40:11.383: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.64:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.64:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.64:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.64:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.64:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.64:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.64:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.64:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.64:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.64:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.64:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.64:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.64:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.64:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.64:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.64:80/\n"
Nov 19 17:40:11.383: INFO: stdout: "\naffinity-clusterip-transition-8mhz4\naffinity-clusterip-transition-8mhz4\naffinity-clusterip-transition-ckt2c\naffinity-clusterip-transition-8mhz4\naffinity-clusterip-transition-pqqbp\naffinity-clusterip-transition-pqqbp\naffinity-clusterip-transition-ckt2c\naffinity-clusterip-transition-pqqbp\naffinity-clusterip-transition-ckt2c\naffinity-clusterip-transition-pqqbp\naffinity-clusterip-transition-8mhz4\naffinity-clusterip-transition-pqqbp\naffinity-clusterip-transition-pqqbp\naffinity-clusterip-transition-8mhz4\naffinity-clusterip-transition-ckt2c\naffinity-clusterip-transition-8mhz4"
Nov 19 17:40:11.383: INFO: Received response from host: affinity-clusterip-transition-8mhz4
Nov 19 17:40:11.383: INFO: Received response from host: affinity-clusterip-transition-8mhz4
Nov 19 17:40:11.383: INFO: Received response from host: affinity-clusterip-transition-ckt2c
Nov 19 17:40:11.383: INFO: Received response from host: affinity-clusterip-transition-8mhz4
Nov 19 17:40:11.383: INFO: Received response from host: affinity-clusterip-transition-pqqbp
Nov 19 17:40:11.383: INFO: Received response from host: affinity-clusterip-transition-pqqbp
Nov 19 17:40:11.383: INFO: Received response from host: affinity-clusterip-transition-ckt2c
Nov 19 17:40:11.383: INFO: Received response from host: affinity-clusterip-transition-pqqbp
Nov 19 17:40:11.383: INFO: Received response from host: affinity-clusterip-transition-ckt2c
Nov 19 17:40:11.383: INFO: Received response from host: affinity-clusterip-transition-pqqbp
Nov 19 17:40:11.383: INFO: Received response from host: affinity-clusterip-transition-8mhz4
Nov 19 17:40:11.383: INFO: Received response from host: affinity-clusterip-transition-pqqbp
Nov 19 17:40:11.383: INFO: Received response from host: affinity-clusterip-transition-pqqbp
Nov 19 17:40:11.383: INFO: Received response from host: affinity-clusterip-transition-8mhz4
Nov 19 17:40:11.383: INFO: Received response from host: affinity-clusterip-transition-ckt2c
Nov 19 17:40:11.383: INFO: Received response from host: affinity-clusterip-transition-8mhz4
Nov 19 17:40:11.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-8589 execpod-affinity9wqmm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.64:80/ ; done'
Nov 19 17:40:11.567: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.64:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.64:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.64:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.64:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.64:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.64:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.64:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.64:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.64:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.64:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.64:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.64:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.64:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.64:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.64:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.64:80/\n"
Nov 19 17:40:11.568: INFO: stdout: "\naffinity-clusterip-transition-pqqbp\naffinity-clusterip-transition-pqqbp\naffinity-clusterip-transition-pqqbp\naffinity-clusterip-transition-pqqbp\naffinity-clusterip-transition-pqqbp\naffinity-clusterip-transition-pqqbp\naffinity-clusterip-transition-pqqbp\naffinity-clusterip-transition-pqqbp\naffinity-clusterip-transition-pqqbp\naffinity-clusterip-transition-pqqbp\naffinity-clusterip-transition-pqqbp\naffinity-clusterip-transition-pqqbp\naffinity-clusterip-transition-pqqbp\naffinity-clusterip-transition-pqqbp\naffinity-clusterip-transition-pqqbp\naffinity-clusterip-transition-pqqbp"
Nov 19 17:40:11.568: INFO: Received response from host: affinity-clusterip-transition-pqqbp
Nov 19 17:40:11.568: INFO: Received response from host: affinity-clusterip-transition-pqqbp
Nov 19 17:40:11.568: INFO: Received response from host: affinity-clusterip-transition-pqqbp
Nov 19 17:40:11.568: INFO: Received response from host: affinity-clusterip-transition-pqqbp
Nov 19 17:40:11.568: INFO: Received response from host: affinity-clusterip-transition-pqqbp
Nov 19 17:40:11.568: INFO: Received response from host: affinity-clusterip-transition-pqqbp
Nov 19 17:40:11.568: INFO: Received response from host: affinity-clusterip-transition-pqqbp
Nov 19 17:40:11.568: INFO: Received response from host: affinity-clusterip-transition-pqqbp
Nov 19 17:40:11.568: INFO: Received response from host: affinity-clusterip-transition-pqqbp
Nov 19 17:40:11.568: INFO: Received response from host: affinity-clusterip-transition-pqqbp
Nov 19 17:40:11.568: INFO: Received response from host: affinity-clusterip-transition-pqqbp
Nov 19 17:40:11.568: INFO: Received response from host: affinity-clusterip-transition-pqqbp
Nov 19 17:40:11.568: INFO: Received response from host: affinity-clusterip-transition-pqqbp
Nov 19 17:40:11.568: INFO: Received response from host: affinity-clusterip-transition-pqqbp
Nov 19 17:40:11.568: INFO: Received response from host: affinity-clusterip-transition-pqqbp
Nov 19 17:40:11.568: INFO: Received response from host: affinity-clusterip-transition-pqqbp
Nov 19 17:40:11.568: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-8589, will wait for the garbage collector to delete the pods
Nov 19 17:40:11.636: INFO: Deleting ReplicationController affinity-clusterip-transition took: 6.031613ms
Nov 19 17:40:12.136: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 500.157406ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:40:27.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8589" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

â€¢ [SLOW TEST:22.876 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":181,"skipped":2948,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:40:27.574: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7987
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-7987
STEP: creating service affinity-clusterip in namespace services-7987
STEP: creating replication controller affinity-clusterip in namespace services-7987
I1119 17:40:27.730573      22 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-7987, replica count: 3
I1119 17:40:30.780843      22 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 19 17:40:30.787: INFO: Creating new exec pod
Nov 19 17:40:33.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-7987 execpod-affinitynvgpp -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Nov 19 17:40:33.940: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Nov 19 17:40:33.940: INFO: stdout: ""
Nov 19 17:40:33.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-7987 execpod-affinitynvgpp -- /bin/sh -x -c nc -zv -t -w 2 10.152.183.180 80'
Nov 19 17:40:34.051: INFO: stderr: "+ nc -zv -t -w 2 10.152.183.180 80\nConnection to 10.152.183.180 80 port [tcp/http] succeeded!\n"
Nov 19 17:40:34.051: INFO: stdout: ""
Nov 19 17:40:34.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-7987 execpod-affinitynvgpp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.180:80/ ; done'
Nov 19 17:40:34.219: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.180:80/\n"
Nov 19 17:40:34.219: INFO: stdout: "\naffinity-clusterip-8dcfb\naffinity-clusterip-8dcfb\naffinity-clusterip-8dcfb\naffinity-clusterip-8dcfb\naffinity-clusterip-8dcfb\naffinity-clusterip-8dcfb\naffinity-clusterip-8dcfb\naffinity-clusterip-8dcfb\naffinity-clusterip-8dcfb\naffinity-clusterip-8dcfb\naffinity-clusterip-8dcfb\naffinity-clusterip-8dcfb\naffinity-clusterip-8dcfb\naffinity-clusterip-8dcfb\naffinity-clusterip-8dcfb\naffinity-clusterip-8dcfb"
Nov 19 17:40:34.219: INFO: Received response from host: affinity-clusterip-8dcfb
Nov 19 17:40:34.219: INFO: Received response from host: affinity-clusterip-8dcfb
Nov 19 17:40:34.219: INFO: Received response from host: affinity-clusterip-8dcfb
Nov 19 17:40:34.219: INFO: Received response from host: affinity-clusterip-8dcfb
Nov 19 17:40:34.219: INFO: Received response from host: affinity-clusterip-8dcfb
Nov 19 17:40:34.219: INFO: Received response from host: affinity-clusterip-8dcfb
Nov 19 17:40:34.219: INFO: Received response from host: affinity-clusterip-8dcfb
Nov 19 17:40:34.219: INFO: Received response from host: affinity-clusterip-8dcfb
Nov 19 17:40:34.219: INFO: Received response from host: affinity-clusterip-8dcfb
Nov 19 17:40:34.219: INFO: Received response from host: affinity-clusterip-8dcfb
Nov 19 17:40:34.219: INFO: Received response from host: affinity-clusterip-8dcfb
Nov 19 17:40:34.219: INFO: Received response from host: affinity-clusterip-8dcfb
Nov 19 17:40:34.219: INFO: Received response from host: affinity-clusterip-8dcfb
Nov 19 17:40:34.219: INFO: Received response from host: affinity-clusterip-8dcfb
Nov 19 17:40:34.219: INFO: Received response from host: affinity-clusterip-8dcfb
Nov 19 17:40:34.219: INFO: Received response from host: affinity-clusterip-8dcfb
Nov 19 17:40:34.219: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-7987, will wait for the garbage collector to delete the pods
Nov 19 17:40:34.289: INFO: Deleting ReplicationController affinity-clusterip took: 6.040187ms
Nov 19 17:40:34.789: INFO: Terminating ReplicationController affinity-clusterip pods took: 500.144928ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:40:47.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7987" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

â€¢ [SLOW TEST:20.045 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":182,"skipped":2983,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:40:47.619: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2470
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service nodeport-test with type=NodePort in namespace services-2470
STEP: creating replication controller nodeport-test in namespace services-2470
I1119 17:40:47.791340      22 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-2470, replica count: 2
I1119 17:40:50.841608      22 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 19 17:40:50.841: INFO: Creating new exec pod
Nov 19 17:40:53.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-2470 execpod8q2fn -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Nov 19 17:40:54.023: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Nov 19 17:40:54.023: INFO: stdout: ""
Nov 19 17:40:54.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-2470 execpod8q2fn -- /bin/sh -x -c nc -zv -t -w 2 10.152.183.77 80'
Nov 19 17:40:54.145: INFO: stderr: "+ nc -zv -t -w 2 10.152.183.77 80\nConnection to 10.152.183.77 80 port [tcp/http] succeeded!\n"
Nov 19 17:40:54.145: INFO: stdout: ""
Nov 19 17:40:54.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-2470 execpod8q2fn -- /bin/sh -x -c nc -zv -t -w 2 172.31.68.240 32376'
Nov 19 17:40:54.282: INFO: stderr: "+ nc -zv -t -w 2 172.31.68.240 32376\nConnection to 172.31.68.240 32376 port [tcp/32376] succeeded!\n"
Nov 19 17:40:54.282: INFO: stdout: ""
Nov 19 17:40:54.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-2470 execpod8q2fn -- /bin/sh -x -c nc -zv -t -w 2 172.31.9.169 32376'
Nov 19 17:40:54.408: INFO: stderr: "+ nc -zv -t -w 2 172.31.9.169 32376\nConnection to 172.31.9.169 32376 port [tcp/32376] succeeded!\n"
Nov 19 17:40:54.408: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:40:54.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2470" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

â€¢ [SLOW TEST:6.801 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":305,"completed":183,"skipped":3013,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:40:54.421: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8788
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 19 17:40:55.568: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:40:55.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8788" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":184,"skipped":3034,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:40:55.589: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3206
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1119 17:41:01.746549      22 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1119 17:41:01.746565      22 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1119 17:41:01.746569      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Nov 19 17:41:01.746: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:41:01.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3206" for this suite.

â€¢ [SLOW TEST:6.165 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":305,"completed":185,"skipped":3049,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:41:01.754: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8459
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 19 17:41:01.897: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6bcc9076-9b22-4779-a2b5-0ba89681cdee" in namespace "downward-api-8459" to be "Succeeded or Failed"
Nov 19 17:41:01.900: INFO: Pod "downwardapi-volume-6bcc9076-9b22-4779-a2b5-0ba89681cdee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.979544ms
Nov 19 17:41:03.905: INFO: Pod "downwardapi-volume-6bcc9076-9b22-4779-a2b5-0ba89681cdee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007421846s
STEP: Saw pod success
Nov 19 17:41:03.905: INFO: Pod "downwardapi-volume-6bcc9076-9b22-4779-a2b5-0ba89681cdee" satisfied condition "Succeeded or Failed"
Nov 19 17:41:03.909: INFO: Trying to get logs from node ip-172-31-20-145 pod downwardapi-volume-6bcc9076-9b22-4779-a2b5-0ba89681cdee container client-container: <nil>
STEP: delete the pod
Nov 19 17:41:03.925: INFO: Waiting for pod downwardapi-volume-6bcc9076-9b22-4779-a2b5-0ba89681cdee to disappear
Nov 19 17:41:03.927: INFO: Pod downwardapi-volume-6bcc9076-9b22-4779-a2b5-0ba89681cdee no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:41:03.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8459" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":186,"skipped":3094,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:41:03.935: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6876
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov 19 17:41:04.099: INFO: Number of nodes with available pods: 0
Nov 19 17:41:04.099: INFO: Node ip-172-31-20-145 is running more than one daemon pod
Nov 19 17:41:05.104: INFO: Number of nodes with available pods: 0
Nov 19 17:41:05.104: INFO: Node ip-172-31-20-145 is running more than one daemon pod
Nov 19 17:41:06.107: INFO: Number of nodes with available pods: 3
Nov 19 17:41:06.107: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Nov 19 17:41:06.137: INFO: Number of nodes with available pods: 2
Nov 19 17:41:06.137: INFO: Node ip-172-31-9-169 is running more than one daemon pod
Nov 19 17:41:07.142: INFO: Number of nodes with available pods: 3
Nov 19 17:41:07.142: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6876, will wait for the garbage collector to delete the pods
Nov 19 17:41:07.208: INFO: Deleting DaemonSet.extensions daemon-set took: 6.639221ms
Nov 19 17:41:07.308: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.143477ms
Nov 19 17:41:11.212: INFO: Number of nodes with available pods: 0
Nov 19 17:41:11.212: INFO: Number of running nodes: 0, number of available pods: 0
Nov 19 17:41:11.214: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6876/daemonsets","resourceVersion":"22243"},"items":null}

Nov 19 17:41:11.216: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6876/pods","resourceVersion":"22243"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:41:11.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6876" for this suite.

â€¢ [SLOW TEST:7.298 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":305,"completed":187,"skipped":3120,"failed":0}
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:41:11.233: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2867
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Nov 19 17:41:11.363: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:41:14.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2867" for this suite.
â€¢{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":305,"completed":188,"skipped":3122,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:41:14.524: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-354
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on tmpfs
Nov 19 17:41:14.667: INFO: Waiting up to 5m0s for pod "pod-0eaa09b3-ac80-4839-86b3-868db19dee21" in namespace "emptydir-354" to be "Succeeded or Failed"
Nov 19 17:41:14.673: INFO: Pod "pod-0eaa09b3-ac80-4839-86b3-868db19dee21": Phase="Pending", Reason="", readiness=false. Elapsed: 5.379528ms
Nov 19 17:41:16.676: INFO: Pod "pod-0eaa09b3-ac80-4839-86b3-868db19dee21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008233508s
STEP: Saw pod success
Nov 19 17:41:16.676: INFO: Pod "pod-0eaa09b3-ac80-4839-86b3-868db19dee21" satisfied condition "Succeeded or Failed"
Nov 19 17:41:16.678: INFO: Trying to get logs from node ip-172-31-20-145 pod pod-0eaa09b3-ac80-4839-86b3-868db19dee21 container test-container: <nil>
STEP: delete the pod
Nov 19 17:41:16.697: INFO: Waiting for pod pod-0eaa09b3-ac80-4839-86b3-868db19dee21 to disappear
Nov 19 17:41:16.700: INFO: Pod pod-0eaa09b3-ac80-4839-86b3-868db19dee21 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:41:16.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-354" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":189,"skipped":3129,"failed":0}
SSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:41:16.709: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-1314
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Nov 19 17:41:16.844: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 19 17:42:16.856: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 17:42:16.859: INFO: Starting informer...
STEP: Starting pods...
Nov 19 17:42:17.079: INFO: Pod1 is running on ip-172-31-20-145. Tainting Node
Nov 19 17:42:49.300: INFO: Pod2 is running on ip-172-31-20-145. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Nov 19 17:42:55.618: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Nov 19 17:43:15.636: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:43:15.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-1314" for this suite.

â€¢ [SLOW TEST:118.955 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":305,"completed":190,"skipped":3136,"failed":0}
SSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:43:15.665: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-8327
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pod templates
Nov 19 17:43:15.811: INFO: created test-podtemplate-1
Nov 19 17:43:15.815: INFO: created test-podtemplate-2
Nov 19 17:43:15.818: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Nov 19 17:43:15.821: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Nov 19 17:43:15.838: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:43:15.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-8327" for this suite.
â€¢{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":305,"completed":191,"skipped":3144,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:43:15.850: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2372
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Nov 19 17:43:15.993: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2372 /api/v1/namespaces/watch-2372/configmaps/e2e-watch-test-watch-closed e7da213f-0f69-4ee8-93ca-c9ca18a8a27e 22804 0 2020-11-19 17:43:15 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-11-19 17:43:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 19 17:43:15.993: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2372 /api/v1/namespaces/watch-2372/configmaps/e2e-watch-test-watch-closed e7da213f-0f69-4ee8-93ca-c9ca18a8a27e 22805 0 2020-11-19 17:43:15 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-11-19 17:43:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Nov 19 17:43:16.008: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2372 /api/v1/namespaces/watch-2372/configmaps/e2e-watch-test-watch-closed e7da213f-0f69-4ee8-93ca-c9ca18a8a27e 22806 0 2020-11-19 17:43:15 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-11-19 17:43:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 19 17:43:16.008: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2372 /api/v1/namespaces/watch-2372/configmaps/e2e-watch-test-watch-closed e7da213f-0f69-4ee8-93ca-c9ca18a8a27e 22807 0 2020-11-19 17:43:15 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-11-19 17:43:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:43:16.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2372" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":305,"completed":192,"skipped":3147,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:43:16.016: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-4908
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 17:43:16.159: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-ab4599c3-6113-411e-aff4-5a8279f0153d" in namespace "security-context-test-4908" to be "Succeeded or Failed"
Nov 19 17:43:16.164: INFO: Pod "alpine-nnp-false-ab4599c3-6113-411e-aff4-5a8279f0153d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.98084ms
Nov 19 17:43:18.168: INFO: Pod "alpine-nnp-false-ab4599c3-6113-411e-aff4-5a8279f0153d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008924358s
Nov 19 17:43:20.171: INFO: Pod "alpine-nnp-false-ab4599c3-6113-411e-aff4-5a8279f0153d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012035452s
Nov 19 17:43:20.171: INFO: Pod "alpine-nnp-false-ab4599c3-6113-411e-aff4-5a8279f0153d" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:43:20.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4908" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":193,"skipped":3194,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:43:20.192: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-204
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-204
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Nov 19 17:43:20.343: INFO: Found 0 stateful pods, waiting for 3
Nov 19 17:43:30.348: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 19 17:43:30.348: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 19 17:43:30.348: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Nov 19 17:43:30.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-204 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 19 17:43:30.744: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 19 17:43:30.744: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 19 17:43:30.744: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Nov 19 17:43:40.781: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Nov 19 17:43:50.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-204 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:43:50.937: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 19 17:43:50.937: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 19 17:43:50.937: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 19 17:44:00.953: INFO: Waiting for StatefulSet statefulset-204/ss2 to complete update
Nov 19 17:44:00.953: INFO: Waiting for Pod statefulset-204/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 19 17:44:00.953: INFO: Waiting for Pod statefulset-204/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 19 17:44:10.960: INFO: Waiting for StatefulSet statefulset-204/ss2 to complete update
Nov 19 17:44:10.960: INFO: Waiting for Pod statefulset-204/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 19 17:44:20.960: INFO: Waiting for StatefulSet statefulset-204/ss2 to complete update
STEP: Rolling back to a previous revision
Nov 19 17:44:30.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-204 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 19 17:44:31.110: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 19 17:44:31.110: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 19 17:44:31.110: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 19 17:44:41.142: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Nov 19 17:44:51.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-204 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:44:51.307: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 19 17:44:51.307: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 19 17:44:51.307: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 19 17:45:11.330: INFO: Waiting for StatefulSet statefulset-204/ss2 to complete update
Nov 19 17:45:11.330: INFO: Waiting for Pod statefulset-204/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Nov 19 17:45:21.343: INFO: Deleting all statefulset in ns statefulset-204
Nov 19 17:45:21.345: INFO: Scaling statefulset ss2 to 0
Nov 19 17:45:41.393: INFO: Waiting for statefulset status.replicas updated to 0
Nov 19 17:45:41.396: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:45:41.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-204" for this suite.

â€¢ [SLOW TEST:141.237 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":305,"completed":194,"skipped":3200,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:45:41.430: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1844
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-1844
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating stateful set ss in namespace statefulset-1844
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1844
Nov 19 17:45:41.592: INFO: Found 0 stateful pods, waiting for 1
Nov 19 17:45:51.595: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Nov 19 17:45:51.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 19 17:45:51.867: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 19 17:45:51.867: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 19 17:45:51.867: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 19 17:45:51.871: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov 19 17:46:01.876: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 19 17:46:01.876: INFO: Waiting for statefulset status.replicas updated to 0
Nov 19 17:46:01.890: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Nov 19 17:46:01.890: INFO: ss-0  ip-172-31-20-145  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:45:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:45:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:45:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:45:41 +0000 UTC  }]
Nov 19 17:46:01.890: INFO: 
Nov 19 17:46:01.890: INFO: StatefulSet ss has not reached scale 3, at 1
Nov 19 17:46:02.895: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997539075s
Nov 19 17:46:03.898: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993457185s
Nov 19 17:46:04.902: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.990454869s
Nov 19 17:46:05.906: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.986297193s
Nov 19 17:46:06.910: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.981694457s
Nov 19 17:46:07.914: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.977765048s
Nov 19 17:46:08.918: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.973910509s
Nov 19 17:46:09.922: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.970137792s
Nov 19 17:46:10.927: INFO: Verifying statefulset ss doesn't scale past 3 for another 966.364644ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1844
Nov 19 17:46:11.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:46:12.078: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 19 17:46:12.078: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 19 17:46:12.078: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 19 17:46:12.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:46:12.215: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Nov 19 17:46:12.215: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 19 17:46:12.215: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 19 17:46:12.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:46:12.361: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Nov 19 17:46:12.361: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 19 17:46:12.361: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 19 17:46:12.364: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Nov 19 17:46:22.368: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 19 17:46:22.368: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 19 17:46:22.368: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Nov 19 17:46:22.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 19 17:46:22.512: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 19 17:46:22.512: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 19 17:46:22.512: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 19 17:46:22.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 19 17:46:22.656: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 19 17:46:22.656: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 19 17:46:22.656: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 19 17:46:22.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 19 17:46:22.818: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 19 17:46:22.818: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 19 17:46:22.818: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 19 17:46:22.818: INFO: Waiting for statefulset status.replicas updated to 0
Nov 19 17:46:22.821: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Nov 19 17:46:32.828: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 19 17:46:32.828: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov 19 17:46:32.828: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov 19 17:46:32.838: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Nov 19 17:46:32.838: INFO: ss-0  ip-172-31-20-145  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:45:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:45:41 +0000 UTC  }]
Nov 19 17:46:32.838: INFO: ss-1  ip-172-31-68-240  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  }]
Nov 19 17:46:32.838: INFO: ss-2  ip-172-31-9-169   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  }]
Nov 19 17:46:32.838: INFO: 
Nov 19 17:46:32.838: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 19 17:46:33.843: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Nov 19 17:46:33.843: INFO: ss-0  ip-172-31-20-145  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:45:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:45:41 +0000 UTC  }]
Nov 19 17:46:33.843: INFO: ss-1  ip-172-31-68-240  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  }]
Nov 19 17:46:33.843: INFO: ss-2  ip-172-31-9-169   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  }]
Nov 19 17:46:33.843: INFO: 
Nov 19 17:46:33.843: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 19 17:46:34.847: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Nov 19 17:46:34.847: INFO: ss-0  ip-172-31-20-145  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:45:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:45:41 +0000 UTC  }]
Nov 19 17:46:34.848: INFO: ss-1  ip-172-31-68-240  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  }]
Nov 19 17:46:34.848: INFO: ss-2  ip-172-31-9-169   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  }]
Nov 19 17:46:34.848: INFO: 
Nov 19 17:46:34.848: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 19 17:46:35.852: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Nov 19 17:46:35.852: INFO: ss-0  ip-172-31-20-145  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:45:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:45:41 +0000 UTC  }]
Nov 19 17:46:35.852: INFO: ss-1  ip-172-31-68-240  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  }]
Nov 19 17:46:35.852: INFO: ss-2  ip-172-31-9-169   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  }]
Nov 19 17:46:35.852: INFO: 
Nov 19 17:46:35.852: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 19 17:46:36.857: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Nov 19 17:46:36.857: INFO: ss-0  ip-172-31-20-145  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:45:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:45:41 +0000 UTC  }]
Nov 19 17:46:36.857: INFO: ss-1  ip-172-31-68-240  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  }]
Nov 19 17:46:36.857: INFO: ss-2  ip-172-31-9-169   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  }]
Nov 19 17:46:36.857: INFO: 
Nov 19 17:46:36.857: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 19 17:46:37.862: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Nov 19 17:46:37.862: INFO: ss-1  ip-172-31-68-240  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  }]
Nov 19 17:46:37.862: INFO: ss-2  ip-172-31-9-169   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  }]
Nov 19 17:46:37.862: INFO: 
Nov 19 17:46:37.862: INFO: StatefulSet ss has not reached scale 0, at 2
Nov 19 17:46:38.865: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Nov 19 17:46:38.865: INFO: ss-1  ip-172-31-68-240  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  }]
Nov 19 17:46:38.865: INFO: ss-2  ip-172-31-9-169   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  }]
Nov 19 17:46:38.865: INFO: 
Nov 19 17:46:38.865: INFO: StatefulSet ss has not reached scale 0, at 2
Nov 19 17:46:39.870: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Nov 19 17:46:39.870: INFO: ss-1  ip-172-31-68-240  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  }]
Nov 19 17:46:39.870: INFO: ss-2  ip-172-31-9-169   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  }]
Nov 19 17:46:39.870: INFO: 
Nov 19 17:46:39.870: INFO: StatefulSet ss has not reached scale 0, at 2
Nov 19 17:46:40.873: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Nov 19 17:46:40.873: INFO: ss-1  ip-172-31-68-240  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  }]
Nov 19 17:46:40.873: INFO: ss-2  ip-172-31-9-169   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  }]
Nov 19 17:46:40.873: INFO: 
Nov 19 17:46:40.873: INFO: StatefulSet ss has not reached scale 0, at 2
Nov 19 17:46:41.878: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Nov 19 17:46:41.878: INFO: ss-1  ip-172-31-68-240  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  }]
Nov 19 17:46:41.878: INFO: ss-2  ip-172-31-9-169   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-19 17:46:01 +0000 UTC  }]
Nov 19 17:46:41.878: INFO: 
Nov 19 17:46:41.878: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1844
Nov 19 17:46:42.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:46:42.961: INFO: rc: 1
Nov 19 17:46:42.961: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 19 17:46:52.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:46:53.022: INFO: rc: 1
Nov 19 17:46:53.022: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 19 17:47:03.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:47:03.082: INFO: rc: 1
Nov 19 17:47:03.082: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 19 17:47:13.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:47:13.140: INFO: rc: 1
Nov 19 17:47:13.140: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 19 17:47:23.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:47:23.210: INFO: rc: 1
Nov 19 17:47:23.210: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 19 17:47:33.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:47:33.270: INFO: rc: 1
Nov 19 17:47:33.270: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 19 17:47:43.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:47:43.342: INFO: rc: 1
Nov 19 17:47:43.342: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 19 17:47:53.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:47:53.401: INFO: rc: 1
Nov 19 17:47:53.401: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 19 17:48:03.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:48:03.462: INFO: rc: 1
Nov 19 17:48:03.462: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 19 17:48:13.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:48:13.521: INFO: rc: 1
Nov 19 17:48:13.521: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 19 17:48:23.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:48:23.579: INFO: rc: 1
Nov 19 17:48:23.579: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 19 17:48:33.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:48:33.638: INFO: rc: 1
Nov 19 17:48:33.638: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 19 17:48:43.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:48:43.698: INFO: rc: 1
Nov 19 17:48:43.698: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 19 17:48:53.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:48:53.758: INFO: rc: 1
Nov 19 17:48:53.758: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 19 17:49:03.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:49:03.817: INFO: rc: 1
Nov 19 17:49:03.817: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 19 17:49:13.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:49:13.876: INFO: rc: 1
Nov 19 17:49:13.876: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 19 17:49:23.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:49:23.942: INFO: rc: 1
Nov 19 17:49:23.942: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 19 17:49:33.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:49:34.001: INFO: rc: 1
Nov 19 17:49:34.001: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 19 17:49:44.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:49:44.059: INFO: rc: 1
Nov 19 17:49:44.059: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 19 17:49:54.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:49:54.132: INFO: rc: 1
Nov 19 17:49:54.133: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 19 17:50:04.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:50:04.194: INFO: rc: 1
Nov 19 17:50:04.194: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 19 17:50:14.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:50:14.255: INFO: rc: 1
Nov 19 17:50:14.255: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 19 17:50:24.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:50:24.314: INFO: rc: 1
Nov 19 17:50:24.314: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 19 17:50:34.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:50:34.372: INFO: rc: 1
Nov 19 17:50:34.372: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 19 17:50:44.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:50:44.431: INFO: rc: 1
Nov 19 17:50:44.432: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 19 17:50:54.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:50:54.492: INFO: rc: 1
Nov 19 17:50:54.492: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 19 17:51:04.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:51:04.553: INFO: rc: 1
Nov 19 17:51:04.553: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 19 17:51:14.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:51:14.617: INFO: rc: 1
Nov 19 17:51:14.617: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 19 17:51:24.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:51:24.678: INFO: rc: 1
Nov 19 17:51:24.678: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 19 17:51:34.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:51:34.738: INFO: rc: 1
Nov 19 17:51:34.738: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Nov 19 17:51:44.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=statefulset-1844 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 19 17:51:44.797: INFO: rc: 1
Nov 19 17:51:44.798: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: 
Nov 19 17:51:44.798: INFO: Scaling statefulset ss to 0
Nov 19 17:51:44.808: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Nov 19 17:51:44.811: INFO: Deleting all statefulset in ns statefulset-1844
Nov 19 17:51:44.813: INFO: Scaling statefulset ss to 0
Nov 19 17:51:44.823: INFO: Waiting for statefulset status.replicas updated to 0
Nov 19 17:51:44.825: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:51:44.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1844" for this suite.

â€¢ [SLOW TEST:363.422 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":305,"completed":195,"skipped":3239,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:51:44.852: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4030
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-5599aa26-fde0-49b5-8592-cb1fcaf1c6a4
STEP: Creating a pod to test consume configMaps
Nov 19 17:51:44.994: INFO: Waiting up to 5m0s for pod "pod-configmaps-d074a2be-6715-40c0-9a46-e2c4fd555c42" in namespace "configmap-4030" to be "Succeeded or Failed"
Nov 19 17:51:44.997: INFO: Pod "pod-configmaps-d074a2be-6715-40c0-9a46-e2c4fd555c42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.380496ms
Nov 19 17:51:47.002: INFO: Pod "pod-configmaps-d074a2be-6715-40c0-9a46-e2c4fd555c42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007092402s
STEP: Saw pod success
Nov 19 17:51:47.002: INFO: Pod "pod-configmaps-d074a2be-6715-40c0-9a46-e2c4fd555c42" satisfied condition "Succeeded or Failed"
Nov 19 17:51:47.005: INFO: Trying to get logs from node ip-172-31-20-145 pod pod-configmaps-d074a2be-6715-40c0-9a46-e2c4fd555c42 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 19 17:51:47.035: INFO: Waiting for pod pod-configmaps-d074a2be-6715-40c0-9a46-e2c4fd555c42 to disappear
Nov 19 17:51:47.037: INFO: Pod pod-configmaps-d074a2be-6715-40c0-9a46-e2c4fd555c42 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:51:47.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4030" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":196,"skipped":3257,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:51:47.045: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2029
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2029.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2029.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2029.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2029.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2029.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2029.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 19 17:51:55.227: INFO: DNS probes using dns-2029/dns-test-2e5f6217-f16a-4fe7-a22c-129815cb21f5 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:51:55.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2029" for this suite.

â€¢ [SLOW TEST:8.201 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":305,"completed":197,"skipped":3285,"failed":0}
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:51:55.245: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1411
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Nov 19 17:51:59.431: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 19 17:51:59.433: INFO: Pod pod-with-prestop-http-hook still exists
Nov 19 17:52:01.433: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 19 17:52:01.439: INFO: Pod pod-with-prestop-http-hook still exists
Nov 19 17:52:03.433: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 19 17:52:03.439: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:52:03.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1411" for this suite.

â€¢ [SLOW TEST:8.210 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":305,"completed":198,"skipped":3287,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:52:03.455: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4662
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Nov 19 17:52:03.593: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
Nov 19 17:52:05.302: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:52:14.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4662" for this suite.

â€¢ [SLOW TEST:11.403 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":305,"completed":199,"skipped":3287,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:52:14.859: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5527
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-5527/configmap-test-3c154271-f489-4562-9b40-0d6a7369dcc3
STEP: Creating a pod to test consume configMaps
Nov 19 17:52:15.005: INFO: Waiting up to 5m0s for pod "pod-configmaps-a7e1485b-3d03-4397-9c6c-4252bacf6cac" in namespace "configmap-5527" to be "Succeeded or Failed"
Nov 19 17:52:15.009: INFO: Pod "pod-configmaps-a7e1485b-3d03-4397-9c6c-4252bacf6cac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.465807ms
Nov 19 17:52:17.013: INFO: Pod "pod-configmaps-a7e1485b-3d03-4397-9c6c-4252bacf6cac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008130937s
STEP: Saw pod success
Nov 19 17:52:17.013: INFO: Pod "pod-configmaps-a7e1485b-3d03-4397-9c6c-4252bacf6cac" satisfied condition "Succeeded or Failed"
Nov 19 17:52:17.017: INFO: Trying to get logs from node ip-172-31-20-145 pod pod-configmaps-a7e1485b-3d03-4397-9c6c-4252bacf6cac container env-test: <nil>
STEP: delete the pod
Nov 19 17:52:17.034: INFO: Waiting for pod pod-configmaps-a7e1485b-3d03-4397-9c6c-4252bacf6cac to disappear
Nov 19 17:52:17.036: INFO: Pod pod-configmaps-a7e1485b-3d03-4397-9c6c-4252bacf6cac no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:52:17.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5527" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":305,"completed":200,"skipped":3299,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:52:17.045: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-2003
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Nov 19 17:52:17.551: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Nov 19 17:52:19.561: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741405137, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741405137, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741405137, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741405137, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 19 17:52:22.577: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 17:52:22.579: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:52:23.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2003" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

â€¢ [SLOW TEST:6.718 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":305,"completed":201,"skipped":3313,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:52:23.763: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2509
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-2603
STEP: Creating secret with name secret-test-38f547b7-7abd-42d9-81e6-49c26e973372
STEP: Creating a pod to test consume secrets
Nov 19 17:52:24.061: INFO: Waiting up to 5m0s for pod "pod-secrets-0af83dde-6d0b-43f5-ae74-0d5a0a2a367c" in namespace "secrets-2509" to be "Succeeded or Failed"
Nov 19 17:52:24.064: INFO: Pod "pod-secrets-0af83dde-6d0b-43f5-ae74-0d5a0a2a367c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.663546ms
Nov 19 17:52:26.068: INFO: Pod "pod-secrets-0af83dde-6d0b-43f5-ae74-0d5a0a2a367c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007254065s
STEP: Saw pod success
Nov 19 17:52:26.068: INFO: Pod "pod-secrets-0af83dde-6d0b-43f5-ae74-0d5a0a2a367c" satisfied condition "Succeeded or Failed"
Nov 19 17:52:26.070: INFO: Trying to get logs from node ip-172-31-20-145 pod pod-secrets-0af83dde-6d0b-43f5-ae74-0d5a0a2a367c container secret-volume-test: <nil>
STEP: delete the pod
Nov 19 17:52:26.086: INFO: Waiting for pod pod-secrets-0af83dde-6d0b-43f5-ae74-0d5a0a2a367c to disappear
Nov 19 17:52:26.089: INFO: Pod pod-secrets-0af83dde-6d0b-43f5-ae74-0d5a0a2a367c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:52:26.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2509" for this suite.
STEP: Destroying namespace "secret-namespace-2603" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":305,"completed":202,"skipped":3315,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:52:26.103: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-775
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-2389ab96-b604-4a21-a5bb-7fcb68608609
STEP: Creating a pod to test consume configMaps
Nov 19 17:52:26.261: INFO: Waiting up to 5m0s for pod "pod-configmaps-490b7a16-389c-452f-8f1d-1bd20e7d5f95" in namespace "configmap-775" to be "Succeeded or Failed"
Nov 19 17:52:26.266: INFO: Pod "pod-configmaps-490b7a16-389c-452f-8f1d-1bd20e7d5f95": Phase="Pending", Reason="", readiness=false. Elapsed: 4.39755ms
Nov 19 17:52:28.269: INFO: Pod "pod-configmaps-490b7a16-389c-452f-8f1d-1bd20e7d5f95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007857238s
STEP: Saw pod success
Nov 19 17:52:28.269: INFO: Pod "pod-configmaps-490b7a16-389c-452f-8f1d-1bd20e7d5f95" satisfied condition "Succeeded or Failed"
Nov 19 17:52:28.273: INFO: Trying to get logs from node ip-172-31-20-145 pod pod-configmaps-490b7a16-389c-452f-8f1d-1bd20e7d5f95 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 19 17:52:28.293: INFO: Waiting for pod pod-configmaps-490b7a16-389c-452f-8f1d-1bd20e7d5f95 to disappear
Nov 19 17:52:28.295: INFO: Pod pod-configmaps-490b7a16-389c-452f-8f1d-1bd20e7d5f95 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:52:28.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-775" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":203,"skipped":3324,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:52:28.303: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1883
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-pv42
STEP: Creating a pod to test atomic-volume-subpath
Nov 19 17:52:28.475: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-pv42" in namespace "subpath-1883" to be "Succeeded or Failed"
Nov 19 17:52:28.481: INFO: Pod "pod-subpath-test-configmap-pv42": Phase="Pending", Reason="", readiness=false. Elapsed: 5.649727ms
Nov 19 17:52:30.486: INFO: Pod "pod-subpath-test-configmap-pv42": Phase="Running", Reason="", readiness=true. Elapsed: 2.010541116s
Nov 19 17:52:32.490: INFO: Pod "pod-subpath-test-configmap-pv42": Phase="Running", Reason="", readiness=true. Elapsed: 4.014911209s
Nov 19 17:52:34.495: INFO: Pod "pod-subpath-test-configmap-pv42": Phase="Running", Reason="", readiness=true. Elapsed: 6.019755407s
Nov 19 17:52:36.498: INFO: Pod "pod-subpath-test-configmap-pv42": Phase="Running", Reason="", readiness=true. Elapsed: 8.023137375s
Nov 19 17:52:38.503: INFO: Pod "pod-subpath-test-configmap-pv42": Phase="Running", Reason="", readiness=true. Elapsed: 10.028033103s
Nov 19 17:52:40.506: INFO: Pod "pod-subpath-test-configmap-pv42": Phase="Running", Reason="", readiness=true. Elapsed: 12.031263063s
Nov 19 17:52:42.510: INFO: Pod "pod-subpath-test-configmap-pv42": Phase="Running", Reason="", readiness=true. Elapsed: 14.034916832s
Nov 19 17:52:44.515: INFO: Pod "pod-subpath-test-configmap-pv42": Phase="Running", Reason="", readiness=true. Elapsed: 16.039951086s
Nov 19 17:52:46.519: INFO: Pod "pod-subpath-test-configmap-pv42": Phase="Running", Reason="", readiness=true. Elapsed: 18.043648174s
Nov 19 17:52:48.522: INFO: Pod "pod-subpath-test-configmap-pv42": Phase="Running", Reason="", readiness=true. Elapsed: 20.047034552s
Nov 19 17:52:50.526: INFO: Pod "pod-subpath-test-configmap-pv42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.050510019s
STEP: Saw pod success
Nov 19 17:52:50.526: INFO: Pod "pod-subpath-test-configmap-pv42" satisfied condition "Succeeded or Failed"
Nov 19 17:52:50.529: INFO: Trying to get logs from node ip-172-31-20-145 pod pod-subpath-test-configmap-pv42 container test-container-subpath-configmap-pv42: <nil>
STEP: delete the pod
Nov 19 17:52:50.555: INFO: Waiting for pod pod-subpath-test-configmap-pv42 to disappear
Nov 19 17:52:50.558: INFO: Pod pod-subpath-test-configmap-pv42 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-pv42
Nov 19 17:52:50.558: INFO: Deleting pod "pod-subpath-test-configmap-pv42" in namespace "subpath-1883"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:52:50.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1883" for this suite.

â€¢ [SLOW TEST:22.266 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":305,"completed":204,"skipped":3337,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:52:50.569: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-7077
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5552
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4958
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:53:03.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7077" for this suite.
STEP: Destroying namespace "nsdeletetest-5552" for this suite.
Nov 19 17:53:04.003: INFO: Namespace nsdeletetest-5552 was already deleted
STEP: Destroying namespace "nsdeletetest-4958" for this suite.

â€¢ [SLOW TEST:13.439 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":305,"completed":205,"skipped":3356,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:53:04.008: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4905
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 17:53:04.139: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:53:06.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4905" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":305,"completed":206,"skipped":3372,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:53:06.189: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-102
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-102
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Nov 19 17:53:06.337: INFO: Found 0 stateful pods, waiting for 3
Nov 19 17:53:16.343: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 19 17:53:16.343: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 19 17:53:16.343: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Nov 19 17:53:16.371: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Nov 19 17:53:26.411: INFO: Updating stateful set ss2
Nov 19 17:53:26.416: INFO: Waiting for Pod statefulset-102/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Nov 19 17:53:36.499: INFO: Found 2 stateful pods, waiting for 3
Nov 19 17:53:46.504: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 19 17:53:46.504: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 19 17:53:46.504: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Nov 19 17:53:46.532: INFO: Updating stateful set ss2
Nov 19 17:53:46.541: INFO: Waiting for Pod statefulset-102/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 19 17:53:56.568: INFO: Updating stateful set ss2
Nov 19 17:53:56.575: INFO: Waiting for StatefulSet statefulset-102/ss2 to complete update
Nov 19 17:53:56.575: INFO: Waiting for Pod statefulset-102/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Nov 19 17:54:06.583: INFO: Deleting all statefulset in ns statefulset-102
Nov 19 17:54:06.586: INFO: Scaling statefulset ss2 to 0
Nov 19 17:54:46.604: INFO: Waiting for statefulset status.replicas updated to 0
Nov 19 17:54:46.606: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:54:46.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-102" for this suite.

â€¢ [SLOW TEST:100.448 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":305,"completed":207,"skipped":3384,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:54:46.637: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8012
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-8012
STEP: creating service affinity-nodeport in namespace services-8012
STEP: creating replication controller affinity-nodeport in namespace services-8012
I1119 17:54:46.799004      22 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-8012, replica count: 3
I1119 17:54:49.849344      22 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 19 17:54:49.856: INFO: Creating new exec pod
Nov 19 17:54:52.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-8012 execpod-affinitywvpv5 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Nov 19 17:54:53.202: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Nov 19 17:54:53.202: INFO: stdout: ""
Nov 19 17:54:53.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-8012 execpod-affinitywvpv5 -- /bin/sh -x -c nc -zv -t -w 2 10.152.183.149 80'
Nov 19 17:54:53.320: INFO: stderr: "+ nc -zv -t -w 2 10.152.183.149 80\nConnection to 10.152.183.149 80 port [tcp/http] succeeded!\n"
Nov 19 17:54:53.320: INFO: stdout: ""
Nov 19 17:54:53.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-8012 execpod-affinitywvpv5 -- /bin/sh -x -c nc -zv -t -w 2 172.31.68.240 32555'
Nov 19 17:54:53.427: INFO: stderr: "+ nc -zv -t -w 2 172.31.68.240 32555\nConnection to 172.31.68.240 32555 port [tcp/32555] succeeded!\n"
Nov 19 17:54:53.427: INFO: stdout: ""
Nov 19 17:54:53.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-8012 execpod-affinitywvpv5 -- /bin/sh -x -c nc -zv -t -w 2 172.31.9.169 32555'
Nov 19 17:54:53.541: INFO: stderr: "+ nc -zv -t -w 2 172.31.9.169 32555\nConnection to 172.31.9.169 32555 port [tcp/32555] succeeded!\n"
Nov 19 17:54:53.541: INFO: stdout: ""
Nov 19 17:54:53.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-8012 execpod-affinitywvpv5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.20.145:32555/ ; done'
Nov 19 17:54:53.719: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32555/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32555/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32555/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32555/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32555/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32555/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32555/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32555/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32555/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32555/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32555/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32555/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32555/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32555/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32555/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.20.145:32555/\n"
Nov 19 17:54:53.719: INFO: stdout: "\naffinity-nodeport-mw4nz\naffinity-nodeport-mw4nz\naffinity-nodeport-mw4nz\naffinity-nodeport-mw4nz\naffinity-nodeport-mw4nz\naffinity-nodeport-mw4nz\naffinity-nodeport-mw4nz\naffinity-nodeport-mw4nz\naffinity-nodeport-mw4nz\naffinity-nodeport-mw4nz\naffinity-nodeport-mw4nz\naffinity-nodeport-mw4nz\naffinity-nodeport-mw4nz\naffinity-nodeport-mw4nz\naffinity-nodeport-mw4nz\naffinity-nodeport-mw4nz"
Nov 19 17:54:53.719: INFO: Received response from host: affinity-nodeport-mw4nz
Nov 19 17:54:53.719: INFO: Received response from host: affinity-nodeport-mw4nz
Nov 19 17:54:53.719: INFO: Received response from host: affinity-nodeport-mw4nz
Nov 19 17:54:53.719: INFO: Received response from host: affinity-nodeport-mw4nz
Nov 19 17:54:53.719: INFO: Received response from host: affinity-nodeport-mw4nz
Nov 19 17:54:53.719: INFO: Received response from host: affinity-nodeport-mw4nz
Nov 19 17:54:53.719: INFO: Received response from host: affinity-nodeport-mw4nz
Nov 19 17:54:53.719: INFO: Received response from host: affinity-nodeport-mw4nz
Nov 19 17:54:53.719: INFO: Received response from host: affinity-nodeport-mw4nz
Nov 19 17:54:53.719: INFO: Received response from host: affinity-nodeport-mw4nz
Nov 19 17:54:53.719: INFO: Received response from host: affinity-nodeport-mw4nz
Nov 19 17:54:53.719: INFO: Received response from host: affinity-nodeport-mw4nz
Nov 19 17:54:53.719: INFO: Received response from host: affinity-nodeport-mw4nz
Nov 19 17:54:53.719: INFO: Received response from host: affinity-nodeport-mw4nz
Nov 19 17:54:53.719: INFO: Received response from host: affinity-nodeport-mw4nz
Nov 19 17:54:53.719: INFO: Received response from host: affinity-nodeport-mw4nz
Nov 19 17:54:53.719: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-8012, will wait for the garbage collector to delete the pods
Nov 19 17:54:53.791: INFO: Deleting ReplicationController affinity-nodeport took: 6.882675ms
Nov 19 17:54:53.891: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.138458ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:55:07.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8012" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

â€¢ [SLOW TEST:20.998 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":208,"skipped":3400,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:55:07.636: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-5864
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Nov 19 17:55:07.778: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 19 17:56:07.792: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:56:07.795: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-7260
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:487
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Nov 19 17:56:09.958: INFO: found a healthy node: ip-172-31-20-145
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 17:56:22.024: INFO: pods created so far: [1 1 1]
Nov 19 17:56:22.024: INFO: length of pods created so far: 3
Nov 19 17:56:34.036: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:56:41.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-7260" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:461
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:56:41.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5864" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

â€¢ [SLOW TEST:93.481 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:450
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":305,"completed":209,"skipped":3442,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:56:41.117: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5701
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl replace
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1581
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Nov 19 17:56:41.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-5701'
Nov 19 17:56:41.321: INFO: stderr: ""
Nov 19 17:56:41.321: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Nov 19 17:56:46.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get pod e2e-test-httpd-pod --namespace=kubectl-5701 -o json'
Nov 19 17:56:46.430: INFO: stderr: ""
Nov 19 17:56:46.430: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2020-11-19T17:56:41Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-11-19T17:56:41Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"10.1.33.168\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-11-19T17:56:42Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5701\",\n        \"resourceVersion\": \"26583\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-5701/pods/e2e-test-httpd-pod\",\n        \"uid\": \"c346b0a4-76c1-4d58-b7dc-dba7f4cb5a22\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-v822q\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-20-145\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-v822q\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-v822q\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-19T17:56:41Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-19T17:56:42Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-19T17:56:42Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-19T17:56:41Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://646b0ae69af9c308f9cb0aa68953a6bff9272b68b3bd808eb0782b6ad55f6f4e\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-11-19T17:56:41Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.20.145\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.1.33.168\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.1.33.168\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-11-19T17:56:41Z\"\n    }\n}\n"
STEP: replace the image in the pod
Nov 19 17:56:46.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 replace -f - --namespace=kubectl-5701'
Nov 19 17:56:46.765: INFO: stderr: ""
Nov 19 17:56:46.765: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1586
Nov 19 17:56:46.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 delete pods e2e-test-httpd-pod --namespace=kubectl-5701'
Nov 19 17:56:57.522: INFO: stderr: ""
Nov 19 17:56:57.522: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:56:57.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5701" for this suite.

â€¢ [SLOW TEST:16.418 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1577
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":305,"completed":210,"skipped":3459,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:56:57.535: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-7905
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 17:56:57.676: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:56:58.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7905" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":305,"completed":211,"skipped":3475,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:56:58.276: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-243
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 19 17:56:58.441: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f41abea6-0d25-425e-a092-7334c7e5c0f2" in namespace "downward-api-243" to be "Succeeded or Failed"
Nov 19 17:56:58.451: INFO: Pod "downwardapi-volume-f41abea6-0d25-425e-a092-7334c7e5c0f2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.310664ms
Nov 19 17:57:00.455: INFO: Pod "downwardapi-volume-f41abea6-0d25-425e-a092-7334c7e5c0f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014556074s
STEP: Saw pod success
Nov 19 17:57:00.455: INFO: Pod "downwardapi-volume-f41abea6-0d25-425e-a092-7334c7e5c0f2" satisfied condition "Succeeded or Failed"
Nov 19 17:57:00.459: INFO: Trying to get logs from node ip-172-31-20-145 pod downwardapi-volume-f41abea6-0d25-425e-a092-7334c7e5c0f2 container client-container: <nil>
STEP: delete the pod
Nov 19 17:57:00.493: INFO: Waiting for pod downwardapi-volume-f41abea6-0d25-425e-a092-7334c7e5c0f2 to disappear
Nov 19 17:57:00.496: INFO: Pod downwardapi-volume-f41abea6-0d25-425e-a092-7334c7e5c0f2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:57:00.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-243" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":305,"completed":212,"skipped":3479,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:57:00.507: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4572
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-2d67b998-5ee6-4a3d-935b-ecd207675ce2
STEP: Creating a pod to test consume secrets
Nov 19 17:57:00.665: INFO: Waiting up to 5m0s for pod "pod-secrets-6c0100a1-fe3f-47ce-84e4-cbb8d6f8c22e" in namespace "secrets-4572" to be "Succeeded or Failed"
Nov 19 17:57:00.673: INFO: Pod "pod-secrets-6c0100a1-fe3f-47ce-84e4-cbb8d6f8c22e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.403097ms
Nov 19 17:57:02.677: INFO: Pod "pod-secrets-6c0100a1-fe3f-47ce-84e4-cbb8d6f8c22e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012401054s
STEP: Saw pod success
Nov 19 17:57:02.677: INFO: Pod "pod-secrets-6c0100a1-fe3f-47ce-84e4-cbb8d6f8c22e" satisfied condition "Succeeded or Failed"
Nov 19 17:57:02.679: INFO: Trying to get logs from node ip-172-31-20-145 pod pod-secrets-6c0100a1-fe3f-47ce-84e4-cbb8d6f8c22e container secret-volume-test: <nil>
STEP: delete the pod
Nov 19 17:57:02.696: INFO: Waiting for pod pod-secrets-6c0100a1-fe3f-47ce-84e4-cbb8d6f8c22e to disappear
Nov 19 17:57:02.698: INFO: Pod pod-secrets-6c0100a1-fe3f-47ce-84e4-cbb8d6f8c22e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:57:02.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4572" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":213,"skipped":3504,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:57:02.707: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4057
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4057.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4057.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4057.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4057.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4057.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4057.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4057.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4057.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4057.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4057.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4057.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4057.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4057.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 185.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.185_udp@PTR;check="$$(dig +tcp +noall +answer +search 185.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.185_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4057.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4057.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4057.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4057.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4057.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4057.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4057.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4057.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4057.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4057.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4057.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4057.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4057.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 185.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.185_udp@PTR;check="$$(dig +tcp +noall +answer +search 185.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.185_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 19 17:57:04.905: INFO: Unable to read wheezy_udp@dns-test-service.dns-4057.svc.cluster.local from pod dns-4057/dns-test-4c67e24e-4f18-461a-b58f-f02c7a2e0583: the server could not find the requested resource (get pods dns-test-4c67e24e-4f18-461a-b58f-f02c7a2e0583)
Nov 19 17:57:04.908: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4057.svc.cluster.local from pod dns-4057/dns-test-4c67e24e-4f18-461a-b58f-f02c7a2e0583: the server could not find the requested resource (get pods dns-test-4c67e24e-4f18-461a-b58f-f02c7a2e0583)
Nov 19 17:57:04.913: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4057.svc.cluster.local from pod dns-4057/dns-test-4c67e24e-4f18-461a-b58f-f02c7a2e0583: the server could not find the requested resource (get pods dns-test-4c67e24e-4f18-461a-b58f-f02c7a2e0583)
Nov 19 17:57:04.915: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4057.svc.cluster.local from pod dns-4057/dns-test-4c67e24e-4f18-461a-b58f-f02c7a2e0583: the server could not find the requested resource (get pods dns-test-4c67e24e-4f18-461a-b58f-f02c7a2e0583)
Nov 19 17:57:04.941: INFO: Unable to read jessie_udp@dns-test-service.dns-4057.svc.cluster.local from pod dns-4057/dns-test-4c67e24e-4f18-461a-b58f-f02c7a2e0583: the server could not find the requested resource (get pods dns-test-4c67e24e-4f18-461a-b58f-f02c7a2e0583)
Nov 19 17:57:04.945: INFO: Unable to read jessie_tcp@dns-test-service.dns-4057.svc.cluster.local from pod dns-4057/dns-test-4c67e24e-4f18-461a-b58f-f02c7a2e0583: the server could not find the requested resource (get pods dns-test-4c67e24e-4f18-461a-b58f-f02c7a2e0583)
Nov 19 17:57:04.948: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4057.svc.cluster.local from pod dns-4057/dns-test-4c67e24e-4f18-461a-b58f-f02c7a2e0583: the server could not find the requested resource (get pods dns-test-4c67e24e-4f18-461a-b58f-f02c7a2e0583)
Nov 19 17:57:04.951: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4057.svc.cluster.local from pod dns-4057/dns-test-4c67e24e-4f18-461a-b58f-f02c7a2e0583: the server could not find the requested resource (get pods dns-test-4c67e24e-4f18-461a-b58f-f02c7a2e0583)
Nov 19 17:57:04.971: INFO: Lookups using dns-4057/dns-test-4c67e24e-4f18-461a-b58f-f02c7a2e0583 failed for: [wheezy_udp@dns-test-service.dns-4057.svc.cluster.local wheezy_tcp@dns-test-service.dns-4057.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4057.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4057.svc.cluster.local jessie_udp@dns-test-service.dns-4057.svc.cluster.local jessie_tcp@dns-test-service.dns-4057.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4057.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4057.svc.cluster.local]

Nov 19 17:57:10.048: INFO: DNS probes using dns-4057/dns-test-4c67e24e-4f18-461a-b58f-f02c7a2e0583 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:57:10.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4057" for this suite.

â€¢ [SLOW TEST:7.420 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":305,"completed":214,"skipped":3524,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:57:10.127: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5346
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 19 17:57:10.568: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 19 17:57:13.584: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:57:13.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5346" for this suite.
STEP: Destroying namespace "webhook-5346-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":305,"completed":215,"skipped":3543,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:57:13.708: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9251
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-a7a67fe3-092f-4254-a6fa-330725e31df3
STEP: Creating a pod to test consume configMaps
Nov 19 17:57:13.905: INFO: Waiting up to 5m0s for pod "pod-configmaps-48844886-eb37-4040-ad75-1362bf55137b" in namespace "configmap-9251" to be "Succeeded or Failed"
Nov 19 17:57:13.908: INFO: Pod "pod-configmaps-48844886-eb37-4040-ad75-1362bf55137b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.285418ms
Nov 19 17:57:15.911: INFO: Pod "pod-configmaps-48844886-eb37-4040-ad75-1362bf55137b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005999557s
STEP: Saw pod success
Nov 19 17:57:15.912: INFO: Pod "pod-configmaps-48844886-eb37-4040-ad75-1362bf55137b" satisfied condition "Succeeded or Failed"
Nov 19 17:57:15.914: INFO: Trying to get logs from node ip-172-31-20-145 pod pod-configmaps-48844886-eb37-4040-ad75-1362bf55137b container configmap-volume-test: <nil>
STEP: delete the pod
Nov 19 17:57:15.933: INFO: Waiting for pod pod-configmaps-48844886-eb37-4040-ad75-1362bf55137b to disappear
Nov 19 17:57:15.936: INFO: Pod pod-configmaps-48844886-eb37-4040-ad75-1362bf55137b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:57:15.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9251" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":216,"skipped":3556,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:57:15.944: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1106
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Nov 19 17:57:16.075: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
Nov 19 17:57:17.784: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:57:27.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1106" for this suite.

â€¢ [SLOW TEST:11.413 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":305,"completed":217,"skipped":3565,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:57:27.357: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5892
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 19 17:57:27.720: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 19 17:57:30.737: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 17:57:30.741: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7513-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:57:32.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5892" for this suite.
STEP: Destroying namespace "webhook-5892-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":305,"completed":218,"skipped":3574,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:57:32.206: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-2714
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 17:57:32.339: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Creating first CR 
Nov 19 17:57:32.948: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-19T17:57:32Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-19T17:57:32Z]] name:name1 resourceVersion:27156 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:862ff8c5-3ab0-49c1-a257-b7547ccd4427] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Nov 19 17:57:42.953: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-19T17:57:42Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-19T17:57:42Z]] name:name2 resourceVersion:27201 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:bd61cba0-6f09-447e-9335-2b825f14eb21] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Nov 19 17:57:52.960: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-19T17:57:32Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-19T17:57:52Z]] name:name1 resourceVersion:27229 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:862ff8c5-3ab0-49c1-a257-b7547ccd4427] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Nov 19 17:58:02.967: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-19T17:57:42Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-19T17:58:02Z]] name:name2 resourceVersion:27256 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:bd61cba0-6f09-447e-9335-2b825f14eb21] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Nov 19 17:58:12.976: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-19T17:57:32Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-19T17:57:52Z]] name:name1 resourceVersion:27283 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:862ff8c5-3ab0-49c1-a257-b7547ccd4427] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Nov 19 17:58:22.984: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-19T17:57:42Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-19T17:58:02Z]] name:name2 resourceVersion:27311 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:bd61cba0-6f09-447e-9335-2b825f14eb21] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:58:33.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-2714" for this suite.

â€¢ [SLOW TEST:61.300 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":305,"completed":219,"skipped":3577,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:58:33.507: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename ingressclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingressclass-6128
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Nov 19 17:58:33.666: INFO: starting watch
STEP: patching
STEP: updating
Nov 19 17:58:33.675: INFO: waiting for watch events with expected annotations
Nov 19 17:58:33.675: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:58:33.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-6128" for this suite.
â€¢{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":305,"completed":220,"skipped":3590,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:58:33.713: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6008
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 17:58:33.847: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Nov 19 17:58:36.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 --namespace=crd-publish-openapi-6008 create -f -'
Nov 19 17:58:37.037: INFO: stderr: ""
Nov 19 17:58:37.037: INFO: stdout: "e2e-test-crd-publish-openapi-3395-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Nov 19 17:58:37.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 --namespace=crd-publish-openapi-6008 delete e2e-test-crd-publish-openapi-3395-crds test-cr'
Nov 19 17:58:37.130: INFO: stderr: ""
Nov 19 17:58:37.130: INFO: stdout: "e2e-test-crd-publish-openapi-3395-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Nov 19 17:58:37.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 --namespace=crd-publish-openapi-6008 apply -f -'
Nov 19 17:58:37.407: INFO: stderr: ""
Nov 19 17:58:37.407: INFO: stdout: "e2e-test-crd-publish-openapi-3395-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Nov 19 17:58:37.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 --namespace=crd-publish-openapi-6008 delete e2e-test-crd-publish-openapi-3395-crds test-cr'
Nov 19 17:58:37.478: INFO: stderr: ""
Nov 19 17:58:37.478: INFO: stdout: "e2e-test-crd-publish-openapi-3395-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Nov 19 17:58:37.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 explain e2e-test-crd-publish-openapi-3395-crds'
Nov 19 17:58:37.634: INFO: stderr: ""
Nov 19 17:58:37.634: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3395-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:58:40.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6008" for this suite.

â€¢ [SLOW TEST:6.633 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":305,"completed":221,"skipped":3632,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:58:40.347: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1765
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl logs
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1415
STEP: creating an pod
Nov 19 17:58:40.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.20 --namespace=kubectl-1765 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Nov 19 17:58:40.551: INFO: stderr: ""
Nov 19 17:58:40.551: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Waiting for log generator to start.
Nov 19 17:58:40.551: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Nov 19 17:58:40.551: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-1765" to be "running and ready, or succeeded"
Nov 19 17:58:40.556: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.931797ms
Nov 19 17:58:42.561: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.009893373s
Nov 19 17:58:42.561: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Nov 19 17:58:42.561: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Nov 19 17:58:42.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 logs logs-generator logs-generator --namespace=kubectl-1765'
Nov 19 17:58:42.633: INFO: stderr: ""
Nov 19 17:58:42.633: INFO: stdout: "I1119 17:58:41.132498       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/ffc 534\nI1119 17:58:41.332547       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/pl4k 265\nI1119 17:58:41.532547       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/c4cm 596\nI1119 17:58:41.732563       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/n89 221\nI1119 17:58:41.932547       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/88p 430\nI1119 17:58:42.132534       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/xqf 418\nI1119 17:58:42.332551       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/gmz 465\nI1119 17:58:42.532560       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/8rl9 344\n"
STEP: limiting log lines
Nov 19 17:58:42.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 logs logs-generator logs-generator --namespace=kubectl-1765 --tail=1'
Nov 19 17:58:42.698: INFO: stderr: ""
Nov 19 17:58:42.698: INFO: stdout: "I1119 17:58:42.532560       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/8rl9 344\n"
Nov 19 17:58:42.698: INFO: got output "I1119 17:58:42.532560       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/8rl9 344\n"
STEP: limiting log bytes
Nov 19 17:58:42.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 logs logs-generator logs-generator --namespace=kubectl-1765 --limit-bytes=1'
Nov 19 17:58:42.763: INFO: stderr: ""
Nov 19 17:58:42.763: INFO: stdout: "I"
Nov 19 17:58:42.763: INFO: got output "I"
STEP: exposing timestamps
Nov 19 17:58:42.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 logs logs-generator logs-generator --namespace=kubectl-1765 --tail=1 --timestamps'
Nov 19 17:58:42.829: INFO: stderr: ""
Nov 19 17:58:42.829: INFO: stdout: "2020-11-19T17:58:42.732629747Z I1119 17:58:42.732545       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/ztb2 484\n"
Nov 19 17:58:42.829: INFO: got output "2020-11-19T17:58:42.732629747Z I1119 17:58:42.732545       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/ztb2 484\n"
STEP: restricting to a time range
Nov 19 17:58:45.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 logs logs-generator logs-generator --namespace=kubectl-1765 --since=1s'
Nov 19 17:58:45.402: INFO: stderr: ""
Nov 19 17:58:45.402: INFO: stdout: "I1119 17:58:44.532543       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/f6cv 569\nI1119 17:58:44.732540       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/4q9 473\nI1119 17:58:44.932540       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/7x75 245\nI1119 17:58:45.132546       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/hpg 463\nI1119 17:58:45.332537       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/d5zn 395\n"
Nov 19 17:58:45.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 logs logs-generator logs-generator --namespace=kubectl-1765 --since=24h'
Nov 19 17:58:45.469: INFO: stderr: ""
Nov 19 17:58:45.469: INFO: stdout: "I1119 17:58:41.132498       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/ffc 534\nI1119 17:58:41.332547       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/pl4k 265\nI1119 17:58:41.532547       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/c4cm 596\nI1119 17:58:41.732563       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/n89 221\nI1119 17:58:41.932547       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/88p 430\nI1119 17:58:42.132534       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/xqf 418\nI1119 17:58:42.332551       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/gmz 465\nI1119 17:58:42.532560       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/8rl9 344\nI1119 17:58:42.732545       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/ztb2 484\nI1119 17:58:42.932544       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/kmmf 506\nI1119 17:58:43.132549       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/skq 392\nI1119 17:58:43.332530       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/zqsc 553\nI1119 17:58:43.532549       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/jm8n 397\nI1119 17:58:43.732540       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/g6b 398\nI1119 17:58:43.932547       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/7m75 385\nI1119 17:58:44.132544       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/2sm 508\nI1119 17:58:44.332530       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/4fr 474\nI1119 17:58:44.532543       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/f6cv 569\nI1119 17:58:44.732540       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/4q9 473\nI1119 17:58:44.932540       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/7x75 245\nI1119 17:58:45.132546       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/hpg 463\nI1119 17:58:45.332537       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/d5zn 395\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
Nov 19 17:58:45.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 delete pod logs-generator --namespace=kubectl-1765'
Nov 19 17:58:57.647: INFO: stderr: ""
Nov 19 17:58:57.647: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:58:57.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1765" for this suite.

â€¢ [SLOW TEST:17.308 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1411
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":305,"completed":222,"skipped":3647,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:58:57.656: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-414
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-414
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-414
STEP: Creating statefulset with conflicting port in namespace statefulset-414
STEP: Waiting until pod test-pod will start running in namespace statefulset-414
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-414
Nov 19 17:59:01.843: INFO: Observed stateful pod in namespace: statefulset-414, name: ss-0, uid: 8ad89c04-89b7-4a21-8895-1205512cf757, status phase: Pending. Waiting for statefulset controller to delete.
Nov 19 17:59:02.310: INFO: Observed stateful pod in namespace: statefulset-414, name: ss-0, uid: 8ad89c04-89b7-4a21-8895-1205512cf757, status phase: Failed. Waiting for statefulset controller to delete.
Nov 19 17:59:02.317: INFO: Observed stateful pod in namespace: statefulset-414, name: ss-0, uid: 8ad89c04-89b7-4a21-8895-1205512cf757, status phase: Failed. Waiting for statefulset controller to delete.
Nov 19 17:59:02.320: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-414
STEP: Removing pod with conflicting port in namespace statefulset-414
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-414 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Nov 19 17:59:06.345: INFO: Deleting all statefulset in ns statefulset-414
Nov 19 17:59:06.348: INFO: Scaling statefulset ss to 0
Nov 19 17:59:26.365: INFO: Waiting for statefulset status.replicas updated to 0
Nov 19 17:59:26.368: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:59:26.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-414" for this suite.

â€¢ [SLOW TEST:28.737 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":305,"completed":223,"skipped":3671,"failed":0}
S
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:59:26.394: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4913
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 17:59:28.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4913" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":224,"skipped":3672,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 17:59:28.563: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8110
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 17:59:28.712: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Nov 19 17:59:28.723: INFO: Number of nodes with available pods: 0
Nov 19 17:59:28.723: INFO: Node ip-172-31-20-145 is running more than one daemon pod
Nov 19 17:59:29.731: INFO: Number of nodes with available pods: 1
Nov 19 17:59:29.731: INFO: Node ip-172-31-68-240 is running more than one daemon pod
Nov 19 17:59:30.730: INFO: Number of nodes with available pods: 3
Nov 19 17:59:30.730: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Nov 19 17:59:30.756: INFO: Wrong image for pod: daemon-set-4jg8c. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:30.756: INFO: Wrong image for pod: daemon-set-cn447. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:30.756: INFO: Wrong image for pod: daemon-set-nr99v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:31.763: INFO: Wrong image for pod: daemon-set-4jg8c. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:31.763: INFO: Wrong image for pod: daemon-set-cn447. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:31.763: INFO: Wrong image for pod: daemon-set-nr99v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:32.763: INFO: Wrong image for pod: daemon-set-4jg8c. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:32.763: INFO: Pod daemon-set-4jg8c is not available
Nov 19 17:59:32.763: INFO: Wrong image for pod: daemon-set-cn447. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:32.763: INFO: Wrong image for pod: daemon-set-nr99v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:33.762: INFO: Wrong image for pod: daemon-set-4jg8c. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:33.762: INFO: Pod daemon-set-4jg8c is not available
Nov 19 17:59:33.762: INFO: Wrong image for pod: daemon-set-cn447. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:33.762: INFO: Wrong image for pod: daemon-set-nr99v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:34.765: INFO: Wrong image for pod: daemon-set-4jg8c. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:34.765: INFO: Pod daemon-set-4jg8c is not available
Nov 19 17:59:34.765: INFO: Wrong image for pod: daemon-set-cn447. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:34.765: INFO: Wrong image for pod: daemon-set-nr99v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:35.762: INFO: Wrong image for pod: daemon-set-4jg8c. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:35.762: INFO: Pod daemon-set-4jg8c is not available
Nov 19 17:59:35.762: INFO: Wrong image for pod: daemon-set-cn447. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:35.762: INFO: Wrong image for pod: daemon-set-nr99v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:36.764: INFO: Wrong image for pod: daemon-set-4jg8c. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:36.764: INFO: Pod daemon-set-4jg8c is not available
Nov 19 17:59:36.764: INFO: Wrong image for pod: daemon-set-cn447. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:36.764: INFO: Wrong image for pod: daemon-set-nr99v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:37.764: INFO: Wrong image for pod: daemon-set-cn447. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:37.764: INFO: Wrong image for pod: daemon-set-nr99v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:37.764: INFO: Pod daemon-set-rchll is not available
Nov 19 17:59:38.763: INFO: Wrong image for pod: daemon-set-cn447. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:38.763: INFO: Wrong image for pod: daemon-set-nr99v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:39.764: INFO: Wrong image for pod: daemon-set-cn447. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:39.764: INFO: Pod daemon-set-cn447 is not available
Nov 19 17:59:39.764: INFO: Wrong image for pod: daemon-set-nr99v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:40.763: INFO: Wrong image for pod: daemon-set-cn447. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:40.763: INFO: Pod daemon-set-cn447 is not available
Nov 19 17:59:40.763: INFO: Wrong image for pod: daemon-set-nr99v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:41.764: INFO: Wrong image for pod: daemon-set-cn447. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:41.764: INFO: Pod daemon-set-cn447 is not available
Nov 19 17:59:41.764: INFO: Wrong image for pod: daemon-set-nr99v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:42.762: INFO: Wrong image for pod: daemon-set-cn447. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:42.762: INFO: Pod daemon-set-cn447 is not available
Nov 19 17:59:42.762: INFO: Wrong image for pod: daemon-set-nr99v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:43.763: INFO: Wrong image for pod: daemon-set-cn447. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:43.763: INFO: Pod daemon-set-cn447 is not available
Nov 19 17:59:43.763: INFO: Wrong image for pod: daemon-set-nr99v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:44.765: INFO: Wrong image for pod: daemon-set-cn447. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:44.765: INFO: Pod daemon-set-cn447 is not available
Nov 19 17:59:44.765: INFO: Wrong image for pod: daemon-set-nr99v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:45.763: INFO: Wrong image for pod: daemon-set-nr99v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:45.763: INFO: Pod daemon-set-wh9ch is not available
Nov 19 17:59:46.763: INFO: Wrong image for pod: daemon-set-nr99v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:47.763: INFO: Wrong image for pod: daemon-set-nr99v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:47.763: INFO: Pod daemon-set-nr99v is not available
Nov 19 17:59:48.764: INFO: Wrong image for pod: daemon-set-nr99v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:48.764: INFO: Pod daemon-set-nr99v is not available
Nov 19 17:59:49.763: INFO: Wrong image for pod: daemon-set-nr99v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:49.763: INFO: Pod daemon-set-nr99v is not available
Nov 19 17:59:50.763: INFO: Wrong image for pod: daemon-set-nr99v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:50.763: INFO: Pod daemon-set-nr99v is not available
Nov 19 17:59:51.763: INFO: Wrong image for pod: daemon-set-nr99v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:51.763: INFO: Pod daemon-set-nr99v is not available
Nov 19 17:59:52.763: INFO: Wrong image for pod: daemon-set-nr99v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:52.763: INFO: Pod daemon-set-nr99v is not available
Nov 19 17:59:53.763: INFO: Wrong image for pod: daemon-set-nr99v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:53.763: INFO: Pod daemon-set-nr99v is not available
Nov 19 17:59:54.763: INFO: Wrong image for pod: daemon-set-nr99v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 19 17:59:54.763: INFO: Pod daemon-set-nr99v is not available
Nov 19 17:59:55.763: INFO: Pod daemon-set-c499h is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Nov 19 17:59:55.771: INFO: Number of nodes with available pods: 2
Nov 19 17:59:55.771: INFO: Node ip-172-31-9-169 is running more than one daemon pod
Nov 19 17:59:56.779: INFO: Number of nodes with available pods: 3
Nov 19 17:59:56.779: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8110, will wait for the garbage collector to delete the pods
Nov 19 17:59:56.856: INFO: Deleting DaemonSet.extensions daemon-set took: 8.157731ms
Nov 19 17:59:56.956: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.146278ms
Nov 19 18:00:07.759: INFO: Number of nodes with available pods: 0
Nov 19 18:00:07.759: INFO: Number of running nodes: 0, number of available pods: 0
Nov 19 18:00:07.761: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8110/daemonsets","resourceVersion":"27974"},"items":null}

Nov 19 18:00:07.764: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8110/pods","resourceVersion":"27974"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:00:07.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8110" for this suite.

â€¢ [SLOW TEST:39.219 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":305,"completed":225,"skipped":3701,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:00:07.783: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4399
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-608b50db-7706-4f7b-aadd-1f9ebac1f049 in namespace container-probe-4399
Nov 19 18:00:09.936: INFO: Started pod liveness-608b50db-7706-4f7b-aadd-1f9ebac1f049 in namespace container-probe-4399
STEP: checking the pod's current state and verifying that restartCount is present
Nov 19 18:00:09.938: INFO: Initial restart count of pod liveness-608b50db-7706-4f7b-aadd-1f9ebac1f049 is 0
Nov 19 18:00:29.978: INFO: Restart count of pod container-probe-4399/liveness-608b50db-7706-4f7b-aadd-1f9ebac1f049 is now 1 (20.03961998s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:00:29.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4399" for this suite.

â€¢ [SLOW TEST:22.213 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":226,"skipped":3719,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:00:29.996: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5296
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with configMap that has name projected-configmap-test-upd-4ea0e06b-d40c-4577-adab-82d0bf1271c2
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-4ea0e06b-d40c-4577-adab-82d0bf1271c2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:00:34.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5296" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":227,"skipped":3732,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:00:34.181: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5950
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Nov 19 18:00:34.313: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:00:36.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5950" for this suite.
â€¢{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":305,"completed":228,"skipped":3742,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:00:36.578: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9445
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 19 18:00:36.721: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aaf19cea-89e0-4382-b538-bb4092bd427c" in namespace "downward-api-9445" to be "Succeeded or Failed"
Nov 19 18:00:36.725: INFO: Pod "downwardapi-volume-aaf19cea-89e0-4382-b538-bb4092bd427c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.136216ms
Nov 19 18:00:38.728: INFO: Pod "downwardapi-volume-aaf19cea-89e0-4382-b538-bb4092bd427c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00791289s
STEP: Saw pod success
Nov 19 18:00:38.728: INFO: Pod "downwardapi-volume-aaf19cea-89e0-4382-b538-bb4092bd427c" satisfied condition "Succeeded or Failed"
Nov 19 18:00:38.732: INFO: Trying to get logs from node ip-172-31-20-145 pod downwardapi-volume-aaf19cea-89e0-4382-b538-bb4092bd427c container client-container: <nil>
STEP: delete the pod
Nov 19 18:00:38.754: INFO: Waiting for pod downwardapi-volume-aaf19cea-89e0-4382-b538-bb4092bd427c to disappear
Nov 19 18:00:38.761: INFO: Pod downwardapi-volume-aaf19cea-89e0-4382-b538-bb4092bd427c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:00:38.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9445" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":229,"skipped":3799,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:00:38.772: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1572
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-ab4cecd2-2ce9-4d3b-92f6-bddecd21249b
STEP: Creating a pod to test consume configMaps
Nov 19 18:00:38.920: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7f62871f-4f36-42a7-b3da-cac2b0b4189c" in namespace "projected-1572" to be "Succeeded or Failed"
Nov 19 18:00:38.926: INFO: Pod "pod-projected-configmaps-7f62871f-4f36-42a7-b3da-cac2b0b4189c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.31368ms
Nov 19 18:00:40.930: INFO: Pod "pod-projected-configmaps-7f62871f-4f36-42a7-b3da-cac2b0b4189c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010232411s
STEP: Saw pod success
Nov 19 18:00:40.930: INFO: Pod "pod-projected-configmaps-7f62871f-4f36-42a7-b3da-cac2b0b4189c" satisfied condition "Succeeded or Failed"
Nov 19 18:00:40.934: INFO: Trying to get logs from node ip-172-31-20-145 pod pod-projected-configmaps-7f62871f-4f36-42a7-b3da-cac2b0b4189c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 19 18:00:40.952: INFO: Waiting for pod pod-projected-configmaps-7f62871f-4f36-42a7-b3da-cac2b0b4189c to disappear
Nov 19 18:00:40.954: INFO: Pod pod-projected-configmaps-7f62871f-4f36-42a7-b3da-cac2b0b4189c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:00:40.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1572" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":230,"skipped":3811,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:00:40.962: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-157
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating cluster-info
Nov 19 18:00:41.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 cluster-info'
Nov 19 18:00:41.152: INFO: stderr: ""
Nov 19 18:00:41.152: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.152.183.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.152.183.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://10.152.183.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:00:41.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-157" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":305,"completed":231,"skipped":3824,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:00:41.162: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9690
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 19 18:00:41.309: INFO: Waiting up to 5m0s for pod "downwardapi-volume-13301072-cbe6-4c68-a9be-6e625ca53b00" in namespace "projected-9690" to be "Succeeded or Failed"
Nov 19 18:00:41.314: INFO: Pod "downwardapi-volume-13301072-cbe6-4c68-a9be-6e625ca53b00": Phase="Pending", Reason="", readiness=false. Elapsed: 4.907442ms
Nov 19 18:00:43.319: INFO: Pod "downwardapi-volume-13301072-cbe6-4c68-a9be-6e625ca53b00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009162088s
STEP: Saw pod success
Nov 19 18:00:43.319: INFO: Pod "downwardapi-volume-13301072-cbe6-4c68-a9be-6e625ca53b00" satisfied condition "Succeeded or Failed"
Nov 19 18:00:43.321: INFO: Trying to get logs from node ip-172-31-20-145 pod downwardapi-volume-13301072-cbe6-4c68-a9be-6e625ca53b00 container client-container: <nil>
STEP: delete the pod
Nov 19 18:00:43.339: INFO: Waiting for pod downwardapi-volume-13301072-cbe6-4c68-a9be-6e625ca53b00 to disappear
Nov 19 18:00:43.341: INFO: Pod downwardapi-volume-13301072-cbe6-4c68-a9be-6e625ca53b00 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:00:43.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9690" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":232,"skipped":3839,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:00:43.347: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4852
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 18:00:43.511: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"8cc3ad74-126a-454c-bfc6-8fa3fdf16793", Controller:(*bool)(0xc00591f38e), BlockOwnerDeletion:(*bool)(0xc00591f38f)}}
Nov 19 18:00:43.524: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"8b42796e-54a9-4738-afa5-6a0f3915d5c7", Controller:(*bool)(0xc00379228e), BlockOwnerDeletion:(*bool)(0xc00379228f)}}
Nov 19 18:00:43.530: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"3abd36cb-6b79-4a76-82bb-d56c2bcfa7ea", Controller:(*bool)(0xc002cff956), BlockOwnerDeletion:(*bool)(0xc002cff957)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:00:48.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4852" for this suite.

â€¢ [SLOW TEST:5.205 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":305,"completed":233,"skipped":3842,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:00:48.552: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-6678
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:00:48.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6678" for this suite.
â€¢{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":234,"skipped":3856,"failed":0}
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:00:48.742: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1068
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Nov 19 18:00:48.885: INFO: PodSpec: initContainers in spec.initContainers
Nov 19 18:01:35.642: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-e52ba596-8ebf-4d19-a92f-fa232ffbfcfa", GenerateName:"", Namespace:"init-container-1068", SelfLink:"/api/v1/namespaces/init-container-1068/pods/pod-init-e52ba596-8ebf-4d19-a92f-fa232ffbfcfa", UID:"81326900-9eeb-4e32-81f7-51d0b9ce584b", ResourceVersion:"28538", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63741405648, loc:(*time.Location)(0x77108c0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"885831552"}, Annotations:map[string]string{"kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc003918fe0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003919000)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc003919020), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003919040)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-bhqzz", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc00816ab40), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-bhqzz", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-bhqzz", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-bhqzz", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00395e588), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-20-145", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001dfd9d0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00395e610)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00395e630)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00395e638), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00395e63c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000959290), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741405648, loc:(*time.Location)(0x77108c0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741405648, loc:(*time.Location)(0x77108c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741405648, loc:(*time.Location)(0x77108c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741405648, loc:(*time.Location)(0x77108c0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.20.145", PodIP:"10.1.33.189", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.1.33.189"}}, StartTime:(*v1.Time)(0xc003919060), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0039190a0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001dfdb20)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://c9f5de32900194ca1c2202da6911bebfdc24d0af56fb5326045bb566a16c5d07", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0039190c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003919080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc00395e6bf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:01:35.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1068" for this suite.

â€¢ [SLOW TEST:46.910 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":305,"completed":235,"skipped":3858,"failed":0}
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:01:35.652: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3918
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-3918
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating statefulset ss in namespace statefulset-3918
Nov 19 18:01:35.812: INFO: Found 0 stateful pods, waiting for 1
Nov 19 18:01:45.816: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Nov 19 18:01:45.838: INFO: Deleting all statefulset in ns statefulset-3918
Nov 19 18:01:45.842: INFO: Scaling statefulset ss to 0
Nov 19 18:02:15.884: INFO: Waiting for statefulset status.replicas updated to 0
Nov 19 18:02:15.886: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:02:15.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3918" for this suite.

â€¢ [SLOW TEST:40.263 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":305,"completed":236,"skipped":3859,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:02:15.916: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6637
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W1119 18:02:56.077830      22 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1119 18:02:56.077847      22 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1119 18:02:56.077852      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Nov 19 18:02:56.077: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Nov 19 18:02:56.077: INFO: Deleting pod "simpletest.rc-5gszn" in namespace "gc-6637"
Nov 19 18:02:56.088: INFO: Deleting pod "simpletest.rc-97nh2" in namespace "gc-6637"
Nov 19 18:02:56.100: INFO: Deleting pod "simpletest.rc-9d8hv" in namespace "gc-6637"
Nov 19 18:02:56.110: INFO: Deleting pod "simpletest.rc-9wzph" in namespace "gc-6637"
Nov 19 18:02:56.121: INFO: Deleting pod "simpletest.rc-brvjh" in namespace "gc-6637"
Nov 19 18:02:56.136: INFO: Deleting pod "simpletest.rc-hqh94" in namespace "gc-6637"
Nov 19 18:02:56.149: INFO: Deleting pod "simpletest.rc-lzqdn" in namespace "gc-6637"
Nov 19 18:02:56.160: INFO: Deleting pod "simpletest.rc-rbqtw" in namespace "gc-6637"
Nov 19 18:02:56.172: INFO: Deleting pod "simpletest.rc-tnl2b" in namespace "gc-6637"
Nov 19 18:02:56.185: INFO: Deleting pod "simpletest.rc-tphf2" in namespace "gc-6637"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:02:56.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6637" for this suite.

â€¢ [SLOW TEST:40.289 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":305,"completed":237,"skipped":3917,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:02:56.205: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8080
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 18:02:56.341: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Nov 19 18:02:58.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 --namespace=crd-publish-openapi-8080 create -f -'
Nov 19 18:02:58.527: INFO: stderr: ""
Nov 19 18:02:58.527: INFO: stdout: "e2e-test-crd-publish-openapi-9191-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Nov 19 18:02:58.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 --namespace=crd-publish-openapi-8080 delete e2e-test-crd-publish-openapi-9191-crds test-cr'
Nov 19 18:02:58.596: INFO: stderr: ""
Nov 19 18:02:58.596: INFO: stdout: "e2e-test-crd-publish-openapi-9191-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Nov 19 18:02:58.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 --namespace=crd-publish-openapi-8080 apply -f -'
Nov 19 18:02:58.901: INFO: stderr: ""
Nov 19 18:02:58.901: INFO: stdout: "e2e-test-crd-publish-openapi-9191-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Nov 19 18:02:58.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 --namespace=crd-publish-openapi-8080 delete e2e-test-crd-publish-openapi-9191-crds test-cr'
Nov 19 18:02:58.967: INFO: stderr: ""
Nov 19 18:02:58.967: INFO: stdout: "e2e-test-crd-publish-openapi-9191-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Nov 19 18:02:58.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 explain e2e-test-crd-publish-openapi-9191-crds'
Nov 19 18:02:59.113: INFO: stderr: ""
Nov 19 18:02:59.113: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9191-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:03:01.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8080" for this suite.

â€¢ [SLOW TEST:5.614 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":305,"completed":238,"skipped":3937,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:03:01.819: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1107
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Nov 19 18:03:01.966: INFO: Waiting up to 5m0s for pod "pod-8f1c4ff9-3fa5-4b03-ad2b-fa83c94b7c86" in namespace "emptydir-1107" to be "Succeeded or Failed"
Nov 19 18:03:01.969: INFO: Pod "pod-8f1c4ff9-3fa5-4b03-ad2b-fa83c94b7c86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.79194ms
Nov 19 18:03:03.972: INFO: Pod "pod-8f1c4ff9-3fa5-4b03-ad2b-fa83c94b7c86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006301788s
STEP: Saw pod success
Nov 19 18:03:03.972: INFO: Pod "pod-8f1c4ff9-3fa5-4b03-ad2b-fa83c94b7c86" satisfied condition "Succeeded or Failed"
Nov 19 18:03:03.976: INFO: Trying to get logs from node ip-172-31-20-145 pod pod-8f1c4ff9-3fa5-4b03-ad2b-fa83c94b7c86 container test-container: <nil>
STEP: delete the pod
Nov 19 18:03:04.012: INFO: Waiting for pod pod-8f1c4ff9-3fa5-4b03-ad2b-fa83c94b7c86 to disappear
Nov 19 18:03:04.014: INFO: Pod pod-8f1c4ff9-3fa5-4b03-ad2b-fa83c94b7c86 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:03:04.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1107" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":239,"skipped":3938,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:03:04.021: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-686
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Nov 19 18:03:04.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 create -f - --namespace=kubectl-686'
Nov 19 18:03:04.402: INFO: stderr: ""
Nov 19 18:03:04.402: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Nov 19 18:03:05.406: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 19 18:03:05.406: INFO: Found 0 / 1
Nov 19 18:03:06.405: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 19 18:03:06.405: INFO: Found 1 / 1
Nov 19 18:03:06.405: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Nov 19 18:03:06.408: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 19 18:03:06.408: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 19 18:03:06.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 patch pod agnhost-primary-nqxdb --namespace=kubectl-686 -p {"metadata":{"annotations":{"x":"y"}}}'
Nov 19 18:03:06.475: INFO: stderr: ""
Nov 19 18:03:06.475: INFO: stdout: "pod/agnhost-primary-nqxdb patched\n"
STEP: checking annotations
Nov 19 18:03:06.478: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 19 18:03:06.478: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:03:06.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-686" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":305,"completed":240,"skipped":3942,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:03:06.487: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-392
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 18:03:06.620: INFO: Creating deployment "test-recreate-deployment"
Nov 19 18:03:06.626: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Nov 19 18:03:06.642: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Nov 19 18:03:08.649: INFO: Waiting deployment "test-recreate-deployment" to complete
Nov 19 18:03:08.652: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Nov 19 18:03:08.661: INFO: Updating deployment test-recreate-deployment
Nov 19 18:03:08.661: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Nov 19 18:03:08.733: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-392 /apis/apps/v1/namespaces/deployment-392/deployments/test-recreate-deployment d2a8f749-4512-4640-bdff-8f68f00e1d0a 29217 2 2020-11-19 18:03:06 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-11-19 18:03:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-11-19 18:03:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0071b3e78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-11-19 18:03:08 +0000 UTC,LastTransitionTime:2020-11-19 18:03:08 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2020-11-19 18:03:08 +0000 UTC,LastTransitionTime:2020-11-19 18:03:06 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Nov 19 18:03:08.735: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-392 /apis/apps/v1/namespaces/deployment-392/replicasets/test-recreate-deployment-f79dd4667 cad1fff7-0868-46a1-94a7-1af3b5253218 29214 1 2020-11-19 18:03:08 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment d2a8f749-4512-4640-bdff-8f68f00e1d0a 0xc007234420 0xc007234421}] []  [{kube-controller-manager Update apps/v1 2020-11-19 18:03:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d2a8f749-4512-4640-bdff-8f68f00e1d0a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0072344a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 19 18:03:08.735: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Nov 19 18:03:08.735: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-c96cf48f  deployment-392 /apis/apps/v1/namespaces/deployment-392/replicasets/test-recreate-deployment-c96cf48f b92dd679-538c-4859-84ad-78c7ce98aed0 29205 2 2020-11-19 18:03:06 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment d2a8f749-4512-4640-bdff-8f68f00e1d0a 0xc00723431f 0xc007234330}] []  [{kube-controller-manager Update apps/v1 2020-11-19 18:03:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d2a8f749-4512-4640-bdff-8f68f00e1d0a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c96cf48f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0072343b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 19 18:03:08.739: INFO: Pod "test-recreate-deployment-f79dd4667-96l8n" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-96l8n test-recreate-deployment-f79dd4667- deployment-392 /api/v1/namespaces/deployment-392/pods/test-recreate-deployment-f79dd4667-96l8n 1fa2c2af-d146-46b8-b0b8-32fc55e6541a 29216 0 2020-11-19 18:03:08 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 cad1fff7-0868-46a1-94a7-1af3b5253218 0xc007234a20 0xc007234a21}] []  [{kube-controller-manager Update v1 2020-11-19 18:03:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cad1fff7-0868-46a1-94a7-1af3b5253218\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-19 18:03:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mnmr8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mnmr8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mnmr8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-20-145,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 18:03:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 18:03:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 18:03:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 18:03:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.20.145,PodIP:,StartTime:2020-11-19 18:03:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:03:08.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-392" for this suite.
â€¢{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":241,"skipped":3985,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:03:08.748: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1735
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Nov 19 18:03:08.888: INFO: Waiting up to 5m0s for pod "pod-18cead62-ccd6-41ce-aa76-836eee010c07" in namespace "emptydir-1735" to be "Succeeded or Failed"
Nov 19 18:03:08.890: INFO: Pod "pod-18cead62-ccd6-41ce-aa76-836eee010c07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.274154ms
Nov 19 18:03:10.895: INFO: Pod "pod-18cead62-ccd6-41ce-aa76-836eee010c07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006704552s
STEP: Saw pod success
Nov 19 18:03:10.895: INFO: Pod "pod-18cead62-ccd6-41ce-aa76-836eee010c07" satisfied condition "Succeeded or Failed"
Nov 19 18:03:10.898: INFO: Trying to get logs from node ip-172-31-20-145 pod pod-18cead62-ccd6-41ce-aa76-836eee010c07 container test-container: <nil>
STEP: delete the pod
Nov 19 18:03:10.918: INFO: Waiting for pod pod-18cead62-ccd6-41ce-aa76-836eee010c07 to disappear
Nov 19 18:03:10.925: INFO: Pod pod-18cead62-ccd6-41ce-aa76-836eee010c07 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:03:10.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1735" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":242,"skipped":4027,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:03:10.932: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-942
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-f54ae22d-edc4-4db6-a27a-247fd08481b6
STEP: Creating a pod to test consume secrets
Nov 19 18:03:11.076: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-782491e3-bf5a-4543-bae8-31378e31d40f" in namespace "projected-942" to be "Succeeded or Failed"
Nov 19 18:03:11.079: INFO: Pod "pod-projected-secrets-782491e3-bf5a-4543-bae8-31378e31d40f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.66898ms
Nov 19 18:03:13.082: INFO: Pod "pod-projected-secrets-782491e3-bf5a-4543-bae8-31378e31d40f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006611503s
STEP: Saw pod success
Nov 19 18:03:13.082: INFO: Pod "pod-projected-secrets-782491e3-bf5a-4543-bae8-31378e31d40f" satisfied condition "Succeeded or Failed"
Nov 19 18:03:13.087: INFO: Trying to get logs from node ip-172-31-20-145 pod pod-projected-secrets-782491e3-bf5a-4543-bae8-31378e31d40f container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 19 18:03:13.104: INFO: Waiting for pod pod-projected-secrets-782491e3-bf5a-4543-bae8-31378e31d40f to disappear
Nov 19 18:03:13.106: INFO: Pod pod-projected-secrets-782491e3-bf5a-4543-bae8-31378e31d40f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:03:13.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-942" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":243,"skipped":4031,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:03:13.114: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3349
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W1119 18:03:14.318419      22 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1119 18:03:14.318437      22 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1119 18:03:14.318441      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Nov 19 18:03:14.318: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:03:14.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3349" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":305,"completed":244,"skipped":4044,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:03:14.325: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5268
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Nov 19 18:03:14.475: INFO: Waiting up to 5m0s for pod "pod-3a8e6233-a9e9-4ed1-b09c-dd1eb21684fc" in namespace "emptydir-5268" to be "Succeeded or Failed"
Nov 19 18:03:14.481: INFO: Pod "pod-3a8e6233-a9e9-4ed1-b09c-dd1eb21684fc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.285216ms
Nov 19 18:03:16.484: INFO: Pod "pod-3a8e6233-a9e9-4ed1-b09c-dd1eb21684fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00844845s
STEP: Saw pod success
Nov 19 18:03:16.484: INFO: Pod "pod-3a8e6233-a9e9-4ed1-b09c-dd1eb21684fc" satisfied condition "Succeeded or Failed"
Nov 19 18:03:16.486: INFO: Trying to get logs from node ip-172-31-20-145 pod pod-3a8e6233-a9e9-4ed1-b09c-dd1eb21684fc container test-container: <nil>
STEP: delete the pod
Nov 19 18:03:16.504: INFO: Waiting for pod pod-3a8e6233-a9e9-4ed1-b09c-dd1eb21684fc to disappear
Nov 19 18:03:16.507: INFO: Pod pod-3a8e6233-a9e9-4ed1-b09c-dd1eb21684fc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:03:16.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5268" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":245,"skipped":4080,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:03:16.517: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3589
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-29f9024d-a08c-4a7e-98b8-5aa1f65273bc
STEP: Creating a pod to test consume configMaps
Nov 19 18:03:16.661: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-43276d3f-c1ad-45c1-b858-b4982f72941f" in namespace "projected-3589" to be "Succeeded or Failed"
Nov 19 18:03:16.663: INFO: Pod "pod-projected-configmaps-43276d3f-c1ad-45c1-b858-b4982f72941f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.469394ms
Nov 19 18:03:18.666: INFO: Pod "pod-projected-configmaps-43276d3f-c1ad-45c1-b858-b4982f72941f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005753496s
STEP: Saw pod success
Nov 19 18:03:18.666: INFO: Pod "pod-projected-configmaps-43276d3f-c1ad-45c1-b858-b4982f72941f" satisfied condition "Succeeded or Failed"
Nov 19 18:03:18.669: INFO: Trying to get logs from node ip-172-31-20-145 pod pod-projected-configmaps-43276d3f-c1ad-45c1-b858-b4982f72941f container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 19 18:03:18.685: INFO: Waiting for pod pod-projected-configmaps-43276d3f-c1ad-45c1-b858-b4982f72941f to disappear
Nov 19 18:03:18.688: INFO: Pod pod-projected-configmaps-43276d3f-c1ad-45c1-b858-b4982f72941f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:03:18.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3589" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":246,"skipped":4107,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:03:18.696: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9422
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 19 18:03:18.857: INFO: Waiting up to 5m0s for pod "downwardapi-volume-494f214b-bf5a-483e-92c6-50686f855964" in namespace "projected-9422" to be "Succeeded or Failed"
Nov 19 18:03:18.862: INFO: Pod "downwardapi-volume-494f214b-bf5a-483e-92c6-50686f855964": Phase="Pending", Reason="", readiness=false. Elapsed: 4.754469ms
Nov 19 18:03:20.865: INFO: Pod "downwardapi-volume-494f214b-bf5a-483e-92c6-50686f855964": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008088831s
STEP: Saw pod success
Nov 19 18:03:20.866: INFO: Pod "downwardapi-volume-494f214b-bf5a-483e-92c6-50686f855964" satisfied condition "Succeeded or Failed"
Nov 19 18:03:20.869: INFO: Trying to get logs from node ip-172-31-20-145 pod downwardapi-volume-494f214b-bf5a-483e-92c6-50686f855964 container client-container: <nil>
STEP: delete the pod
Nov 19 18:03:20.884: INFO: Waiting for pod downwardapi-volume-494f214b-bf5a-483e-92c6-50686f855964 to disappear
Nov 19 18:03:20.886: INFO: Pod downwardapi-volume-494f214b-bf5a-483e-92c6-50686f855964 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:03:20.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9422" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":247,"skipped":4114,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:03:20.895: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5782
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 19 18:03:21.037: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7861ce60-5c9d-49f6-888c-c025a4ed0972" in namespace "projected-5782" to be "Succeeded or Failed"
Nov 19 18:03:21.039: INFO: Pod "downwardapi-volume-7861ce60-5c9d-49f6-888c-c025a4ed0972": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009857ms
Nov 19 18:03:23.042: INFO: Pod "downwardapi-volume-7861ce60-5c9d-49f6-888c-c025a4ed0972": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005471404s
STEP: Saw pod success
Nov 19 18:03:23.042: INFO: Pod "downwardapi-volume-7861ce60-5c9d-49f6-888c-c025a4ed0972" satisfied condition "Succeeded or Failed"
Nov 19 18:03:23.046: INFO: Trying to get logs from node ip-172-31-20-145 pod downwardapi-volume-7861ce60-5c9d-49f6-888c-c025a4ed0972 container client-container: <nil>
STEP: delete the pod
Nov 19 18:03:23.064: INFO: Waiting for pod downwardapi-volume-7861ce60-5c9d-49f6-888c-c025a4ed0972 to disappear
Nov 19 18:03:23.066: INFO: Pod downwardapi-volume-7861ce60-5c9d-49f6-888c-c025a4ed0972 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:03:23.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5782" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":248,"skipped":4128,"failed":0}

------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:03:23.073: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4967
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in volume subpath
Nov 19 18:03:23.222: INFO: Waiting up to 5m0s for pod "var-expansion-d1cf2c54-a145-460e-929b-47de10e65571" in namespace "var-expansion-4967" to be "Succeeded or Failed"
Nov 19 18:03:23.224: INFO: Pod "var-expansion-d1cf2c54-a145-460e-929b-47de10e65571": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048046ms
Nov 19 18:03:25.228: INFO: Pod "var-expansion-d1cf2c54-a145-460e-929b-47de10e65571": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006404034s
STEP: Saw pod success
Nov 19 18:03:25.228: INFO: Pod "var-expansion-d1cf2c54-a145-460e-929b-47de10e65571" satisfied condition "Succeeded or Failed"
Nov 19 18:03:25.232: INFO: Trying to get logs from node ip-172-31-20-145 pod var-expansion-d1cf2c54-a145-460e-929b-47de10e65571 container dapi-container: <nil>
STEP: delete the pod
Nov 19 18:03:25.248: INFO: Waiting for pod var-expansion-d1cf2c54-a145-460e-929b-47de10e65571 to disappear
Nov 19 18:03:25.251: INFO: Pod var-expansion-d1cf2c54-a145-460e-929b-47de10e65571 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:03:25.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4967" for this suite.
â€¢{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":305,"completed":249,"skipped":4128,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:03:25.260: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-685
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:03:27.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-685" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":305,"completed":250,"skipped":4150,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:03:27.453: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3619
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 19 18:03:27.891: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 19 18:03:30.911: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Nov 19 18:03:32.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 attach --namespace=webhook-3619 to-be-attached-pod -i -c=container1'
Nov 19 18:03:33.015: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:03:33.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3619" for this suite.
STEP: Destroying namespace "webhook-3619-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:5.628 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":305,"completed":251,"skipped":4178,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:03:33.081: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5002
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-28d1b21d-2881-45f0-81ab-9927a93f34cf
STEP: Creating configMap with name cm-test-opt-upd-8328ff3e-0548-4d15-96b6-759b94189bf5
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-28d1b21d-2881-45f0-81ab-9927a93f34cf
STEP: Updating configmap cm-test-opt-upd-8328ff3e-0548-4d15-96b6-759b94189bf5
STEP: Creating configMap with name cm-test-opt-create-420df79c-dcda-4bfd-b6b6-2174c5b5dbe5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:03:39.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5002" for this suite.

â€¢ [SLOW TEST:6.239 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":252,"skipped":4180,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:03:39.320: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5451
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Nov 19 18:03:39.462: INFO: Waiting up to 5m0s for pod "pod-0033e3ad-db15-4f7b-8417-127864b90b00" in namespace "emptydir-5451" to be "Succeeded or Failed"
Nov 19 18:03:39.467: INFO: Pod "pod-0033e3ad-db15-4f7b-8417-127864b90b00": Phase="Pending", Reason="", readiness=false. Elapsed: 5.063984ms
Nov 19 18:03:41.472: INFO: Pod "pod-0033e3ad-db15-4f7b-8417-127864b90b00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010342563s
STEP: Saw pod success
Nov 19 18:03:41.472: INFO: Pod "pod-0033e3ad-db15-4f7b-8417-127864b90b00" satisfied condition "Succeeded or Failed"
Nov 19 18:03:41.475: INFO: Trying to get logs from node ip-172-31-68-240 pod pod-0033e3ad-db15-4f7b-8417-127864b90b00 container test-container: <nil>
STEP: delete the pod
Nov 19 18:03:41.508: INFO: Waiting for pod pod-0033e3ad-db15-4f7b-8417-127864b90b00 to disappear
Nov 19 18:03:41.511: INFO: Pod pod-0033e3ad-db15-4f7b-8417-127864b90b00 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:03:41.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5451" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":253,"skipped":4190,"failed":0}

------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:03:41.518: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4421
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Nov 19 18:03:41.659: INFO: Created pod &Pod{ObjectMeta:{dns-4421  dns-4421 /api/v1/namespaces/dns-4421/pods/dns-4421 d4873f50-2c0c-4b55-b33d-ebe0b224a938 29746 0 2020-11-19 18:03:41 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2020-11-19 18:03:41 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zxxb2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zxxb2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zxxb2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 19 18:03:41.666: INFO: The status of Pod dns-4421 is Pending, waiting for it to be Running (with Ready = true)
Nov 19 18:03:43.670: INFO: The status of Pod dns-4421 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Nov 19 18:03:43.670: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-4421 PodName:dns-4421 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 19 18:03:43.670: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Verifying customized DNS server is configured on pod...
Nov 19 18:03:43.756: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-4421 PodName:dns-4421 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 19 18:03:43.756: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
Nov 19 18:03:43.821: INFO: Deleting pod dns-4421...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:03:43.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4421" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":305,"completed":254,"skipped":4190,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:03:43.843: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5191
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 19 18:03:44.421: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 19 18:03:47.441: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:03:59.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5191" for this suite.
STEP: Destroying namespace "webhook-5191-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:15.773 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":305,"completed":255,"skipped":4239,"failed":0}
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:03:59.616: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5202
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:04:01.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5202" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":305,"completed":256,"skipped":4239,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:04:01.792: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6961
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's args
Nov 19 18:04:01.930: INFO: Waiting up to 5m0s for pod "var-expansion-c7b33958-60ee-4fcf-9ac3-e6e58f7cc493" in namespace "var-expansion-6961" to be "Succeeded or Failed"
Nov 19 18:04:01.933: INFO: Pod "var-expansion-c7b33958-60ee-4fcf-9ac3-e6e58f7cc493": Phase="Pending", Reason="", readiness=false. Elapsed: 2.340648ms
Nov 19 18:04:03.938: INFO: Pod "var-expansion-c7b33958-60ee-4fcf-9ac3-e6e58f7cc493": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007550648s
STEP: Saw pod success
Nov 19 18:04:03.938: INFO: Pod "var-expansion-c7b33958-60ee-4fcf-9ac3-e6e58f7cc493" satisfied condition "Succeeded or Failed"
Nov 19 18:04:03.942: INFO: Trying to get logs from node ip-172-31-20-145 pod var-expansion-c7b33958-60ee-4fcf-9ac3-e6e58f7cc493 container dapi-container: <nil>
STEP: delete the pod
Nov 19 18:04:03.958: INFO: Waiting for pod var-expansion-c7b33958-60ee-4fcf-9ac3-e6e58f7cc493 to disappear
Nov 19 18:04:03.960: INFO: Pod var-expansion-c7b33958-60ee-4fcf-9ac3-e6e58f7cc493 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:04:03.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6961" for this suite.
â€¢{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":305,"completed":257,"skipped":4267,"failed":0}
S
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:04:03.969: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9305
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-bec0a6cb-ebd6-4ec2-9b26-20ff79ced7c9 in namespace container-probe-9305
Nov 19 18:04:06.130: INFO: Started pod liveness-bec0a6cb-ebd6-4ec2-9b26-20ff79ced7c9 in namespace container-probe-9305
STEP: checking the pod's current state and verifying that restartCount is present
Nov 19 18:04:06.134: INFO: Initial restart count of pod liveness-bec0a6cb-ebd6-4ec2-9b26-20ff79ced7c9 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:08:06.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9305" for this suite.

â€¢ [SLOW TEST:242.712 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":305,"completed":258,"skipped":4268,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:08:06.681: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4220
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-d2c485ec-6a13-4913-aed0-7ed1ffa7dfd5
STEP: Creating secret with name s-test-opt-upd-110883e2-e800-408e-b3eb-6035d868c410
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-d2c485ec-6a13-4913-aed0-7ed1ffa7dfd5
STEP: Updating secret s-test-opt-upd-110883e2-e800-408e-b3eb-6035d868c410
STEP: Creating secret with name s-test-opt-create-8b46657e-7542-494d-9390-d52d056e7acc
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:08:10.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4220" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":259,"skipped":4272,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:08:10.934: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5222
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:08:22.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5222" for this suite.

â€¢ [SLOW TEST:11.180 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":305,"completed":260,"skipped":4274,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:08:22.114: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9473
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Nov 19 18:08:22.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-9473'
Nov 19 18:08:22.320: INFO: stderr: ""
Nov 19 18:08:22.320: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Nov 19 18:08:22.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 get pod e2e-test-httpd-pod -o json --namespace=kubectl-9473'
Nov 19 18:08:22.376: INFO: stderr: ""
Nov 19 18:08:22.376: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2020-11-19T18:08:22Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-11-19T18:08:22Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:message\": {},\n                                \"f:reason\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:message\": {},\n                                \"f:reason\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-11-19T18:08:22Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9473\",\n        \"resourceVersion\": \"30774\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-9473/pods/e2e-test-httpd-pod\",\n        \"uid\": \"70cafe1e-818b-4702-88f7-2167ceaefefa\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-zdksl\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-20-145\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-zdksl\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-zdksl\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-19T18:08:22Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-19T18:08:22Z\",\n                \"message\": \"containers with unready status: [e2e-test-httpd-pod]\",\n                \"reason\": \"ContainersNotReady\",\n                \"status\": \"False\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-19T18:08:22Z\",\n                \"message\": \"containers with unready status: [e2e-test-httpd-pod]\",\n                \"reason\": \"ContainersNotReady\",\n                \"status\": \"False\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-19T18:08:22Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": false,\n                \"restartCount\": 0,\n                \"started\": false,\n                \"state\": {\n                    \"waiting\": {\n                        \"reason\": \"ContainerCreating\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.20.145\",\n        \"phase\": \"Pending\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-11-19T18:08:22Z\"\n    }\n}\n"
Nov 19 18:08:22.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 replace -f - --dry-run server --namespace=kubectl-9473'
Nov 19 18:08:22.521: INFO: stderr: "W1119 18:08:22.418555    1391 helpers.go:553] --dry-run is deprecated and can be replaced with --dry-run=client.\n"
Nov 19 18:08:22.521: INFO: stdout: "pod/e2e-test-httpd-pod replaced (dry run)\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Nov 19 18:08:22.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 delete pods e2e-test-httpd-pod --namespace=kubectl-9473'
Nov 19 18:08:24.071: INFO: stderr: ""
Nov 19 18:08:24.071: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:08:24.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9473" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":305,"completed":261,"skipped":4285,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:08:24.081: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4743
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-4743
Nov 19 18:08:26.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-4743 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Nov 19 18:08:26.367: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Nov 19 18:08:26.367: INFO: stdout: "iptables"
Nov 19 18:08:26.367: INFO: proxyMode: iptables
Nov 19 18:08:26.375: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 19 18:08:26.378: INFO: Pod kube-proxy-mode-detector still exists
Nov 19 18:08:28.378: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 19 18:08:28.383: INFO: Pod kube-proxy-mode-detector still exists
Nov 19 18:08:30.378: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 19 18:08:30.382: INFO: Pod kube-proxy-mode-detector still exists
Nov 19 18:08:32.378: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 19 18:08:32.382: INFO: Pod kube-proxy-mode-detector still exists
Nov 19 18:08:34.378: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 19 18:08:34.383: INFO: Pod kube-proxy-mode-detector still exists
Nov 19 18:08:36.378: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 19 18:08:36.382: INFO: Pod kube-proxy-mode-detector still exists
Nov 19 18:08:38.378: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 19 18:08:38.383: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-4743
STEP: creating replication controller affinity-clusterip-timeout in namespace services-4743
I1119 18:08:38.411018      22 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-4743, replica count: 3
I1119 18:08:41.461294      22 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 19 18:08:41.469: INFO: Creating new exec pod
Nov 19 18:08:44.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-4743 execpod-affinityp7gwn -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Nov 19 18:08:44.621: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Nov 19 18:08:44.621: INFO: stdout: ""
Nov 19 18:08:44.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-4743 execpod-affinityp7gwn -- /bin/sh -x -c nc -zv -t -w 2 10.152.183.210 80'
Nov 19 18:08:44.748: INFO: stderr: "+ nc -zv -t -w 2 10.152.183.210 80\nConnection to 10.152.183.210 80 port [tcp/http] succeeded!\n"
Nov 19 18:08:44.748: INFO: stdout: ""
Nov 19 18:08:44.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-4743 execpod-affinityp7gwn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.210:80/ ; done'
Nov 19 18:08:44.919: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.210:80/\n"
Nov 19 18:08:44.919: INFO: stdout: "\naffinity-clusterip-timeout-887vs\naffinity-clusterip-timeout-887vs\naffinity-clusterip-timeout-887vs\naffinity-clusterip-timeout-887vs\naffinity-clusterip-timeout-887vs\naffinity-clusterip-timeout-887vs\naffinity-clusterip-timeout-887vs\naffinity-clusterip-timeout-887vs\naffinity-clusterip-timeout-887vs\naffinity-clusterip-timeout-887vs\naffinity-clusterip-timeout-887vs\naffinity-clusterip-timeout-887vs\naffinity-clusterip-timeout-887vs\naffinity-clusterip-timeout-887vs\naffinity-clusterip-timeout-887vs\naffinity-clusterip-timeout-887vs"
Nov 19 18:08:44.919: INFO: Received response from host: affinity-clusterip-timeout-887vs
Nov 19 18:08:44.919: INFO: Received response from host: affinity-clusterip-timeout-887vs
Nov 19 18:08:44.919: INFO: Received response from host: affinity-clusterip-timeout-887vs
Nov 19 18:08:44.919: INFO: Received response from host: affinity-clusterip-timeout-887vs
Nov 19 18:08:44.919: INFO: Received response from host: affinity-clusterip-timeout-887vs
Nov 19 18:08:44.919: INFO: Received response from host: affinity-clusterip-timeout-887vs
Nov 19 18:08:44.919: INFO: Received response from host: affinity-clusterip-timeout-887vs
Nov 19 18:08:44.919: INFO: Received response from host: affinity-clusterip-timeout-887vs
Nov 19 18:08:44.919: INFO: Received response from host: affinity-clusterip-timeout-887vs
Nov 19 18:08:44.919: INFO: Received response from host: affinity-clusterip-timeout-887vs
Nov 19 18:08:44.919: INFO: Received response from host: affinity-clusterip-timeout-887vs
Nov 19 18:08:44.919: INFO: Received response from host: affinity-clusterip-timeout-887vs
Nov 19 18:08:44.919: INFO: Received response from host: affinity-clusterip-timeout-887vs
Nov 19 18:08:44.919: INFO: Received response from host: affinity-clusterip-timeout-887vs
Nov 19 18:08:44.919: INFO: Received response from host: affinity-clusterip-timeout-887vs
Nov 19 18:08:44.919: INFO: Received response from host: affinity-clusterip-timeout-887vs
Nov 19 18:08:44.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-4743 execpod-affinityp7gwn -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.152.183.210:80/'
Nov 19 18:08:45.043: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.152.183.210:80/\n"
Nov 19 18:08:45.043: INFO: stdout: "affinity-clusterip-timeout-887vs"
Nov 19 18:09:00.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 exec --namespace=services-4743 execpod-affinityp7gwn -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.152.183.210:80/'
Nov 19 18:09:00.188: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.152.183.210:80/\n"
Nov 19 18:09:00.188: INFO: stdout: "affinity-clusterip-timeout-wzkz6"
Nov 19 18:09:00.188: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-4743, will wait for the garbage collector to delete the pods
Nov 19 18:09:00.262: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 7.965181ms
Nov 19 18:09:00.763: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 500.148396ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:09:15.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4743" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

â€¢ [SLOW TEST:51.112 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":262,"skipped":4289,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:09:15.194: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8033
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 19 18:09:15.343: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2c39d0e5-e925-4ad0-aaea-5a7dff9f0b93" in namespace "downward-api-8033" to be "Succeeded or Failed"
Nov 19 18:09:15.345: INFO: Pod "downwardapi-volume-2c39d0e5-e925-4ad0-aaea-5a7dff9f0b93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.207839ms
Nov 19 18:09:17.351: INFO: Pod "downwardapi-volume-2c39d0e5-e925-4ad0-aaea-5a7dff9f0b93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007697262s
STEP: Saw pod success
Nov 19 18:09:17.351: INFO: Pod "downwardapi-volume-2c39d0e5-e925-4ad0-aaea-5a7dff9f0b93" satisfied condition "Succeeded or Failed"
Nov 19 18:09:17.353: INFO: Trying to get logs from node ip-172-31-20-145 pod downwardapi-volume-2c39d0e5-e925-4ad0-aaea-5a7dff9f0b93 container client-container: <nil>
STEP: delete the pod
Nov 19 18:09:17.370: INFO: Waiting for pod downwardapi-volume-2c39d0e5-e925-4ad0-aaea-5a7dff9f0b93 to disappear
Nov 19 18:09:17.372: INFO: Pod downwardapi-volume-2c39d0e5-e925-4ad0-aaea-5a7dff9f0b93 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:09:17.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8033" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":263,"skipped":4319,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:09:17.381: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1662
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-9e22f1d4-0a76-4a68-81f1-296c0b969236
STEP: Creating configMap with name cm-test-opt-upd-dab547c9-701e-435b-9205-375f99d16e11
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-9e22f1d4-0a76-4a68-81f1-296c0b969236
STEP: Updating configmap cm-test-opt-upd-dab547c9-701e-435b-9205-375f99d16e11
STEP: Creating configMap with name cm-test-opt-create-e482d1bf-cf01-4819-a2fa-07bbf8969af0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:09:23.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1662" for this suite.

â€¢ [SLOW TEST:6.235 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":264,"skipped":4354,"failed":0}
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:09:23.616: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9839
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override command
Nov 19 18:09:23.767: INFO: Waiting up to 5m0s for pod "client-containers-61c1b5b2-95b1-4046-9b01-e4064132bea7" in namespace "containers-9839" to be "Succeeded or Failed"
Nov 19 18:09:23.772: INFO: Pod "client-containers-61c1b5b2-95b1-4046-9b01-e4064132bea7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.402208ms
Nov 19 18:09:25.777: INFO: Pod "client-containers-61c1b5b2-95b1-4046-9b01-e4064132bea7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009401159s
STEP: Saw pod success
Nov 19 18:09:25.777: INFO: Pod "client-containers-61c1b5b2-95b1-4046-9b01-e4064132bea7" satisfied condition "Succeeded or Failed"
Nov 19 18:09:25.781: INFO: Trying to get logs from node ip-172-31-20-145 pod client-containers-61c1b5b2-95b1-4046-9b01-e4064132bea7 container test-container: <nil>
STEP: delete the pod
Nov 19 18:09:25.806: INFO: Waiting for pod client-containers-61c1b5b2-95b1-4046-9b01-e4064132bea7 to disappear
Nov 19 18:09:25.808: INFO: Pod client-containers-61c1b5b2-95b1-4046-9b01-e4064132bea7 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:09:25.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9839" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":305,"completed":265,"skipped":4357,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:09:25.816: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-700
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:09:25.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-700" for this suite.
â€¢{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":305,"completed":266,"skipped":4366,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:09:26.002: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-427
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 18:09:26.150: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-795f459b-3295-4221-8d00-b11863f9f8a5" in namespace "security-context-test-427" to be "Succeeded or Failed"
Nov 19 18:09:26.155: INFO: Pod "busybox-privileged-false-795f459b-3295-4221-8d00-b11863f9f8a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.85012ms
Nov 19 18:09:28.159: INFO: Pod "busybox-privileged-false-795f459b-3295-4221-8d00-b11863f9f8a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008905799s
Nov 19 18:09:28.159: INFO: Pod "busybox-privileged-false-795f459b-3295-4221-8d00-b11863f9f8a5" satisfied condition "Succeeded or Failed"
Nov 19 18:09:28.164: INFO: Got logs for pod "busybox-privileged-false-795f459b-3295-4221-8d00-b11863f9f8a5": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:09:28.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-427" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":267,"skipped":4381,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:09:28.174: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5097
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5097.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-5097.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5097.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5097.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5097.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-5097.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5097.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-5097.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5097.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5097.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-5097.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5097.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-5097.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5097.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-5097.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5097.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-5097.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5097.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 19 18:09:30.345: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5097.svc.cluster.local from pod dns-5097/dns-test-51c8e087-5cc4-4a16-994f-493725b78b00: the server could not find the requested resource (get pods dns-test-51c8e087-5cc4-4a16-994f-493725b78b00)
Nov 19 18:09:30.349: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5097.svc.cluster.local from pod dns-5097/dns-test-51c8e087-5cc4-4a16-994f-493725b78b00: the server could not find the requested resource (get pods dns-test-51c8e087-5cc4-4a16-994f-493725b78b00)
Nov 19 18:09:30.354: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5097.svc.cluster.local from pod dns-5097/dns-test-51c8e087-5cc4-4a16-994f-493725b78b00: the server could not find the requested resource (get pods dns-test-51c8e087-5cc4-4a16-994f-493725b78b00)
Nov 19 18:09:30.357: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5097.svc.cluster.local from pod dns-5097/dns-test-51c8e087-5cc4-4a16-994f-493725b78b00: the server could not find the requested resource (get pods dns-test-51c8e087-5cc4-4a16-994f-493725b78b00)
Nov 19 18:09:30.369: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5097.svc.cluster.local from pod dns-5097/dns-test-51c8e087-5cc4-4a16-994f-493725b78b00: the server could not find the requested resource (get pods dns-test-51c8e087-5cc4-4a16-994f-493725b78b00)
Nov 19 18:09:30.373: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5097.svc.cluster.local from pod dns-5097/dns-test-51c8e087-5cc4-4a16-994f-493725b78b00: the server could not find the requested resource (get pods dns-test-51c8e087-5cc4-4a16-994f-493725b78b00)
Nov 19 18:09:30.377: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5097.svc.cluster.local from pod dns-5097/dns-test-51c8e087-5cc4-4a16-994f-493725b78b00: the server could not find the requested resource (get pods dns-test-51c8e087-5cc4-4a16-994f-493725b78b00)
Nov 19 18:09:30.380: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5097.svc.cluster.local from pod dns-5097/dns-test-51c8e087-5cc4-4a16-994f-493725b78b00: the server could not find the requested resource (get pods dns-test-51c8e087-5cc4-4a16-994f-493725b78b00)
Nov 19 18:09:30.388: INFO: Lookups using dns-5097/dns-test-51c8e087-5cc4-4a16-994f-493725b78b00 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5097.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5097.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5097.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5097.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5097.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5097.svc.cluster.local jessie_udp@dns-test-service-2.dns-5097.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5097.svc.cluster.local]

Nov 19 18:09:35.430: INFO: DNS probes using dns-5097/dns-test-51c8e087-5cc4-4a16-994f-493725b78b00 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:09:35.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5097" for this suite.

â€¢ [SLOW TEST:7.309 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":305,"completed":268,"skipped":4428,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:09:35.483: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1963
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 19 18:09:35.632: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7b3ffaaa-e269-41c0-8b9a-d121b9bc37b5" in namespace "downward-api-1963" to be "Succeeded or Failed"
Nov 19 18:09:35.638: INFO: Pod "downwardapi-volume-7b3ffaaa-e269-41c0-8b9a-d121b9bc37b5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.17514ms
Nov 19 18:09:37.642: INFO: Pod "downwardapi-volume-7b3ffaaa-e269-41c0-8b9a-d121b9bc37b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010331793s
STEP: Saw pod success
Nov 19 18:09:37.642: INFO: Pod "downwardapi-volume-7b3ffaaa-e269-41c0-8b9a-d121b9bc37b5" satisfied condition "Succeeded or Failed"
Nov 19 18:09:37.646: INFO: Trying to get logs from node ip-172-31-20-145 pod downwardapi-volume-7b3ffaaa-e269-41c0-8b9a-d121b9bc37b5 container client-container: <nil>
STEP: delete the pod
Nov 19 18:09:37.665: INFO: Waiting for pod downwardapi-volume-7b3ffaaa-e269-41c0-8b9a-d121b9bc37b5 to disappear
Nov 19 18:09:37.669: INFO: Pod downwardapi-volume-7b3ffaaa-e269-41c0-8b9a-d121b9bc37b5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:09:37.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1963" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":269,"skipped":4447,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:09:37.675: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4716
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 18:09:37.811: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Nov 19 18:09:39.839: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:09:40.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4716" for this suite.
â€¢{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":305,"completed":270,"skipped":4480,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:09:40.858: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6614
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 19 18:09:41.002: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d91d0bb7-2bad-4ebc-8aa2-716a3cd3bfc5" in namespace "projected-6614" to be "Succeeded or Failed"
Nov 19 18:09:41.005: INFO: Pod "downwardapi-volume-d91d0bb7-2bad-4ebc-8aa2-716a3cd3bfc5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.810248ms
Nov 19 18:09:43.010: INFO: Pod "downwardapi-volume-d91d0bb7-2bad-4ebc-8aa2-716a3cd3bfc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008184442s
STEP: Saw pod success
Nov 19 18:09:43.010: INFO: Pod "downwardapi-volume-d91d0bb7-2bad-4ebc-8aa2-716a3cd3bfc5" satisfied condition "Succeeded or Failed"
Nov 19 18:09:43.014: INFO: Trying to get logs from node ip-172-31-20-145 pod downwardapi-volume-d91d0bb7-2bad-4ebc-8aa2-716a3cd3bfc5 container client-container: <nil>
STEP: delete the pod
Nov 19 18:09:43.029: INFO: Waiting for pod downwardapi-volume-d91d0bb7-2bad-4ebc-8aa2-716a3cd3bfc5 to disappear
Nov 19 18:09:43.032: INFO: Pod downwardapi-volume-d91d0bb7-2bad-4ebc-8aa2-716a3cd3bfc5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:09:43.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6614" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":271,"skipped":4526,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:09:43.041: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6482
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's command
Nov 19 18:09:43.183: INFO: Waiting up to 5m0s for pod "var-expansion-3b45bb2a-0463-4dad-acbb-94562aa4fe47" in namespace "var-expansion-6482" to be "Succeeded or Failed"
Nov 19 18:09:43.187: INFO: Pod "var-expansion-3b45bb2a-0463-4dad-acbb-94562aa4fe47": Phase="Pending", Reason="", readiness=false. Elapsed: 3.867806ms
Nov 19 18:09:45.190: INFO: Pod "var-expansion-3b45bb2a-0463-4dad-acbb-94562aa4fe47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007092097s
STEP: Saw pod success
Nov 19 18:09:45.190: INFO: Pod "var-expansion-3b45bb2a-0463-4dad-acbb-94562aa4fe47" satisfied condition "Succeeded or Failed"
Nov 19 18:09:45.194: INFO: Trying to get logs from node ip-172-31-20-145 pod var-expansion-3b45bb2a-0463-4dad-acbb-94562aa4fe47 container dapi-container: <nil>
STEP: delete the pod
Nov 19 18:09:45.210: INFO: Waiting for pod var-expansion-3b45bb2a-0463-4dad-acbb-94562aa4fe47 to disappear
Nov 19 18:09:45.212: INFO: Pod var-expansion-3b45bb2a-0463-4dad-acbb-94562aa4fe47 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:09:45.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6482" for this suite.
â€¢{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":305,"completed":272,"skipped":4534,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:09:45.222: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-871
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Nov 19 18:09:45.353: INFO: namespace kubectl-871
Nov 19 18:09:45.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 create -f - --namespace=kubectl-871'
Nov 19 18:09:45.559: INFO: stderr: ""
Nov 19 18:09:45.559: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Nov 19 18:09:46.564: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 19 18:09:46.564: INFO: Found 0 / 1
Nov 19 18:09:47.563: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 19 18:09:47.563: INFO: Found 1 / 1
Nov 19 18:09:47.563: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 19 18:09:47.567: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 19 18:09:47.567: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 19 18:09:47.567: INFO: wait on agnhost-primary startup in kubectl-871 
Nov 19 18:09:47.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 logs agnhost-primary-bvdxn agnhost-primary --namespace=kubectl-871'
Nov 19 18:09:47.634: INFO: stderr: ""
Nov 19 18:09:47.634: INFO: stdout: "Paused\n"
STEP: exposing RC
Nov 19 18:09:47.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-871'
Nov 19 18:09:47.711: INFO: stderr: ""
Nov 19 18:09:47.711: INFO: stdout: "service/rm2 exposed\n"
Nov 19 18:09:47.714: INFO: Service rm2 in namespace kubectl-871 found.
STEP: exposing service
Nov 19 18:09:49.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-871'
Nov 19 18:09:49.799: INFO: stderr: ""
Nov 19 18:09:49.799: INFO: stdout: "service/rm3 exposed\n"
Nov 19 18:09:49.804: INFO: Service rm3 in namespace kubectl-871 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:09:51.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-871" for this suite.

â€¢ [SLOW TEST:6.600 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1246
    should create services for rc  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":305,"completed":273,"skipped":4549,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:09:51.823: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9665
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:10:05.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9665" for this suite.

â€¢ [SLOW TEST:13.193 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":305,"completed":274,"skipped":4567,"failed":0}
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:10:05.015: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4508
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 19 18:10:05.156: INFO: Waiting up to 5m0s for pod "downwardapi-volume-988e71f5-2e84-401a-a051-ffae4380981d" in namespace "projected-4508" to be "Succeeded or Failed"
Nov 19 18:10:05.159: INFO: Pod "downwardapi-volume-988e71f5-2e84-401a-a051-ffae4380981d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.528319ms
Nov 19 18:10:07.164: INFO: Pod "downwardapi-volume-988e71f5-2e84-401a-a051-ffae4380981d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007725053s
STEP: Saw pod success
Nov 19 18:10:07.164: INFO: Pod "downwardapi-volume-988e71f5-2e84-401a-a051-ffae4380981d" satisfied condition "Succeeded or Failed"
Nov 19 18:10:07.166: INFO: Trying to get logs from node ip-172-31-20-145 pod downwardapi-volume-988e71f5-2e84-401a-a051-ffae4380981d container client-container: <nil>
STEP: delete the pod
Nov 19 18:10:07.183: INFO: Waiting for pod downwardapi-volume-988e71f5-2e84-401a-a051-ffae4380981d to disappear
Nov 19 18:10:07.185: INFO: Pod downwardapi-volume-988e71f5-2e84-401a-a051-ffae4380981d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:10:07.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4508" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":275,"skipped":4567,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:10:07.193: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-4243
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-4243, will wait for the garbage collector to delete the pods
Nov 19 18:10:09.400: INFO: Deleting Job.batch foo took: 7.629849ms
Nov 19 18:10:09.900: INFO: Terminating Job.batch foo pods took: 500.15643ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:10:42.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4243" for this suite.

â€¢ [SLOW TEST:35.117 seconds]
[sig-apps] Job
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":305,"completed":276,"skipped":4577,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:10:42.310: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3143
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 19 18:10:42.775: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Nov 19 18:10:44.785: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741406242, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741406242, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741406242, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741406242, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 19 18:10:47.799: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 18:10:47.803: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:10:48.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3143" for this suite.
STEP: Destroying namespace "webhook-3143-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.672 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":305,"completed":277,"skipped":4586,"failed":0}
SSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:10:48.982: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename limitrange
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in limitrange-8079
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Nov 19 18:10:49.123: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Nov 19 18:10:49.130: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Nov 19 18:10:49.130: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Nov 19 18:10:49.141: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Nov 19 18:10:49.141: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Nov 19 18:10:49.149: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Nov 19 18:10:49.149: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Nov 19 18:10:56.188: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:10:56.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-8079" for this suite.

â€¢ [SLOW TEST:7.226 seconds]
[sig-scheduling] LimitRange
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":305,"completed":278,"skipped":4589,"failed":0}
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:10:56.209: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-8196
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 18:10:56.348: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-2c85dac7-2e2c-41ae-af06-32b6ba6e5a98" in namespace "security-context-test-8196" to be "Succeeded or Failed"
Nov 19 18:10:56.350: INFO: Pod "busybox-readonly-false-2c85dac7-2e2c-41ae-af06-32b6ba6e5a98": Phase="Pending", Reason="", readiness=false. Elapsed: 1.94544ms
Nov 19 18:10:58.354: INFO: Pod "busybox-readonly-false-2c85dac7-2e2c-41ae-af06-32b6ba6e5a98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00554887s
Nov 19 18:10:58.354: INFO: Pod "busybox-readonly-false-2c85dac7-2e2c-41ae-af06-32b6ba6e5a98" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:10:58.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8196" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":305,"completed":279,"skipped":4589,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:10:58.362: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-410
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov 19 18:10:58.493: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 19 18:10:58.499: INFO: Waiting for terminating namespaces to be deleted...
Nov 19 18:10:58.501: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-20-145 before test
Nov 19 18:10:58.506: INFO: nginx-ingress-controller-kubernetes-worker-4gqmk from ingress-nginx-kubernetes-worker started at 2020-11-19 17:43:15 +0000 UTC (1 container statuses recorded)
Nov 19 18:10:58.506: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Nov 19 18:10:58.506: INFO: busybox-readonly-false-2c85dac7-2e2c-41ae-af06-32b6ba6e5a98 from security-context-test-8196 started at 2020-11-19 18:10:56 +0000 UTC (1 container statuses recorded)
Nov 19 18:10:58.506: INFO: 	Container busybox-readonly-false-2c85dac7-2e2c-41ae-af06-32b6ba6e5a98 ready: false, restart count 0
Nov 19 18:10:58.506: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-68-240 before test
Nov 19 18:10:58.509: INFO: default-http-backend-kubernetes-worker-6494cbc7fd-vtwt8 from ingress-nginx-kubernetes-worker started at 2020-11-19 16:40:15 +0000 UTC (1 container statuses recorded)
Nov 19 18:10:58.509: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
Nov 19 18:10:58.509: INFO: nginx-ingress-controller-kubernetes-worker-qkjj5 from ingress-nginx-kubernetes-worker started at 2020-11-19 16:40:11 +0000 UTC (1 container statuses recorded)
Nov 19 18:10:58.509: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Nov 19 18:10:58.509: INFO: metrics-server-v0.3.6-7685c8469-xspvp from kube-system started at 2020-11-19 16:40:36 +0000 UTC (2 container statuses recorded)
Nov 19 18:10:58.509: INFO: 	Container metrics-server ready: true, restart count 0
Nov 19 18:10:58.509: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Nov 19 18:10:58.509: INFO: sonobuoy from sonobuoy started at 2020-11-19 16:56:31 +0000 UTC (1 container statuses recorded)
Nov 19 18:10:58.509: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 19 18:10:58.509: INFO: sonobuoy-e2e-job-51d1badd77b04798 from sonobuoy started at 2020-11-19 16:56:36 +0000 UTC (2 container statuses recorded)
Nov 19 18:10:58.509: INFO: 	Container e2e ready: true, restart count 0
Nov 19 18:10:58.509: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 19 18:10:58.509: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-9-169 before test
Nov 19 18:10:58.512: INFO: nginx-ingress-controller-kubernetes-worker-47fcn from ingress-nginx-kubernetes-worker started at 2020-11-19 16:40:06 +0000 UTC (1 container statuses recorded)
Nov 19 18:10:58.512: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Nov 19 18:10:58.512: INFO: coredns-7bb4d77796-wgc4n from kube-system started at 2020-11-19 16:40:12 +0000 UTC (1 container statuses recorded)
Nov 19 18:10:58.512: INFO: 	Container coredns ready: true, restart count 0
Nov 19 18:10:58.512: INFO: kube-state-metrics-6f586bb967-xlnpk from kube-system started at 2020-11-19 17:42:49 +0000 UTC (1 container statuses recorded)
Nov 19 18:10:58.512: INFO: 	Container kube-state-metrics ready: true, restart count 0
Nov 19 18:10:58.512: INFO: dashboard-metrics-scraper-74757fb5b7-qfnkc from kubernetes-dashboard started at 2020-11-19 16:40:12 +0000 UTC (1 container statuses recorded)
Nov 19 18:10:58.512: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Nov 19 18:10:58.512: INFO: kubernetes-dashboard-64f87676d4-ctkcd from kubernetes-dashboard started at 2020-11-19 17:42:49 +0000 UTC (1 container statuses recorded)
Nov 19 18:10:58.512: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-1d3c3a60-3dd0-4a69-baa5-2374cd44af50 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-1d3c3a60-3dd0-4a69-baa5-2374cd44af50 off the node ip-172-31-20-145
STEP: verifying the node doesn't have the label kubernetes.io/e2e-1d3c3a60-3dd0-4a69-baa5-2374cd44af50
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:16:02.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-410" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

â€¢ [SLOW TEST:304.244 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":305,"completed":280,"skipped":4590,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:16:02.606: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-775
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 18:16:02.745: INFO: Waiting up to 5m0s for pod "busybox-user-65534-54ac8bdf-b4fb-414a-b610-7285ffb75e1c" in namespace "security-context-test-775" to be "Succeeded or Failed"
Nov 19 18:16:02.747: INFO: Pod "busybox-user-65534-54ac8bdf-b4fb-414a-b610-7285ffb75e1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.268086ms
Nov 19 18:16:04.752: INFO: Pod "busybox-user-65534-54ac8bdf-b4fb-414a-b610-7285ffb75e1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007719906s
Nov 19 18:16:04.752: INFO: Pod "busybox-user-65534-54ac8bdf-b4fb-414a-b610-7285ffb75e1c" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:16:04.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-775" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":281,"skipped":4609,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:16:04.761: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-4637
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Nov 19 18:16:04.894: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 19 18:17:04.905: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 18:17:04.908: INFO: Starting informer...
STEP: Starting pod...
Nov 19 18:17:05.124: INFO: Pod is running on ip-172-31-20-145. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Nov 19 18:17:05.152: INFO: Pod wasn't evicted. Proceeding
Nov 19 18:17:05.152: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Nov 19 18:18:20.173: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:18:20.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-4637" for this suite.

â€¢ [SLOW TEST:135.421 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":305,"completed":282,"skipped":4624,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:18:20.183: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-1756
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:18:20.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1756" for this suite.
â€¢{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":305,"completed":283,"skipped":4643,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:18:20.358: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5365
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-c0edbfdc-ca4a-41b7-b773-6cc83a3873c2
STEP: Creating a pod to test consume configMaps
Nov 19 18:18:20.500: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1d9d178f-9484-4335-9996-231950d67670" in namespace "projected-5365" to be "Succeeded or Failed"
Nov 19 18:18:20.503: INFO: Pod "pod-projected-configmaps-1d9d178f-9484-4335-9996-231950d67670": Phase="Pending", Reason="", readiness=false. Elapsed: 2.279936ms
Nov 19 18:18:22.506: INFO: Pod "pod-projected-configmaps-1d9d178f-9484-4335-9996-231950d67670": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005496652s
STEP: Saw pod success
Nov 19 18:18:22.506: INFO: Pod "pod-projected-configmaps-1d9d178f-9484-4335-9996-231950d67670" satisfied condition "Succeeded or Failed"
Nov 19 18:18:22.510: INFO: Trying to get logs from node ip-172-31-20-145 pod pod-projected-configmaps-1d9d178f-9484-4335-9996-231950d67670 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 19 18:18:22.539: INFO: Waiting for pod pod-projected-configmaps-1d9d178f-9484-4335-9996-231950d67670 to disappear
Nov 19 18:18:22.541: INFO: Pod pod-projected-configmaps-1d9d178f-9484-4335-9996-231950d67670 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:18:22.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5365" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":284,"skipped":4653,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:18:22.548: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1320
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating secret secrets-1320/secret-test-9e33d400-f807-4ef2-894b-b41f5cfe3e2a
STEP: Creating a pod to test consume secrets
Nov 19 18:18:22.696: INFO: Waiting up to 5m0s for pod "pod-configmaps-ae299350-733f-4fa8-82b9-a2b5d41da9ac" in namespace "secrets-1320" to be "Succeeded or Failed"
Nov 19 18:18:22.700: INFO: Pod "pod-configmaps-ae299350-733f-4fa8-82b9-a2b5d41da9ac": Phase="Pending", Reason="", readiness=false. Elapsed: 3.176128ms
Nov 19 18:18:24.706: INFO: Pod "pod-configmaps-ae299350-733f-4fa8-82b9-a2b5d41da9ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009289767s
STEP: Saw pod success
Nov 19 18:18:24.706: INFO: Pod "pod-configmaps-ae299350-733f-4fa8-82b9-a2b5d41da9ac" satisfied condition "Succeeded or Failed"
Nov 19 18:18:24.708: INFO: Trying to get logs from node ip-172-31-20-145 pod pod-configmaps-ae299350-733f-4fa8-82b9-a2b5d41da9ac container env-test: <nil>
STEP: delete the pod
Nov 19 18:18:24.725: INFO: Waiting for pod pod-configmaps-ae299350-733f-4fa8-82b9-a2b5d41da9ac to disappear
Nov 19 18:18:24.728: INFO: Pod pod-configmaps-ae299350-733f-4fa8-82b9-a2b5d41da9ac no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:18:24.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1320" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":285,"skipped":4682,"failed":0}
SSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:18:24.735: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9540
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-9540/configmap-test-c5942964-dc4f-463b-a384-315803b61c29
STEP: Creating a pod to test consume configMaps
Nov 19 18:18:24.888: INFO: Waiting up to 5m0s for pod "pod-configmaps-661bce4b-991f-487b-85d4-8a1e34148b3a" in namespace "configmap-9540" to be "Succeeded or Failed"
Nov 19 18:18:24.892: INFO: Pod "pod-configmaps-661bce4b-991f-487b-85d4-8a1e34148b3a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.550075ms
Nov 19 18:18:26.895: INFO: Pod "pod-configmaps-661bce4b-991f-487b-85d4-8a1e34148b3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006871662s
STEP: Saw pod success
Nov 19 18:18:26.895: INFO: Pod "pod-configmaps-661bce4b-991f-487b-85d4-8a1e34148b3a" satisfied condition "Succeeded or Failed"
Nov 19 18:18:26.899: INFO: Trying to get logs from node ip-172-31-20-145 pod pod-configmaps-661bce4b-991f-487b-85d4-8a1e34148b3a container env-test: <nil>
STEP: delete the pod
Nov 19 18:18:26.914: INFO: Waiting for pod pod-configmaps-661bce4b-991f-487b-85d4-8a1e34148b3a to disappear
Nov 19 18:18:26.916: INFO: Pod pod-configmaps-661bce4b-991f-487b-85d4-8a1e34148b3a no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:18:26.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9540" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":286,"skipped":4688,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:18:26.923: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1898
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Nov 19 18:18:27.065: INFO: Waiting up to 5m0s for pod "downward-api-d03cc7e6-911d-46bf-bb61-ad2fb8bd1a4e" in namespace "downward-api-1898" to be "Succeeded or Failed"
Nov 19 18:18:27.067: INFO: Pod "downward-api-d03cc7e6-911d-46bf-bb61-ad2fb8bd1a4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.183768ms
Nov 19 18:18:29.071: INFO: Pod "downward-api-d03cc7e6-911d-46bf-bb61-ad2fb8bd1a4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005993549s
STEP: Saw pod success
Nov 19 18:18:29.071: INFO: Pod "downward-api-d03cc7e6-911d-46bf-bb61-ad2fb8bd1a4e" satisfied condition "Succeeded or Failed"
Nov 19 18:18:29.074: INFO: Trying to get logs from node ip-172-31-20-145 pod downward-api-d03cc7e6-911d-46bf-bb61-ad2fb8bd1a4e container dapi-container: <nil>
STEP: delete the pod
Nov 19 18:18:29.107: INFO: Waiting for pod downward-api-d03cc7e6-911d-46bf-bb61-ad2fb8bd1a4e to disappear
Nov 19 18:18:29.109: INFO: Pod downward-api-d03cc7e6-911d-46bf-bb61-ad2fb8bd1a4e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:18:29.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1898" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":305,"completed":287,"skipped":4720,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:18:29.118: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6589
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 18:18:29.249: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Nov 19 18:18:29.257: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 19 18:18:34.260: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 19 18:18:34.260: INFO: Creating deployment "test-rolling-update-deployment"
Nov 19 18:18:34.266: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Nov 19 18:18:34.271: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Nov 19 18:18:36.279: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Nov 19 18:18:36.281: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Nov 19 18:18:36.290: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6589 /apis/apps/v1/namespaces/deployment-6589/deployments/test-rolling-update-deployment 3ec54fb7-f8af-4c3c-a8c9-8a512aaa77d4 33526 1 2020-11-19 18:18:34 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2020-11-19 18:18:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-11-19 18:18:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00725e8b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-11-19 18:18:34 +0000 UTC,LastTransitionTime:2020-11-19 18:18:34 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" has successfully progressed.,LastUpdateTime:2020-11-19 18:18:35 +0000 UTC,LastTransitionTime:2020-11-19 18:18:34 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 19 18:18:36.293: INFO: New ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9  deployment-6589 /apis/apps/v1/namespaces/deployment-6589/replicasets/test-rolling-update-deployment-c4cb8d6d9 8a196b45-57d0-4f21-9c88-1ddba50f8944 33516 1 2020-11-19 18:18:34 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 3ec54fb7-f8af-4c3c-a8c9-8a512aaa77d4 0xc00725ee40 0xc00725ee41}] []  [{kube-controller-manager Update apps/v1 2020-11-19 18:18:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ec54fb7-f8af-4c3c-a8c9-8a512aaa77d4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: c4cb8d6d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00725eeb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 19 18:18:36.293: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Nov 19 18:18:36.293: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6589 /apis/apps/v1/namespaces/deployment-6589/replicasets/test-rolling-update-controller 9cad6dc4-ebb9-4abc-be6f-8d71aa0aa6ef 33525 2 2020-11-19 18:18:29 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 3ec54fb7-f8af-4c3c-a8c9-8a512aaa77d4 0xc00725ed37 0xc00725ed38}] []  [{e2e.test Update apps/v1 2020-11-19 18:18:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-11-19 18:18:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ec54fb7-f8af-4c3c-a8c9-8a512aaa77d4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00725edd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 19 18:18:36.295: INFO: Pod "test-rolling-update-deployment-c4cb8d6d9-zgzl6" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9-zgzl6 test-rolling-update-deployment-c4cb8d6d9- deployment-6589 /api/v1/namespaces/deployment-6589/pods/test-rolling-update-deployment-c4cb8d6d9-zgzl6 afd0327a-ac2e-4301-8916-9c242bcd8892 33515 0 2020-11-19 18:18:34 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-c4cb8d6d9 8a196b45-57d0-4f21-9c88-1ddba50f8944 0xc00725f390 0xc00725f391}] []  [{kube-controller-manager Update v1 2020-11-19 18:18:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8a196b45-57d0-4f21-9c88-1ddba50f8944\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-19 18:18:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.33.243\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8z5qm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8z5qm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8z5qm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-20-145,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 18:18:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 18:18:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 18:18:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 18:18:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.20.145,PodIP:10.1.33.243,StartTime:2020-11-19 18:18:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-19 18:18:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:containerd://34d2fc237e54db42ffffe26e6f23b0cf491a3306a52bb932d4b72ec294d3639d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.33.243,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:18:36.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6589" for this suite.

â€¢ [SLOW TEST:7.186 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":288,"skipped":4728,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:18:36.304: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7505
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 19 18:18:36.911: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 19 18:18:39.929: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:18:40.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7505" for this suite.
STEP: Destroying namespace "webhook-7505-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":305,"completed":289,"skipped":4731,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:18:40.192: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7108
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Nov 19 18:18:40.337: INFO: Pod name pod-release: Found 0 pods out of 1
Nov 19 18:18:45.343: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:18:45.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7108" for this suite.

â€¢ [SLOW TEST:5.183 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":305,"completed":290,"skipped":4738,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:18:45.375: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7279
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:19:02.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7279" for this suite.

â€¢ [SLOW TEST:17.201 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":305,"completed":291,"skipped":4773,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:19:02.576: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6764
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-46b8294d-9405-4154-8899-aac2b20b28b7
STEP: Creating a pod to test consume secrets
Nov 19 18:19:02.729: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1eeb4c5e-89d5-422b-81c1-88aa916dce19" in namespace "projected-6764" to be "Succeeded or Failed"
Nov 19 18:19:02.735: INFO: Pod "pod-projected-secrets-1eeb4c5e-89d5-422b-81c1-88aa916dce19": Phase="Pending", Reason="", readiness=false. Elapsed: 5.611586ms
Nov 19 18:19:04.738: INFO: Pod "pod-projected-secrets-1eeb4c5e-89d5-422b-81c1-88aa916dce19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009182899s
STEP: Saw pod success
Nov 19 18:19:04.738: INFO: Pod "pod-projected-secrets-1eeb4c5e-89d5-422b-81c1-88aa916dce19" satisfied condition "Succeeded or Failed"
Nov 19 18:19:04.741: INFO: Trying to get logs from node ip-172-31-20-145 pod pod-projected-secrets-1eeb4c5e-89d5-422b-81c1-88aa916dce19 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 19 18:19:04.759: INFO: Waiting for pod pod-projected-secrets-1eeb4c5e-89d5-422b-81c1-88aa916dce19 to disappear
Nov 19 18:19:04.762: INFO: Pod pod-projected-secrets-1eeb4c5e-89d5-422b-81c1-88aa916dce19 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:19:04.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6764" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":292,"skipped":4776,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:19:04.769: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2795
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Nov 19 18:19:04.908: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2795 /api/v1/namespaces/watch-2795/configmaps/e2e-watch-test-configmap-a a2518201-e4ec-4458-937b-46c6390d78dd 33804 0 2020-11-19 18:19:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-19 18:19:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 19 18:19:04.908: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2795 /api/v1/namespaces/watch-2795/configmaps/e2e-watch-test-configmap-a a2518201-e4ec-4458-937b-46c6390d78dd 33804 0 2020-11-19 18:19:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-19 18:19:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Nov 19 18:19:14.917: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2795 /api/v1/namespaces/watch-2795/configmaps/e2e-watch-test-configmap-a a2518201-e4ec-4458-937b-46c6390d78dd 33850 0 2020-11-19 18:19:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-19 18:19:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 19 18:19:14.917: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2795 /api/v1/namespaces/watch-2795/configmaps/e2e-watch-test-configmap-a a2518201-e4ec-4458-937b-46c6390d78dd 33850 0 2020-11-19 18:19:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-19 18:19:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Nov 19 18:19:24.925: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2795 /api/v1/namespaces/watch-2795/configmaps/e2e-watch-test-configmap-a a2518201-e4ec-4458-937b-46c6390d78dd 33879 0 2020-11-19 18:19:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-19 18:19:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 19 18:19:24.925: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2795 /api/v1/namespaces/watch-2795/configmaps/e2e-watch-test-configmap-a a2518201-e4ec-4458-937b-46c6390d78dd 33879 0 2020-11-19 18:19:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-19 18:19:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Nov 19 18:19:34.934: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2795 /api/v1/namespaces/watch-2795/configmaps/e2e-watch-test-configmap-a a2518201-e4ec-4458-937b-46c6390d78dd 33907 0 2020-11-19 18:19:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-19 18:19:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 19 18:19:34.934: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2795 /api/v1/namespaces/watch-2795/configmaps/e2e-watch-test-configmap-a a2518201-e4ec-4458-937b-46c6390d78dd 33907 0 2020-11-19 18:19:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-19 18:19:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Nov 19 18:19:44.944: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2795 /api/v1/namespaces/watch-2795/configmaps/e2e-watch-test-configmap-b c42829c3-bbb7-4948-b004-294193dc183b 33935 0 2020-11-19 18:19:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-11-19 18:19:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 19 18:19:44.944: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2795 /api/v1/namespaces/watch-2795/configmaps/e2e-watch-test-configmap-b c42829c3-bbb7-4948-b004-294193dc183b 33935 0 2020-11-19 18:19:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-11-19 18:19:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Nov 19 18:19:54.953: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2795 /api/v1/namespaces/watch-2795/configmaps/e2e-watch-test-configmap-b c42829c3-bbb7-4948-b004-294193dc183b 33963 0 2020-11-19 18:19:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-11-19 18:19:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 19 18:19:54.953: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2795 /api/v1/namespaces/watch-2795/configmaps/e2e-watch-test-configmap-b c42829c3-bbb7-4948-b004-294193dc183b 33963 0 2020-11-19 18:19:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-11-19 18:19:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:20:04.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2795" for this suite.

â€¢ [SLOW TEST:60.193 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":305,"completed":293,"skipped":4789,"failed":0}
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:20:04.962: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-3444
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Nov 19 18:20:09.137: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3444 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 19 18:20:09.137: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
Nov 19 18:20:09.221: INFO: Exec stderr: ""
Nov 19 18:20:09.221: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3444 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 19 18:20:09.221: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
Nov 19 18:20:09.273: INFO: Exec stderr: ""
Nov 19 18:20:09.273: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3444 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 19 18:20:09.273: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
Nov 19 18:20:09.329: INFO: Exec stderr: ""
Nov 19 18:20:09.329: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3444 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 19 18:20:09.329: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
Nov 19 18:20:09.368: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Nov 19 18:20:09.368: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3444 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 19 18:20:09.368: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
Nov 19 18:20:09.437: INFO: Exec stderr: ""
Nov 19 18:20:09.437: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3444 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 19 18:20:09.437: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
Nov 19 18:20:09.474: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Nov 19 18:20:09.474: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3444 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 19 18:20:09.474: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
Nov 19 18:20:09.524: INFO: Exec stderr: ""
Nov 19 18:20:09.524: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3444 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 19 18:20:09.524: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
Nov 19 18:20:09.599: INFO: Exec stderr: ""
Nov 19 18:20:09.599: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3444 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 19 18:20:09.599: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
Nov 19 18:20:09.648: INFO: Exec stderr: ""
Nov 19 18:20:09.648: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3444 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 19 18:20:09.648: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
Nov 19 18:20:09.690: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:20:09.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-3444" for this suite.
â€¢{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":294,"skipped":4789,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:20:09.701: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7337
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov 19 18:20:09.836: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 19 18:20:09.841: INFO: Waiting for terminating namespaces to be deleted...
Nov 19 18:20:09.844: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-20-145 before test
Nov 19 18:20:09.848: INFO: test-host-network-pod from e2e-kubelet-etc-hosts-3444 started at 2020-11-19 18:20:07 +0000 UTC (2 container statuses recorded)
Nov 19 18:20:09.848: INFO: 	Container busybox-1 ready: true, restart count 0
Nov 19 18:20:09.848: INFO: 	Container busybox-2 ready: true, restart count 0
Nov 19 18:20:09.848: INFO: test-pod from e2e-kubelet-etc-hosts-3444 started at 2020-11-19 18:20:05 +0000 UTC (3 container statuses recorded)
Nov 19 18:20:09.848: INFO: 	Container busybox-1 ready: true, restart count 0
Nov 19 18:20:09.848: INFO: 	Container busybox-2 ready: true, restart count 0
Nov 19 18:20:09.848: INFO: 	Container busybox-3 ready: true, restart count 0
Nov 19 18:20:09.848: INFO: nginx-ingress-controller-kubernetes-worker-n476v from ingress-nginx-kubernetes-worker started at 2020-11-19 18:17:27 +0000 UTC (1 container statuses recorded)
Nov 19 18:20:09.848: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Nov 19 18:20:09.848: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-68-240 before test
Nov 19 18:20:09.851: INFO: default-http-backend-kubernetes-worker-6494cbc7fd-vtwt8 from ingress-nginx-kubernetes-worker started at 2020-11-19 16:40:15 +0000 UTC (1 container statuses recorded)
Nov 19 18:20:09.851: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
Nov 19 18:20:09.851: INFO: nginx-ingress-controller-kubernetes-worker-qkjj5 from ingress-nginx-kubernetes-worker started at 2020-11-19 16:40:11 +0000 UTC (1 container statuses recorded)
Nov 19 18:20:09.851: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Nov 19 18:20:09.851: INFO: metrics-server-v0.3.6-7685c8469-xspvp from kube-system started at 2020-11-19 16:40:36 +0000 UTC (2 container statuses recorded)
Nov 19 18:20:09.851: INFO: 	Container metrics-server ready: true, restart count 0
Nov 19 18:20:09.851: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Nov 19 18:20:09.851: INFO: sonobuoy from sonobuoy started at 2020-11-19 16:56:31 +0000 UTC (1 container statuses recorded)
Nov 19 18:20:09.851: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 19 18:20:09.851: INFO: sonobuoy-e2e-job-51d1badd77b04798 from sonobuoy started at 2020-11-19 16:56:36 +0000 UTC (2 container statuses recorded)
Nov 19 18:20:09.851: INFO: 	Container e2e ready: true, restart count 0
Nov 19 18:20:09.851: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 19 18:20:09.851: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-9-169 before test
Nov 19 18:20:09.854: INFO: nginx-ingress-controller-kubernetes-worker-47fcn from ingress-nginx-kubernetes-worker started at 2020-11-19 16:40:06 +0000 UTC (1 container statuses recorded)
Nov 19 18:20:09.854: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Nov 19 18:20:09.854: INFO: coredns-7bb4d77796-wgc4n from kube-system started at 2020-11-19 16:40:12 +0000 UTC (1 container statuses recorded)
Nov 19 18:20:09.854: INFO: 	Container coredns ready: true, restart count 0
Nov 19 18:20:09.854: INFO: kube-state-metrics-6f586bb967-xlnpk from kube-system started at 2020-11-19 17:42:49 +0000 UTC (1 container statuses recorded)
Nov 19 18:20:09.854: INFO: 	Container kube-state-metrics ready: true, restart count 0
Nov 19 18:20:09.854: INFO: dashboard-metrics-scraper-74757fb5b7-qfnkc from kubernetes-dashboard started at 2020-11-19 16:40:12 +0000 UTC (1 container statuses recorded)
Nov 19 18:20:09.854: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Nov 19 18:20:09.854: INFO: kubernetes-dashboard-64f87676d4-ctkcd from kubernetes-dashboard started at 2020-11-19 17:42:49 +0000 UTC (1 container statuses recorded)
Nov 19 18:20:09.854: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-b6f1b53b-7290-4b7f-9d7e-cde1595d57c7 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-b6f1b53b-7290-4b7f-9d7e-cde1595d57c7 off the node ip-172-31-68-240
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b6f1b53b-7290-4b7f-9d7e-cde1595d57c7
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:20:13.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7337" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
â€¢{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":305,"completed":295,"skipped":4803,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:20:13.934: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4989
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 18:20:14.069: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Nov 19 18:20:16.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 --namespace=crd-publish-openapi-4989 create -f -'
Nov 19 18:20:17.326: INFO: stderr: ""
Nov 19 18:20:17.326: INFO: stdout: "e2e-test-crd-publish-openapi-9581-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Nov 19 18:20:17.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 --namespace=crd-publish-openapi-4989 delete e2e-test-crd-publish-openapi-9581-crds test-cr'
Nov 19 18:20:17.388: INFO: stderr: ""
Nov 19 18:20:17.388: INFO: stdout: "e2e-test-crd-publish-openapi-9581-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Nov 19 18:20:17.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 --namespace=crd-publish-openapi-4989 apply -f -'
Nov 19 18:20:17.645: INFO: stderr: ""
Nov 19 18:20:17.645: INFO: stdout: "e2e-test-crd-publish-openapi-9581-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Nov 19 18:20:17.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 --namespace=crd-publish-openapi-4989 delete e2e-test-crd-publish-openapi-9581-crds test-cr'
Nov 19 18:20:17.715: INFO: stderr: ""
Nov 19 18:20:17.715: INFO: stdout: "e2e-test-crd-publish-openapi-9581-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Nov 19 18:20:17.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 explain e2e-test-crd-publish-openapi-9581-crds'
Nov 19 18:20:17.869: INFO: stderr: ""
Nov 19 18:20:17.869: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9581-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:20:20.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4989" for this suite.

â€¢ [SLOW TEST:6.653 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":305,"completed":296,"skipped":4840,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:20:20.586: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6070
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-b52a1c19-ad8c-46c8-9d85-3d158a90d1bb
STEP: Creating a pod to test consume secrets
Nov 19 18:20:20.731: INFO: Waiting up to 5m0s for pod "pod-secrets-5958d535-daf2-411d-8811-dba5b4b6e6b8" in namespace "secrets-6070" to be "Succeeded or Failed"
Nov 19 18:20:20.735: INFO: Pod "pod-secrets-5958d535-daf2-411d-8811-dba5b4b6e6b8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.508903ms
Nov 19 18:20:22.739: INFO: Pod "pod-secrets-5958d535-daf2-411d-8811-dba5b4b6e6b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007922366s
STEP: Saw pod success
Nov 19 18:20:22.739: INFO: Pod "pod-secrets-5958d535-daf2-411d-8811-dba5b4b6e6b8" satisfied condition "Succeeded or Failed"
Nov 19 18:20:22.741: INFO: Trying to get logs from node ip-172-31-9-169 pod pod-secrets-5958d535-daf2-411d-8811-dba5b4b6e6b8 container secret-volume-test: <nil>
STEP: delete the pod
Nov 19 18:20:22.769: INFO: Waiting for pod pod-secrets-5958d535-daf2-411d-8811-dba5b4b6e6b8 to disappear
Nov 19 18:20:22.771: INFO: Pod pod-secrets-5958d535-daf2-411d-8811-dba5b4b6e6b8 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:20:22.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6070" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":297,"skipped":4841,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:20:22.780: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5478
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 18:20:22.911: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Nov 19 18:20:25.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 --namespace=crd-publish-openapi-5478 create -f -'
Nov 19 18:20:26.048: INFO: stderr: ""
Nov 19 18:20:26.048: INFO: stdout: "e2e-test-crd-publish-openapi-4378-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Nov 19 18:20:26.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 --namespace=crd-publish-openapi-5478 delete e2e-test-crd-publish-openapi-4378-crds test-foo'
Nov 19 18:20:26.113: INFO: stderr: ""
Nov 19 18:20:26.113: INFO: stdout: "e2e-test-crd-publish-openapi-4378-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Nov 19 18:20:26.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 --namespace=crd-publish-openapi-5478 apply -f -'
Nov 19 18:20:26.372: INFO: stderr: ""
Nov 19 18:20:26.372: INFO: stdout: "e2e-test-crd-publish-openapi-4378-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Nov 19 18:20:26.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 --namespace=crd-publish-openapi-5478 delete e2e-test-crd-publish-openapi-4378-crds test-foo'
Nov 19 18:20:26.435: INFO: stderr: ""
Nov 19 18:20:26.435: INFO: stdout: "e2e-test-crd-publish-openapi-4378-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Nov 19 18:20:26.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 --namespace=crd-publish-openapi-5478 create -f -'
Nov 19 18:20:26.586: INFO: rc: 1
Nov 19 18:20:26.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 --namespace=crd-publish-openapi-5478 apply -f -'
Nov 19 18:20:26.737: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Nov 19 18:20:26.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 --namespace=crd-publish-openapi-5478 create -f -'
Nov 19 18:20:26.948: INFO: rc: 1
Nov 19 18:20:26.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 --namespace=crd-publish-openapi-5478 apply -f -'
Nov 19 18:20:27.114: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Nov 19 18:20:27.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 explain e2e-test-crd-publish-openapi-4378-crds'
Nov 19 18:20:27.260: INFO: stderr: ""
Nov 19 18:20:27.260: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4378-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Nov 19 18:20:27.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 explain e2e-test-crd-publish-openapi-4378-crds.metadata'
Nov 19 18:20:27.406: INFO: stderr: ""
Nov 19 18:20:27.406: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4378-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Nov 19 18:20:27.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 explain e2e-test-crd-publish-openapi-4378-crds.spec'
Nov 19 18:20:27.604: INFO: stderr: ""
Nov 19 18:20:27.604: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4378-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Nov 19 18:20:27.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 explain e2e-test-crd-publish-openapi-4378-crds.spec.bars'
Nov 19 18:20:27.751: INFO: stderr: ""
Nov 19 18:20:27.751: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4378-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Nov 19 18:20:27.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-898325863 explain e2e-test-crd-publish-openapi-4378-crds.spec.bars2'
Nov 19 18:20:27.966: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:20:30.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5478" for this suite.

â€¢ [SLOW TEST:7.901 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":305,"completed":298,"skipped":4859,"failed":0}
S
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:20:30.681: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-9422
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:20:36.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9422" for this suite.

â€¢ [SLOW TEST:6.160 seconds]
[sig-apps] Job
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":305,"completed":299,"skipped":4860,"failed":0}
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:20:36.841: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8982
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-706efeff-076b-47db-aab4-21c966020b60
STEP: Creating a pod to test consume secrets
Nov 19 18:20:36.986: INFO: Waiting up to 5m0s for pod "pod-secrets-e748f38e-62bf-498f-a861-0e507a3b1e23" in namespace "secrets-8982" to be "Succeeded or Failed"
Nov 19 18:20:36.988: INFO: Pod "pod-secrets-e748f38e-62bf-498f-a861-0e507a3b1e23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.111449ms
Nov 19 18:20:38.991: INFO: Pod "pod-secrets-e748f38e-62bf-498f-a861-0e507a3b1e23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005245125s
STEP: Saw pod success
Nov 19 18:20:38.991: INFO: Pod "pod-secrets-e748f38e-62bf-498f-a861-0e507a3b1e23" satisfied condition "Succeeded or Failed"
Nov 19 18:20:38.995: INFO: Trying to get logs from node ip-172-31-68-240 pod pod-secrets-e748f38e-62bf-498f-a861-0e507a3b1e23 container secret-env-test: <nil>
STEP: delete the pod
Nov 19 18:20:39.024: INFO: Waiting for pod pod-secrets-e748f38e-62bf-498f-a861-0e507a3b1e23 to disappear
Nov 19 18:20:39.033: INFO: Pod pod-secrets-e748f38e-62bf-498f-a861-0e507a3b1e23 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:20:39.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8982" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":305,"completed":300,"skipped":4860,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:20:39.043: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8230
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 19 18:20:39.187: INFO: Pod name rollover-pod: Found 0 pods out of 1
Nov 19 18:20:44.192: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 19 18:20:44.192: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Nov 19 18:20:46.195: INFO: Creating deployment "test-rollover-deployment"
Nov 19 18:20:46.203: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Nov 19 18:20:48.211: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Nov 19 18:20:48.217: INFO: Ensure that both replica sets have 1 created replica
Nov 19 18:20:48.222: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Nov 19 18:20:48.232: INFO: Updating deployment test-rollover-deployment
Nov 19 18:20:48.232: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Nov 19 18:20:50.237: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Nov 19 18:20:50.244: INFO: Make sure deployment "test-rollover-deployment" is complete
Nov 19 18:20:50.249: INFO: all replica sets need to contain the pod-template-hash label
Nov 19 18:20:50.250: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741406846, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741406846, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741406849, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741406846, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 19 18:20:52.257: INFO: all replica sets need to contain the pod-template-hash label
Nov 19 18:20:52.257: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741406846, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741406846, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741406849, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741406846, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 19 18:20:54.258: INFO: all replica sets need to contain the pod-template-hash label
Nov 19 18:20:54.258: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741406846, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741406846, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741406849, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741406846, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 19 18:20:56.258: INFO: all replica sets need to contain the pod-template-hash label
Nov 19 18:20:56.258: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741406846, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741406846, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741406849, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741406846, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 19 18:20:58.258: INFO: all replica sets need to contain the pod-template-hash label
Nov 19 18:20:58.258: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741406846, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741406846, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741406849, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741406846, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 19 18:21:00.256: INFO: 
Nov 19 18:21:00.256: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Nov 19 18:21:00.265: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-8230 /apis/apps/v1/namespaces/deployment-8230/deployments/test-rollover-deployment 9fb74a71-035e-47b6-9d1b-1b8f63358b59 34537 2 2020-11-19 18:20:46 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-11-19 18:20:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-11-19 18:20:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004429d48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-11-19 18:20:46 +0000 UTC,LastTransitionTime:2020-11-19 18:20:46 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-5797c7764" has successfully progressed.,LastUpdateTime:2020-11-19 18:20:59 +0000 UTC,LastTransitionTime:2020-11-19 18:20:46 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 19 18:21:00.267: INFO: New ReplicaSet "test-rollover-deployment-5797c7764" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-5797c7764  deployment-8230 /apis/apps/v1/namespaces/deployment-8230/replicasets/test-rollover-deployment-5797c7764 1602001e-5d26-409c-ab6b-2cb09fdeb891 34525 2 2020-11-19 18:20:48 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 9fb74a71-035e-47b6-9d1b-1b8f63358b59 0xc004476a10 0xc004476a11}] []  [{kube-controller-manager Update apps/v1 2020-11-19 18:20:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9fb74a71-035e-47b6-9d1b-1b8f63358b59\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5797c7764,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004476a98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 19 18:21:00.267: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Nov 19 18:21:00.267: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8230 /apis/apps/v1/namespaces/deployment-8230/replicasets/test-rollover-controller 557efcba-a511-48cb-9006-5b11c13d5739 34536 2 2020-11-19 18:20:39 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 9fb74a71-035e-47b6-9d1b-1b8f63358b59 0xc004476887 0xc004476888}] []  [{e2e.test Update apps/v1 2020-11-19 18:20:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-11-19 18:20:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9fb74a71-035e-47b6-9d1b-1b8f63358b59\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004476948 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 19 18:21:00.267: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-8230 /apis/apps/v1/namespaces/deployment-8230/replicasets/test-rollover-deployment-78bc8b888c 1f5a1be8-0804-4817-8823-40ad1d78c644 34481 2 2020-11-19 18:20:46 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 9fb74a71-035e-47b6-9d1b-1b8f63358b59 0xc004476b27 0xc004476b28}] []  [{kube-controller-manager Update apps/v1 2020-11-19 18:20:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9fb74a71-035e-47b6-9d1b-1b8f63358b59\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004476bd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 19 18:21:00.270: INFO: Pod "test-rollover-deployment-5797c7764-l58kz" is available:
&Pod{ObjectMeta:{test-rollover-deployment-5797c7764-l58kz test-rollover-deployment-5797c7764- deployment-8230 /api/v1/namespaces/deployment-8230/pods/test-rollover-deployment-5797c7764-l58kz 867418c1-ff5b-49ab-99eb-83cc03dc573b 34490 0 2020-11-19 18:20:48 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-5797c7764 1602001e-5d26-409c-ab6b-2cb09fdeb891 0xc004477420 0xc004477421}] []  [{kube-controller-manager Update v1 2020-11-19 18:20:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1602001e-5d26-409c-ab6b-2cb09fdeb891\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-19 18:20:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.31.132\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v4cc4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v4cc4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v4cc4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-9-169,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 18:20:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 18:20:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 18:20:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-19 18:20:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.9.169,PodIP:10.1.31.132,StartTime:2020-11-19 18:20:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-19 18:20:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:containerd://31f66f713a037eeddc54e7b8102f739bbd9893ab151c5832768930cb7f7b29c6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.31.132,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:21:00.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8230" for this suite.

â€¢ [SLOW TEST:21.236 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":305,"completed":301,"skipped":4875,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:21:00.279: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6818
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 19 18:21:02.451: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:21:02.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6818" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":302,"skipped":4903,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:21:02.475: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4867
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 19 18:21:02.861: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 19 18:21:05.883: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:21:15.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4867" for this suite.
STEP: Destroying namespace "webhook-4867-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:13.560 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":305,"completed":303,"skipped":4906,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:21:16.035: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7547
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Nov 19 18:21:16.186: INFO: Waiting up to 5m0s for pod "pod-9b972c98-c810-466e-adea-3a93f452de39" in namespace "emptydir-7547" to be "Succeeded or Failed"
Nov 19 18:21:16.190: INFO: Pod "pod-9b972c98-c810-466e-adea-3a93f452de39": Phase="Pending", Reason="", readiness=false. Elapsed: 3.702024ms
Nov 19 18:21:18.193: INFO: Pod "pod-9b972c98-c810-466e-adea-3a93f452de39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007228684s
STEP: Saw pod success
Nov 19 18:21:18.193: INFO: Pod "pod-9b972c98-c810-466e-adea-3a93f452de39" satisfied condition "Succeeded or Failed"
Nov 19 18:21:18.197: INFO: Trying to get logs from node ip-172-31-20-145 pod pod-9b972c98-c810-466e-adea-3a93f452de39 container test-container: <nil>
STEP: delete the pod
Nov 19 18:21:18.219: INFO: Waiting for pod pod-9b972c98-c810-466e-adea-3a93f452de39 to disappear
Nov 19 18:21:18.223: INFO: Pod pod-9b972c98-c810-466e-adea-3a93f452de39 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:21:18.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7547" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":304,"skipped":4920,"failed":0}
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 19 18:21:18.229: INFO: >>> kubeConfig: /tmp/kubeconfig-898325863
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1621
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 19 18:21:41.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1621" for this suite.

â€¢ [SLOW TEST:23.363 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":305,"completed":305,"skipped":4924,"failed":0}
SSSSSNov 19 18:21:41.592: INFO: Running AfterSuite actions on all nodes
Nov 19 18:21:41.592: INFO: Running AfterSuite actions on node 1
Nov 19 18:21:41.592: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":305,"completed":305,"skipped":4929,"failed":0}

Ran 305 of 5234 Specs in 5096.109 seconds
SUCCESS! -- 305 Passed | 0 Failed | 0 Pending | 4929 Skipped
PASS

Ginkgo ran 1 suite in 1h24m57.25580697s
Test Suite Passed
